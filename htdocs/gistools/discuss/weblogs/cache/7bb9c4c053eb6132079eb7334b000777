O:9:"MagpieRSS":19:{s:6:"parser";i:0;s:12:"current_item";a:0:{}s:5:"items";a:25:{i:0;a:12:{s:4:"info";s:403:"<id >tag:blogger.com,1999:blog-21224994.post-3503738291563522515</id><published >2015-07-30T10:48:00.001-07:00</published><updated >2015-07-30T10:48:32.403-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="conference"></category><category scheme="http://www.blogger.com/atom/ns#" term="ICSE"></category><category scheme="http://www.blogger.com/atom/ns#" term="Software"></category>";s:5:"title";s:53:"ICSE 2015 and Software Engineering Research at Google";s:12:"atom_content";s:3793:"<span class="byline-author">Posted by Mohsen Vakilian, Software Engineer</span><br /><br />The large scale of our software engineering efforts at Google often pushes us to develop cutting-edge infrastructure. In May 2015, at the <a href="http://2015.icse-conferences.org/">International Conference on Software Engineering</a> (ICSE 2015), we shared some of our software engineering tools and practices and collaborated with the research community through a combination of publications, committee memberships, and workshops. Learn more about some of our research below (Googlers highlighted in <span style="color: #3d85c6;">blue</span>).<br /><br />Google was a Gold supporter of ICSE 2015.<br /><br /><b>Technical Research Papers:</b><br /><a href="http://hdl.handle.net/2060/20150011052">A Flexible and Non-intrusive Approach for Computing Complex Structural Coverage Metrics</a><br /><i>Michael W. Whalen, Suzette Person, Neha Rungta, <span style="color: #3d85c6;">Matt Staats</span>, Daniela Grijincu</i><br /><br /><a href="http://research.google.com/pubs/archive/42249.pdf">Automated Decomposition of Build Targets</a><br /><i><span style="color: #3d85c6;">Mohsen Vakilian</span>, <span style="color: #3d85c6;">Raluca Sauciuc</span>, <span style="color: #3d85c6;">David Morgenthaler</span>, <span style="color: #3d85c6;">Vahab Mirrokni</span></i><br /><br /><a href="http://research.google.com/pubs/archive/43322.pdf">Tricorder: Building a Program Analysis Ecosystem</a><br /><i><span style="color: #3d85c6;">Caitlin Sadowski</span>, <span style="color: #3d85c6;">Jeffrey van Gogh</span>, <span style="color: #3d85c6;">Ciera Jaspan</span>, <span style="color: #3d85c6;">Emma Soederberg</span>, <span style="color: #3d85c6;">Collin Winter</span></i><br /><br /><b>Software Engineering in Practice (SEIP) Papers:</b><br /><a href="https://ece.uwaterloo.ca/~lintan/publications/archrec-icse15.pdf">Comparing Software Architecture Recovery Techniques Using Accurate Dependencies</a><br /><i>Thibaud Lutellier, Devin Chollak, Joshua Garcia, Lin Tan, Derek Rayside, Nenad Medvidovic, <span style="color: #3d85c6;">Robert Kroeger</span></i><br /><br /><b>Technical Briefings:</b><br /><a href="http://eprints.lancs.ac.uk/73464/1/anthonysRashid_icse15TechBrief.pdf">Software Engineering for Privacy in-the-Large</a><br /><i><span style="color: #3d85c6;">Pauline Anthonysamy</span>, Awais Rashid</i><br /><br /><b>Workshop Organizers:</b><br /><a href="http://ret.cs.lth.se/15/">2nd International Workshop on Requirements Engineering and Testing (RET 2015)</a><br /><i>Elizabeth Bjarnason, Mirko Morandini, Markus Borg, Michael Unterkalmsteiner, Michael Felderer, <span style="color: #3d85c6;">Matthew Staats</span></i><br /><br /><b>Committee Members:</b><br /><i><span style="color: #3d85c6;">Caitlin Sadowski </span></i>-  Program Committee Member and Distinguished Reviewer Award Winner<br /><span style="color: #3d85c6;"><i>James Andrews</i></span> -  Review Committee Member<br /><i><span style="color: #3d85c6;">Ray Buse</span></i> - Software Engineering in Practice (SEIP) Committee Member and Demonstrations Committee Member<br /><i><span style="color: #3d85c6;">John Penix</span></i> - Software Engineering in Practice (SEIP) Committee Member<br /><i><span style="color: #3d85c6;">Marija Mikic</span></i> - Poster Co-chair<br /><i><span style="color: #3d85c6;">Daniel Popescu</span></i> and <i><span style="color: #3d85c6;">Ivo Krka</span></i> - Poster Committee Members<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=68fn5UG-1UY:QoazL2retbU:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/68fn5UG-1UY" height="1" width="1" alt=""/>";s:12:"link_replies";s:172:"http://googleresearch.blogspot.com/feeds/3503738291563522515/comments/defaulthttp://googleresearch.blogspot.com/2015/07/icse-2015-and-software-engineering.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3503738291563522515";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3503738291563522515";s:4:"link";s:99:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/68fn5UG-1UY/icse-2015-and-software-engineering.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:82:"http://googleresearch.blogspot.com/2015/07/icse-2015-and-software-engineering.html";}}i:1;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-7277475040605726656";s:9:"published";s:29:"2015-07-29T06:00:00.000-07:00";s:7:"updated";s:29:"2015-07-29T11:02:16.010-07:00";s:5:"title";s:56:"How Google Translate squeezes deep learning onto a phone";s:12:"atom_content";s:7816:"<span class="byline-author">Posted by Otavio Good, Software Engineer, Google Translate</span><br /><br />Today we <a href="http://googleblog.blogspot.com/2015/07/see-world-in-your-language-with-google.html">announced</a> that the <a href="http://translate.google.com/about/intl/en_ALL/#">Google Translate app</a> now does real-time visual translation of 20 more languages. So the next time you’re in Prague and can’t read a menu, we’ve got your back. But how are we able to recognize these new languages?<br /><br />In short: deep neural nets. When the Word Lens team joined Google, we were excited for the opportunity to work with some of the leading researchers in deep learning. Neural nets have gotten a lot of attention in the last few years because they’ve set all kinds of records in <a href="http://googleresearch.blogspot.com/2014/09/building-deeper-understanding-of-images.html">image recognition</a>. Five years ago, if you gave a computer an image of a cat or a dog, it had trouble telling which was which. Thanks to convolutional neural networks, not only can computers tell the difference between cats and dogs, they can even recognize different breeds of dogs. Yes, they’re good for more than just <a href="http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html">trippy art</a>—if you're translating a foreign menu or sign with the latest version of Google's Translate app, you're now using a deep neural net. And the amazing part is it can all work on your phone, without an Internet connection. Here’s how.<br /><br /><b>Step by step</b><br /><br />First, when a camera image comes in, the Google Translate app has to find the letters in the picture. It needs to weed out background objects like trees or cars, and pick up on the words we want translated. It looks at blobs of pixels that have similar color to each other that are also near other similar blobs of pixels. Those are possibly letters, and if they’re near each other, that makes a continuous line we should read.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-D7MiFwqGuNw/Vbf3lq9z8-I/AAAAAAAAApw/2Pd1hMCsnBg/s1600/image00.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="160" src="http://4.bp.blogspot.com/-D7MiFwqGuNw/Vbf3lq9z8-I/AAAAAAAAApw/2Pd1hMCsnBg/s640/image00.png" width="640" /></a></div>Second, Translate has to recognize what each letter actually is. This is where deep learning comes in. We use a convolutional neural network, training it on letters and non-letters so it can learn what different letters look like.<br /><br />But interestingly, if we train just on very “clean”-looking letters, we risk not understanding what real-life letters look like. Letters out in the real world are marred by reflections, dirt, smudges, and all kinds of weirdness. So we built our letter generator to create all kinds of fake “dirt” to convincingly mimic the noisiness of the real world—fake reflections, fake smudges, fake weirdness all around.<br /><br />Why not just train on real-life photos of letters? Well, it’s tough to find enough examples in all the languages we need, and it’s harder to maintain the fine control over what examples we use when we’re aiming to train a really efficient, compact neural network. So it’s more effective to simulate the dirt.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-oldoWmfzgKs/Vbf3vFYbh8I/AAAAAAAAAp4/K27DMUF0tbM/s1600/image01.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="113" src="http://3.bp.blogspot.com/-oldoWmfzgKs/Vbf3vFYbh8I/AAAAAAAAAp4/K27DMUF0tbM/s400/image01.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Some of the “dirty” letters we use for training. Dirt, highlights, and rotation, but not too much because we don’t want to confuse our neural net.</td></tr></tbody></table>The third step is to take those recognized letters, and look them up in a dictionary to get translations. Since every previous step could have failed in some way, the dictionary lookup needs to be approximate. That way, if we read an ‘S’ as a ‘5’, we’ll still be able to find the word ‘5uper’.<br /><br />Finally, we render the translation on top of the original words in the same style as the original. We can do this because we’ve already found and read the letters in the image, so we know exactly where they are. We can look at the colors surrounding the letters and use that to erase the original letters. And then we can draw the translation on top using the original foreground color.<br /><br /><b>Crunching it down for mobile</b><br /><br />Now, if we could do this visual translation in <a href="http://www.google.com/about/datacenters/">our data centers</a>, it wouldn’t be too hard. But a lot of our users, especially those getting online for the very first time, have slow or intermittent network connections and smartphones starved for computing power. These low-end phones can be about 50 times slower than a good laptop—and a good laptop is already much slower than the data centers that typically run our image recognition systems. So how do we get visual translation on these phones, with no connection to the cloud, translating in real-time as the camera moves around?<br /><br />We needed to develop a very small neural net, and put severe limits on how much we tried to teach it—in essence, put an upper bound on the density of information it handles. The challenge here was in creating the most effective training data. Since we’re generating our own training data, we put a lot of effort into including just the right data and nothing more. For instance, we want to be able to recognize a letter with a small amount of rotation, but not too much. If we overdo the rotation, the neural network will use too much of its information density on unimportant things. So we put effort into making tools that would give us a fast iteration time and good visualizations. Inside of a few minutes, we can change the algorithms for generating training data, generate it, retrain, and visualize. From there we can look at what kind of letters are failing and why. At one point, we were warping our training data too much, and ‘$’ started to be recognized as ‘S’. We were able to quickly identify that and adjust the warping parameters to fix the problem. It was like trying to paint a picture of letters that you’d see in real life with all their imperfections painted just perfectly.<br /><br />To achieve real-time, we also heavily optimized and hand-tuned the math operations. That meant using the mobile processor’s <a href="https://en.wikipedia.org/wiki/SIMD">SIMD</a> instructions and tuning things like matrix multiplies to fit processing into all levels of cache memory.<br /><br />In the end, we were able to get our networks to give us significantly better results while running about as fast as our old system—great for translating what you see around you on the fly. Sometimes new technology can seem very abstract, and it's not always obvious what the applications for things like convolutional neural nets could be. We think breaking down language barriers is one great use.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=l-1z-6zCdTw:dYdJhTH7cMg:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/l-1z-6zCdTw" height="1" width="1" alt=""/>";s:12:"link_replies";s:172:"http://googleresearch.blogspot.com/feeds/7277475040605726656/comments/defaulthttp://googleresearch.blogspot.com/2015/07/how-google-translate-squeezes-deep.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7277475040605726656";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7277475040605726656";s:4:"link";s:99:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/l-1z-6zCdTw/how-google-translate-squeezes-deep.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:82:"http://googleresearch.blogspot.com/2015/07/how-google-translate-squeezes-deep.html";}}i:2;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-3688680459028500303";s:9:"published";s:29:"2015-07-16T09:00:00.000-07:00";s:7:"updated";s:29:"2015-07-16T09:01:18.633-07:00";s:5:"title";s:44:"The Thorny Issue of CS Teacher Certification";s:12:"atom_content";s:4777:"<span class="byline-author">Posted by Chris Stephenson, Head of Computer Science Education Programs</span> <br /><br />(<i>Cross-posted on the <a href="http://googleforeducation.blogspot.com/2015/07/the-thorny-issue-of-cs-teacher.html">Google for Education Blog</a></i>)<br /><br />There is a tremendous focus on computer science education in K-12. Educators, policy makers, the non-profit sector and industry are sharing a common message about the benefits of computer science knowledge and the opportunities it provides. In this wider effort to improve access to computer science education, one of the challenges we face is how to ensure that there is a pipeline of computer science teachers to meet the <a href="http://techcrunch.com/2014/05/25/from-the-ivy-league-to-state-schools-demand-for-computer-science-is-booming/" target="_blank">growing demand</a> for this expertise in schools.<br /><br />In 2013 the <a href="http://www.csta.acm.org/" target="_blank">Computer Science Teachers Association (CSTA)</a> released <a href="http://csta.acm.org/Communications/sub/Reports.html" target="_blank">Bugs in the System: Computer Science Teacher Certification in the U.S.</a> Based on 18 months of intensive Google-funded research, this report characterized the current state of teacher certification as being  rife with “bugs in the system” that prevent it from functioning as intended. Examples of current challenges included states where someone with no knowledge of computer science can teach it, states where the requirements for teacher certification are impossible to meet, and states where certification administrators are confused about what computer science is. The report also demonstrated that this is actually a circular problem - States are hesitant to require certification when they have no programs to train the teachers, and teacher training programs are hesitant to create programs for which there is no clear certification pathway.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-QMQ4Q_CK2TA/VafArGKNO0I/AAAAAAAAEVU/ScWzvWzTlio/s1600/CS-Teacher-Certification-Stats-2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="640" src="http://4.bp.blogspot.com/-QMQ4Q_CK2TA/VafArGKNO0I/AAAAAAAAEVU/ScWzvWzTlio/s640/CS-Teacher-Certification-Stats-2.png" width="513" /></a></div>Addressing the issues with the current teacher preparation and certification system is a complex challenge and it requires the commitment of the entire computer science community. Fortunately, some of this work is already underway. CSTA’s report provides a set of recommendations aimed at addressing these issues. Educators, advocates, and policymakers are also beginning to examine their systems and how to reform them. <br /><br />Google is also exploring how we might help. We convened a group of teacher preparation faculty, researchers, and administrators from across the country to brainstorm how we might work with teacher preparation programs to support the inclusion of computational thinking into teacher preparation programs. As a result of this meeting, <a href="http://www.amanyadav.org/" target="_blank">Dr. Aman Yadav</a>, Professor of Educational Psychology and Educational Technology at Michigan State University, is now working on two research articles aimed at helping teacher preparation program leaders better understand what computational thinking is, and how it supports learning across multiple disciplines. <br /><br />Google will also be launching a new online course called <a href="http://computationalthinkingcourse.withgoogle.com/" target="_blank">Computational Thinking for Educators</a>.  In this free course, educators working with students between the ages of 13 and 18 will learn how incorporating computational thinking can enhance and enrich learning in diverse academic disciplines and can help boost students’ confidence when dealing with ambiguous, complex or open-ended problems. The course will run from July 15 to September 30, 2015.<br /><br />These kind of community partnerships are one way that Google can contribute to practitioner-centered solutions and help further the computer science education community’s efforts to  help everyone understand that computer science is a deeply important academic discipline that deserves a place in the K-12 canon and well-prepared teachers to share this knowledge with students.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=qPFNEpbGRq8:q2NFdyzspik:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/qPFNEpbGRq8" height="1" width="1" alt=""/>";s:12:"link_replies";s:168:"http://googleresearch.blogspot.com/feeds/3688680459028500303/comments/defaulthttp://googleresearch.blogspot.com/2015/07/the-thorny-issue-of-cs-teacher.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3688680459028500303";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3688680459028500303";s:4:"link";s:95:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/qPFNEpbGRq8/the-thorny-issue-of-cs-teacher.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:78:"http://googleresearch.blogspot.com/2015/07/the-thorny-issue-of-cs-teacher.html";}}i:3;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-8002588473816383978";s:9:"published";s:29:"2015-07-14T09:00:00.000-07:00";s:7:"updated";s:29:"2015-07-14T09:00:11.420-07:00";s:5:"title";s:28:"Should My Kid Learn to Code?";s:12:"atom_content";s:6453:"<span class="byline-author">Posted by Maggie Johnson, Director of Education and University Relations, Google</span><br /><br />(<i>Cross-posted on the <a href="http://googleforeducation.blogspot.com/2015/07/should-my-kid-learn-to-code.html">Google for Education Blog</a></i>)<br /><br />Over the last few years, successful marketing campaigns such as <a href="https://hourofcode.com/us">Hour of Code</a> and <a href="https://www.madewithcode.com/">Made with Code</a> have helped K12 students become increasingly aware of the power and relevance of computer programming across all fields. In addition, there has been <a href="https://www.coursereport.com/2014-programming-bootcamp-survey.pdf">growth in developer bootcamps</a>, online “learn to code” programs (<a href="https://code.org/">code.org</a>, <a href="http://www.cs-first.com/">CS First</a>, <a href="https://www.khanacademy.org/computing/computer-programming">Khan Academy</a>, <a href="http://www.codecademy.com/">Codecademy</a>, <a href="https://blockly-games.appspot.com/">Blockly Games</a>, etc.), and non-profits focused specifically on girls and underrepresented minorities (URMs) (<a href="http://www.technovationchallenge.org/home/">Technovation</a>, <a href="https://girlswhocode.com/">Girls who Code</a>, <a href="http://www.blackgirlscode.com/">Black Girls Code</a>, <a href="http://www.yeswecode.org/">#YesWeCode</a>, etc.). <br /><br />This is good news, as we need <a href="http://googleresearch.blogspot.com/2015/07/the-computer-science-pipeline-and.html">many more computing professionals</a> than are currently graduating from Computer Science (CS) and Information Technology (IT) programs. There is evidence that students are starting to respond positively too, given undergraduate departments are <a href="http://googleresearch.blogspot.com/2015/07/the-computer-science-pipeline-and_9.html">experiencing capacity issues</a> in accommodating all the students who want to study CS. <br /><br />Most educators agree that basic application and internet skills (typing, word processing, spreadsheets, web literacy and safety, etc.) are fundamental, and thus, “digital literacy” is a part of K12 curriculum. But is coding now a fundamental literacy, like reading or writing, that all K12 students need to learn as well?<br /><br />In order to gain a deeper understanding of the devices and applications they use everyday, it’s important for all students to try coding. In doing so, this also has the positive effect of inspiring more potential future programmers.  Furthermore, there are a set of relevant skills, often consolidated as “<a href="https://www.google.com/edu/resources/programs/exploring-computational-thinking/">computational thinking</a>”, that are becoming more important for all students, given the growth in the use of computers, algorithms and data in many fields. These include:<br /><ul><li>Abstraction, which is the replacement of a complex real-world situation with a simple model within which we can solve problems. CS is the science of abstraction: creating the right model for a problem, representing it in a computer, and then devising appropriate automated techniques to solve the problem within the model. A spreadsheet is an abstraction of an accountant’s worksheet; a word processor is an abstraction of a typewriter; a game like <a href="http://www.civilization5.com/#/civilizations/">Civilization</a> is an abstraction of history.</li><li>An algorithm is a procedure for solving a problem in a finite number of steps that can involve repetition of operations, or branching to one set of operations or another based on a condition. Being able to represent a problem-solving process as an algorithm is becoming increasingly important in any field that uses computing as a primary tool (business, economics, statistics, medicine, engineering, etc.). Success in these fields requires algorithm design skills.</li><li>As computers become essential in a particular field, more domain-specific data is collected, analyzed and used to make decisions. Students need to understand how to find the data; how to collect it appropriately and with respect to privacy considerations; how much data is needed for a particular problem; how to remove noise from data; what techniques are most appropriate for analysis; how to use an analysis to make a decision; etc. Such data skills are already required in many fields.</li></ul>These computational thinking skills are becoming more important as computers, algorithms and data become ubiquitous. Coding will also become more common, particularly with the growth in the use of visual programming languages, like <a href="https://developers.google.com/blockly/about/examples">Blockly</a>, that remove the need to learn programming language syntax, and via custom blocks, can be used as an abstraction for many different applications. <br /><br />One way to represent these different skill sets and the students who need them is as follows: <br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-bk-usmk_Iq0/VaPv8ZFqtEI/AAAAAAAAApA/JFe_gZqprcY/s1600/image03.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="398" src="http://4.bp.blogspot.com/-bk-usmk_Iq0/VaPv8ZFqtEI/AAAAAAAAApA/JFe_gZqprcY/s400/image03.png" width="400" /></a></div>All students need digital literacy, many need computational thinking depending on their career choice, and some will actually do the software development in high-tech companies, IT departments, or other specialized areas. I don’t believe all kids should learn to code seriously, but all kids should try it via programs like <a href="https://code.org/">code.org</a>, <a href="http://www.cs-first.com/">CS First</a> or <a href="https://www.khanacademy.org/computing/computer-programming/programming/drawing-basics/p/intro-to-drawing">Khan Academy</a>. This gives students a good introduction to computational thinking and coding, and provides them with a basis for making an informed decision on whether CS or IT is something they wish to pursue as a career. <div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=9XWisY7orb4:Rwjsbzy6BOU:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/9XWisY7orb4" height="1" width="1" alt=""/>";s:12:"link_replies";s:168:"http://googleresearch.blogspot.com/feeds/8002588473816383978/comments/defaulthttp://googleresearch.blogspot.com/2015/07/should-my-kid-learn-to-code_14.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/8002588473816383978";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/8002588473816383978";s:4:"link";s:95:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9XWisY7orb4/should-my-kid-learn-to-code_14.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:78:"http://googleresearch.blogspot.com/2015/07/should-my-kid-learn-to-code_14.html";}}i:4;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-9058230675085825306";s:9:"published";s:29:"2015-07-13T13:00:00.000-07:00";s:7:"updated";s:29:"2015-07-13T13:00:00.762-07:00";s:5:"title";s:68:"Simulating fermionic particles with superconducting quantum hardware";s:12:"atom_content";s:6642:"<span class="byline-author">Posted by Rami Barends and Julian Kelly, Quantum Electronics Engineers and John Martinis, Research Scientist</span><br /><br />Digital quantum simulation is one of the key applications of a future, viable quantum computer. Researchers around the world hope that quantum computing will not only be able to process certain calculations faster than any classical computer, but also help simulate nature more accurately and answer longstanding questions with regard to high temperature superconductivity, complex quantum materials, and applications in quantum chemistry. <br /><br />A crucial part in describing nature is simulating electrons. Without electrons, you cannot describe metals and their conductivity, or the interatomic bonds which hold molecules together. But simulating systems with many electrons makes for a very tough problem on classical computers, due to some of their peculiar quantum properties.<br /><br />Electrons are <a href="https://en.wikipedia.org/wiki/Fermion">fermionic particles</a>, and as such obey the well-known <a href="https://en.wikipedia.org/wiki/Pauli_exclusion_principle">Pauli exclusion principle</a> which states that no fermions in a system can occupy the same <a href="https://en.wikipedia.org/wiki/Quantum_state">quantum state</a>. This is due to a property called <a href="https://en.wikipedia.org/wiki/Anticommutativity">anticommutation</a>, an inherent quantum mechanical behavior of all fermions, that makes it very tricky to fully simulate anything that is composed of complex interactions between electrons. The upshot of this anticommutative property is that if you have identical electrons, one at position A and another at position B, and you swap them, you end up with a different quantum state. If your simulation has many electrons you need to carefully keep track of these changes, while ensuring <i>all</i> the interactions between electrons can be completely, yet separately tunable.<br /><br />Add to that the memory errors caused by fluctuation or noise from their environment and the fact that quantum physics prevents one from directly monitoring the superconducting quantum bits (“<a href="https://en.wikipedia.org/wiki/Qubit">qubits</a>”) of a quantum computer directly to account for those errors, and you've got your hands full.  However, earlier this year we reported on some exciting steps towards <a href="http://googleresearch.blogspot.com/2015/03/a-step-closer-to-quantum-computation.html">Quantum Error Correction</a> - as it turns out, the hardware we built isn't only useable for error correction, but can also be used for quantum simulation. <br /><br />In <a href="http://www.nature.com/ncomms/2015/150708/ncomms8654/full/ncomms8654.html">Digital quantum simulation of fermionic models with a superconducting circuit</a>, published in <a href="http://www.nature.com/ncomms/index.html">Nature Communications</a>, we present digital methods that enable the simulation of the complex interactions between fermionic particles, by using single-qubit and two-qubit quantum logic gates as building blocks. And with the recent advances in hardware and control we can now implement them. <br /><br />We took our qubits and made them act like interacting fermions. We experimentally verified that the simulated particles anticommute, and implemented static and time-varying models. With over 300 logic gates, it is the largest digital quantum simulation to date, and the first implementation in a solid-state device.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-mxSN3356NQ0/VaP3wLa3XII/AAAAAAAAApU/eMqGdg5JdAM/s1600/Screen%2BShot%2B2015-07-13%2Bat%2B10.25.56%2BAM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="194" src="http://2.bp.blogspot.com/-mxSN3356NQ0/VaP3wLa3XII/AAAAAAAAApU/eMqGdg5JdAM/s640/Screen%2BShot%2B2015-07-13%2Bat%2B10.25.56%2BAM.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><b>Left:</b> Model picture with four fermionic modes in two sites. The modes are occupied or unoccupied. For example, we can start with two fermionic particles in the right well, by occupying the blue and green mode. If the particles repel each other, there's a good chance that one of the them will hop to the left well through the process of quantum tunneling through the barrier. It will then occupy the red or purple mode. This interplay of on-site interaction and hopping lies at the core of describing processes in physics and chemistry, ranging from the conductivity of metals to the binding between atoms in molecules. <b>Right:</b> The false-colored cross-shaped structures are the superconducting quantum bits. The colors correspond to the modes, so if we have two fermionic particles in the blue and red modes, the rightmost two quantum bits are excited.</td></tr></tbody></table>Coming up with an efficient sequence of logic gates that can accurately model the interactions for systems of fermions wasn’t easy. So we teamed up with <a href="https://sites.google.com/site/lucaslamata/">Dr. Lucas Lamata</a>, <a href="http://www.qutisgroup.com/m-sc-laura-garcia-alvarez/">M.Sc. Laura García-Álvarez</a>, and <a href="http://www.qutisgroup.com/prof-enrique-solano/">Prof. Enrique Solano</a> from the <a href="http://www.qutisgroup.com/">QUTIS group</a> at the University of the Basque Country (UPV/EHU) in Bilbao, Spain, who are experts in constructing algorithms and translating them into the streams of logic gates we can implement with our hardware.<br /><br />For the future, digital quantum simulation holds the promise that it can be run on an error-corrected quantum computer. But before that, we foresee the construction of larger testbeds for simulation with improvements in logic gates and architecture. This experiment is a critical step on the path to creating a quantum simulator capable of modeling fermions as well as <a href="https://en.wikipedia.org/wiki/Boson">bosons</a> (particles which can be interchanged, as opposed to fermions), opening up exciting possibilities for simulating physical and chemical processes in nature. <div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=PE4RmQXeaVw:Y1_JDD2kYDk:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/PE4RmQXeaVw" height="1" width="1" alt=""/>";s:12:"link_replies";s:173:"http://googleresearch.blogspot.com/feeds/9058230675085825306/comments/defaulthttp://googleresearch.blogspot.com/2015/07/simulating-fermionic-particles-with.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/9058230675085825306";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/9058230675085825306";s:4:"link";s:100:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/PE4RmQXeaVw/simulating-fermionic-particles-with.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:83:"http://googleresearch.blogspot.com/2015/07/simulating-fermionic-particles-with.html";}}i:5;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-1054192587012600981";s:9:"published";s:29:"2015-07-09T07:00:00.000-07:00";s:7:"updated";s:29:"2015-07-09T07:55:53.898-07:00";s:5:"title";s:105:"The Computer Science Pipeline and Diversity: Part 2 - Some positive signs, and looking towards the future";s:12:"atom_content";s:12854:"<span class="byline-author">Posted by Maggie Johnson, Director of Education and University Relations, Google</span><br /><br />(<i>Cross-posted on the <a href="http://googleforeducation.blogspot.com/2015/07/the-computer-science-pipeline-and_9.html">Google for Education Blog</a></i>)<br /><br />The disparity between the growing demand for computing professionals and the number of graduates in Computer Science (CS) and Information Technology (IT) has been highlighted in many <a href="http://googleresearch.blogspot.com/2015/07/the-computer-science-pipeline-and.html">recent publications</a>. The tiny pipeline of diverse students (women and underrepresented minorities (URMs)) is even more troubling. Some of the factors causing these issues are:<br /><ul><li>The historical lack of STEM (Science, Technology, Engineering and Mathematics) capabilities in our younger students; lack of proficiency has had a substantial impact on the overall number of students pursuing technical careers. (<a href="https://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-stemed-report.pdf">PCAST Stem Ed report, 2010</a>)</li><li>On the lack of girls in computing, boys often come into computing knowing more than girls because they have been doing it longer.  This can cause girls to lose confidence with the perception that computing is a man’s world. Lack of role models, encouragement and relevant curriculum are additional factors that discourage girls’ participation. (<a href="http://books.google.com/books/about/Unlocking_the_Clubhouse.html?id=StwGQw45YoEC">Margolis 2003</a>)</li><li>On the lack of URMs in computing, the best and most enthusiastic minority students are effectively discouraged from pursuing technical careers because of systemic and structural issues in our high schools and communities, and because of unconscious bias of teachers and administrators. (<a href="http://books.google.com/books/about/Stuck_in_the_Shallow_End.html?id=WOI9rGJSFCcC">Margolis, 2010</a>)</li></ul>Over the last 3-4 years, however, we have seen some significant positive signals in STEM education in general, and in CS/IT in particular.<br /><ul><li>Math<sup>1</sup> and Science<sup>2</sup> results as measured by the National Assessment of Educational Progress (NAEP) have improved slightly since 2009, both in general and for female and minority students.</li><li>Over the last 10 years, there has been an increase in the number of students earning STEM degrees, but the news on women graduates is not as positive.</li></ul><div style="margin-left: 2.5em;"><i>“Overall, 40 percent of bachelor's degrees earned by men and 29 percent earned by women are now in STEM fields. At the doctoral level, more than half of the degrees earned by men (58 percent) and one-third earned by women (33 percent) are in STEM fields. At the bachelor's degree level, though, women are losing ground. Between 2004 and 2014, the share of STEM-related bachelor's degrees earned by women decreased in all seven discipline areas: engineering; computer science; earth, atmospheric and ocean sciences; physical sciences; mathematics; biological and agricultural sciences; and social sciences and psychology. The biggest decrease was in computer science, where women now earn 18 percent of bachelor's degrees (18 percent). In 2004, women earned nearly a quarter of computer science bachelor's degrees, at 23 percent.”</i> - (<a href="http://www.usnews.com/news/articles/2015/01/27/more-students-earning-degrees-in-stem-fields-report-shows">U.S. News, 2015</a>)</div><ul><li>There has been a steady growth in investment in education companies, particularly those focused on innovative uses of technology.</li></ul><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-PtamAsxO2s8/VZwW_KtKMMI/AAAAAAAAAoE/0pGvbWtZ46M/s1600/image01.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="340" src="http://1.bp.blogspot.com/-PtamAsxO2s8/VZwW_KtKMMI/AAAAAAAAAoE/0pGvbWtZ46M/s640/image01.png" width="640" /></a></div><ul><li>The number of publications in <a href="https://scholar.google.com/">Google Scholar</a> on STEM education that focus on gender issues or minority students has steadily increased over the last several years.</li></ul><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-mZR_Cp49jrg/VZwXddFMtRI/AAAAAAAAAoM/8R5Inn_lfpA/s1600/image02.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://3.bp.blogspot.com/-mZR_Cp49jrg/VZwXddFMtRI/AAAAAAAAAoM/8R5Inn_lfpA/s640/image02.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Results from Google Scholar, using “STEM education minority” and “STEM education gender” as search terms</td></tr></tbody></table><ul><li>Successful marketing campaigns such as <a href="https://hourofcode.com/us">Hour of Code</a> and <a href="https://www.madewithcode.com/">Made with Code</a> have helped raise awareness on the accessibility and importance of coding, and the diverse career opportunities in CS.</li><li>There has been growth in <a href="https://www.coursereport.com/2014-programming-bootcamp-survey.pdf">developer bootcamps</a> over the last few years, as well as online “learn to code” programs (<a href="https://code.org/">code.org</a>, <a href="http://www.cs-first.com/">CS First</a>, <a href="https://www.khanacademy.org/computing/computer-programming">Khan Academy</a>, <a href="http://www.codecademy.com/">Codecademy</a>, <a href="https://blockly-games.appspot.com/">Blockly Games</a>, <a href="http://pencilcode.net/">PencilCode</a>, etc.), and an increase in opportunities for K12 students to <a href="http://blog.code.org/post/91961669383/ap-computer-science">learn coding in their schools</a>.  We have also seen non-profits emerge focused specifically on girls and URMs (<a href="http://www.technovationchallenge.org/home/">Technovation</a>, <a href="https://girlswhocode.com/">Girls who Code</a>, <a href="http://www.blackgirlscode.com/">Black Girls Code</a>, <a href="http://www.yeswecode.org/">#YesWeCode</a>, etc.)</li><li>One of the most positive signals has been the growth of graduates in CS over the past few years.</li></ul><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-HUbCxD2uFIk/VZwX4ct8g5I/AAAAAAAAAoY/lCeIy3z5q7Y/s1600/image00.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="322" src="http://4.bp.blogspot.com/-HUbCxD2uFIk/VZwX4ct8g5I/AAAAAAAAAoY/lCeIy3z5q7Y/s640/image00.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Source: <a href="http://cra.org/uploads/documents/resources/crndocs/2013-Taulbee-Survey.pdf">2013 Taulbee Survey</a>, <a href="http://cra.org/">Computing Research Association</a></td></tr></tbody></table>So we are seeing small improvements in K-12 STEM proficiency and undergraduate STEM and CS degrees earned, a significant growth in investment in education innovation, more and more research on the issues of gender and ethnicity in STEM fields and increased opportunities for <b><i>all</i></b> students to learn coding skills online, through non-profit programs, through developer boot camps or in their schools. <br /><br />However, an interesting, and potentially threatening development resulting from this positive momentum is the <a href="http://techcrunch.com/2014/05/25/from-the-ivy-league-to-state-schools-demand-for-computer-science-is-booming/">lack of capacity and faculty</a> in CS departments to handle the increased number of enrollments and majors in CS. Colleges and universities, as a whole, aren’t adequately prepared to handle the surge in CS education demand - Currently there just aren’t enough instructors to teach all the students who want to learn. <br /><br />This has happened in the past. In the 80’s, with the introduction of the PC, and again during the dot-com boom, interest in CS surged. CS departments managed the load by increasing class sizes as much as they possibly could, and/or they put enrollment caps in place and made CS classes harder. The effect of the former was some faculty left for industry while the effect of the latter was a decrease in the diversity pipeline.<br /><div style="margin-left: 2.5em;"><br />“<i>These kinds of caps have two effects which limit access by women and under-represented minorities:</i><br /><ul><li><i>First, the students who succeed the most in intro CS are the ones with prior experience.</i></li><li><i>Second, creating these kinds of caps creates a perception of CS as a highly competitive field, which is a deterrent to many students.  Those students may not even try to get into CS.”</i></li></ul>-(<a href="https://computinged.wordpress.com/2014/10/30/npr-when-women-stopped-coding-in-1980s-are-we-about-to-repeat-the-past/">Guzdial, 2014</a>)</div><br />If we allow the past to repeat itself, we may again find CS faculty leaving for industry and less diversity students going into the field. In addition, unlike the dot-com boom where interest in CS plummeted with the bust, it’s unlikely we will see a decrease in enrollments, particularly in the introductory CS courses. “CS+X”, which represents the application of CS in other fields, is illustrated by the following sample list of interdisciplinary majors in various universities:<br /><ul><li>Yale: "Computer Science and Psychology is an interdepartmental major..."</li><li>USC: "B.S in Physics/Computer Science for students with dual interests..."</li><li>Stanford: "Mathematical and Computational Sciences for students interested in..."</li><li>Northeastern: "Computer Science/Music Technology dual major for students who want to explore connections between..."</li><li>Lehigh: "BS in Computer Science and Business integrates..."</li><li>Dartmouth: "The M.D.-Ph.D. Program in Computational Biology..."</li></ul>The number of non-major students taking CS courses, particularly the introductory ones, is growing, which makes the capacity issues worse. <br /><br />At Google, we recently funded a number of universities via our <a href="http://googleresearch.blogspot.com/2015/03/google-computer-science-capacity-awards.html">3X3 award program</a> (3 times the number of students in 3 years), which aims to facilitate innovative, inclusive, and sustainable approaches to address these scaling issues in university CS programs. Our hope is to disseminate and scale the most successful approaches that our university partners develop. A positive development, which was not present when this happened in the past, is the recent innovation in online education and technology. The increase in bandwidth, high-quality content and interactive learning opportunities may help us get ahead of this challenging capacity issue. <br /><br /><hr size="1" width="95%" /><sup>1</sup><i>Average mathematics scores for fourth- and eighth-graders in 2013 were 1 point higher than in 2011, and 28 and 22 points higher respectively in comparison to the first assessment year in 1990. Hispanic students made gains in mathematics from 2011 to 2013 at both grades 4 and 8. Fourth- and eighth-grade female students scored higher in mathematics in 2013 than in 2011, but the scores for fourth- and eighth-grade male students did not change significantly over the same period. (<a href="http://www.nationsreportcard.gov/reading_math_2013/#/executive-summary">Nation’s Report Card</a>)</i><br /><i><br /></i> <sup>2</sup><i>The average eighth-grade science score increased two points, from 150 in 2009 to 152 in 2011. Scores also rose among public school students in 16 of 47 states that participated in both 2009 and 2011, and no state showed a decline in science scores from 2009 to 2011. A five-point gain from 2009 to 2011 by Hispanic students was larger than the one-point gain for White students, an improvement that narrowed the score gap between those two groups. Black students scored three points higher in 2011 than in 2009, narrowing the achievement gap with White students. (<a href="http://www.nationsreportcard.gov/subject/_commonobjects/media/pdf/PressRelease_Science_2011.pdf">Nation’s Report Card</a>)</i><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=mu-csMGmF4U:2d5m4KeudhQ:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/mu-csMGmF4U" height="1" width="1" alt=""/>";s:12:"link_replies";s:173:"http://googleresearch.blogspot.com/feeds/1054192587012600981/comments/defaulthttp://googleresearch.blogspot.com/2015/07/the-computer-science-pipeline-and_9.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/1054192587012600981";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/1054192587012600981";s:4:"link";s:100:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/mu-csMGmF4U/the-computer-science-pipeline-and_9.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:83:"http://googleresearch.blogspot.com/2015/07/the-computer-science-pipeline-and_9.html";}}i:6;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-3718274937266429229";s:9:"published";s:29:"2015-07-08T07:00:00.000-07:00";s:7:"updated";s:29:"2015-07-08T07:00:02.938-07:00";s:5:"title";s:74:"The Computer Science Pipeline and Diversity: Part 1 - How did we get here?";s:12:"atom_content";s:10801:"<span class="byline-author">Posted by Maggie Johnson, Director of Education and University Relations, Google</span><br /><br />(<i>Cross-posted on the <a href="http://googleforeducation.blogspot.com/2015/07/the-computer-science-pipeline-and.html">Google for Education Blog</a></i>)<br /><br />For many years, the Computer Science industry has struggled with a pipeline problem. Since 2009, when the number of undergraduate computer science (CS) graduates hit a low mark, there have been many efforts to increase the supply to meet an ever-increasing demand. Despite these efforts, the projected demand over the next seven years is significant.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-ZS8XFggfQuI/VZq4bLhGt0I/AAAAAAAAAnk/fShJX3X62f4/s1600/image00.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="322" src="http://3.bp.blogspot.com/-ZS8XFggfQuI/VZq4bLhGt0I/AAAAAAAAAnk/fShJX3X62f4/s640/image00.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Source: <a href="http://cra.org/uploads/documents/resources/crndocs/2013-Taulbee-Survey.pdf">2013 Taulbee Survey</a>, <a href="http://cra.org/">Computing Research Association</a></td></tr></tbody></table>Even if we are able to sustain a positive growth in graduation rates over the next 7 years, we will only fill 30-40% of the available jobs. <br /><br /><div style="margin-left: 2em;"><i>“By 2022, the computer and mathematical occupations group is expected to yield more than 1.3 million job openings. However, unlike in most occupational groups, more job openings will stem from growth than from the need to replace workers who change occupations or leave the labor force.” </i>-<a href="http://www.bls.gov/opub/mlr/2013/article/occupational-employment-projections-to-2022.htm">Bureau of Labor Statistics Occupational Projection Report, 2012</a>.</div><br />More than 3 in 4 of these 1.3M jobs will require at least a Bachelor’s degree in CS or an Information Technology (IT) area. With our current production of only 16,000 CS undergraduates per year, we are way off the mark. Furthermore, within this too-small pipeline of CS graduates, is an even smaller supply of diverse - women and underrepresented minority (URM) - students. In 2013, only 14% of graduates were women and 20% URM.  Why is this lack of representation important?<br /><ul><li>The workforce that creates technology should be representative of the people who use it, or there will be an inherent bias in design and interfaces.</li><li>If we get women and URMs involved, we will fill more than 30-40% of the projected jobs over the next 7 years.</li><li>Getting more women and URMs to choose computing occupations will reduce social inequity, since computing occupations are among the fastest-growing and pay the most.</li></ul>Why are so few students interested in pursuing computing as a career, particularly women and URMs? How did we get here?<br /><br />One fundamental reason is the lack of STEM (Science, Technology, Engineering and Mathematics) capabilities in our younger students. Over the last several years, international comparisons of K12 students’ performance in science and mathematics place the U.S. in the middle of the ranking or lower. On the National Assessment of Educational Progress, <a href="https://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-stemed-report.pdf">less than one-third of U.S. eighth graders show proficiency in science and mathematics</a>. Lack of proficiency has led to lack of engagement in technical degree programs, which include CS and IT. <br /><br /><div style="margin-left: 2em;"><i>“In the United States, about 4% of all bachelor’s degrees awarded in 2008 were in engineering. This compares with about 19% throughout Asia and 31% in China specifically.  In computer sciences, the number of bachelor’s and master’s degrees awarded decreased sharply from 2004 to 2007.”&nbsp;</i> -<a href="http://www.nsf.gov/statistics/seind12/pdf/c02.pdf">NSF: Higher Education in Science and Engineering</a>.</div><br />The lack of proficiency has had a substantial impact on the overall number of students pursuing technical careers, but there have also been shifts resulting from trends and events in the technology sector that compound the issue. For example, we saw an increase in CS graduates from 1997 to the early 2000’s which reflected the growth of the dot-com bubble. Students, seeing the financial opportunities, moved increasingly toward technical degree programs. This continued until the collapse, after which a steady decrease occurred, perhaps as a result of disillusionment or caution. <br /><br />Importantly, there are additional factors that are minimizing the diversity of individuals, particularly women, pursuing these fields. It’s important to note that there are no biological or cognitive reasons that justify a gender disparity in individuals participating in computing (<a href="http://www.montana.edu/wrt/Science06GendSim.pdf">Hyde 2006</a>). With similar training and experience, women perform just as well as men in computer-related activities (<a href="http://books.google.com/books/about/Unlocking_the_Clubhouse.html?id=StwGQw45YoEC">Margolis 2003</a>). But there can be important differences in reinforced predilections and interests during childhood that affect the diversity of those choosing to pursue computer science . <br /><br />In general, most young boys build and explore; play with blocks, trains, etc.; and engage in activity and movement. For a typical boy, a computer can be the ultimate toy that allows him to pursue his interests, and this can develop into an intense passion early on. Many girls like to build, play with blocks, etc. too. For the most part, however, girls tend to prefer social interaction. Most girls develop an interest in computing later through social media and YouTubers, girl-focused games, or through math, science and computing courses.  They typically do not develop the intense interest in computing at an early age like some boys do – they may never experience that level of interest (<a href="http://books.google.com/books/about/Unlocking_the_Clubhouse.html?id=StwGQw45YoEC">Margolis 2003</a>).<br /><br />Thus, some boys come into computing knowing more than girls because they have been doing it longer. This can cause many girls to lose confidence and drive during adolescence with the perception that technology is a man’s world - Both girls and boys perceive computing to be a largely masculine field (<a href="http://life-slc.org/docs/Mercier_etal-Imagesofself.pdf">Mercier 2006</a>). Furthermore, there are few role models at home, school or in the media changing the perception that computing is just not for girls. This overall lack of support and encouragement keeps many girls from considering computing as a career. (<a href="https://docs.google.com/a/google.com/file/d/0B-E2rcvhnlQ_a1Q4VUxWQ2dtTHM/edit">Google white paper 2014</a>)<br /><br />In addition, many teachers are oblivious to or support the gender stereotypes by assigning problems and projects that are oriented more toward boys, or are not of interest to girls. This lack of relevant curriculum is important. Many women who have pursued technology as a career cite relevant courses as critical to their decision (<a href="http://www.ncwit.org/sites/default/files/legacy/pdf/NCWIT-GSUSAPhaseIIIReport_FINAL.pdf">Liston 2008</a>). <br /><br />While gender differences exist with URM groups as well, there are compelling additional factors that affect them. <a href="http://gseis.ucla.edu/directory/jane-margolis/">Jane Margolis</a>, a senior researcher at UCLA, did a study in 2000 resulting in the book <a href="http://books.google.com/books/about/Stuck_in_the_Shallow_End.html?id=WOI9rGJSFCcC"><i>Stuck in the Shallow End</i></a>. She and her research group studied three very different high schools in Los Angeles, with different student demographics. The results of the study show that across all three schools, minority students do not get the same opportunities. While all of the students have access to basic technology courses (word processor, spreadsheet skills, etc.), advanced CS courses are typically only made available to students who, because of opportunities they already have outside school, need it less. Additionally, the best and most enthusiastic minority students can be effectively discouraged because of systemic and structural issues, and belief systems of teachers and administrators. The result is a small, mostly homogeneous group of students have all the opportunities and are introduced to CS, while the rest are relegated to the “shallow end of computing skills”, which perpetuates inequities and keeps minority students from pursuing computing careers. <br /><br />These are some of the reasons why the pipeline for technical talent is so small and why the diversity pipeline is even smaller. Over the last two years, however, we are starting to see some positive signs.<br /><ul><li>Many students are becoming more aware of the relevance and accessibility of coding through campaigns such as <a href="https://hourofcode.com/us">Hour of Code</a> and <a href="https://www.madewithcode.com/">Made with Code</a>.</li><li>This increase in awareness has helped to produce a steady increase in CS and IT graduates, and there’s every indication this growth will continue.</li><li>More opportunities to participate in CS-related activities are becoming available for girls and URMs, such as <a href="http://www.cs-first.com/">CS First</a>, <a href="http://www.technovationchallenge.org/home/">Technovation</a>, <a href="https://girlswhocode.com/">Girls who Code</a>, <a href="http://www.blackgirlscode.com/">Black Girls Code</a>, <a href="http://www.yeswecode.org/">#YesWeCode</a>, etc.</li></ul>There’s much more that can be done to reinforce these positive trends, and to get more students of all types to pursue computing as a career. This is important not only to high tech, but is critical for our nation to compete globally. In the next post of this series, we will explore some of the positive steps that have been taken in increasing the diversity of graduates in Computer Science (CS) and Information Technology (IT) fields.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=0imLjod21as:nPaJHqoozHE:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/0imLjod21as" height="1" width="1" alt=""/>";s:12:"link_replies";s:171:"http://googleresearch.blogspot.com/feeds/3718274937266429229/comments/defaulthttp://googleresearch.blogspot.com/2015/07/the-computer-science-pipeline-and.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3718274937266429229";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3718274937266429229";s:4:"link";s:98:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/0imLjod21as/the-computer-science-pipeline-and.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:81:"http://googleresearch.blogspot.com/2015/07/the-computer-science-pipeline-and.html";}}i:7;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-2842256359318124212";s:9:"published";s:29:"2015-07-05T21:00:00.000-07:00";s:7:"updated";s:29:"2015-07-05T21:00:05.606-07:00";s:5:"title";s:49:"ICML 2015 and Machine Learning Research at Google";s:12:"atom_content";s:8611:"<span class="byline-author">Posted by Corinna Cortes, Head, Google Research NY</span><br /><br />This week, Lille, France hosts the <a href="http://icml.cc/2015/">2015 International Conference on Machine Learning</a> (ICML 2015), a premier annual Machine Learning event supported by the <a href="http://www.machinelearning.org/">International Machine Learning Society</a> (IMLS). As a leader in Machine Learning research, Google will have a strong presence at ICML 2015, with many Googlers publishing work and hosting workshops. If you’re attending, we hope you’ll visit the Google booth and talk with the Googlers to learn more about the hard work, creativity and fun that goes into solving interesting ML problems that impacts millions of people. You can also learn more about our research being presented at ICML 2015 in the list below (Googlers highlighted in <span style="color: #3d85c6;">blue</span>).<br /><br />Google is an Platinum Sponsor of ICML 2015.<br /><br /><b><u>ICML Program Committee</u></b><br /><i>Area Chair - <span style="color: #3d85c6;">Corinna Cortes</span> &amp; <span style="color: #3d85c6;">Samy Bengio</span></i><br /><i>IMLS Board Member - <span style="color: #3d85c6;">Corinna Cortes</span></i><br /><br /><b><u>Papers:</u></b><br /><a href="http://jmlr.org/proceedings/papers/v37/piech15.pdf">Learning Program Embeddings to Propagate Feedback on Student Code</a><br /><i>Chris Piech, <span style="color: #3d85c6;">Jonathan Huang</span>, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas</i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/gouws15.pdf">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</a><br /><i><span style="color: #3d85c6;">Stephan Gouws</span>, Yoshua Bengio, <span style="color: #3d85c6;">Greg Corrado</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">An Empirical Exploration of Recurrent Network Architectures</a><br /><i><span style="color: #3d85c6;">Rafal Jozefowicz</span>, Wojciech Zaremba, <span style="color: #3d85c6;">Ilya Sutskever</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a><br /><i><span style="color: #3d85c6;">Sergey Ioffe</span>, <span style="color: #3d85c6;">Christian Szegedy</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/gregor15.pdf">DRAW: A Recurrent Neural Network For Image Generation</a><br /><i><span style="color: #3d85c6;">Karol Gregor</span>, <span style="color: #3d85c6;">Ivo Danihelka</span>, <span style="color: #3d85c6;">Alex Graves</span>, <span style="color: #3d85c6;">Danilo Rezende</span>, <span style="color: #3d85c6;">Daan Wierstra</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/rezende15.pdf">Variational Inference with Normalizing Flows</a><br /><i><span style="color: #3d85c6;">Danilo Rezende</span>, <span style="color: #3d85c6;">Shakir Mohamed</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/cortes15.pdf">Structural Maxent Models</a><br /><i><span style="color: #3d85c6;">Corinna Cortes</span>, Vitaly Kuznetsov, <span style="color: #3d85c6;">Mehryar Mohri</span>, <span style="color: #3d85c6;">Umar Syed</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/blundell15.pdf">Weight Uncertainty in Neural Network</a><br /><i><span style="color: #3d85c6;">Charles Blundell</span>, <span style="color: #3d85c6;">Julien Cornebise</span>, <span style="color: #3d85c6;">Koray Kavukcuoglu</span>, <span style="color: #3d85c6;">Daan Wierstra</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/germain15.pdf">MADE: Masked Autoencoder for Distribution Estimation</a><br /><i>Mathieu Germain, <span style="color: #3d85c6;">Karol Gregor</span>, Iain Murray, Hugo Larochelle</i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/heinrich15.pdf">Fictitious Self-Play in Extensive-Form Games</a><br /><i>Johannes Heinrich, <span style="color: #3d85c6;">Marc Lanctot</span>, <span style="color: #3d85c6;">David Silver</span></i><br /><br /><a href="http://jmlr.org/proceedings/papers/v37/schaul15.pdf">Universal Value Function Approximators</a><br /><i><span style="color: #3d85c6;">Tom Schaul</span>, <span style="color: #3d85c6;">Daniel Horgan</span>, <span style="color: #3d85c6;">Karol Gregor</span>, <span style="color: #3d85c6;">David Silver</span></i><br /><br /><b><u>Workshops:</u></b><br /><a href="https://sites.google.com/site/extremeclassification/">Extreme Classification: Learning with a Very Large Number of Labels</a><br /><i><span style="color: #3d85c6;">Samy Bengio</span> - Organizing Committee</i><br /><br /><a href="http://dsp.rice.edu/ML4Ed_ICML2015">Machine Learning for Education</a><br /><i><span style="color: #3d85c6;">Jonathan Huang</span> - Organizing Committee</i><br /><br /><a href="https://mloss.org/workshop/icml15/">Workshop on Machine Learning Open Source Software 2015: Open Ecosystems</a><br /><i><span style="color: #3d85c6;">Ian Goodfellow</span> - Program Committee</i><br /><br /><a href="http://sites.google.com/site/ml4md2015/">Machine Learning for Music Recommendation</a><br /><i><span style="color: #3d85c6;">Philippe Hamel </span>- Invited Speaker</i><br /><br /><a href="https://sites.google.com/site/largescalekernelwsicml15/">Large-Scale Kernel Learning: Challenges and New Opportunities</a><br /><i>Poster - <a href="https://096a3cce-a-62cb3a1a-s-sites.googlegroups.com/site/largescalekernelwsicml15/files/Jitkrittum.pdf?attachauth=ANoY7copOax5bzYxXd064pjHuAveCr-IEtAlVEVbY1jBBaamVijZtDVqOYimVzdFj9oC7wj2lSqlqWrBI2k9TV2fth8-Y6CkKQTbLy4YOsDy-MDD889Y6qtvz2b5VHTP4P00rAPRSwn7K1VTXA8kwkqqz5jePuV6mYWv1dwyxoi33A7ZAErrs0QtdHBqzDt_eEF2uaCbNgIyG6KadtHvlwE3rl6qsXwR5aFvdQeVqu5kS4IHJG2Yppk%3D&amp;attredirects=0">Just-In-Time Kernel Regression for Expectation Propagation</a></i><br /><i>Wittawat Jitkrittum, Arthur Gretton, <span style="color: #3d85c6;">Nicolas Heess</span>, <span style="color: #3d85c6;">S.M. Ali Eslami</span>, Balaji Lakshminarayanan, Dino Sejdinovic, Zoltan Szabo</i><br /><br /><a href="https://ewrl.wordpress.com/ewrl12-2015/">European Workshop on Reinforcement Learning (EWRL)</a><br /><i><span style="color: #3d85c6;">Rémi Munos</span> - Organizing Committee</i><br /><i><span style="color: #3d85c6;">David Silver</span> - Keynote</i><br /><br /><a href="https://sites.google.com/site/deeplearning2015/">Workshop on Deep Learning</a><br /><i><span style="color: #3d85c6;">Geoff Hinton</span> - Organizer</i><br /><i><span style="color: #3d85c6;">Tara Sainath</span>, <span style="color: #3d85c6;">Oriol Vinyals</span>, <span style="color: #3d85c6;">Ian Goodfellow</span>, <span style="color: #3d85c6;">Karol Gregor</span> - Invited Speakers</i><br /><i>Poster - <a href="https://sites.google.com/site/deeplearning2015/36.pdf?attredirects=0">A Neural Conversational Model </a></i><br /><i><span style="color: #3d85c6;">Oriol Vinyals</span>, <span style="color: #3d85c6;">Quoc Le</span></i><br /><i>Oral Presentation - <a href="https://8109f4a4-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearning2015/1.pdf?attachauth=ANoY7cofLIDes0UcI4oh-TOrvCErHWOPk-YUpcKLbSJggFujDjKOXgz33VmpQLlxOm238vkzkqGvqRDcd0abXBNzIjlAtlP6aTfI_mjEQr_l0qEzbPwgULocpAc7jh9avkTZJv7WBhLkjSgEknxe4mo1ElaSDk7rSqgKm66n_BeNhVcDnRsPRpLZl-u_FxYw1wMXf63RU9tsNqXXih4fG-D8ayCzjIzo2w%3D%3D&amp;attredirects=0">Massively Parallel Methods for Deep Reinforcement Learning</a></i><br /><i><span style="color: #3d85c6;">Arun Nair</span>, <span style="color: #3d85c6;">Praveen Srinivasan</span>, <span style="color: #3d85c6;">Sam Blackwell</span>, <span style="color: #3d85c6;">Cagdas Alcicek</span>, <span style="color: #3d85c6;">Rory Fearon</span>, <span style="color: #3d85c6;">Alessandro De Maria</span>, <span style="color: #3d85c6;">Vedavyas Panneershelvam</span>, <span style="color: #3d85c6;">Mustafa Suleyman</span>, <span style="color: #3d85c6;">Charles Beattie</span>, <span style="color: #3d85c6;">Stig Petersen</span>, <span style="color: #3d85c6;">Shane Legg</span>, <span style="color: #3d85c6;">Volodymyr Mnih</span>, <span style="color: #3d85c6;">Koray Kavukcuoglu</span>, <span style="color: #3d85c6;">David Silver</span></i><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=luqvRsCcHTA:uE4ZJSm_kvI:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/luqvRsCcHTA" height="1" width="1" alt=""/>";s:12:"link_replies";s:177:"http://googleresearch.blogspot.com/feeds/2842256359318124212/comments/defaulthttp://googleresearch.blogspot.com/2015/07/icml-2015-and-machine-learning-research.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/2842256359318124212";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/2842256359318124212";s:4:"link";s:104:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/luqvRsCcHTA/icml-2015-and-machine-learning-research.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:87:"http://googleresearch.blogspot.com/2015/07/icml-2015-and-machine-learning-research.html";}}i:8;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-8115528194584375572";s:9:"published";s:29:"2015-07-01T15:00:00.000-07:00";s:7:"updated";s:29:"2015-07-01T15:48:17.074-07:00";s:5:"title";s:58:"DeepDream - a code example for visualizing Neural Networks";s:12:"atom_content";s:3254:"<span class="byline-author">Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer</span><br /><br />Two weeks ago <a href="http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html">we blogged about a visualization tool</a> designed to help us understand how neural networks work and what each layer has learned. In addition to gaining some insight on how these networks carry out classification tasks, we found that this process also generated some beautiful art.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-CdUrPm7x5Ig/VZQIGjJzP0I/AAAAAAAAAnI/qhqchfzdaOc/s1600/image00.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="640" src="http://1.bp.blogspot.com/-CdUrPm7x5Ig/VZQIGjJzP0I/AAAAAAAAAnI/qhqchfzdaOc/s640/image00.jpg" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Top: Input image. Bottom: output image made using a network trained on places by <a href="http://places.csail.mit.edu/">MIT Computer Science and AI Laboratory</a>.</td></tr></tbody></table>We have seen a lot of interest and received some great questions, from programmers and artists alike, about the details of how these visualizations are made. We have decided to open source the code we used to generate these images in an <a href="https://github.com/google/deepdream">IPython notebook</a>, so now you can make neural network inspired images yourself! <br /><br />The code is based on <a href="http://caffe.berkeleyvision.org/">Caffe</a> and uses available open source packages, and is designed to have as few dependencies as possible. To get started, you will need the following (full details in the notebook):<br /><br /><ul><li><a href="http://www.numpy.org/">NumPy</a>, <a href="http://www.scipy.org/">SciPy</a>, <a href="http://www.pythonware.com/products/pil/">PIL</a>, <a href="http://ipython.org/">IPython</a>, or a scientific python distribution such as <a href="http://continuum.io/downloads">Anaconda</a> or <a href="https://store.enthought.com/">Canopy</a>.</li><li><a href="http://caffe.berkeleyvision.org/">Caffe</a> deep learning framework (<a href="http://caffe.berkeleyvision.org/installation.html">Installation instructions</a>)</li></ul><br />Once you’re set up, you can supply an image and choose which layers in the network to enhance, how many iterations to apply and how far to zoom in. Alternatively, different pre-trained networks can be plugged in. <br /><br />It'll be interesting to see what imagery people are able to generate. If you post images to Google+, Facebook, or Twitter, be sure to tag them with #deepdream so other researchers can check them out too.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=9u-Q0pcR6sk:TlfneEF8wfc:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/9u-Q0pcR6sk" height="1" width="1" alt=""/>";s:12:"link_replies";s:176:"http://googleresearch.blogspot.com/feeds/8115528194584375572/comments/defaulthttp://googleresearch.blogspot.com/2015/07/deepdream-code-example-for-visualizing.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/8115528194584375572";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/8115528194584375572";s:4:"link";s:103:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9u-Q0pcR6sk/deepdream-code-example-for-visualizing.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:86:"http://googleresearch.blogspot.com/2015/07/deepdream-code-example-for-visualizing.html";}}i:9;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-8594303534017236098";s:9:"published";s:29:"2015-06-18T22:00:00.000-07:00";s:7:"updated";s:29:"2015-06-25T13:27:28.362-07:00";s:5:"title";s:64:"Google Computational Journalism Research Awards launch in Europe";s:12:"atom_content";s:3114:"<span class="byline-author">Posted by Andrea Held, Google University Relations &amp; Matt Cooke, Google News Lab Europe</span><br /><br />Journalism is evolving fast in the digital age, and researchers across Europe are working on exciting projects to create innovative new tools and open source software that will support online  journalism and benefit readers.  As part of the wider Google <a href="http://www.digitalnewsinitiative.com/">Digital News Initiative</a> (DNI), we invited academic researchers across Europe to submit proposals for the Computational Journalism Research Awards.  <br /><br />After careful review by Google’s News Lab and Research teams, the following projects were selected:<br /><br /><b>SCAN: Systematic Content Analysis of User Comments for Journalists</b><br /><a href="https://mobis.informatik.uni-hamburg.de/people/walidmaalej/">Walid Maalej</a>, Professor of Informatics, University of Hamburg<br /><a href="http://www.hans-bredow-institut.de/en/node/3398">Wiebke Loosen</a>, Senior Researcher for Journalism, Hans-Bredow-Institute, Hamburg, Germany<br />This project aims at developing a framework for the systematic, semi-automated analysis of audience feedback on journalistic content to better reflect the voice of users, mitigate the analysis efforts, and help journalists generate new content from the user comments. <br /><br /><b>Event Thread Extraction for Viewpoint Analysis</b><br /><a href="http://pages.saclay.inria.fr/ioana.manolescu/">Ioana Manolescu</a>, Senior Researcher, INRIA Saclay, France<br /><a href="https://perso.limsi.fr/xtannier/en/">Xavier Tannier</a>, Professor of Computer Science, University Paris-Sud, France<br />The goal of the project is to automatically build topic "event threads" that will help journalists and citizens decode claims made by public figures, in order to distinguish between personal opinion, communication tools and voluntary distortions of the reality.<br /><br /><b>Computational Support for Creative Story Development by Journalists</b><br /><a href="http://www.city.ac.uk/people/academics/neil-maiden">Neil Maiden</a>, Professor of Systems Engineering <br /><a href="http://www.city.ac.uk/people/academics/george-brock">George Brock</a>, Professor of Journalism, City University London, UK<br />This project will develop a new software prototype to implement creative search strategies that journalists could use to strengthen investigative storytelling more efficiently than with current news content management and search tools.<br /><br />We congratulate the recipients of these awards and we look forward to the results of their research. Each award includes funding of up to $60,000 in cash and $20,000 in computing credits on Google’s Cloud Platform. Stay tuned for updates on their progress.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=xmT-_hsTL_4:S3D30MQ7uqo:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/xmT-_hsTL_4" height="1" width="1" alt=""/>";s:12:"link_replies";s:169:"http://googleresearch.blogspot.com/feeds/8594303534017236098/comments/defaulthttp://googleresearch.blogspot.com/2015/06/google-computational-journalism.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/8594303534017236098";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/8594303534017236098";s:4:"link";s:96:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xmT-_hsTL_4/google-computational-journalism.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:79:"http://googleresearch.blogspot.com/2015/06/google-computational-journalism.html";}}i:10;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-5675413448691562451";s:9:"published";s:29:"2015-06-17T19:00:00.000-07:00";s:7:"updated";s:29:"2015-07-13T17:23:29.914-07:00";s:5:"title";s:47:"Inceptionism: Going Deeper into Neural Networks";s:12:"atom_content";s:13519:"<span class="byline-author">Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer</span><br /><br /><span style="font-size: x-small;"><b>Update - 13/07/2015</b></span><br /><span style="font-size: x-small;"><i>Images in this blog post are licensed by Google Inc. under a <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. However, images based on places by <a href="http://places.csail.mit.edu/">MIT Computer Science and AI Laboratory</a> require additional permissions from MIT for use.</i></span><br /><br /><a href="http://en.wikipedia.org/wiki/Artificial_neural_network">Artificial Neural Networks</a> have spurred remarkable recent progress in <a href="http://googleresearch.blogspot.com/2014/09/building-deeper-understanding-of-images.html#uds-search-results">image classification</a> and <a href="https://www.youtube.com/watch?v=yxxRAHVtafI">speech recognition</a>. But even though these are very useful tools based on well-known mathematical methods, we actually understand surprisingly little of why certain models work and others don’t. So let’s take a look at some simple techniques for peeking inside these networks.<br /><br />We train an artificial neural network by showing it millions of training examples and <a href="https://en.wikipedia.org/?title=Backpropagation">gradually adjusting the network parameters</a> until it gives the classifications we want. The network typically consists of 10-30 stacked layers of artificial neurons. Each image is fed into the input layer, which then talks to the next layer, until eventually the “output” layer is reached. The network’s “answer” comes from this final output layer.<br /><br />One of the challenges of neural networks is understanding what exactly goes on at each layer. We know that after training, each layer progressively extracts higher and higher-level features of the image, until the final layer essentially makes a decision on what the image shows. For example, the first layer maybe looks for edges or corners. Intermediate layers interpret the basic features to look for overall shapes or components, like a door or a leaf. The final few layers assemble those into complete interpretations—these neurons activate in response to very complex things such as entire buildings or trees. <br /><br />One way to visualize what goes on is to turn the network upside down and ask it to enhance an input image in such a way as to elicit a particular interpretation. Say you want to know what sort of image would result in “Banana.” Start with an image full of random noise, then gradually tweak the image towards what the neural net considers a banana (see related work in <a href="http://arxiv.org/pdf/1412.1897v4.pdf">[1]</a>, <a href="http://arxiv.org/pdf/1412.0035v1.pdf">[2]</a>, <a href="http://arxiv.org/pdf/1506.02753.pdf">[3]</a>, <a href="http://arxiv.org/pdf/1312.6034v2.pdf">[4]</a>). By itself, that doesn’t work very well, but it does if we impose a prior constraint that the image should have similar statistics to natural images, such as neighboring pixels needing to be correlated. <br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-tTYZpdJ18bg/VYITAO4s_uI/AAAAAAAAAlE/L7VMImFFt_M/s1600/noise-to-banana.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="257" src="http://4.bp.blogspot.com/-tTYZpdJ18bg/VYITAO4s_uI/AAAAAAAAAlE/L7VMImFFt_M/s640/noise-to-banana.png" width="640" /></a></div>So here’s one surprise: neural networks that were trained to discriminate between different kinds of images have quite a bit of the information needed to <i>generate</i> images too. Check out some more examples across different classes:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-17ajatawCW4/VYITTA1NkDI/AAAAAAAAAlM/eZmy5_Uu9TQ/s1600/classvis.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="374" src="http://2.bp.blogspot.com/-17ajatawCW4/VYITTA1NkDI/AAAAAAAAAlM/eZmy5_Uu9TQ/s640/classvis.png" width="640" /></a></div>Why is this important? Well, we train networks by simply showing them many examples of what we want them to learn, hoping they extract the essence of the matter at hand (e.g., a fork needs a handle and 2-4 tines), and learn to ignore what doesn’t matter (a fork can be any shape, size, color or orientation). But how do you check that the network has correctly learned the right features? It can help to visualize the network’s representation of a fork.<br /><br />Indeed, in some cases, this reveals that the neural net isn’t quite looking for the thing we thought it was. For example, here’s what one neural net we designed thought dumbbells looked like:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-dc6B2h_o1fc/VYITir_QCgI/AAAAAAAAAlU/Ysi0_reQTpI/s1600/dumbbells.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="154" src="http://3.bp.blogspot.com/-dc6B2h_o1fc/VYITir_QCgI/AAAAAAAAAlU/Ysi0_reQTpI/s640/dumbbells.png" width="640" /></a></div>There are dumbbells in there alright, but it seems no picture of a dumbbell is complete without a muscular weightlifter there to lift them. In this case, the network failed to completely distill the essence of a dumbbell. Maybe it’s never been shown a dumbbell without an arm holding it. Visualization can help us correct these kinds of training mishaps.<br /><br />Instead of exactly prescribing which feature we want the network to amplify, we can also let the network make that decision. In this case we simply feed the network an arbitrary image or photo and let the network analyze the picture. We then pick a layer and ask the network to enhance whatever it detected. Each layer of the network deals with features at a different level of abstraction, so the complexity of features we generate depends on which layer we choose to enhance. For example, lower layers tend to produce strokes or simple ornament-like patterns, because those layers are sensitive to basic features such as edges and their orientations.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-4Uj3hPFupok/VYIT6s_c9OI/AAAAAAAAAlc/_yGdbbsmGiw/s1600/ibis.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="214" src="http://3.bp.blogspot.com/-4Uj3hPFupok/VYIT6s_c9OI/AAAAAAAAAlc/_yGdbbsmGiw/s640/ibis.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><i>Left: Original photo by <a href="https://www.flickr.com/photos/zachievenor/8258092492/in/set-72157630014410078">Zachi Evenor</a>. Right: processed by Günther Noack, Software Engineer</i></td></tr></tbody></table><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-PK_bEYY91cw/VYIVBYw63uI/AAAAAAAAAlo/iUsA4leua10/s1600/seurat-layout.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="282" src="http://4.bp.blogspot.com/-PK_bEYY91cw/VYIVBYw63uI/AAAAAAAAAlo/iUsA4leua10/s640/seurat-layout.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><i>Left: Original painting by G<a href="https://en.wikipedia.org/wiki/A_Sunday_Afternoon_on_the_Island_of_La_Grande_Jatte#/media/File:Georges_Seurat_-_A_Sunday_on_La_Grande_Jatte_--_1884_-_Google_Art_Project.jpg">eorges Seurat</a>. Right:  processed images by Matthew McNaughton, Software Engineer</i></td></tr></tbody></table>If we choose higher-level layers, which identify more sophisticated features in images, complex features or even whole objects tend to emerge. Again, we just start with an existing image and give it to our neural net. We ask the network: “Whatever you see there, I want more of it!” This creates a feedback loop: if a cloud looks a little bit like a bird, the network will make it look more like a bird. This in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-FPDgxlc-WPU/VYIV1bK50HI/AAAAAAAAAlw/YIwOPjoulcs/s1600/skyarrow.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="170" src="http://4.bp.blogspot.com/-FPDgxlc-WPU/VYIV1bK50HI/AAAAAAAAAlw/YIwOPjoulcs/s640/skyarrow.png" width="640" /></a></div>The results are intriguing—even a relatively simple neural network can be used to over-interpret an image, just like as children we enjoyed watching clouds and interpreting the random shapes. This network was trained mostly on images of animals, so naturally it tends to interpret shapes as animals. But because the data is stored at such a high abstraction, the results are an interesting remix of these learned features.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-R15_fyB-ZpE/VYIV-Uu9iwI/AAAAAAAAAl4/o3heQNGpVRU/s1600/Funny-Animals.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="170" src="http://3.bp.blogspot.com/-R15_fyB-ZpE/VYIV-Uu9iwI/AAAAAAAAAl4/o3heQNGpVRU/s640/Funny-Animals.png" width="640" /></a></div>Of course, we can do more than cloud watching with this technique. We can apply it to any kind of image. The results vary quite a bit with the kind of image, because the features that are entered bias the network towards certain interpretations. For example, horizon lines tend to get filled with towers and pagodas. Rocks and trees turn into buildings. Birds and insects appear in images of leaves.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-nxPKPYA8otk/VYIWRcpjZfI/AAAAAAAAAmE/8dSuxLnSNQ4/s1600/image-dream-map.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="534" src="http://2.bp.blogspot.com/-nxPKPYA8otk/VYIWRcpjZfI/AAAAAAAAAmE/8dSuxLnSNQ4/s640/image-dream-map.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><i>The original image influences what kind of objects form in the processed image.</i></td></tr></tbody></table>This technique gives us a qualitative sense of the level of abstraction that a particular layer has achieved in its understanding of images. We call this technique “Inceptionism” in reference to the <a href="http://arxiv.org/pdf/1409.4842.pdf">neural net architecture</a> used. See our <a href="https://goo.gl/photos/fFcivHZ2CDhqCkZdA">Inceptionism gallery</a> for more pairs of images and their processed results, plus some cool video animations.<br /><br /><b>We must go deeper: Iterations </b><br /><br />If we apply the algorithm iteratively on its own outputs and apply some zooming after each iteration, we get an endless stream of new impressions, exploring the set of things the network knows about. We can even start this process from a random-noise image, so that the result becomes purely the result of the neural network, as seen in the following images:<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-XZ0i0zXOhQk/VYIXdyIL9kI/AAAAAAAAAmQ/UbA6j41w28o/s1600/building-dreams.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="300" src="http://1.bp.blogspot.com/-XZ0i0zXOhQk/VYIXdyIL9kI/AAAAAAAAAmQ/UbA6j41w28o/s640/building-dreams.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><i>Neural net “dreams”— generated purely from random noise, using a network trained on places by <a href="http://places.csail.mit.edu/">MIT Computer Science and AI Laboratory</a>. See our <a href="https://goo.gl/photos/fFcivHZ2CDhqCkZdA">Inceptionism gallery</a>&nbsp;for hi-res versions of the images above and more (Images marked “Places205-GoogLeNet” were made using this network).</i></td></tr></tbody></table>The techniques presented here help us understand and visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training. It also makes us wonder whether neural networks could become a tool for artists—a new way to remix visual concepts—or perhaps even shed a little light on the roots of the creative process in general.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=vmOHph4QVSU:1qH4NsZX73s:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/vmOHph4QVSU" height="1" width="1" alt=""/>";s:12:"link_replies";s:175:"http://googleresearch.blogspot.com/feeds/5675413448691562451/comments/defaulthttp://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/5675413448691562451";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/5675413448691562451";s:4:"link";s:102:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/vmOHph4QVSU/inceptionism-going-deeper-into-neural.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:85:"http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html";}}i:11;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-2464106895057395344";s:9:"published";s:29:"2015-06-17T09:00:00.000-07:00";s:7:"updated";s:29:"2015-06-17T09:00:00.592-07:00";s:5:"title";s:43:"New ways to add Reminders in Inbox by Gmail";s:12:"atom_content";s:3100:"<span class="byline-author">Posted by Dave Orr, Google Research Product Manager</span><br /><br />Last week, Inbox by Gmail <a href="http://gmailblog.blogspot.com/2015/05/thanks-to-you-inbox-by-gmail-is-now.html">opened up</a> and improved many of your favorite features, including two new ways to add Reminders.<br /><br />First up, when someone emails you a to-do, Inbox can now suggest adding a Reminder so you don’t forget. Here's how it looks if your spouse emails you and asks you to buy milk on the way home:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-f4JPQulhCs8/VYCbdZqoI9I/AAAAAAAAAkQ/Hc9BI3GrHt4/s1600/image01.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="247" src="http://2.bp.blogspot.com/-f4JPQulhCs8/VYCbdZqoI9I/AAAAAAAAAkQ/Hc9BI3GrHt4/s400/image01.png" width="400" /></a></div>To help you add Reminders, the Google Research team used <a href="http://en.wikipedia.org/wiki/Natural_language_understanding">natural language understanding</a> technology to teach Inbox to recognize to-dos in email.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-7EYWPHzU-rc/VYCbjOtPc_I/AAAAAAAAAkY/KPvQmIxoZvU/s1600/image02.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="37" src="http://4.bp.blogspot.com/-7EYWPHzU-rc/VYCbjOtPc_I/AAAAAAAAAkY/KPvQmIxoZvU/s400/image02.png" width="400" /></a></div>And much like Gmail and Inbox get better when you report spam, your feedback helps improve these suggested Reminders. You can accept or reject them with a single click:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-1FWlWWZvC4k/VYCbqyUL8vI/AAAAAAAAAkg/DXojgcYDfsk/s1600/image03.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="371" src="http://2.bp.blogspot.com/-1FWlWWZvC4k/VYCbqyUL8vI/AAAAAAAAAkg/DXojgcYDfsk/s400/image03.png" width="400" /></a></div>The other new way to add Reminders in Inbox is to create Reminders in Google Keep--they will appear in Inbox with a link back to the full note in Google Keep.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/--5WmU3wJZVc/VYCbz99JzYI/AAAAAAAAAko/7ZdQ0CzfaWw/s1600/image00.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="400" src="http://3.bp.blogspot.com/--5WmU3wJZVc/VYCbz99JzYI/AAAAAAAAAko/7ZdQ0CzfaWw/s400/image00.png" width="400" /></a></div>Hopefully, this little extra help gets you back to what matters more quickly and easily. Try the new features out, and as always, let us know what you think using the feedback link in the app.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=MdFXkgTV0Fg:RAjjQI9_JlQ:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/MdFXkgTV0Fg" height="1" width="1" alt=""/>";s:12:"link_replies";s:175:"http://googleresearch.blogspot.com/feeds/2464106895057395344/comments/defaulthttp://googleresearch.blogspot.com/2015/06/new-ways-to-add-reminders-in-inbox-by.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/2464106895057395344";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/2464106895057395344";s:4:"link";s:102:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/MdFXkgTV0Fg/new-ways-to-add-reminders-in-inbox-by.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:85:"http://googleresearch.blogspot.com/2015/06/new-ways-to-add-reminders-in-inbox-by.html";}}i:12;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-5236130123391242147";s:9:"published";s:29:"2015-06-07T09:00:00.000-07:00";s:7:"updated";s:29:"2015-06-08T07:12:47.722-07:00";s:5:"title";s:44:"Google Computer Vision research at CVPR 2015";s:12:"atom_content";s:15524:"<span class="byline-author">Posted by Vincent Vanhoucke, Google Research Scientist</span><br /><br />Much of the world's data is in the form of visual media. In order to utilize meaningful information from multimedia and deliver innovative products, such as <a href="http://www.google.com/photos/about/">Google Photos</a>, Google builds machine-learning systems that are designed to enable computer perception of visual input, in addition to pursuing image and video analysis techniques focused on image/scene reconstruction and understanding.<br /><br />This week, Boston hosts the <a href="http://www.pamitc.org/cvpr15/">2015 Conference on Computer Vision and Pattern Recognition</a> (CVPR 2015), the premier annual computer vision event comprising the main CVPR conference and several co-located workshops and short courses. As a leader in <a href="http://en.wikipedia.org/wiki/Computer_vision">computer vision</a> research, Google will have a strong presence at CVPR 2015, with many Googlers presenting publications in addition to hosting workshops and tutorials on topics covering image/video annotation and enhancement, 3D analysis and processing, development of semantic similarity measures for visual objects, synthesis of meaningful composites for visualization/browsing of large image/video collections and more.<br /><br />Learn more about some of our research in the list below (Googlers highlighted in <span style="color: #3d85c6;">blue</span>). If you are attending CVPR this year, we hope you’ll stop by our booth and chat with our researchers about the projects and opportunities at Google that go into solving interesting problems for hundreds of millions of people.  Members of the <a href="http://g.co/jump">Jump</a> team will also have a prototype of the camera on display and will be showing videos produced using the Jump system on <a href="http://g.co/cardboard">Google Cardboard</a>. <br /><br /><u><b>Tutorials:</b></u><br /><a href="http://torch.ch/docs/cvpr15.html">Applied Deep Learning for Computer Vision with Torch</a><br /><i><span style="color: #3d85c6;">Koray Kavukcuoglu</span>, Ronan Collobert, Soumith Chintala</i><br /><br /><a href="http://tutorial.caffe.berkeleyvision.org/">DIY Deep Learning: a Hands-On Tutorial with Caffe</a><br /><i>Evan Shelhamer, Jeff Donahue, <span style="color: #3d85c6;">Yangqing Jia</span>, Jonathan Long, Ross Girshick</i><br /><br /><a href="http://image-net.org/tutorials/cvpr2015/">ImageNet Large Scale Visual Recognition Challenge Tutorial</a><br /><i>Olga Russakovsky, Jonathan Krause, <span style="color: #3d85c6;">Karen Simonyan</span>, <span style="color: #3d85c6;">Yangqing Jia</span>, Jia Deng, Alex Berg, Fei-Fei Li</i><br /><br /><a href="http://halide-lang.org/cvpr2015.html">Fast Image Processing With Halide</a><br /><i>Jonathan Ragan-Kelley, <span style="color: #3d85c6;">Andrew Adams</span>, Fredo Durand</i><br /><br /><a href="http://www.kitware.com/cvpr2015-tutorial.html">Open Source Structure-from-Motion</a><br /><i>Matt Leotta, <span style="color: #3d85c6;">Sameer Agarwal</span>, Frank Dellaert, Pierre Moulon, Vincent Rabaud</i><br /><br /><b><u>Oral Sessions:</u></b><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Papandreou_Modeling_Local_and_2015_CVPR_paper.pdf">Modeling Local and Global Deformations in Deep Learning: Epitomic Convolution, Multiple Instance Learning, and Sliding Window Detection</a><br /><i><span style="color: #3d85c6;">George Papandreou</span>, Iasonas Kokkinos, Pierre-André Savalle</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/1A_001_ext.pdf">Going Deeper with Convolutions </a><br /><i><span style="color: #3d85c6;">Christian Szegedy</span>, Wei Liu, <span style="color: #3d85c6;">Yangqing Jia</span>, <span style="color: #3d85c6;">Pierre Sermanet</span>, Scott Reed, <span style="color: #3d85c6;">Dragomir Anguelov</span>, <span style="color: #3d85c6;">Dumitru Erhan</span>, <span style="color: #3d85c6;">Vincent Vanhoucke</span>, Andrew Rabinovich</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/1A_038_ext.pdf">DynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-Time</a><br /><i>Richard A. Newcombe, Dieter Fox, </i><span style="color: #3d85c6;"><i>Steven M. Seitz</i></span><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">Show and Tell: A Neural Image Caption Generator </a><br /><i><span style="color: #3d85c6;">Oriol Vinyals</span>, <span style="color: #3d85c6;">Alexander Toshev</span>, <span style="color: #3d85c6;">Samy Bengio</span>, <span style="color: #3d85c6;">Dumitru Erhan</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_044_ext.pdf">Long-Term Recurrent Convolutional Networks for Visual Recognition and Description </a><br /><i>Jeffrey Donahue, Lisa Anne Hendricks, <span style="color: #3d85c6;">Sergio Guadarrama</span>, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/3B_097_ext.pdf">Visual Vibrometry: Estimating Material Properties from Small Motion in Video </a><br /><i>Abe Davis, Katherine L. Bouman, Justin G. Chen, <span style="color: #3d85c6;">Michael Rubinstein</span>, Frédo Durand, <span style="color: #3d85c6;">William T. Freeman</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Barron_Fast_Bilateral-Space_Stereo_2015_CVPR_paper.pdf">Fast Bilateral-Space Stereo for Synthetic Defocus </a><br /><i><span style="color: #3d85c6;">Jonathan T. Barron</span>, <span style="color: #3d85c6;">Andrew Adams</span>, YiChang Shih, <span style="color: #3d85c6;">Carlos Hernández</span></i><br /><br /><b><u>Poster Sessions:</u></b><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ramanathan_Learning_Semantic_Relationships_2015_CVPR_paper.pdf">Learning Semantic Relationships for Better Action Retrieval in Images</a><br /><i>Vignesh Ramanathan, <span style="color: #3d85c6;">Congcong Li</span>, Jia Deng, Wei Han, <span style="color: #3d85c6;">Zhen Li</span>, <span style="color: #3d85c6;">Kunlong Gu</span>, <span style="color: #3d85c6;">Yang Song</span>, <span style="color: #3d85c6;">Samy Bengio</span>, <span style="color: #3d85c6;">Charles Rosenberg</span>, Li Fei-Fei</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf">FaceNet: A Unified Embedding for Face Recognition and Clustering</a><br /><i><span style="color: #3d85c6;">Florian Schroff</span>, <span style="color: #3d85c6;">Dmitry Kalenichenko</span>, <span style="color: #3d85c6;">James Philbin</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Peng_A_Mixed_Bag_2015_CVPR_paper.pdf">A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions</a><br /><i>Kuan-Chuan Peng, Tsuhan Chen, Amir Sadovnik, <span style="color: #3d85c6;">Andrew C. Gallagher</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dekel_Best-Buddies_Similarity_for_2015_CVPR_paper.pdf">Best-Buddies Similarity for Robust Template Matching</a><br /><i>Tali Dekel, Shaul Oron, <span style="color: #3d85c6;">Michael Rubinstein</span>, Shai Avidan, <span style="color: #3d85c6;">William T. Freeman</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pero_Articulated_Motion_Discovery_2015_CVPR_paper.pdf">Articulated Motion Discovery Using Pairs of Trajectories</a><br /><i>Luca Del Pero, <span style="color: #3d85c6;">Susanna Ricco</span>, <span style="color: #3d85c6;">Rahul Sukthankar</span>, Vittorio Ferrari</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Shih_Reflection_Removal_Using_2015_CVPR_paper.pdf">Reflection Removal Using Ghosting Cues</a><br /><i>YiChang Shih, <span style="color: #3d85c6;">Dilip Krishnan</span>, Frédo Durand, <span style="color: #3d85c6;">William T. Freeman</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wu_P3.5P_Pose_Estimation_2015_CVPR_paper.pdf">P3.5P: Pose Estimation with Unknown Focal Length</a><br /><span style="color: #3d85c6;"><i>Changchang Wu</i></span><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Han_MatchNet_Unifying_Feature_2015_CVPR_paper.pdf">MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching</a><br /><i>Xufeng Han, <span style="color: #3d85c6;">Thomas Leung</span>, <span style="color: #3d85c6;">Yangqing Jia</span>, <span style="color: #3d85c6;">Rahul Sukthankar</span>, Alexander C. Berg</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pan_Inferring_3D_Layout_2015_CVPR_paper.pdf">Inferring 3D Layout of Building Facades from a Single Image</a><br /><span style="color: #3d85c6;"><i>Jiyan Pan</i></span><i>, Martial Hebert, Takeo Kanade</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xue_The_Aperture_Problem_2015_CVPR_paper.pdf">The Aperture Problem for Refractive Motion</a><br /><i>Tianfan Xue, Hossein Mobahei, Frédo Durand, <span style="color: #3d85c6;">William T. Freeman</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Elgharib_Video_Magnification_in_2015_CVPR_paper.pdf">Video Magnification in Presence of Large Motions</a><br /><i>Mohamed Elgharib, Mohamed Hefeeda, Frédo Durand, <span style="color: #3d85c6;">William T. Freeman</span></i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wu_Robust_Video_Segment_2015_CVPR_paper.pdf">Robust Video Segment Proposals with Painless Occlusion Handling</a><br /><i>Zhengyang Wu, Fuxin Li, <span style="color: #3d85c6;">Rahul Sukthankar</span>, James M. Rehg</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Movshovitz-Attias_Ontological_Supervision_for_2015_CVPR_paper.pdf">Ontological Supervision for Fine Grained Classification of Street View Storefronts</a><br /><i>Yair Movshovitz-Attias, <span style="color: #3d85c6;">Qian Yu</span>, <span style="color: #3d85c6;">Martin C. Stumpe</span>, <span style="color: #3d85c6;">Vinay Shet</span>, <span style="color: #3d85c6;">Sacha Arnoud</span>, Liron Yatziv</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mathialagan_VIP_Finding_Important_2015_CVPR_paper.pdf">VIP: Finding Important People in Images</a><br /><i>Clint Solomon Mathialagan, <span style="color: #3d85c6;">Andrew C. Gallagher</span>, Dhruv Batra</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_Fusing_Subcategory_Probabilities_2015_CVPR_paper.pdf">Fusing Subcategory Probabilities for Texture Classification</a><br /><i><span style="color: #3d85c6;">Yang Song</span>, Weidong Cai, Qing Li, Fan Zhang</i><br /><br /><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf">Beyond Short Snippets: Deep Networks for Video Classification</a><br /><i>Joe Yue-Hei Ng, Matthew Hausknecht, <span style="background-color: white;"><span style="color: #3d85c6;">Sudheendra Vijayanarasimhan</span></span>, <span style="color: #3d85c6;">Oriol Vinyals</span>, <span style="color: #3d85c6;">Rajat Monga</span>, <span style="color: #3d85c6;">George Toderici</span></i><br /><br /><b><u>Workshops:</u></b><br /><a href="http://www.thumos.info/">THUMOS Challenge 2015</a><br /><i>Program organizers include: <span style="color: #3d85c6;">Alexander Gorban</span>, <span style="color: #3d85c6;">Rahul Sukthankar</span></i><br /><br /><a href="http://www.deep-vision.net/">DeepVision: Deep Learning in Computer Vision 2015</a><br /><i>Invited Speaker: <span style="color: #3d85c6;">Rahul Sukthankar</span></i><br /><br /><a href="http://lsviscom.org/">Large Scale Visual Commerce (LSVisCom)</a><br /><i>Panelist: <span style="color: #3d85c6;">Luc Vincent</span></i><br /><br /><a href="http://www.ntu.edu.sg/home/jsyuan/lsvsm15/index.html">Large-Scale Video Search and Mining (LSVSM)</a><br /><i>Invited Speaker and Panelist: <span style="color: #3d85c6;">Rahul Sukthankar</span></i><br /><i>Program Committee includes: <span style="color: #3d85c6;">Apostol Natsev</span></i><br /><br /><a href="http://www.visionmeetscognition.org/">Vision meets Cognition: Functionality, Physics, Intentionality and Causality</a><br /><i>Program Organizers include: <span style="color: #3d85c6;">Peter Battaglia</span></i><br /><br /><a href="https://sites.google.com/site/bigvision2015cvpr">Big Data Meets Computer Vision: 3rd International Workshop on Large Scale Visual Recognition and Retrieval (BigVision 2015)</a><br /><i>Program Organizers include: <span style="color: #3d85c6;">Samy Bengio</span></i><br /><i>Includes speaker <span style="color: #3d85c6;">Christian Szegedy</span> - “Scalable approaches for large scale vision”</i><br /><br /><a href="http://www.ics.uci.edu/~jsupanci/HANDS-2015/index.html">Observing and Understanding Hands in Action (Hands 2015)</a><br /><i>Program Committee includes: <span style="color: #3d85c6;">Murphy Stein</span></i><br /><br /><a href="http://www.fgvc.org/">Fine-Grained Visual Categorization (FGVC3)</a><br /><i>Program Organizers include: <span style="color: #3d85c6;">Anelia Angelova</span></i><br /><br /><a href="http://lsun.cs.princeton.edu/">Large-scale Scene Understanding Challenge (LSUN)</a><br /><i>Winners of the Scene Classification Challenge: <span style="color: #3d85c6;">Julian Ibarz</span>, <span style="color: #3d85c6;">Christian Szegedy</span> and <span style="color: #3d85c6;">Vincent Vanhoucke</span></i><br /><i>Winners of the Caption Generation Challenge: <span style="background-color: white;"><span style="color: #3d85c6;">Oriol Vinyals</span></span>, <span style="color: #3d85c6;">Alexander Toshev</span>, <span style="color: #3d85c6;">Samy Bengio</span>, and <span style="color: #3d85c6;">Dumitru Erhan</span></i><br /><br /><a href="http://www.grss-ieee.org/earthvision2015/">Looking from above: when Earth observation meets vision (EARTHVISION)</a><br /><i>Technical Committee includes: <span style="color: #3d85c6;">Andreas Wendel</span></i><br /><br /><a href="http://www.cvc.uab.es/adas/cvvt2015/">Computer Vision in Vehicle Technology: Assisted Driving, Exploration Rovers, Aerial and Underwater Vehicles</a><br /><i>Invited Speaker: <span style="color: #3d85c6;">Andreas Wendel</span></i><br /><i>Program Committee includes: <span style="color: #3d85c6;">Andreas Wendel</span></i><br /><br /><a href="https://sites.google.com/site/wicv2015/home">Women in Computer Vision (WiCV)</a><br /><i>Invited Speaker: <span style="color: #3d85c6;">Mei Han</span></i><br /><br /><a href="http://gesture.chalearn.org/">ChaLearn Looking at People</a> (<i>sponsor</i>)<br /><br /><a href="http://www.fgvc.org/">Fine-Grained Visual Categorization (FGVC3)</a> (<i>sponsor</i>)<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=2-stYzDcULM:wsFGjRGgHi4:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/2-stYzDcULM" height="1" width="1" alt=""/>";s:12:"link_replies";s:177:"http://googleresearch.blogspot.com/feeds/5236130123391242147/comments/defaulthttp://googleresearch.blogspot.com/2015/06/google-computer-vision-research-at-cvpr.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/5236130123391242147";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/5236130123391242147";s:4:"link";s:104:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2-stYzDcULM/google-computer-vision-research-at-cvpr.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:87:"http://googleresearch.blogspot.com/2015/06/google-computer-vision-research-at-cvpr.html";}}i:13;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-6997339361778593153";s:9:"published";s:29:"2015-06-05T09:00:00.000-07:00";s:7:"updated";s:29:"2015-06-12T06:48:01.994-07:00";s:5:"title";s:52:"Announcing the 2015 Google European Doctoral Fellows";s:12:"atom_content";s:3573:"<span class="byline-author">Posted by David Harper, University Relations and Beate List, Research Programs</span><br /><br />In 2009, Google created the <a href="http://research.google.com/university/student-support/">PhD Fellowship program</a> to recognize and support outstanding graduate students doing exceptional work in Computer Science and related disciplines. The following year, we launched the program in Europe as the <a href="http://research.google.com/university/relations/doctoral_fellowships_europe.html">Google European Doctoral Fellowship program</a>. Alumni of the European program occupy a variety of positions including faculty positions (<a href="http://www.ttic.edu/meshi.php">Ofer Meshi</a>, <a href="http://mmc.tudelft.nl/users/cynthia-liem">Cynthia Liem</a>), academic research positions (<a href="http://rangst.github.io/#/people/rangst">Roland Angst</a>, <a href="http://www-desir.lip6.fr/~doerr/">Carola Doerr</a> née Winzen) and positions in industry (<a href="https://www.linkedin.com/pub/yair-adato/36/6b2/493">Yair Adato</a>, <a href="https://www.linkedin.com/in/petrhosek">Peter Hosek</a>, <a href="https://www.linkedin.com/pub/neil-houlsby/75/523/711">Neil Houlsby</a>).<br /><br />Reflecting our continuing commitment to building strong relations with the European academic community, we are delighted to announce the 2015 Google European Doctoral Fellows. The following fifteen fellowship recipients were selected from an outstanding set of PhD students nominated by our partner universities:<br /><br /><ul><li><b>Heike Adel</b>, Fellowship in Natural Language Processing (University of Munich)</li><li><b>Thang Bui,</b> Fellowship in Speech Technology (University of Cambridge)</li><li><b>Victoria Caparrós Cabezas</b>, Fellowship in Distributed Systems (ETH Zurich)</li><li><b>Nadav Cohen</b>, Fellowship in Machine Learning (The Hebrew University of Jerusalem)</li><li><b>Josip Djolonga</b>, Fellowship in Probabilistic Inference (ETH Zurich)</li><li><b>Jakob Julian Engel</b>, Fellowship in Computer Vision (Technische Universität München)</li><li><b>Nikola Gvozdiev</b>, Fellowship in Computer Networking (University College London)</li><li><b>Felix Hill</b>, Fellowship in Language Understanding (University of Cambridge)</li><li><b>Durk Kingma</b>, Fellowship in Deep Learning (University of Amsterdam)</li><li><b>Massimo Nicosia</b>, Fellowship in Statistical Natural Language Processing (University of Trento)</li><li><b>George Prekas</b>, Fellowship in Operating Systems (École Polytechnique Fédérale de Lausanne)</li><li><b>Roman Prutkin</b>, Fellowship in Graph Algorithms (Karlsruhe Institute of Technology)</li><li><b>Siva Reddy</b>, Fellowship in Multilingual Semantic Parsing (The University of Edinburgh)</li><li><b>Immanuel Trummer</b>, Fellowship in Structured Data Analysis (École Polytechnique Fédérale de Lausanne)</li><li><b>Margarita Vald</b>, Fellowship in Security (Tel Aviv University)</li></ul><br />This group of students represent the next generation of researchers who will endeavor to solve some of the most interesting challenges in Computer Science. We offer our congratulations, and look forward to their future contributions to the research community with high expectation.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=MH5imyNjdzw:CotrEDgCmlg:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/MH5imyNjdzw" height="1" width="1" alt=""/>";s:12:"link_replies";s:169:"http://googleresearch.blogspot.com/feeds/6997339361778593153/comments/defaulthttp://googleresearch.blogspot.com/2015/06/announcing-2015-google-european.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/6997339361778593153";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/6997339361778593153";s:4:"link";s:96:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/MH5imyNjdzw/announcing-2015-google-european.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:79:"http://googleresearch.blogspot.com/2015/06/announcing-2015-google-european.html";}}i:14;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-7452772025437567806";s:9:"published";s:29:"2015-06-02T09:00:00.000-07:00";s:7:"updated";s:29:"2015-06-02T15:00:02.295-07:00";s:5:"title";s:73:"A Multilingual Corpus of Automatically Extracted Relations from Wikipedia";s:12:"atom_content";s:4418:"<span class="byline-author">Posted by Shankar Kumar, Google Research Scientist and Manaal Faruqui, Carnegie Mellon University PhD candidate</span><br /><br />In <a href="http://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a>, relation extraction is the task of assigning a semantic relationship between a pair of arguments. As an example, a relationship between the phrases “<i>Ottawa</i>” and “<i>Canada</i>” is “<i>is the capital of</i>”. These extracted relations could be used in a variety of applications ranging from <a href="http://en.wikipedia.org/wiki/Question_answering">Question Answering</a> to building databases from unstructured text.  <br /><br />While relation extraction systems work accurately for English and a few other languages, where tools for syntactic analysis such as parsers, part-of-speech taggers and named entity analyzers are readily available, there is relatively little work in developing such systems for most of the world's languages where linguistic analysis tools do not yet exist. Fortunately, because we do have translation systems between English and many other languages (such as <a href="https://translate.google.com/">Google Translate</a>), we can translate text from a non-English language to English, perform relation extraction and project these relations back to the foreign language. <br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-stV0NoTMvMU/VW4mH89d3-I/AAAAAAAAAj0/yPDYN7b3otE/s1600/Screen%2BShot%2B2015-06-02%2Bat%2B2.54.16%2BPM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="320" src="http://2.bp.blogspot.com/-stV0NoTMvMU/VW4mH89d3-I/AAAAAAAAAj0/yPDYN7b3otE/s640/Screen%2BShot%2B2015-06-02%2Bat%2B2.54.16%2BPM.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Relation extraction in a Spanish sentence using the cross-lingual relation extraction pipeline.</td></tr></tbody></table><div class="separator" style="clear: both; text-align: center;"></div>In <a href="http://research.google.com/pubs/pub43449.html">Multilingual Open Relation Extraction Using Cross-lingual Projection</a>, that will appear at the <a href="http://naacl.org/naacl-hlt-2015/">2015 Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies</a> (NAACL HLT 2015), we use this idea of cross-lingual projection to develop an algorithm that extracts open-domain relation <a href="http://en.wikipedia.org/wiki/Tuple">tuples</a>, i.e. where an arbitrary phrase can describe the relation between the arguments, in multiple languages from <a href="https://www.wikipedia.org/">Wikipedia</a>. In this work, we also evaluated the performance of extracted relations using human annotations in French, Hindi and Russian. <br /><br />Since there is no such publicly available corpus of multilingual relations, we are <a href="https://storage.cloud.google.com/wikipedia_multilingual_relations_v1/multilingual_relations_data/">releasing a dataset</a> of automatically extracted relations from the Wikipedia corpus in 61 languages, along with the manually annotated relations in 3 languages (French, Hindi and Russian). It is our hope that our data will help researchers working on natural language processing and encourage novel applications in a wide variety of languages. More details on the corpus and the file formats can be found in this <a href="https://storage.cloud.google.com/wikipedia_multilingual_relations_v1/multilingual_relations_data/README">README file</a>. <br /><br />We wish to thank Bruno Cartoni, Vitaly Nikolaev, Hidetoshi Shimokawa, Kishore Papineni, John Giannandrea and their teams for making this data release possible. This dataset is licensed by Google Inc. under the <a href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 License</a>.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=N_ygJ_BQRu8:Amo8PPTFhug:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/N_ygJ_BQRu8" height="1" width="1" alt=""/>";s:12:"link_replies";s:176:"http://googleresearch.blogspot.com/feeds/7452772025437567806/comments/defaulthttp://googleresearch.blogspot.com/2015/06/a-multilingual-corpus-of-automatically.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7452772025437567806";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7452772025437567806";s:4:"link";s:103:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/N_ygJ_BQRu8/a-multilingual-corpus-of-automatically.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:86:"http://googleresearch.blogspot.com/2015/06/a-multilingual-corpus-of-automatically.html";}}i:15;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-3325571989750709270";s:9:"published";s:29:"2015-05-22T11:00:00.000-07:00";s:7:"updated";s:29:"2015-05-22T11:00:19.973-07:00";s:5:"title";s:67:"Sergey and Larry awarded the Seoul Test-of-Time Award from WWW 2015";s:12:"atom_content";s:4100:"<span class="byline-author">Posted by Andrei Broder, Google Distinguished Scientist</span><br /><br />Today, at the <a href="http://www.www2015.it/">24th International World Wide Web Conference</a> (WWW) in Florence, Italy, our company founders, Sergey Brin and Larry Page, received the inaugural <a href="http://www.www2015.it/brin-and-page-win-the-first-seoul-test-of-time-award/">Seoul Test-of-Time Award</a> for their 1998 paper “<a href="http://www7.scu.edu.au/1921/com1921.htm">The Anatomy of a Large-Scale Hypertextual Web Search Engine</a>”, which introduced Google to the world at the <a href="http://www7.scu.edu.au/00/">7th WWW conference in Brisbane, Australia</a>. I had the pleasure and honor to accept the award on behalf of Larry and Sergey from <a href="http://islab.kaist.ac.kr/chungcw/">Professor Chin-Wan Chung</a>, who led the committee that created the award.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-g3UyoCr7K6A/VV9slsQm4bI/AAAAAAAAAi8/i7cWyh9aouU/s1600/IMG_3377.JPG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="480" src="http://2.bp.blogspot.com/-g3UyoCr7K6A/VV9slsQm4bI/AAAAAAAAAi8/i7cWyh9aouU/s640/IMG_3377.JPG" width="640" /></a></div>Except for the fact that I was myself in Brisbane, it is hard to believe that Google began just as  a two-student research project at Stanford University 17 years ago with the goal to “produce much more satisfying search results than existing systems.” Their paper presented two  breakthrough concepts: first, using a distributed system built on inexpensive commodity hardware to deal with the size of the index, and second, using the hyperlink structure of the Web as a powerful new relevance signal. By now these ideas are common wisdom, but their paper continues to be very influential: it has over <a href="https://goo.gl/KVrVUV">13,000 citations</a> so far and more are added every day. <br /><br />Since those beginnings Google has continued to grow, with tools that enable <a href="https://www.youtube.com/watch?v=LCzri4tXuF0">small business owners to reach customers</a>, <a href="https://www.youtube.com/watch?v=gHGDN9-oFJE">help long lost friends to reunite</a>, and <a href="https://www.youtube.com/watch?v=DVwHCGAr_OE">empower users to discover answers</a>. We keep pursuing new ideas and products, generating discoveries that both affect the world and advance the state-of-the-art in Computer Science and related disciplines. From products like <a href="http://gmailblog.blogspot.com/">Gmail</a>, <a href="http://www.google.com/maps/about/">Google Maps</a> and <a href="https://earthengine.google.org/#intro">Google Earth Engine</a> to advances in <a href="http://googleresearch.blogspot.com/2015/02/from-pixels-to-actions-human-level.html">Machine Intelligence</a>, <a href="http://googleresearch.blogspot.com/2014/11/a-picture-is-worth-thousand-coherent.html">Computer Vision</a>, and <a href="http://googleresearch.blogspot.com/2014/08/teaching-machines-to-read-between-lines.html">Natural Language Understanding</a>, it is our continuing goal to create useful tools and services that benefit our users.<br /><br />Larry and Sergey sent a video message to the conference expressing their thanks and their encouragement for future research, in which Sergey said “There is still a ton of work left to do in Search, and on the Web as a whole and I couldn’t think of a more exciting time to be working in this space.” I certainly share this view, and was very gratified by the number of young computer scientists from all over the world that came by the Google booth at the conference to share their thoughts about the future of search, and to explore the possibility of joining our efforts.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=BR5yDpxmyA4:-8AoohIjfy4:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/BR5yDpxmyA4" height="1" width="1" alt=""/>";s:12:"link_replies";s:176:"http://googleresearch.blogspot.com/feeds/3325571989750709270/comments/defaulthttp://googleresearch.blogspot.com/2015/05/sergey-and-larry-awarded-seoul-test-of.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3325571989750709270";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/3325571989750709270";s:4:"link";s:103:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/BR5yDpxmyA4/sergey-and-larry-awarded-seoul-test-of.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:86:"http://googleresearch.blogspot.com/2015/05/sergey-and-larry-awarded-seoul-test-of.html";}}i:16;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-2618647264326953993";s:9:"published";s:29:"2015-05-19T10:00:00.000-07:00";s:7:"updated";s:29:"2015-05-29T12:06:04.390-07:00";s:5:"title";s:69:"Tone: An experimental Chrome extension for instant sharing over audio";s:12:"atom_content";s:3914:"<span class="byline-author">Posted by Alex Kauffmann, Interaction Researcher, and Boris Smus, Software Engineer</span><br /><br />Sometimes in the course of exploring new ideas, we'll stumble upon a technology application that gets us excited. <a href="http://g.co/tone">Tone</a> is a perfect example: it's a <a href="http://www.google.com/chrome/">Chrome</a> extension that broadcasts the URL of the current tab to any machine within earshot that also has the extension installed. Tone is an experiment that we’ve enjoyed and found useful, and we think you may as well.<br /><br />As digital devices have multiplied, so has the complexity of coordinating them and moving stuff between them. Tone grew out of the idea that while digital communication methods like email and chat have made it infinitely easier, cheaper, and faster to share things with people across the globe, they've actually made it more complicated to share things with the people standing right next to you. Tone aims to make sharing digital things with nearby people as easy as talking to them.<br /><div class="separator" style="clear: both; text-align: center;"><iframe allowfullscreen="" class="YOUTUBE-iframe-video" data-thumbnail-src="https://i.ytimg.com/vi/_FT6u2JDGyY/0.jpg" frameborder="0" height="400" src="https://www.youtube.com/embed/_FT6u2JDGyY?feature=player_embedded" width="480"></iframe></div>The first version was built in an afternoon for fun (which resulted in numerous <a href="https://goo.gl/f0VH3V">rickrolls</a>), but we increasingly found ourselves using it to share documents with everyone in a meeting quickly, to exchange design files back and forth while collaborating on UI design, and to contribute relevant links without interrupting conversations.<br /><br />Tone provides an easy-to-understand broadcast mechanism that behaves like the human voice—it doesn't pass through walls like radio or require pairing or addressing. The initial prototype used an efficient audio transmission scheme that sounded terrible, so we played it beyond the range of human hearing. However, because many laptop microphones and nearly all video conferencing systems are optimized for voice, it improved reliability considerably to also include a minimal <a href="http://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling">DTMF</a>-based audible codec. The combination is reliable for short distances in the majority of audio environments even at low volumes, and it even works over Hangouts.<br /><br />Because it's audio based, Tone behaves like speech in interesting ways. The orientation of laptops relative to each other, the acoustic characteristics of the space, the particular speaker volume and mic sensitivity, and even where you're standing will all affect Tone's reliability. Not every nearby machine will always receive every broadcast, just like not everyone will always hear every word someone says. But resending is painless and debugging generally just requires raising the volume. Many groups at Google have found that the tradeoffs between ease and reliability worthwhile—it is our hope that small teams, students in classrooms, and families with multiple computers will too.<br /><br />To get started, first install the <a href="http://g.co/tone">Tone extension for Chrome</a>. Then simply open a tab with the URL you want to share, make sure your volume is on, and press the Tone button. Your machine will then emit a short sequence of beeps. Nearby machines receive a clickable notification that will open the same tab. Getting everyone on the same page has never been so easy!<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=Xkvhy56Jyo8:eZ_EjKRaPt0:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/Xkvhy56Jyo8" height="1" width="1" alt=""/>";s:12:"link_replies";s:176:"http://googleresearch.blogspot.com/feeds/2618647264326953993/comments/defaulthttp://googleresearch.blogspot.com/2015/05/tone-experimental-chrome-extension-for.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/2618647264326953993";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/2618647264326953993";s:4:"link";s:103:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/Xkvhy56Jyo8/tone-experimental-chrome-extension-for.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:86:"http://googleresearch.blogspot.com/2015/05/tone-experimental-chrome-extension-for.html";}}i:17;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-4262489306806200245";s:9:"published";s:29:"2015-05-06T10:00:00.000-07:00";s:7:"updated";s:29:"2015-05-06T10:17:16.349-07:00";s:5:"title";s:34:"Paper to Digital in 200+ languages";s:12:"atom_content";s:6144:"<span class="byline-author">Posted by Dmitriy Genzel and Ashok Popat, Research Scientists and Dhyanesh Narayanan, Product Manager</span><br /><br />Many of the world’s important sources of information - books, newspapers, magazines, pamphlets, and historical documents - are not digital. Unlike digital documents, these paper-based sources of information are difficult to search through or edit, or worse, completely inaccessible to some people. Part of the solution is <i>scanning</i>, getting a digital image of the page, but raw image pixels aren’t yet recognized as textual content from the computer’s point of view.  <br /><br /><a href="http://en.wikipedia.org/wiki/Optical_character_recognition">Optical Character Recognition</a> (OCR) technology aims to turn pictures of text into computer text that can be indexed, searched, and edited.  For some time, <a href="http://drive.google.com/">Google Drive</a> has provided OCR capabilities. Recently, we expanded this state-of-the-art technology to support all of the world’s major languages - that’s over <a href="https://support.google.com/drive/answer/176692">200 languages</a> in more than 25 writing systems. This technology is available to users in 2 easy steps: <br /><br />1. Upload a scanned document in its current form (say, as an image or PDF). The example below shows a scanned document in Hindi uploaded to a user’s Drive account as a PNG.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-EwxkZyqGBDo/VUo6Ls_TUUI/AAAAAAAAAhY/mt4Y7tDa1WE/s1600/image01.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-EwxkZyqGBDo/VUo6Ls_TUUI/AAAAAAAAAhY/mt4Y7tDa1WE/s1600/image01.png" height="281" width="400" /></a></div>2. Right-click on the document in the Drive interface, and select ‘Open with’ -&gt; ‘Google Docs’.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-o-XtjI67KPU/VUo6bWnS51I/AAAAAAAAAhg/nXEAxtfqdPU/s1600/image02.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-o-XtjI67KPU/VUo6bWnS51I/AAAAAAAAAhg/nXEAxtfqdPU/s1600/image02.png" height="152" width="400" /></a></div>This opens a Google document with the original image followed by the extracted text.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-qP_ywMgNVY8/VUo6kgcSKAI/AAAAAAAAAho/pxZbZcBOVWA/s1600/image00.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-qP_ywMgNVY8/VUo6kgcSKAI/AAAAAAAAAho/pxZbZcBOVWA/s1600/image00.png" height="271" width="400" /></a></div>You don’t even need to specify which language the document is in; the system will determine that automatically. Or, you can use the <a href="https://developers.google.com/drive/v2/reference/files/insert">Google Drive API</a> for more explicit control over the language detection in documents. For example, here is an invocation of the Drive API in Python:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-Z_c0ra6GKHE/VUpMR5-3ymI/AAAAAAAAAiY/s6xXGMC4-rY/s1600/Screen%2BShot%2B2015-05-06%2Bat%2B10.15.25%2BAM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-Z_c0ra6GKHE/VUpMR5-3ymI/AAAAAAAAAiY/s6xXGMC4-rY/s1600/Screen%2BShot%2B2015-05-06%2Bat%2B10.15.25%2BAM.png" height="105" width="400" /></a></div>The OCR capability in Drive is also available in the <a href="http://googledrive.blogspot.com/2013/05/a-smoother-drive-app-for-android.html">Drive App for Android</a>.<br /><br />To make this possible, engineering teams across Google pursued an approach to OCR focused on broad language coverage, with a goal of designing an architecture that could potentially work with all existing languages and writing systems. We do this in part by using <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Models</a> (HMMs) to make sense of the input as a whole sequence, rather than first trying to break it apart into pieces. This is similar to how modern <a href="https://www.youtube.com/watch?v=yxxRAHVtafI">speech recognition systems</a> recognize audio input.<br /><br />OCR and speech recognition share some challenges - like dealing with background “noise,” different languages, and low-quality inputs. But some challenges are specific to OCR: the variety of typefaces, the different types of scanners and cameras, and the need to work on older material that may contain archaic <a href="http://en.wikipedia.org/wiki/Orthography">orthographic</a> and linguistic elements. In addition to utilizing HMMs, we leveraged many of the same technologies used in the <a href="http://googleresearch.blogspot.com/2015/04/google-handwriting-input-in-82.html">Google Handwriting Input</a> app to allow automatic learning of features and to give preference to more likely output, as well as <a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35496.pdf">minimum-error-rate training</a> to allow effective combination of multiple sources of information, and modern methods in machine learning to minimize manual design and maximize use of data.  We also take advantage of advances in internationalization and typesetting, by using synthetic data in our training.<br /><br />Currently, the OCR works best on cleanly scanned, high-resolution documents in the most commonly used typefaces. We are working to improve performance on poor quality scans and challenging text layouts. Give it a try and <a href="https://productforums.google.com/forum/#!categories/drive">let us know</a> how it works for you.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=fTp8tM9QR8E:Q05TW6Jd5Ac:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fTp8tM9QR8E" height="1" width="1" alt=""/>";s:12:"link_replies";s:171:"http://googleresearch.blogspot.com/feeds/4262489306806200245/comments/defaulthttp://googleresearch.blogspot.com/2015/05/paper-to-digital-in-200-languages.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/4262489306806200245";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/4262489306806200245";s:4:"link";s:98:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fTp8tM9QR8E/paper-to-digital-in-200-languages.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:81:"http://googleresearch.blogspot.com/2015/05/paper-to-digital-in-200-languages.html";}}i:18;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-5643581224638765229";s:9:"published";s:29:"2015-04-15T10:00:00.000-07:00";s:7:"updated";s:29:"2015-04-15T12:00:32.588-07:00";s:5:"title";s:70:"Google Handwriting Input in 82 languages on your Android mobile device";s:12:"atom_content";s:6473:"<span class="byline-author">Posted by Thomas Deselaers, Daniel Keysers, Henry Rowley, Li-Lun Wang, Victor Cărbune, Ashok Popat, Dhyanesh Narayanan, Handwriting Team, Google Research</span><br /><br />Entering text on mobile devices is still considered inconvenient by many; touchscreen keyboards, although much improved over the years, require a lot of attention to hit the right buttons. Voice input is an option, but there are situations where it is not feasible, such as in a noisy environment or during a meeting. Using handwriting as an input method can allow for natural and intuitive input method for text entry which complements typing and speech input methods. However, until recently there have been many languages where enabling this functionality presented significant challenges. <br /><br />Today we launched <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.handwriting.ime">Google Handwriting Input</a>, which lets users handwrite text on their Android mobile device as an additional input method for <i>any</i> Android app. Google Handwriting Input supports 82 languages in 20 distinct scripts, and works with both printed and cursive writing input with or without a stylus. Beyond text input, it also provides a fun way to enter hundreds of emojis by drawing them (simply press and hold the ‘enter’ button to switch modes). Google Handwriting Input works with or without an Internet connection.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-RGb2PaehAmg/VS6S3oePTTI/AAAAAAAAAg0/oSgR-Vlm9SI/s1600/Screen%2BShot%2B2015-04-15%2Bat%2B9.29.42%2BAM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-RGb2PaehAmg/VS6S3oePTTI/AAAAAAAAAg0/oSgR-Vlm9SI/s1600/Screen%2BShot%2B2015-04-15%2Bat%2B9.29.42%2BAM.png" height="336" width="400" /></a></div>By building on <a href="http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html">large-scale language modeling</a>, <a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/37260.pdf">robust multi-language OCR</a>, and incorporating <a href="http://research.google.com/archive/large_deep_networks_nips2012.html">large-scale neural-networks</a> and <a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41694.pdf">approximate nearest neighbor search</a> for character classification, Google Handwriting Input supports languages that can be challenging to type on a virtual keyboard. For example, keyboards for ideographic languages (such as Chinese) are often based on a particular dialect of the language, but if a user does not know that dialect, they may be hard to use. Additionally, keyboards for complex script languages (like many South Asian languages) are less standardized and may be unfamiliar. Even for languages where virtual keyboards are more widely used (like English or Spanish), some users find that handwriting is more intuitive, faster, and generally more comfortable.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-PyVO0KMgePo/VS6S_sUcz0I/AAAAAAAAAg8/n0fuPSZ18T0/s1600/image01.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://4.bp.blogspot.com/-PyVO0KMgePo/VS6S_sUcz0I/AAAAAAAAAg8/n0fuPSZ18T0/s1600/image01.png" height="152" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Writing 'Hello' in Chinese, German, and Tamil.</td></tr></tbody></table>Google Handwriting Input is the result of many years of research at Google. Initially, cloud based handwriting recognition supported the <a href="http://googletranslate.blogspot.com/2012/01/sometimes-its-easier-just-to-write-it.html">Translate Apps</a> on <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.translate">Android</a> and <a href="https://itunes.apple.com/us/app/google-translate/id414706506">iOS</a>, <a href="http://www.google.com/insidesearch/features/search/handwritinginput/">Mobile Search</a>, and <a href="http://www.google.com/inputtools/try/">Google Input Tools</a> (in <a href="https://chrome.google.com/webstore/detail/google-input-tools/mclkkofklkfljcocdinagocijmpgbhab">Chrome</a>, <a href="https://support.google.com/chromebook/answer/6076237?hl=en">ChromeOS</a>, <a href="http://gmailblog.blogspot.com/2013/10/handwriting-input-comes-to-gmail-and.html">Gmail and Docs</a>, <a href="http://googletranslate.blogspot.com/2013/07/sometimes-its-easiest-to-just-write-it.html">translate.google.com</a>, and the <a href="https://support.google.com/docs/answer/3371015?hl=en">Docs symbol picker</a>). However, other products required recognizers to run directly on an Android device without an Internet connection. So we worked to make recognition models smaller and faster for use in Android handwriting input methods for <a href="https://play.google.com/store/apps/details?id=com.google.android.inputmethod.pinyin">Simplified</a> and <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.inputmethod.zhuyin">Traditional</a> Chinese, <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.inputmethod.cantonese">Cantonese</a>, and <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.inputmethod.hindi">Hindi</a>, as well as multi-language support in <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.gesturesearch">Gesture Search</a>. Google Handwriting Input combines these efforts, allowing recognition both on-device and in the cloud (by tapping on the cloud icon) in any Android app.<br /><br />You can install Google Handwriting Input from the Play Store <a href="https://play.google.com/store/apps/details?id=com.google.android.apps.handwriting.ime">here</a>. More information and FAQs can be found <a href="https://support.google.com/faqs/faq/6188721">here</a>.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=nnbZv7u52OE:5i47L6JDzCs:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/nnbZv7u52OE" height="1" width="1" alt=""/>";s:12:"link_replies";s:168:"http://googleresearch.blogspot.com/feeds/5643581224638765229/comments/defaulthttp://googleresearch.blogspot.com/2015/04/google-handwriting-input-in-82.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/5643581224638765229";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/5643581224638765229";s:4:"link";s:95:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/nnbZv7u52OE/google-handwriting-input-in-82.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:78:"http://googleresearch.blogspot.com/2015/04/google-handwriting-input-in-82.html";}}i:19;a:14:{s:2:"id";s:58:"tag:blogger.com,1999:blog-21224994.post-277677070840664188";s:9:"published";s:29:"2015-04-08T10:00:00.000-07:00";s:7:"updated";s:29:"2015-04-08T10:00:04.084-07:00";s:5:"title";s:61:"Beyond Short Snippets: Deep Networks for Video Classification";s:12:"atom_content";s:9043:"<span class="byline-author">Posted by Software Engineers George Toderici and Sudheendra Vijayanarasimhan</span><br /><br /><a href="http://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a> (CNNs) have recently shown rapid progress in advancing the state of the art of <a href="http://googleresearch.blogspot.com/2014/09/building-deeper-understanding-of-images.html">detecting and classifying objects in static images</a>, <a href="http://googleresearch.blogspot.com/2014/11/a-picture-is-worth-thousand-coherent.html">automatically learning complex features in pictures</a> without the need for manually annotated features. But what if one wanted not only to identify objects in static images, but also analyze what a video is about? After all, a video isn’t much more than a string of static images linked together in time. <br /><br />As it turns out, video analysis provides even more information to the object detection and recognition task performed by CNN’s by adding a temporal component through which motion and other information can be also be used to improve classification. However, analyzing entire videos is challenging from a modeling perspective because one must model variable length videos with a fixed number of parameters. Not to mention that modeling variable length videos is computationally very intensive.<br /><br />In <a href="http://arxiv.org/abs/1503.08909">Beyond Short Snippets: Deep Networks for Video Classification</a>, to be presented at the <a href="http://www.pamitc.org/cvpr15/">2015 Computer Vision and Pattern Recognition</a>&nbsp;conference (CVPR 2015), we<a href="#1" name="top1"><sup>1</sup></a> evaluated two approaches - <a href="http://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer">feature pooling</a> networks and <a href="http://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> (RNNs) - capable of modeling variable length videos with a fixed number of parameters while maintaining a low computational footprint. In doing so, we were able to not only show that learning a high level global description of the video’s temporal evolution is very important for accurate video classification, but that our <a href="http://arxiv.org/abs/1503.08909">best networks</a> exhibited significant performance improvements over <a href="http://research.google.com/pubs/pub42455.html">previously published</a> results on the <a href="https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md">Sports 1 million</a> dataset (Sports-1M).<br /><br />In <a href="https://plus.sandbox.google.com/+ResearchatGoogle/posts/eqSPSviY2CH">previous work</a>, we employed 3D-convolutions (meaning convolutions over time and space) over short video clips - typically just a few seconds - to learn motion features from raw frames implicitly and then aggregate predictions at the video level. For purposes of video classification, the low level motion features were only marginally outperforming models in which no motion was modeled.<br /><br />To understand why, consider the following two images which are very similar visually but obtain drastically different scores from a CNN model trained on static images:<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-wKWEVfII7aU/VSVPkARXPNI/AAAAAAAAAgM/ELOzvagitW0/s1600/image03.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://1.bp.blogspot.com/-wKWEVfII7aU/VSVPkARXPNI/AAAAAAAAAgM/ELOzvagitW0/s1600/image03.png" height="258" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Slight differences in object poses/context can change the predicted class/confidence of CNNs trained on static images.</td></tr></tbody></table>Since each individual video frame forms only a small part of the video’s story, static frames and short video snippets (2-3 secs) use incomplete information and could easily confuse subtle fine-grained distinctions between classes (e.g: Tae Kwon Do vs. Systema) or use portions of the video irrelevant to the action of interest.<br /><br />To get around this frame-by-frame confusion, we used feature pooling networks that independently process each frame and then pool/aggregate the frame-level features over the entire video at various stages. Another approach we took was to utilize an RNN (derived from <a href="http://en.wikipedia.org/wiki/Long_short_term_memory">Long Short Term Memory units</a>) instead of feature pooling, allowing the network itself to decide which parts of the video are important for classification. By sharing parameters through time, both feature pooling and RNN architectures are able to maintain a constant number of parameters while capturing a global description of the video’s temporal evolution. <br /><br />In order to feed the two aggregation approaches, we compute an image “pixel-based” CNN model, based on the raw pixels in the frames of a video. We processed videos for the “pixel-based” CNNs at one frame per second to reduce computational complexity. Of course, at this frame rate implicit motion information is lost. <br /><br />To compensate, we incorporate explicit motion information in the form of <a href="http://en.wikipedia.org/wiki/Optical_flow">optical flow</a> - the apparent motion of objects across a camera's viewfinder due to the motion of the objects or the motion of the camera. We compute optical flow images over adjacent frames to learn an additional “optical flow” CNN model. <br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-jk7MKpOUry8/VSVQUNN9QPI/AAAAAAAAAgc/-x2ASmzHSvA/s1600/Screen%2BShot%2B2015-04-08%2Bat%2B8.58.22%2BAM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-jk7MKpOUry8/VSVQUNN9QPI/AAAAAAAAAgc/-x2ASmzHSvA/s1600/Screen%2BShot%2B2015-04-08%2Bat%2B8.58.22%2BAM.png" height="178" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Left: Image used for the pixel-based CNN; Right: Dense optical flow image used for optical flow CNN</td></tr></tbody></table>The pixel-based and optical flow based CNN model outputs are provided as inputs to both the RNN and pooling approaches described earlier. These two approaches then separately aggregate the frame-level predictions from each CNN model input, and average the results. This allows our video-level prediction to take advantage of both image information and motion information to accurately label videos of similar activities even when the visual content of those videos varies greatly.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-pl6lvFO9nKk/VSVP0cM4OvI/AAAAAAAAAgU/FSWRgJzywqg/s1600/image01.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-pl6lvFO9nKk/VSVP0cM4OvI/AAAAAAAAAgU/FSWRgJzywqg/s1600/image01.png" height="478" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Badminton (top 25 videos according to the max-pooling model). Our methods accurately label all 25 videos as badminton despite the variety of scenes in the various videos because they use the entire video’s context for prediction.</td></tr></tbody></table>We conclude by observing that although very different in concept, the max-pooling and the recurrent neural network methods perform similarly when using both images and optical flow. Currently, these two architectures are the top performers on the Sports-1M dataset. The main difference between the two was that the RNN approach was more robust when using optical flow alone on this dataset. Check out a <a href="http://youtu.be/oDRl3-X1KkI">short video showing some example outputs</a> from the deep convolutional networks presented in our paper.<br /><hr width="80%"><p><span class="Apple-style-span" style="font-size: x-small;"><br /><a name="1"><b>1 </b></a>Research carried out in collaboration with University of Maryland, College Park PhD student Joe Yue-Hei Ng and University of Texas at Austin PhD student Matthew Hausknecht, as part of a Google Software Engineering Internship<a href="#top1"><sup>↩</sup></a><br /></span><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=Hg3i0N8_Klw:y4uj_xuxCno:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/Hg3i0N8_Klw" height="1" width="1" alt=""/>";s:12:"link_replies";s:176:"http://googleresearch.blogspot.com/feeds/277677070840664188/comments/defaulthttp://googleresearch.blogspot.com/2015/04/beyond-short-snippets-deep-networks-for.html#comment-form";s:9:"link_edit";s:70:"http://www.blogger.com/feeds/21224994/posts/default/277677070840664188";s:9:"link_self";s:70:"http://www.blogger.com/feeds/21224994/posts/default/277677070840664188";s:4:"link";s:104:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/Hg3i0N8_Klw/beyond-short-snippets-deep-networks-for.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:87:"http://googleresearch.blogspot.com/2015/04/beyond-short-snippets-deep-networks-for.html";}}i:20;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-7399992755811533594";s:9:"published";s:29:"2015-04-06T12:31:00.000-07:00";s:7:"updated";s:29:"2015-04-06T12:31:51.592-07:00";s:5:"title";s:65:"Skill maps, analytics and more with Google’s Course Builder 1.8";s:12:"atom_content";s:6357:"<span class="byline-author">Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team</span><br /><br />Over the past couple of years, Google’s <a href="https://www.google.com/edu/openonline/">Course Builder</a> has been used to create and deliver hundreds of online courses on a variety of subjects (from <a href="https://ourenergyfuture-ucsd.appspot.com/003">sustainable energy</a> to <a href="https://making-comics.appspot.com/course">comic books</a>), making learning more scalable and accessible through open source technology. With the help of Course Builder, over a million students of all ages have learned something new.<br /><br />Today, we’re increasing our commitment to Course Builder by bringing rich, new functionality to the platform with a new release.  Of course, we will also continue to work with edX and others to contribute to the entire ecosystem.<br /><br />This <a href="https://code.google.com/p/course-builder/wiki/ReleaseNotes">new version</a> enables instructors and students to understand prerequisites and skills explicitly, introduces several improvements to the instructor experience, and even allows you to export data to Google BigQuery for in depth analysis.<br /><ul><li><b>Drag and drop, simplified tabs, and student feedback</b></li></ul><div style="margin-left: 2.5em;">We’ve made major enhancements to the instructor interface, such as simplifying the tabs and clarifying which part of the page you’re editing, so you can spend more time teaching and less time configuring.  You can also structure your course on the fly by dragging and dropping elements directly in the outline.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-8JtEtohZVTg/VSLDqLwwmpI/AAAAAAAAAfo/ZOPFbFbcpsI/s1600/image00.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-8JtEtohZVTg/VSLDqLwwmpI/AAAAAAAAAfo/ZOPFbFbcpsI/s1600/image00.png" height="206" width="640" /></a></div><br />Additionally, we’ve added the option to include a feedback box at the bottom of each lesson, making it easy for your students to tell you their thoughts (though we can't promise you'll always enjoy reading them).</div><ul><li><b>Skill Mapping</b></li></ul><div style="margin-left: 2.5em;">You can now define prerequisites and skills learned for each lesson.  For instance, in a course about arithmetic, addition might be a prerequisite for the lesson on multiplying numbers, while multiplication is a skill learned.  Once an instructor has defined the skill relationships, they will have a consolidated view of all their skills and the lessons they appear in, such as this list for <a href="http://www.powersearchingwithgoogle.com/">Power Searching with Google</a>:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-qdnEY0QSxvA/VSLED6AIFPI/AAAAAAAAAfw/fCbdajM2TO0/s1600/image01.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-qdnEY0QSxvA/VSLED6AIFPI/AAAAAAAAAfw/fCbdajM2TO0/s1600/image01.png" height="284" width="640" /></a></div>Instructors can then enable a skills widget that shows at the top of each lesson and which lets students see exactly what they should know before and after completing a lesson.  Below are the prerequisites and goals for the <i>Thinking More Deeply About Your Search</i> lesson.  A student can easily see what they should know beforehand and which lessons to explore next to learn more.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-mVdTRH-Rak0/VSLES52k80I/AAAAAAAAAf4/exVZy57jqp4/s1600/image02.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-mVdTRH-Rak0/VSLES52k80I/AAAAAAAAAf4/exVZy57jqp4/s1600/image02.png" height="248" width="640" /></a></div>Skill maps help a student better understand which content is right for them.  And, they lay the groundwork for our future forays into adaptive and personalized learning.  Learn more about Course Builder skill maps in this <a href="https://www.youtube.com/watch?v=Dl9tMMLffhw">video</a>.</div><ul><li><b>Analytics through BigQuery</b></li></ul><div style="margin-left: 2.5em;">One of the core tenets of Course Builder is that quality online learning requires a feedback loop between instructor and student, which is why we’ve always had a focus on providing rich analytical information about a course.  But no matter how complete, sometimes the built-in reports just aren’t enough.  So Course Builder now includes a pipeline to <a href="https://cloud.google.com/bigquery/">Google BigQuery</a>, allowing course owners to issue super-fast queries in a SQL-like syntax using the processing power of Google’s infrastructure. This allows you to slice and dice the data in an infinite number of ways, giving you just the information you need to help your students and optimize your course.  Watch these videos on <a href="https://www.youtube.com/watch?v=2ticBJcZGZ8">configuring</a> and <a href="https://www.youtube.com/watch?v=cz80K9DPtxg">sending data</a>.</div><br />To get started with your own course, follow these simple <a href="https://www.google.com/edu/openonline/tech/cb/index.html">instructions</a>.  Please <a href="https://groups.google.com/forum/?fromgroups#!forum/course-builder-forum">let us know</a> how you use these new features and what you’d like to see in Course Builder next.  Need some inspiration?  Check out our <a href="https://code.google.com/p/course-builder/wiki/ListOfCourses">list of courses</a> (and <a href="https://docs.google.com/a/google.com/forms/viewform?id=s3UFJsP0JeqY2t4YIIm40gA&amp;sid=2ab0c1ab6da6db04&amp;token=OW4tvjoBAAA.iA2VQyyRWufn4YyMKREosg.vrs_oH-FCjoNn9YxmglGmg">tell us</a> when you launch yours).<br /><br />Keep on learning!<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=07IHIV9e-P8:COIcUMD15Z4:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/07IHIV9e-P8" height="1" width="1" alt=""/>";s:12:"link_replies";s:172:"http://googleresearch.blogspot.com/feeds/7399992755811533594/comments/defaulthttp://googleresearch.blogspot.com/2015/04/skill-maps-analytics-and-more-with.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7399992755811533594";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7399992755811533594";s:4:"link";s:99:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/07IHIV9e-P8/skill-maps-analytics-and-more-with.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:82:"http://googleresearch.blogspot.com/2015/04/skill-maps-analytics-and-more-with.html";}}i:21;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-4761024791768788209";s:9:"published";s:29:"2015-03-16T10:00:00.000-07:00";s:7:"updated";s:29:"2015-03-25T22:01:08.723-07:00";s:5:"title";s:39:"Google Computer Science Capacity Awards";s:12:"atom_content";s:4677:"<span class="byline-author">By Maggie Johnson, Director of Education and University Relations and Chris Busselle, Google.org</span><br /><br />One of Google's goals is to surface successful strategies that support the expansion of high-quality Computer Science (CS) programs at the undergraduate level. Innovations in teaching and technologies, while additionally ensuring better engagement of women and underrepresented minority students, is necessary in creating inclusive, sustainable, and scalable educational programs. <br /><br />To address issues arising from the <a href="http://www.cra.org/uploads/documents/resources/taulbee/CRA_Taulbee_CS_Degrees_and_Enrollment_2012-13.pdf">dramatic increase in undergraduate CS enrollments</a>, we recently launched the Computer Science Capacity Awards program. For this three-year program, select educational institutions were invited to contribute proposals for innovative, inclusive, and sustainable approaches to address current scaling issues in university CS educational programs.<br /><br />Today, after an extensive proposal review process, we are pleased to announce the recipients of the Capacity Awards program:<br /><br /><b>Carnegie Mellon University - Professor Jacobo Carrasquel</b><br /><b><i>Alternate Instructional Model for Introductory Computer Science Classes</i></b><br />CMU will develop a new instructional model consisting of two optional mini lectures per week given by the instructor, and problem-solving  sessions with flexible group meetings that are coordinated by undergraduate and graduate teaching assistants.<br /><br /><b>Duke University - Professor Jeffrey Forbes</b><br /><b>North Carolina State University - Professor Kristy Boyer </b><br /><b>University of North Carolina - Professor Ketan Mayer-Patel</b><br /><b><i>RESEARCH TRIANGLE PEER TEACHING FELLOWS: Scalable Evidence-Based Peer Teaching for Improving CS Capacity and Diversity</i></b><br />The project hopes to increase CS retention and diversity by developing a highly scalable, effective, evidence-based peer training program across three universities in the North Carolina Research Triangle.<br /><br /><b>Mount Holyoke College - Professor Heather Pon-Barry</b><br /><b><i>MaGE (Megas and Gigas Educate): Growing Computer Science Capacity at Mount Holyoke College</i></b><br />Mount Holyoke’s <a href="https://www.mtholyoke.edu/media/google-funds-new-computer-science-initiative">MaGE program</a> includes a plan to grow enrollment in introductory CS courses, particularly for women and other underrepresented groups. The program also includes a plan of action for CS students to educate, mentor, and support others in inclusive ways.<br /><br /><b>George Mason University - Professor Jeff Offutt</b><br /><b><i>SPARC: Self-PAced Learning increases Retention and Capacity</i></b><br />George Mason University wants to replace the traditional course model for CS-1 and CS-2 with an innovative teaching model of self- paced introductory programming courses. Students will periodically demonstrate competency with practical skills demonstrations similar to  those used in martial arts.<br /><br /><b>Rutgers University - Professor Andrew Tjang</b><br /><b><i>Increasing the Scalability and Diversity in the Face of Large Growth in Computer Science Enrollment</i></b><br />Rutger’s program addresses scalability issues with technology tools, as well as collaborative spaces. It also emphasizes outreach to Rutgers’ women’s college and includes original research on success in CS programs to create new courses that cater to the changing environment.<br /><br /><b>University of California, Berkeley -  Professor John DeNero</b><br /><b><i>Scaling Computer Science through Targeted Engagement</i></b><br />Berkeley’s program plans to increase Software Engineering and UI Design enrollment by 500 total students/year, as well as increase the number of women and underrepresented minority CS majors by a factor of three.<br /><br />Each of the selected schools brings a unique and innovative approach to addressing current scaling issues, and we are excited to collaborate in developing concrete strategies to develop sustainable and inclusive educational programs. Stay tuned over the coming year, where we will report on program recipients' progress and share results with the broader CS education community.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=rSzVp3VnXIY:5YQWKQ1NAkA:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/rSzVp3VnXIY" height="1" width="1" alt=""/>";s:12:"link_replies";s:177:"http://googleresearch.blogspot.com/feeds/4761024791768788209/comments/defaulthttp://googleresearch.blogspot.com/2015/03/google-computer-science-capacity-awards.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/4761024791768788209";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/4761024791768788209";s:4:"link";s:104:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/rSzVp3VnXIY/google-computer-science-capacity-awards.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:87:"http://googleresearch.blogspot.com/2015/03/google-computer-science-capacity-awards.html";}}i:22;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-1911138756127114617";s:9:"published";s:29:"2015-03-09T09:00:00.000-07:00";s:7:"updated";s:29:"2015-03-09T09:27:39.549-07:00";s:5:"title";s:50:"Announcing the Google MOOC Focused Research Awards";s:12:"atom_content";s:3184:"<span class="byline-author">Posted by Maggie Johnson, Director of Education and University Relations, and Aimin Zhu, University Relations Manager, APAC</span><br /><br />Last year, Google and <a href="http://www.tsinghua.edu.cn/publish/newthu/index.html">Tsinghua University</a> hosted the <a href="https://sites.google.com/site/2014apacmooc/home">2014 APAC MOOC Focused Faculty Workshop</a>, an event designed to share, brainstorm and generate ideas aimed at fostering MOOC innovation. As a result of the <a href="http://googleresearch.blogspot.com/2014/12/mooc-research-and-innovation.html">ideas generated at the workshop</a>, we solicited proposals from the attendees for research collaborations that would advance important topics in MOOC development.<br /><br />After expert reviews and committee discussions, we are pleased to announce the following recipients of the MOOC <a href="http://research.google.com/university/relations/focused_research_awards.html">Focused Research Awards</a>. These awards cover research exploring new interactions to enhance learning experience, personalized learning, online community building, interoperability of online learning platforms and education accessibility:<br /><br /><ul><li>“MOOC Visual Analytics” - Michael Ginda, Indiana University, United States</li><li>“Improvement of students’ interaction in MOOCs using participative networks” - Pedro A. Pernías Peco, Universidad de Alicante, Spain</li><li>“Automated Analysis of MOOC Discussion Content to Support Personalised Learning” - Katrina Falkner, The University of Adelaide, Australia</li><li>“Extending the Offline Capability of Spoken Tutorial Methodology” - Kannan Moudgalya, Indian Institute of Technology Bombay, India</li><li>“Launching the Pan Pacific ISTP (Information Science and Technology Program) through MOOCs” - Yasushi Kodama, Hosei University, Japan</li><li>“Fostering Engagement and Social Learning with Incentive Schemes and Gamification Elements in MOOCs” - Thomas Schildhauer, Alexander von Humboldt Institute for Internet and Society, Germany</li><li>“Reusability Measurement and Social Community Analysis from MOOC Content Users” - Timothy K. Shih, National Central University, Taiwan</li></ul><br />In order to further support these projects and foster collaboration, we have begun pairing the award recipients with Googlers pursuing online education research as well as product development teams.<br /><br />Google is committed to supporting innovation in <a href="http://www.google.com/edu/openonline/index.html">online learning at scale</a>, and we congratulate the recipients of the MOOC Focused Research Awards. It is our belief that these collaborations will further develop the potential of online education, and we are very pleased to work with these researchers to jointly push the frontier of MOOCs.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=4H64kUjG9zY:lEt7OU-Z7CA:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/4H64kUjG9zY" height="1" width="1" alt=""/>";s:12:"link_replies";s:177:"http://googleresearch.blogspot.com/feeds/1911138756127114617/comments/defaulthttp://googleresearch.blogspot.com/2015/03/announcing-google-mooc-focused-research.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/1911138756127114617";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/1911138756127114617";s:4:"link";s:104:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/4H64kUjG9zY/announcing-google-mooc-focused-research.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:87:"http://googleresearch.blogspot.com/2015/03/announcing-google-mooc-focused-research.html";}}i:23;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-7486685858534330957";s:9:"published";s:29:"2015-03-04T10:00:00.000-08:00";s:7:"updated";s:29:"2015-03-04T10:09:13.275-08:00";s:5:"title";s:66:"A step closer to quantum computation with Quantum Error Correction";s:12:"atom_content";s:5307:"<span class="byline-author">Posted by Julian Kelly, Rami Barends, and Austin Fowler, Quantum Electronics Engineers</span><br /><br />Computer scientists have dreamt of large-scale <a href="http://en.wikipedia.org/wiki/Quantum_computing">quantum computation</a> since at least <a href="http://en.wikipedia.org/wiki/Shor%27s_algorithm">1994</a> -- the hope is that quantum computers will be able to process certain calculations much more quickly than any classical computer, helping to solve problems ranging from complicated physics or chemistry simulations to solving optimization problems to accelerating machine learning tasks.<br /><br />One of the primary challenges is that quantum memory elements (“<a href="http://en.wikipedia.org/wiki/Qubit">qubits</a>”) have always been too prone to errors. They’re fragile and easily disturbed -- any fluctuation or noise from their environment can introduce memory errors, rendering the computations useless. As it turns out, getting even just a small number of qubits together to repeatedly perform the required quantum logic operations and still be nearly error-free is just plain <i>hard</i>. But our team has been developing the quantum logic operations and qubit architectures to do just that. <br /><br />In our paper “<a href="http://nature.com/articles/doi:10.1038/nature14270">State preservation by repetitive error detection in a superconducting quantum circuit</a>”, published in the journal <a href="http://www.nature.com/"><i>Nature</i></a>, we describe a superconducting quantum circuit with nine qubits where, for the first time, the qubits are able to detect and effectively protect each other from bit errors. This <a href="http://en.wikipedia.org/wiki/Quantum_error_correction">quantum error correction</a> (QEC) can overcome memory errors by applying a carefully choreographed series of logic operations on the qubits to detect where errors have occurred.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-3wAVMhQqQkE/VPc732VajzI/AAAAAAAAAew/lgXOjDu4maU/s1600/image01.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://4.bp.blogspot.com/-3wAVMhQqQkE/VPc732VajzI/AAAAAAAAAew/lgXOjDu4maU/s1600/image01.png" height="317" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Photograph of the device containing nine quantum bits (qubits). Each qubit interacts with its neighbors to protect them from error.</td></tr></tbody></table><br />So how does QEC work? In a classical computer, we can monitor bits directly to detect errors. However, qubits are much more fickle -- measuring a qubit directly will collapse <a href="http://en.wikipedia.org/wiki/Quantum_entanglement">entanglement</a> and <a href="http://en.wikipedia.org/wiki/Quantum_superposition">superposition</a> states, removing the quantum elements that make it useful for computation.<br /><br />To get around this, we introduce additional ‘measurement’ qubits, and perform a series of quantum logic operations that look at the 'measurement' and 'data' qubits in combination. By looking at the state of these pairwise combinations (using quantum <a href="http://en.wikipedia.org/wiki/XOR_gate">XOR gates</a>), and performing some careful cross-checking, we can pull out just enough information to detect errors without altering the information in any individual qubit.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-1ELK_8tl17k/VPc8JjN4EQI/AAAAAAAAAe4/BLFH_vGprmI/s1600/image00.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://4.bp.blogspot.com/-1ELK_8tl17k/VPc8JjN4EQI/AAAAAAAAAe4/BLFH_vGprmI/s1600/image00.png" height="181" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The basics of error correction. ‘Measurement’ qubits can detect errors on ‘data’ qubits through the use of quantum XOR gates.</td></tr></tbody></table><br />We’ve also shown that storing information in five qubits works better than just storing it in one, and that with nine qubits the error correction works even better. That’s a key result -- it shows that the quantum logic operations are trustworthy enough that by adding more qubits, we can detect more complex errors that otherwise may cause algorithmic failure.<br /><br />While the basic physical processes behind quantum error correction are feasible, many challenges remain, such as improving the logic operations behind error correction and testing protection from phase-flip errors. We’re excited to tackle these challenges on the way towards making real computations possible.<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=HWIMDIXcIZo:ugLKot0WxRE:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/HWIMDIXcIZo" height="1" width="1" alt=""/>";s:12:"link_replies";s:174:"http://googleresearch.blogspot.com/feeds/7486685858534330957/comments/defaulthttp://googleresearch.blogspot.com/2015/03/a-step-closer-to-quantum-computation.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7486685858534330957";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7486685858534330957";s:4:"link";s:101:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/HWIMDIXcIZo/a-step-closer-to-quantum-computation.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:84:"http://googleresearch.blogspot.com/2015/03/a-step-closer-to-quantum-computation.html";}}i:24;a:14:{s:2:"id";s:59:"tag:blogger.com,1999:blog-21224994.post-7792873938266074619";s:9:"published";s:29:"2015-03-02T10:00:00.000-08:00";s:7:"updated";s:29:"2015-03-05T08:18:47.449-08:00";s:5:"title";s:47:"Large-Scale Machine Learning for Drug Discovery";s:12:"atom_content";s:6326:"<span class="byline-author">Posted by Patrick Riley and Dale Webster, Google Research and <a href="http://web.stanford.edu/~rbharath/">Bharath Ramsundar</a>, Google Research Intern and Stanford Ph.D. candidate</span> <br /><br />Discovering new treatments for human diseases is an immensely complicated challenge; Even after extensive research to develop a biological understanding of a disease, an effective therapeutic that can improve the quality of life must still be found. This process often takes years of research, requiring the creation and testing of millions of drug-like compounds in an effort to find a just a few viable drug treatment candidates.   These <a href="http://en.wikipedia.org/wiki/High-throughput_screening">high-throughput screens</a> are often automated in sophisticated labs and are expensive to perform. <br /><br />Recently, <a href="http://en.wikipedia.org/wiki/Deep_learning">deep learning</a> with neural networks has been applied in <a href="http://en.wikipedia.org/wiki/Virtual_screening">virtual drug screening</a><sup><i>1,2,3</i></sup>, which attempts to replace or augment the high-throughput screening process with the use of computational methods in order to improve its speed and success rate.<sup><i>4</i></sup> Traditionally, virtual drug screening has used only the experimental data from the particular disease being studied. However, as the volume of experimental drug screening data across many diseases continues to grow, several research groups have demonstrated that data from multiple diseases can be leveraged with <a href="http://en.wikipedia.org/wiki/Multi-task_learning">multitask</a> neural networks to improve the virtual screening effectiveness.<br /><br />In collaboration with the <a href="http://pande.stanford.edu/">Pande Lab</a> at <a href="http://www.stanford.edu/">Stanford University</a>, we’ve released a paper titled "<a href="http://arxiv.org/abs/1502.02072">Massively Multitask Networks for Drug Discovery</a>", investigating how data from a variety of sources can be used to improve the accuracy of determining which chemical compounds would be effective drug treatments for a variety of diseases. In particular, we carefully quantified how the amount and diversity of screening data from a variety of diseases with very different biological processes can be used to improve the virtual drug screening predictions. <br /><br />Using our <a href="http://research.google.com/archive/large_deep_networks_nips2012.html">large-scale neural network training system</a>, we trained at a scale 18x larger than previous work with a total of 37.8M data points across more than 200 distinct biological processes. Because of our large scale, we were able to carefully probe the sensitivity of these models to a variety of changes in model structure and input data. In the paper, we examine not just the performance of the model but why it performs well and what we can expect for similar models in the future. The data in the paper represents more than 50M total CPU hours.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-rgiVKCdKvGs/VPIRMAFEfmI/AAAAAAAAAec/xfeCYFY0eeM/s1600/image00.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://1.bp.blogspot.com/-rgiVKCdKvGs/VPIRMAFEfmI/AAAAAAAAAec/xfeCYFY0eeM/s1600/image00.png" height="400" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">This graph shows a measure of prediction accuracy (ROC AUC is the <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">area under the receiver operating characteristic curve</a>) for virtual screening on a fixed set of 10 biological processes as more datasets are added.</td></tr></tbody></table><br />One encouraging conclusion from this work is that our models are able to utilize data from many different experiments to increase prediction accuracy across many diseases. To our knowledge, this is the first time the effect of adding additional data has been quantified in this domain, and our results suggest that even more data could improve performance even further.<br /><br />Machine learning at scale has significant potential to accelerate drug discovery and improve human health. We look forward to continued improvement in virtual drug screening and its increasing impact in the discovery process for future drugs.<br /><br />Thank you to our other collaborators <a href="http://www.konerding.com/~dek/">David Konerding</a> (Google), <a href="http://pande.stanford.edu/people/#StevenKearnes">Steven Kearnes</a> (Stanford), and <a href="http://pande.stanford.edu/">Vijay Pande</a> (Stanford).<br /><br /><b>References:</b><br /><br /><i>1.</i> Thomas Unterthiner, Andreas Mayr, Günter Klambauer, Marvin Steijaert, Jörg Kurt Wegner, Hugo Ceulemans, Sepp Hochreiter. <a href="http://www.dlworkshop.org/23.pdf?attredirects=0"><i>Deep Learning as an Opportunity in Virtual Screening</i></a>. Deep Learning and Representation Learning Workshop: NIPS 2014<br /><br /><i>2.</i> Dahl, George E, Jaitly, Navdeep, and Salakhutdinov, Ruslan. <a href="http://arxiv.org/abs/1406.1231"><i>Multi-task neural networks for QSAR predictions</i></a>. arXiv preprint arXiv:1406.1231, 2014.<br /><br /><i>3.</i> Ma, Junshui, Sheridan, Robert P, Liaw, Andy, Dahl, George, and Svetnik, Vladimir. <a href="http://pubs.acs.org/doi/abs/10.1021/ci500747n"><i>Deep neural nets as a method for quantitative structure-activity relationships</i></a>. Journal of Chemical Information and Modeling, 2015.<br /><br /><i>4.</i> Peter Ripphausen, Britta Nisius, Lisa Peltason, and Jürgen Bajorath. <a href="http://pubs.acs.org/doi/full/10.1021/jm101020z"><i>Quo Vadis, Virtual Screening? A Comprehensive Survey of Prospective Applications</i></a>. Journal of Medicinal Chemistry 2010 53 (24), 8461-8467<div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=22KldIDh56I:zqoIBTpddKo:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"></img></a>
</div><img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/22KldIDh56I" height="1" width="1" alt=""/>";s:12:"link_replies";s:175:"http://googleresearch.blogspot.com/feeds/7792873938266074619/comments/defaulthttp://googleresearch.blogspot.com/2015/03/large-scale-machine-learning-for-drug.html#comment-form";s:9:"link_edit";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7792873938266074619";s:9:"link_self";s:71:"http://www.blogger.com/feeds/21224994/posts/default/7792873938266074619";s:4:"link";s:102:"http://feedproxy.google.com/~r/blogspot/gJZg/~3/22KldIDh56I/large-scale-machine-learning-for-drug.html";s:11:"author_name";s:13:"Research Blog";s:10:"author_uri";s:45:"https://plus.google.com/101673966767287570260";s:12:"author_email";s:19:"noreply@blogger.com";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:10:"feedburner";a:1:{s:8:"origlink";s:85:"http://googleresearch.blogspot.com/2015/03/large-scale-machine-learning-for-drug.html";}}}s:7:"channel";a:13:{s:2:"id";s:34:"tag:blogger.com,1999:blog-21224994";s:7:"updated";s:29:"2015-07-30T10:48:32.386-07:00";s:5:"title";s:20:"Google Research Blog";s:8:"subtitle";s:35:"The latest news on Google Research.";s:4:"link";s:35:"http://googleresearch.blogspot.com/";s:9:"link_next";s:96:"http://www.blogger.com/feeds/21224994/posts/default?start-index=26&max-results=25&redirect=false";s:11:"author_name";s:7:"Melanie";s:10:"author_uri";s:51:"http://www.blogger.com/profile/18294365194687572330";s:12:"author_email";s:19:"noreply@blogger.com";s:9:"generator";s:7:"Blogger";s:10:"opensearch";a:3:{s:12:"totalresults";s:3:"366";s:10:"startindex";s:1:"1";s:12:"itemsperpage";s:2:"25";}s:6:"atom10";a:1:{s:9:"link_self";s:41:"http://feeds.feedburner.com/blogspot/gJZg";}s:4:"info";s:140:"<feedburner:info /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/"><atom10:link />";}s:9:"textinput";a:0:{}s:5:"image";a:0:{}s:9:"feed_type";s:4:"Atom";s:12:"feed_version";N;s:5:"stack";a:0:{}s:9:"inchannel";b:0;s:6:"initem";b:0;s:9:"incontent";b:0;s:11:"intextinput";b:0;s:7:"inimage";b:0;s:13:"current_field";s:0:"";s:17:"current_namespace";b:0;s:5:"ERROR";s:0:"";s:19:"_CONTENT_CONSTRUCTS";a:6:{i:0;s:7:"content";i:1;s:7:"summary";i:2;s:4:"info";i:3;s:5:"title";i:4;s:7:"tagline";i:5;s:9:"copyright";}s:13:"last_modified";s:31:"Fri, 31 Jul 2015 11:25:15 GMT
";}