O:9:"MagpieRSS":20:{s:6:"parser";i:0;s:12:"current_item";a:0:{}s:5:"items";a:25:{i:0;a:10:{s:4:"guid";s:58:"tag:blogger.com,1999:blog-14903426.post-665148054178130825";s:7:"pubdate";s:31:"Mon, 11 Jan 2016 21:29:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2016-01-11T13:29:42.889-08:00";}s:8:"category";s:17:"cartodbitossvideo";s:5:"title";s:23:"The Future and All That";s:11:"description";s:611:"<p>I gave this talk in December, at the CartoDB 2015 partners conference, at the galactic headquarters in glamorous Bushwick, Brooklyn. A bit of a late posting, but hopefully I can still sneak under the "new year predictions bar".</p> <iframe src="https://player.vimeo.com/video/149429837" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href="https://vimeo.com/149429837">CartoDB15 - Trends In Business and Technology, The Future and All That</a> from <a href="https://vimeo.com/cartodb">CartoDB</a> on <a href="https://vimeo.com">Vimeo</a>.</p>";s:4:"link";s:66:"http://blog.cleverelephant.ca/2016/01/the-future-and-all-that.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:7:"summary";s:611:"<p>I gave this talk in December, at the CartoDB 2015 partners conference, at the galactic headquarters in glamorous Bushwick, Brooklyn. A bit of a late posting, but hopefully I can still sneak under the "new year predictions bar".</p> <iframe src="https://player.vimeo.com/video/149429837" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href="https://vimeo.com/149429837">CartoDB15 - Trends In Business and Technology, The Future and All That</a> from <a href="https://vimeo.com/cartodb">CartoDB</a> on <a href="https://vimeo.com">Vimeo</a>.</p>";}i:1;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-4975812417521534389";s:7:"pubdate";s:31:"Fri, 18 Dec 2015 03:05:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-12-18T10:23:35.665-08:00";}s:8:"category";s:27:"archivebcemailfoiitpolitics";s:5:"title";s:30:"What's up with Mr. Loukidelis?";s:11:"description";s:13976:"<p>Ever feel like people are talking about you behind your back? Usually it's just <a href="http://www.goodreads.com/quotes/95117-all-through-my-life-i-ve-had-this-strange-unaccountable-feeling">perfectly normal paranoia</a>. But sometimes, they actually are. Maybe. </p><p><small><b>Backgrounder for those from abroad:</b> Our <a href="http://www.gov.bc.ca">provincial government</a> was recently caught destroying public records by an Officer of the Legislature, who produced a <a href="https://www.oipc.bc.ca/investigation-reports/1874">detailed report</a> with a dozen recommendations on how to stop breaking the law so much. But rather than simply implementing the recommendations, the Premier instead appointed her <b>own</b> smart important guy, <a href="https://www.linkedin.com/in/david-loukidelis-bba05449">David Loukidelis</a>, to go over those recommendations and produce yet another set of <b>This Time It's For Real</b> recommendations for her to take <b>Very, Very Seriously</b>. Mr. Loukidelis produced <a href="http://www.cio.gov.bc.ca/local/cio/d_loukidelis_report.pdf">his report</a> on Wednesday, and <a href="http://thetyee.ca/News/2015/12/16/Loukidelis-Stop-Deleting/">the government said</a> it would "accept them all" (for certain definitions of the words "all" and "accept").</small></p><p><img alt="David Loukidelis" style="float: right; padding: 12px;" border="0" width="150" src="http://1.bp.blogspot.com/-B0AXkiaWftQ/VnN3gdd-TyI/AAAAAAAAAkk/tG5Beceh7E4/s320/img_09-12_loukidelis.jpg" />Anyways, I wasn't even through reading the introduction to the <a href="http://www.cio.gov.bc.ca/local/cio/d_loukidelis_report.pdf">Loukedlis report</a> on the <a href="https://www.oipc.bc.ca/investigation-reports/1874">Denham report</a> on government information access policy when I hit this line:</p><blockquote>"Nonetheless, some observers have suggested in the wake of the investigation report that all emails should be kept." </blockquote><p>As far as I know, I've been the only "observer" to <a href="http://blog.cleverelephant.ca/2015/10/if-i-hear-words-triple-delete-one-more.html">suggest that government emails should be archived and retained</a> more-or-less in their entirety, as we expect Canadian financial institutions to do, and as the US government expects all public corporations to do. So I took this as a little bit of a throw down.  </p><p>David Loukidelis wants to get it on! Is it on? Oh yes, it's on, baby! </p><p>(This would be a good moment to go do something a lot more engaging, like picking lint out of your toes, or feeling that sensitive place at the back of your second left molar. I'm about to take apart Recommendation #2 of a 70 page report that, despite <a href="http://www.theglobeandmail.com/news/british-columbia/bc-to-pay-ex-privacy-watchdog-50000-to-review-deleted-e-mails-report/article27073274/">costing $50,000</a>, is about as interesting as the last 70 pages of the phone book.) </p> <p><b>Chapter 1: It's too big!</b></p> <p>After calling out us "observers", Loukedlis then procedes to lay out his Luddite credentials in full, first by calculating the number of pages represented by the 43 terabytes of annual government emails:</p> <blockquote>"Using the above averages of emails received and sent, each year there would be roughly 426,000,000 pages of received emails and some 129,000,000 pages of sent emails, for a total of roughly 555,000,000 pages of emails. No one would suggest that all emails should be printed, but this gives a sense of the order-of-magnitude implications of the suggestions that, contrary to prudent information management principles, all emails should be kept, or should be vetted by others for retention. The same would be true even if these estimates were reduced by one or even two orders of magnitude, to 55,000,000 pages or 5,500,000 pages." </blockquote> <p>Staggering! Shocking! <b>Half a billion!</b> I'm surprised he didn't express it in terms of <a href="https://www.reddit.com/r/explainlikeimfive/comments/3hsjbu/eli5_why_are_football_fields_such_a_popular_unit/">football fields</a> to help the folks at home grasp the staggering immensity. (Because you need to know: 500M pages stack to about 700 football fields high.)</p> <p>Let's recast this problem in more computer-centric terms:</p> <ul><li>The government produces/receives 43TB of email per year.</li><li>A 4TB hard-drive can be purchased for between <a href="http://www.tigerdirect.ca/applications/Category/guidedSearch.asp?CatId=8&sel=Detail%3B17_35_33010_50290">$200 and $400</a>. </li><li>So depending on the amount of redundency you want, and the quality of hard-drive you purchase, it's possible to store the entire years worth of government email data on between $8,600 and $50,000 worth of hardware. Or, to put it in terms Mr. Loukidelis might understand, for about the cost of one overly wordy report.</li></ul> <p>Now I'm not suggesting the OCIO buy a dozen 4TB drives and stick a server in the closet, but the numbers above should reassure us that storing 44TB of email per year is not exactly at the far reaches of today's computing capabilities. There are companies that provide <a href="https://www.globalrelay.com/services/archive/email">cloud-based email archiving</a> services, particularly for organizations with privacy issues and sensitive data (financial companies). In fact, one of the <a href="https://www.globalrelay.com">leaders in the field</a> is headquartered right here in BC. I asked them if they could handle the government's data volume.</p> <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/pwramsey">@pwramsey</a> Yes, we are capable of that. If you have any additional questions, feel free to reach out to info@globalrelay.net!</p>&mdash; Global Relay (@globalRelay) <a href="https://twitter.com/globalRelay/status/677255946150658048">December 16, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> <p>So, we have the technology, we just lack the will.</p> <p><b>Chapter 2: It's not searchable!</b></p> <p>Unfortunately, Mr. Loukedelis doesn't stop trying to explain technology to the unwashed with his "pages of paper" analogy. He's got yet more reasoning by analogy to share.</p> <blockquote>"At all costs, the provincial government should not entertain any notion that all electronic records must, regardless of their value, be retained. ... To suggest, as some have, that all information should be kept is akin to suggesting it is good household management for homeowners to never throw away rotten food, grocery lists, old newspapers, broken toys or worn-out clothes. No one keeps their garbage. Hoarding is not healthy." </blockquote> <p>Except of course, we aren't talking about rotten food, grocery lists, old newspapers, and broken toys here. We're talking about digital data, which can be sifted, filtered and analyzed in microseconds, without human effort of any kind. These are not differences in <em>degree</em>, these are differences in <em>kind</em>.</p> <p>Mr. Loukedelis might be too young to remember this, but when Google introduced GMail in 2004, they did two remarkable things: they gave every user an unprecedented 1GB of free storage (that number is now 15GB); and, they hid the "delete" button in favor of an "archive" button. The archive button does not delete mails, it just removes them from the Inbox. Google served notice a decade ago: you don't <b>have</b> to delete your mail, and you shouldn't <b>bother</b> to delete your mail, because it's too <b>valuable</b> as a record, and so very easy to search and find what you want.</p> <p>I'm surprised Mr. Loukidelis, as a lawyer, isn't following the progress of <a href="https://en.wikipedia.org/wiki/Electronic_discovery">e-discovery</a> technology, rapidly moving from keyword based searching to applying <a href="http://bommaritollc.com/2012/10/natural-language-processing-and-machine-learning-for-e-discovery-slides-from-guest-lecture-at-msu-college-of-law/">natural language</a> and <a href="http://e-discoveryteam.com/car/">AI</a> (well, statistical pattern recognition) tools to finding relevant documents in huge corpuses of electronic data.</p> <p>Suffice it to say, it's early days. Present technology is more than satisfactory to do a <em>much better job</em> than the poor old FOI clerks are doing searching mail boxes. And in the future, we can expect AI tools to easily sort through as much "garbage" as we care to throw at them.</p> <p>The time to start archiving everything, and letting the computers sort out the mess, is now.</p> <p><b>Chapter 3: It's not relevant!</b></p> <p>There's one more vignette Mr. Loukedelis shares, a folksy thing, which is also worth looking at:</p> <blockquote>"This is true even if an individual engages in a transaction that generates records. Take the example of an individual who shops at an online store and arranges to pick up the television they buy at a bricks-and-mortar location. The order confirmation is emailed to them and they print it for pickup purposes. They cannot pick the television up within the allotted window, so they email the retailer to extend the time. The retailer responds. They then email the retailer about whether the television comes with an HDMI cable. The retailer responds. Once the television is picked up, the purchaser keeps the receipt for warranty purposes. This is surely the only documentation that truly matters. It would make no sense to keep all of the emails back and forth, or the printed pickup notice."</blockquote> <p>Valueless! Cluttering up the important documentary record of government! If we had to store all this back-and-forth nonsense, we'd never be able to find the "good stuff" amongst the trash. Right?</p> <p>What if the individual were picked up for a murder he didn't commit, and his only alibi was that he sent an email from his desk to the television store, right when the act was committed? What if, after delivery, the individual opens the box and finds no HDMI cable! The store insists there isn't <b>supposed</b> to be one. How can the individual prove otherwise? On and on it goes.</p> <p>The most trivial pieces of information can have value, in the right circumstances. And since they cost practically <b>nothing</b> to store, why not keep them, particularly in light of the alternative Mr. Loukedelis proposes.</p> <p><b>Chapter 4: What's the alternative?</b></p> <p>It's important to weigh Mr. Loukidelis' strong <b>rejection</b> of email archiving against the alternative, which is basically the current system.</p> <ul><li>Most policy discussion and decisions are handled in email.</li><li>That email may be discarded very easily by any staff member.</li><li>Only if printed and filed will a permanent record be kept.</li><li>If deleted, a copy in the trash folder may find its way to a backup file.</li><li>Once deleted, FOI searches for the record will start to come up empty, as individual searches on staff computers don't necessarily hit the trash folder.</li><li>Also, FOI searches can only find the record if run on the right staff member's computer (unlike with a government-wide archive).</li><li>The copy in the backups will only be retrievable if HP Advanced Solutions restores the backup file (and if you think storing 44TB of data a year is expensive, compare it to <a href="http://blog.cleverelephant.ca/2015/07/bc-it-outsourcing-201415.html">having HPAS do really anything at all</a> for you).</li><li>The backups themselves will be purged after 13 months. At that point, the record is gone, forever.</li></ul> <p>On top of this system, Mr. Loukidelis proposes some sensible tweaks and improvements, but let's be crystal clear: <a href="http://blog.cleverelephant.ca/2013/08/bc-government-email-defective-by-design.html">the current system sucks</a>, it's really collosally bad, and there's no excuse for that <a href="http://www.cbc.ca/news/politics/canada-trudeau-liberal-government-cabinet-1.3304590">in 2015</a>.</p> <p>Mr. Loukidelis should have proposed a real improvement, but instead he wiffed, and he wiffed hard.</p> <hr/> <p><b>Appendix A: Optional Conspiracy Theory Section</b></p> <p>Mr. Loukidelis' recommendation #2 is really striking, here it is:</p> <blockquote>"It is recommended <b>in the strongest possible terms</b> that government resist any notion that all emails should be kept"</blockquote> <p>Emphasis mine. Not just recommended, but "in the strongest possible terms". None of the other recommendations is remotely so strong. And here's an odd thing: the other recommendations are all addressed to Commissioner Denham's original report, but Denham has nothing at all to say about archiving all email. It's like this particular topic dropped into the Loukidelis report from out of the blue sky, and was greated by a phalanx of flame-throwers.</p> <p>Why? What's going on? Why spend so much ink, and such strong language, killing an idea that Denham didn't even raise?</p> <p>I find it hard to believe that Loukidelis really cared that much about "observers" like me and my blog. But he cared enough to not only put in a section about email archiving, but also to beat the topic to death with a shovel.</p> <p>I think there must have been some internal debate in government about permanently ending the controversy over bad email management by <b>adopting an email archive</b>. And Loukidelis was instructed by political staff on one side of that debate to ensure that the idea was terminated with dispatch.</p> <p>Maybe Finance Minister Mike "Mr Transparency" de Jong made an email archive a personal hobby-horse and started talking it up in cabinet. If so, having the Loukidelis report kill the idea dead would be a quick and dirty way for the Premier to make sure the discussion went no further.</p> <p>Regardless, I think there's probably an interesting story behind recommendation #2, and I hope someday I get to hear what it was.</p>";s:4:"link";s:70:"http://blog.cleverelephant.ca/2015/12/whats-up-with-mr-loukidelis.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"1";}s:7:"summary";s:13976:"<p>Ever feel like people are talking about you behind your back? Usually it's just <a href="http://www.goodreads.com/quotes/95117-all-through-my-life-i-ve-had-this-strange-unaccountable-feeling">perfectly normal paranoia</a>. But sometimes, they actually are. Maybe. </p><p><small><b>Backgrounder for those from abroad:</b> Our <a href="http://www.gov.bc.ca">provincial government</a> was recently caught destroying public records by an Officer of the Legislature, who produced a <a href="https://www.oipc.bc.ca/investigation-reports/1874">detailed report</a> with a dozen recommendations on how to stop breaking the law so much. But rather than simply implementing the recommendations, the Premier instead appointed her <b>own</b> smart important guy, <a href="https://www.linkedin.com/in/david-loukidelis-bba05449">David Loukidelis</a>, to go over those recommendations and produce yet another set of <b>This Time It's For Real</b> recommendations for her to take <b>Very, Very Seriously</b>. Mr. Loukidelis produced <a href="http://www.cio.gov.bc.ca/local/cio/d_loukidelis_report.pdf">his report</a> on Wednesday, and <a href="http://thetyee.ca/News/2015/12/16/Loukidelis-Stop-Deleting/">the government said</a> it would "accept them all" (for certain definitions of the words "all" and "accept").</small></p><p><img alt="David Loukidelis" style="float: right; padding: 12px;" border="0" width="150" src="http://1.bp.blogspot.com/-B0AXkiaWftQ/VnN3gdd-TyI/AAAAAAAAAkk/tG5Beceh7E4/s320/img_09-12_loukidelis.jpg" />Anyways, I wasn't even through reading the introduction to the <a href="http://www.cio.gov.bc.ca/local/cio/d_loukidelis_report.pdf">Loukedlis report</a> on the <a href="https://www.oipc.bc.ca/investigation-reports/1874">Denham report</a> on government information access policy when I hit this line:</p><blockquote>"Nonetheless, some observers have suggested in the wake of the investigation report that all emails should be kept." </blockquote><p>As far as I know, I've been the only "observer" to <a href="http://blog.cleverelephant.ca/2015/10/if-i-hear-words-triple-delete-one-more.html">suggest that government emails should be archived and retained</a> more-or-less in their entirety, as we expect Canadian financial institutions to do, and as the US government expects all public corporations to do. So I took this as a little bit of a throw down.  </p><p>David Loukidelis wants to get it on! Is it on? Oh yes, it's on, baby! </p><p>(This would be a good moment to go do something a lot more engaging, like picking lint out of your toes, or feeling that sensitive place at the back of your second left molar. I'm about to take apart Recommendation #2 of a 70 page report that, despite <a href="http://www.theglobeandmail.com/news/british-columbia/bc-to-pay-ex-privacy-watchdog-50000-to-review-deleted-e-mails-report/article27073274/">costing $50,000</a>, is about as interesting as the last 70 pages of the phone book.) </p> <p><b>Chapter 1: It's too big!</b></p> <p>After calling out us "observers", Loukedlis then procedes to lay out his Luddite credentials in full, first by calculating the number of pages represented by the 43 terabytes of annual government emails:</p> <blockquote>"Using the above averages of emails received and sent, each year there would be roughly 426,000,000 pages of received emails and some 129,000,000 pages of sent emails, for a total of roughly 555,000,000 pages of emails. No one would suggest that all emails should be printed, but this gives a sense of the order-of-magnitude implications of the suggestions that, contrary to prudent information management principles, all emails should be kept, or should be vetted by others for retention. The same would be true even if these estimates were reduced by one or even two orders of magnitude, to 55,000,000 pages or 5,500,000 pages." </blockquote> <p>Staggering! Shocking! <b>Half a billion!</b> I'm surprised he didn't express it in terms of <a href="https://www.reddit.com/r/explainlikeimfive/comments/3hsjbu/eli5_why_are_football_fields_such_a_popular_unit/">football fields</a> to help the folks at home grasp the staggering immensity. (Because you need to know: 500M pages stack to about 700 football fields high.)</p> <p>Let's recast this problem in more computer-centric terms:</p> <ul><li>The government produces/receives 43TB of email per year.</li><li>A 4TB hard-drive can be purchased for between <a href="http://www.tigerdirect.ca/applications/Category/guidedSearch.asp?CatId=8&sel=Detail%3B17_35_33010_50290">$200 and $400</a>. </li><li>So depending on the amount of redundency you want, and the quality of hard-drive you purchase, it's possible to store the entire years worth of government email data on between $8,600 and $50,000 worth of hardware. Or, to put it in terms Mr. Loukidelis might understand, for about the cost of one overly wordy report.</li></ul> <p>Now I'm not suggesting the OCIO buy a dozen 4TB drives and stick a server in the closet, but the numbers above should reassure us that storing 44TB of email per year is not exactly at the far reaches of today's computing capabilities. There are companies that provide <a href="https://www.globalrelay.com/services/archive/email">cloud-based email archiving</a> services, particularly for organizations with privacy issues and sensitive data (financial companies). In fact, one of the <a href="https://www.globalrelay.com">leaders in the field</a> is headquartered right here in BC. I asked them if they could handle the government's data volume.</p> <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/pwramsey">@pwramsey</a> Yes, we are capable of that. If you have any additional questions, feel free to reach out to info@globalrelay.net!</p>&mdash; Global Relay (@globalRelay) <a href="https://twitter.com/globalRelay/status/677255946150658048">December 16, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> <p>So, we have the technology, we just lack the will.</p> <p><b>Chapter 2: It's not searchable!</b></p> <p>Unfortunately, Mr. Loukedelis doesn't stop trying to explain technology to the unwashed with his "pages of paper" analogy. He's got yet more reasoning by analogy to share.</p> <blockquote>"At all costs, the provincial government should not entertain any notion that all electronic records must, regardless of their value, be retained. ... To suggest, as some have, that all information should be kept is akin to suggesting it is good household management for homeowners to never throw away rotten food, grocery lists, old newspapers, broken toys or worn-out clothes. No one keeps their garbage. Hoarding is not healthy." </blockquote> <p>Except of course, we aren't talking about rotten food, grocery lists, old newspapers, and broken toys here. We're talking about digital data, which can be sifted, filtered and analyzed in microseconds, without human effort of any kind. These are not differences in <em>degree</em>, these are differences in <em>kind</em>.</p> <p>Mr. Loukedelis might be too young to remember this, but when Google introduced GMail in 2004, they did two remarkable things: they gave every user an unprecedented 1GB of free storage (that number is now 15GB); and, they hid the "delete" button in favor of an "archive" button. The archive button does not delete mails, it just removes them from the Inbox. Google served notice a decade ago: you don't <b>have</b> to delete your mail, and you shouldn't <b>bother</b> to delete your mail, because it's too <b>valuable</b> as a record, and so very easy to search and find what you want.</p> <p>I'm surprised Mr. Loukidelis, as a lawyer, isn't following the progress of <a href="https://en.wikipedia.org/wiki/Electronic_discovery">e-discovery</a> technology, rapidly moving from keyword based searching to applying <a href="http://bommaritollc.com/2012/10/natural-language-processing-and-machine-learning-for-e-discovery-slides-from-guest-lecture-at-msu-college-of-law/">natural language</a> and <a href="http://e-discoveryteam.com/car/">AI</a> (well, statistical pattern recognition) tools to finding relevant documents in huge corpuses of electronic data.</p> <p>Suffice it to say, it's early days. Present technology is more than satisfactory to do a <em>much better job</em> than the poor old FOI clerks are doing searching mail boxes. And in the future, we can expect AI tools to easily sort through as much "garbage" as we care to throw at them.</p> <p>The time to start archiving everything, and letting the computers sort out the mess, is now.</p> <p><b>Chapter 3: It's not relevant!</b></p> <p>There's one more vignette Mr. Loukedelis shares, a folksy thing, which is also worth looking at:</p> <blockquote>"This is true even if an individual engages in a transaction that generates records. Take the example of an individual who shops at an online store and arranges to pick up the television they buy at a bricks-and-mortar location. The order confirmation is emailed to them and they print it for pickup purposes. They cannot pick the television up within the allotted window, so they email the retailer to extend the time. The retailer responds. They then email the retailer about whether the television comes with an HDMI cable. The retailer responds. Once the television is picked up, the purchaser keeps the receipt for warranty purposes. This is surely the only documentation that truly matters. It would make no sense to keep all of the emails back and forth, or the printed pickup notice."</blockquote> <p>Valueless! Cluttering up the important documentary record of government! If we had to store all this back-and-forth nonsense, we'd never be able to find the "good stuff" amongst the trash. Right?</p> <p>What if the individual were picked up for a murder he didn't commit, and his only alibi was that he sent an email from his desk to the television store, right when the act was committed? What if, after delivery, the individual opens the box and finds no HDMI cable! The store insists there isn't <b>supposed</b> to be one. How can the individual prove otherwise? On and on it goes.</p> <p>The most trivial pieces of information can have value, in the right circumstances. And since they cost practically <b>nothing</b> to store, why not keep them, particularly in light of the alternative Mr. Loukedelis proposes.</p> <p><b>Chapter 4: What's the alternative?</b></p> <p>It's important to weigh Mr. Loukidelis' strong <b>rejection</b> of email archiving against the alternative, which is basically the current system.</p> <ul><li>Most policy discussion and decisions are handled in email.</li><li>That email may be discarded very easily by any staff member.</li><li>Only if printed and filed will a permanent record be kept.</li><li>If deleted, a copy in the trash folder may find its way to a backup file.</li><li>Once deleted, FOI searches for the record will start to come up empty, as individual searches on staff computers don't necessarily hit the trash folder.</li><li>Also, FOI searches can only find the record if run on the right staff member's computer (unlike with a government-wide archive).</li><li>The copy in the backups will only be retrievable if HP Advanced Solutions restores the backup file (and if you think storing 44TB of data a year is expensive, compare it to <a href="http://blog.cleverelephant.ca/2015/07/bc-it-outsourcing-201415.html">having HPAS do really anything at all</a> for you).</li><li>The backups themselves will be purged after 13 months. At that point, the record is gone, forever.</li></ul> <p>On top of this system, Mr. Loukidelis proposes some sensible tweaks and improvements, but let's be crystal clear: <a href="http://blog.cleverelephant.ca/2013/08/bc-government-email-defective-by-design.html">the current system sucks</a>, it's really collosally bad, and there's no excuse for that <a href="http://www.cbc.ca/news/politics/canada-trudeau-liberal-government-cabinet-1.3304590">in 2015</a>.</p> <p>Mr. Loukidelis should have proposed a real improvement, but instead he wiffed, and he wiffed hard.</p> <hr/> <p><b>Appendix A: Optional Conspiracy Theory Section</b></p> <p>Mr. Loukidelis' recommendation #2 is really striking, here it is:</p> <blockquote>"It is recommended <b>in the strongest possible terms</b> that government resist any notion that all emails should be kept"</blockquote> <p>Emphasis mine. Not just recommended, but "in the strongest possible terms". None of the other recommendations is remotely so strong. And here's an odd thing: the other recommendations are all addressed to Commissioner Denham's original report, but Denham has nothing at all to say about archiving all email. It's like this particular topic dropped into the Loukidelis report from out of the blue sky, and was greated by a phalanx of flame-throwers.</p> <p>Why? What's going on? Why spend so much ink, and such strong language, killing an idea that Denham didn't even raise?</p> <p>I find it hard to believe that Loukidelis really cared that much about "observers" like me and my blog. But he cared enough to not only put in a section about email archiving, but also to beat the topic to death with a shovel.</p> <p>I think there must have been some internal debate in government about permanently ending the controversy over bad email management by <b>adopting an email archive</b>. And Loukidelis was instructed by political staff on one side of that debate to ensure that the idea was terminated with dispatch.</p> <p>Maybe Finance Minister Mike "Mr Transparency" de Jong made an email archive a personal hobby-horse and started talking it up in cabinet. If so, having the Loukidelis report kill the idea dead would be a quick and dirty way for the Premier to make sure the discussion went no further.</p> <p>Regardless, I think there's probably an interesting story behind recommendation #2, and I hope someday I get to hear what it was.</p>";}i:2;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-7634031033025977219";s:7:"pubdate";s:31:"Sun, 29 Nov 2015 00:53:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-12-17T19:07:00.199-08:00";}s:8:"category";s:20:"pgconfsvpostgisvideo";s:5:"title";s:31:"PostGIS Gotchas @ PgConfSV 2015";s:11:"description";s:605:"<P>I attended PgConf Silicon Valley a couple weeks ago and gave a new talk about aspects of PostGIS that come as a surprise to new users. Folks in Silicon Valley arrive at PostGIS with lots of technical chops, but often little experience with geospatial concepts, which can lead to fun misunderstandings. Also, PostGIS just has a lot of historical behaviours we've kept in place for backwards compatibility over the years. </p> <iframe allowfullscreen="" frameborder="0" height="270" src="https://www.youtube.com/embed/GSuZP89UdGs" width="480"></iframe> <p>Thanks to everyone who turned out to attend!</p>";s:4:"link";s:72:"http://blog.cleverelephant.ca/2015/11/postgis-gotchas-pgconfsv-2015.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:7:"summary";s:605:"<P>I attended PgConf Silicon Valley a couple weeks ago and gave a new talk about aspects of PostGIS that come as a surprise to new users. Folks in Silicon Valley arrive at PostGIS with lots of technical chops, but often little experience with geospatial concepts, which can lead to fun misunderstandings. Also, PostGIS just has a lot of historical behaviours we've kept in place for backwards compatibility over the years. </p> <iframe allowfullscreen="" frameborder="0" height="270" src="https://www.youtube.com/embed/GSuZP89UdGs" width="480"></iframe> <p>Thanks to everyone who turned out to attend!</p>";}i:3;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-2228092726852358322";s:7:"pubdate";s:31:"Wed, 28 Oct 2015 03:19:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-10-30T13:09:41.895-07:00";}s:8:"category";s:24:"bcemailfoiittripledelete";s:5:"title";s:41:"Government email deleting: intent matters";s:11:"description";s:2883:"<p><img src="https://pbs.twimg.com/profile_images/1195985377/Baldrey_twitter_photo_400x400.jpg" style="float:right; width:150px;padding:10px;" alt="Keith Baldrey" /> I caught Keith Baldrey on the aether-box today (CKNW) and he was being generous in his distribution of benefit of the doubt to the poor, poor government staffers trying to handle their email:</p> <blockquote>“I’ve talked to government staffers about this, and they are confused on what the rules are, it’s very unclear and unevenly applied over what should be deleted and what should not be.”<br/>&mdash; Keith Baldrey, Tuesday, October 27, 15:24 on CKNW</blockquote> <p>Before we get to remedies, let's review what these poor confused dears <a href="https://www.oipc.bc.ca/investigation-reports/1874">are doing</a>. For whatever reason, because they believe the email is not an important record, or a duplicate, or they just can't bear to burden the taxpayers of BC with storing a further 85KB of data, the beleaguered staffers are doing the following:</p> <ol><li>They select the email in question and hit <b>Delete</b>.</li><li>Then they go to their <b>Trash</b> folder and select the option to purge that folder.</li><li>Finally they open up a special folder called <b>Recover Deleted</b>, and select the option to purge <b>that</b> folder.</li></ol> <p>Let's be clear. If the poor confused staffers were just plain vanilla innocently deleting emails that they thought were transitory but were not, they would be <b>stopping at step number one</b>. But they aren't. So there's a very particular intent in play here, and that's to make sure that <b>nobody ever sees what's in these emails ever, ever, ever again</b>. And that intent is not consistent with the (current) cover story about innocently not understanding the rules in play with respect to email management.</p> <p>Moving on to remedies.</p> <p>We don't need to train them more (or maybe we do, but not for this). We need to establish a corporate email archive that simply takes a copy of every email, sent and received and dumps it into a searchable vault. <a href="http://blog.cleverelephant.ca/2015/10/if-i-hear-words-triple-delete-one-more.html">This is widely available technology, used by public companies and investment dealers around the world.</a></p> <p>Once the archive is in place, staffers can manage their email <b>any way they like</b>. They can keep a pristine, empty mail box, the way <a href="http://www.cbc.ca/news/canada/british-columbia/highway-tears-emails-triple-deleted-1.3284985">Minister Todd Stone apparently likes to operate</a>. Or they can keep a complete record of all their email, ready to search and aid their work. Or some happy mixture of the two. They'll be more effective public servants, and the public won't need to worry about records going down the memory hole any more.</p> <p>Let's get it done, OK?</p>";s:4:"link";s:83:"http://blog.cleverelephant.ca/2015/10/government-email-deleting-intent-matters.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"1";}s:7:"summary";s:2883:"<p><img src="https://pbs.twimg.com/profile_images/1195985377/Baldrey_twitter_photo_400x400.jpg" style="float:right; width:150px;padding:10px;" alt="Keith Baldrey" /> I caught Keith Baldrey on the aether-box today (CKNW) and he was being generous in his distribution of benefit of the doubt to the poor, poor government staffers trying to handle their email:</p> <blockquote>“I’ve talked to government staffers about this, and they are confused on what the rules are, it’s very unclear and unevenly applied over what should be deleted and what should not be.”<br/>&mdash; Keith Baldrey, Tuesday, October 27, 15:24 on CKNW</blockquote> <p>Before we get to remedies, let's review what these poor confused dears <a href="https://www.oipc.bc.ca/investigation-reports/1874">are doing</a>. For whatever reason, because they believe the email is not an important record, or a duplicate, or they just can't bear to burden the taxpayers of BC with storing a further 85KB of data, the beleaguered staffers are doing the following:</p> <ol><li>They select the email in question and hit <b>Delete</b>.</li><li>Then they go to their <b>Trash</b> folder and select the option to purge that folder.</li><li>Finally they open up a special folder called <b>Recover Deleted</b>, and select the option to purge <b>that</b> folder.</li></ol> <p>Let's be clear. If the poor confused staffers were just plain vanilla innocently deleting emails that they thought were transitory but were not, they would be <b>stopping at step number one</b>. But they aren't. So there's a very particular intent in play here, and that's to make sure that <b>nobody ever sees what's in these emails ever, ever, ever again</b>. And that intent is not consistent with the (current) cover story about innocently not understanding the rules in play with respect to email management.</p> <p>Moving on to remedies.</p> <p>We don't need to train them more (or maybe we do, but not for this). We need to establish a corporate email archive that simply takes a copy of every email, sent and received and dumps it into a searchable vault. <a href="http://blog.cleverelephant.ca/2015/10/if-i-hear-words-triple-delete-one-more.html">This is widely available technology, used by public companies and investment dealers around the world.</a></p> <p>Once the archive is in place, staffers can manage their email <b>any way they like</b>. They can keep a pristine, empty mail box, the way <a href="http://www.cbc.ca/news/canada/british-columbia/highway-tears-emails-triple-deleted-1.3284985">Minister Todd Stone apparently likes to operate</a>. Or they can keep a complete record of all their email, ready to search and aid their work. Or some happy mixture of the two. They'll be more effective public servants, and the public won't need to worry about records going down the memory hole any more.</p> <p>Let's get it done, OK?</p>";}i:4;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-8850519691258948922";s:7:"pubdate";s:31:"Sat, 24 Oct 2015 19:05:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-10-25T09:40:33.434-07:00";}s:8:"category";s:18:"bcfoiitoipcscandal";s:5:"title";s:52:"If I hear the words "triple delete" one more time...";s:11:"description";s:8045:"<p><img src="http://www.firstaidadvice.info/fm-21-11/images/fig3-19.PNG" style="float: right; height: 200px;" /> ... I'm going to tear my ears off. Also "transitory email". Just bam, going to rip them right off. </p> <p><small><b>Note for those not following the British Columbia political news:</b> While we have known for many years that high-level government staff <a href="http://blog.cleverelephant.ca/2013/08/bc-government-email-defective-by-design.html">routinely delete their work email</a>, a smoking gun came to light in the spring. A <a href="https://s3.amazonaws.com/s3.documentcloud.org/documents/2089546/foi-letter.pdf">former staffer told</a> how his superior personally deleted emails that were subject to an FOI request and then memorably said "It's done. Now you don't have to worry anymore." (A line which really should only be delivered over a fresh mound of dirt with a shovel in hand.) The BC FOI Commissioner investigated his allegation and <a href="https://www.oipc.bc.ca/investigation-reports/1874">reported back</a> that, yep, it really did happen and that the government basically does it all the time.</small></p> <p>The Microsoft Outlook tricks and the contortions of policy around what is "transitory" or not, are all beside the point, since: </p><ol><li>there is <b>no reason electronic document destruction should be allowed</b>, in any circumstance, ever, because</li><li>electronic message <b>archival and retrieval is a solved problem</b>. </ol> <p>The <a href="http://www.bclaws.ca/Recon/document/ID/freeside/96165_00">BC Freedom of Information Act</a>, with its careful parsing of "<a href="http://www.gov.bc.ca/citz/iao/records_mgmt/guides/transitoryug.pdf">transitory</a>" versus real e-mails, was written in the early 1990s, when there was a tangible, physical cost to retaining duplicative and short-lived records -- they took up space, and cost money to store. </p> <p><img src="http://www.staples-3p.com/s7/is/image/Staples/s0404113_sc7" style="float: right; height: 200px" />Oh, yes, digital documents cost money to store, but please note, my old CD collection (already a very information dense media) takes up a 2-cube box in my garage, but barely dents the storage capacity of an $10 memory stick in MP3 form. My book collection (6 shelves) hardly even registers in digital form. You use more data streaming an episode of Breaking Bad. <b>Things have changed since 1995. And since 2005.</b></p> <p>So why are we still having this conversation, and why does the government have such lax rules around message retention? And let me be clear, the government rules are <b>very, very, lax</b>. </p> <p>In the USA, public companies are under the <a href="https://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act">Sarbanes-Oxley</a> rules and have extremely <a href="http://www.sox-online.com/act_section_802.html">strict requirements</a> for <a href="http://www.creditworthy.com/3jm/articles/cw90507.html">document retention</a>, with punishments to match: </p> <blockquote>"Whoever knowingly alters, destroys, mutilates, conceals, covers up, falsifies, or makes a false entry in any record, document, or tangible object with the intent to impede, obstruct, or influence the investigation or proper administration of any matter within the jurisdiction of any department or agency of the United States or any case filed under title 11, or in relation to or contemplation of any such matter or case, shall be fined under this title, imprisoned not more than 20 years, or both."</blockquote> <p>Similarly, in Canada investment companies must keep complete archives of all messages, in all kinds of media: </p> <blockquote>Pursuant to National Instrument 31-103 ... firms must retain records of their business activities, financial affairs, client transactions and communication. ... <b>The type of device used to transmit the communication or whether it is a firm issued or personal device is irrelevant.</b> Dealer Members must therefore design systems and programs with compliant record retention and retrieval functionalities for those methods of communication permitted at the firm. For instance, the content posted on social media websites, such as <B>Twitter, Facebook, blogs, chat rooms and all material transmitted through emails</b>, are subject to the above-noted legislative and regulatory requirements. <br/>&mdash; IIROC <a href="http://www.iiroc.ca/Documents/2011/dbed7d6a-ed1c-4a8b-b3d9-bef60412aa27_en.pdf">Guidelines for the review, supervision and retention of advertisements, sales literature and correspondence</a>, Section II </blockquote> <p>Wow! That sounds really hard! I wonder how US public companies and Canadian investment dealers can do this, while the government can't even upgrade their email servers without losing 8 months worth of archival data: </p> <blockquote>As it turned out, the entire migration process would take eight months. When the process extended beyond June 2014, MTICS forgot to instruct HPAS to do backups on a monthly basis. This meant that every government mailbox that migrated onto the new system went without a monthly backup until all mailboxes were migrated. Any daily backup that existed was expunged after 31 days. At its peak, some 48,000 government mailboxes were without monthly email backups.<br/>&mdash; OIPC <a href="https://www.oipc.bc.ca/investigation-reports/1874">Investigation Report F15-03</a>, Page 32 </blockquote> <p><img src="https://lh3.googleusercontent.com/-XHk-iEPpMBE/AAAAAAAAAAI/AAAAAAAAAAA/6ZPvqo28YIc/photo.jpg" style="float:right; height: 100px;"/>Corporations and investment banks can do this because high volume <a href="https://www.google.ca/?q=enterprise+email+archive">enterprise email archiving</a> has been a solved problem for well over a decade. So there are lots of options, <a href="http://www.messagesolution.com/email_archiving.htm">proprietary</a>, <a href="https://www.mailarchiva.com/#firstPage">open source</a>, and even <a href="http://globalrelay.com/">British Columbian</a>!</p> <p>Yep, one of the top companies in the electronic message archiving space, <a href="http://globalrelay.com/">Global Relay</a>, is actually headquartered in Vancouver! Guys! Wake up! Put a salesperson on the float-plane to Victoria on Monday!</p> <p>Right now, British Columbia doesn't have an enterprise email archive. It has an email server farm, with infrequent backup files, retained for only 18 months and requiring substantial effort to restore and search. Some of the advantages of an archive are: </p> <ul><li>The archive is separate from the users, they do not individually determine the retention schedule using their [DELETE] key, retention is applied enterprise-wide on the archive.</li><li>Archive searches are not done by users, they are done by the people who need access to the archive. In the case of corporate archives, that's usually the legal team. In the case of the government it would be the legal team and the FOI officers.</li><li>Archive searches can address the <b>whole</b> collection of email in one search. Current government FOI email searches are done computer-by-computer, by line staff who probably have better things to do.</li><li>The archive is separate from the operational mail delivery and mail box servers, so upgrades on the operation equipment do not affect the archive.</li></ul> <p>So, for the next little while, the Commissioner's narrow technical recommendations are fine (even though they make me want to tear my ears off): </p> <a href="http://2.bp.blogspot.com/-WcPaHLhhssY/VivThIrDjtI/AAAAAAAAAi8/pSu1CFJCGEU/s1600/screenshot_340.png" imageanchor="1" ><img border="0" src="http://2.bp.blogspot.com/-WcPaHLhhssY/VivThIrDjtI/AAAAAAAAAi8/pSu1CFJCGEU/s1600/screenshot_340.png" /></a> <p>But the real long-term technical solution to treating email as a document of record is... <b>start treating it as a document of record</b>! Archive it, permanently, in a searchable form, and don't let the end users set the retention policy. It's not rocket science, it's just computers. </p>";s:4:"link";s:81:"http://blog.cleverelephant.ca/2015/10/if-i-hear-words-triple-delete-one-more.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"1";}s:7:"summary";s:8045:"<p><img src="http://www.firstaidadvice.info/fm-21-11/images/fig3-19.PNG" style="float: right; height: 200px;" /> ... I'm going to tear my ears off. Also "transitory email". Just bam, going to rip them right off. </p> <p><small><b>Note for those not following the British Columbia political news:</b> While we have known for many years that high-level government staff <a href="http://blog.cleverelephant.ca/2013/08/bc-government-email-defective-by-design.html">routinely delete their work email</a>, a smoking gun came to light in the spring. A <a href="https://s3.amazonaws.com/s3.documentcloud.org/documents/2089546/foi-letter.pdf">former staffer told</a> how his superior personally deleted emails that were subject to an FOI request and then memorably said "It's done. Now you don't have to worry anymore." (A line which really should only be delivered over a fresh mound of dirt with a shovel in hand.) The BC FOI Commissioner investigated his allegation and <a href="https://www.oipc.bc.ca/investigation-reports/1874">reported back</a> that, yep, it really did happen and that the government basically does it all the time.</small></p> <p>The Microsoft Outlook tricks and the contortions of policy around what is "transitory" or not, are all beside the point, since: </p><ol><li>there is <b>no reason electronic document destruction should be allowed</b>, in any circumstance, ever, because</li><li>electronic message <b>archival and retrieval is a solved problem</b>. </ol> <p>The <a href="http://www.bclaws.ca/Recon/document/ID/freeside/96165_00">BC Freedom of Information Act</a>, with its careful parsing of "<a href="http://www.gov.bc.ca/citz/iao/records_mgmt/guides/transitoryug.pdf">transitory</a>" versus real e-mails, was written in the early 1990s, when there was a tangible, physical cost to retaining duplicative and short-lived records -- they took up space, and cost money to store. </p> <p><img src="http://www.staples-3p.com/s7/is/image/Staples/s0404113_sc7" style="float: right; height: 200px" />Oh, yes, digital documents cost money to store, but please note, my old CD collection (already a very information dense media) takes up a 2-cube box in my garage, but barely dents the storage capacity of an $10 memory stick in MP3 form. My book collection (6 shelves) hardly even registers in digital form. You use more data streaming an episode of Breaking Bad. <b>Things have changed since 1995. And since 2005.</b></p> <p>So why are we still having this conversation, and why does the government have such lax rules around message retention? And let me be clear, the government rules are <b>very, very, lax</b>. </p> <p>In the USA, public companies are under the <a href="https://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act">Sarbanes-Oxley</a> rules and have extremely <a href="http://www.sox-online.com/act_section_802.html">strict requirements</a> for <a href="http://www.creditworthy.com/3jm/articles/cw90507.html">document retention</a>, with punishments to match: </p> <blockquote>"Whoever knowingly alters, destroys, mutilates, conceals, covers up, falsifies, or makes a false entry in any record, document, or tangible object with the intent to impede, obstruct, or influence the investigation or proper administration of any matter within the jurisdiction of any department or agency of the United States or any case filed under title 11, or in relation to or contemplation of any such matter or case, shall be fined under this title, imprisoned not more than 20 years, or both."</blockquote> <p>Similarly, in Canada investment companies must keep complete archives of all messages, in all kinds of media: </p> <blockquote>Pursuant to National Instrument 31-103 ... firms must retain records of their business activities, financial affairs, client transactions and communication. ... <b>The type of device used to transmit the communication or whether it is a firm issued or personal device is irrelevant.</b> Dealer Members must therefore design systems and programs with compliant record retention and retrieval functionalities for those methods of communication permitted at the firm. For instance, the content posted on social media websites, such as <B>Twitter, Facebook, blogs, chat rooms and all material transmitted through emails</b>, are subject to the above-noted legislative and regulatory requirements. <br/>&mdash; IIROC <a href="http://www.iiroc.ca/Documents/2011/dbed7d6a-ed1c-4a8b-b3d9-bef60412aa27_en.pdf">Guidelines for the review, supervision and retention of advertisements, sales literature and correspondence</a>, Section II </blockquote> <p>Wow! That sounds really hard! I wonder how US public companies and Canadian investment dealers can do this, while the government can't even upgrade their email servers without losing 8 months worth of archival data: </p> <blockquote>As it turned out, the entire migration process would take eight months. When the process extended beyond June 2014, MTICS forgot to instruct HPAS to do backups on a monthly basis. This meant that every government mailbox that migrated onto the new system went without a monthly backup until all mailboxes were migrated. Any daily backup that existed was expunged after 31 days. At its peak, some 48,000 government mailboxes were without monthly email backups.<br/>&mdash; OIPC <a href="https://www.oipc.bc.ca/investigation-reports/1874">Investigation Report F15-03</a>, Page 32 </blockquote> <p><img src="https://lh3.googleusercontent.com/-XHk-iEPpMBE/AAAAAAAAAAI/AAAAAAAAAAA/6ZPvqo28YIc/photo.jpg" style="float:right; height: 100px;"/>Corporations and investment banks can do this because high volume <a href="https://www.google.ca/?q=enterprise+email+archive">enterprise email archiving</a> has been a solved problem for well over a decade. So there are lots of options, <a href="http://www.messagesolution.com/email_archiving.htm">proprietary</a>, <a href="https://www.mailarchiva.com/#firstPage">open source</a>, and even <a href="http://globalrelay.com/">British Columbian</a>!</p> <p>Yep, one of the top companies in the electronic message archiving space, <a href="http://globalrelay.com/">Global Relay</a>, is actually headquartered in Vancouver! Guys! Wake up! Put a salesperson on the float-plane to Victoria on Monday!</p> <p>Right now, British Columbia doesn't have an enterprise email archive. It has an email server farm, with infrequent backup files, retained for only 18 months and requiring substantial effort to restore and search. Some of the advantages of an archive are: </p> <ul><li>The archive is separate from the users, they do not individually determine the retention schedule using their [DELETE] key, retention is applied enterprise-wide on the archive.</li><li>Archive searches are not done by users, they are done by the people who need access to the archive. In the case of corporate archives, that's usually the legal team. In the case of the government it would be the legal team and the FOI officers.</li><li>Archive searches can address the <b>whole</b> collection of email in one search. Current government FOI email searches are done computer-by-computer, by line staff who probably have better things to do.</li><li>The archive is separate from the operational mail delivery and mail box servers, so upgrades on the operation equipment do not affect the archive.</li></ul> <p>So, for the next little while, the Commissioner's narrow technical recommendations are fine (even though they make me want to tear my ears off): </p> <a href="http://2.bp.blogspot.com/-WcPaHLhhssY/VivThIrDjtI/AAAAAAAAAi8/pSu1CFJCGEU/s1600/screenshot_340.png" imageanchor="1" ><img border="0" src="http://2.bp.blogspot.com/-WcPaHLhhssY/VivThIrDjtI/AAAAAAAAAi8/pSu1CFJCGEU/s1600/screenshot_340.png" /></a> <p>But the real long-term technical solution to treating email as a document of record is... <b>start treating it as a document of record</b>! Archive it, permanently, in a searchable form, and don't let the end users set the retention policy. It's not rocket science, it's just computers. </p>";}i:5;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-4549607568200498022";s:7:"pubdate";s:31:"Thu, 15 Oct 2015 17:12:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-10-15T10:19:08.096-07:00";}s:8:"category";s:11:"foss4gvideo";s:5:"title";s:22:"Keynote at FOSS4G 2015";s:11:"description";s:1026:"<p>On my usual bi-annual schedule, I gave a keynote talk at <a href="http://2015.foss4g.org">FOSS4G this year in Seoul</a>, about the parallel pressures on open source that the move to cloud computing is providing. On the one hand, the cloud runs on open source. On the other hand, below the API layer the cloud is pretty much the opposite of open: it's as much a black box as the old Win32 API. And the growth of cloud is paralleled by the shrinkage of infrastructure maintainers in other venues; the kinds of folks who currently use and produce OSS. It's a big change coming down the highway.</p><p><iframe src="https://player.vimeo.com/video/142334723" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href="https://vimeo.com/142334723">Keynote Lecture 5: Where do we go from here? The next 10 years of open source geospatial &mdash; Paul Ramsey</a> from <a href="https://vimeo.com/foss4g">FOSS4G</a> on <a href="https://vimeo.com">Vimeo</a>.</p><p>&nbsp;</p>";s:4:"link";s:65:"http://blog.cleverelephant.ca/2015/10/keynote-at-foss4g-2015.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"1";}s:7:"summary";s:1026:"<p>On my usual bi-annual schedule, I gave a keynote talk at <a href="http://2015.foss4g.org">FOSS4G this year in Seoul</a>, about the parallel pressures on open source that the move to cloud computing is providing. On the one hand, the cloud runs on open source. On the other hand, below the API layer the cloud is pretty much the opposite of open: it's as much a black box as the old Win32 API. And the growth of cloud is paralleled by the shrinkage of infrastructure maintainers in other venues; the kinds of folks who currently use and produce OSS. It's a big change coming down the highway.</p><p><iframe src="https://player.vimeo.com/video/142334723" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href="https://vimeo.com/142334723">Keynote Lecture 5: Where do we go from here? The next 10 years of open source geospatial &mdash; Paul Ramsey</a> from <a href="https://vimeo.com/foss4g">FOSS4G</a> on <a href="https://vimeo.com">Vimeo</a>.</p><p>&nbsp;</p>";}i:6;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-2842206594566289376";s:7:"pubdate";s:31:"Fri, 09 Oct 2015 19:22:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-10-09T12:23:56.184-07:00";}s:8:"category";s:7:"bcmedia";s:5:"title";s:11:"Krugman FTW";s:11:"description";s:500:"<blockquote>"Sometimes I have the impression that many people in the media consider it uncouth to acknowledge, even to themselves, the fraudulence of much political posturing. The done thing, it seems, is to pretend that we’re having real debates about national security or economics even when it’s both obvious and easy to show that nothing of the kind is actually taking place."<br/>&mdash; <a href="http://www.nytimes.com/2015/10/09/opinion/its-all-benghazi.html">Paul Krugman</a></blockquote>";s:4:"link";s:54:"http://blog.cleverelephant.ca/2015/10/krugman-ftw.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:7:"summary";s:500:"<blockquote>"Sometimes I have the impression that many people in the media consider it uncouth to acknowledge, even to themselves, the fraudulence of much political posturing. The done thing, it seems, is to pretend that we’re having real debates about national security or economics even when it’s both obvious and easy to show that nothing of the kind is actually taking place."<br/>&mdash; <a href="http://www.nytimes.com/2015/10/09/opinion/its-all-benghazi.html">Paul Krugman</a></blockquote>";}i:7;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-3161806318875477758";s:7:"pubdate";s:31:"Tue, 11 Aug 2015 01:47:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-10-28T09:44:04.304-07:00";}s:8:"category";s:29:"data sciencehadooprstatistics";s:5:"title";s:37:"Big Data and Data Science Piss Me Off";s:11:"description";s:2510:"<p>Get off my lawn!</p> <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">.<a href="https://twitter.com/galvanize">@galvanize</a> bringing its 12-week Data Science boot camp to Denver. &#10;&#10;You still stoked about studying for the GRE ? <a href="http://t.co/EMFgUq0OFV">pic.twitter.com/EMFgUq0OFV</a></p>&mdash; Brian Timoney (@briantimoney) <a href="https://twitter.com/briantimoney/status/630906976508121088">August 11, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> <p>I don't talk about this much, but I actually trained in statistics, not in computer science, and I've been getting slowly but progressively weirded out by the whole "big data" / "data science" thing. Because so much of it is bogus, or boys-with-toys or something.</p> <p>Basically, my objections to the big data thing are the usual: probably your data is not big. It really isn't, and there are some <a href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">great</a> <a href="https://news.ycombinator.com/item?id=5696451">blog</a> <a href="https://www.compose.io/articles/you-dont-have-big-data/">posts</a> all about <a href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">that</a>.</p> <p>So that's point number one: most people blabbing on about big data can fit their problem onto a big vertical machine and analyze it to their heart's content in R or something.</p> <p>Point number two is less frequently touched upon: sure, you have 2 trillion records, but why do you need to look at all of them? The whole point of an education in statistics is to learn how to reason about a population using a <b>random sample</b>. So why are all these alleged "data scientists" firing up massive compute clusters to summarize <b>every single record in their collections</b>?</p> <p><img border="0" src="http://2.bp.blogspot.com/-1V7vEO_LsZc/VjD7F3ZypDI/AAAAAAAAAjQ/piC2pyiUQ40/s320/350px-Normal_Distribution_PDF.svg.png" style="float:right; padding:10px;" />I'm guessing it's the usual reason: because they can. And because the current meme is that they should. They should stand up a 100 node cluster on AWS and bloody well count all 2 trillion of them. Because: CPUs.</p> <p>But honestly, if you want to know the age distribution of people buying red socks, draw a sample of a couple hundred thousand records, and find out to within a fraction of a percentage point 19-times-out-of-20. After all, you're a freaking "data scientist", right?</p>";s:4:"link";s:80:"http://blog.cleverelephant.ca/2015/08/big-data-and-data-science-piss-me-off.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"9";}s:7:"summary";s:2510:"<p>Get off my lawn!</p> <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">.<a href="https://twitter.com/galvanize">@galvanize</a> bringing its 12-week Data Science boot camp to Denver. &#10;&#10;You still stoked about studying for the GRE ? <a href="http://t.co/EMFgUq0OFV">pic.twitter.com/EMFgUq0OFV</a></p>&mdash; Brian Timoney (@briantimoney) <a href="https://twitter.com/briantimoney/status/630906976508121088">August 11, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script> <p>I don't talk about this much, but I actually trained in statistics, not in computer science, and I've been getting slowly but progressively weirded out by the whole "big data" / "data science" thing. Because so much of it is bogus, or boys-with-toys or something.</p> <p>Basically, my objections to the big data thing are the usual: probably your data is not big. It really isn't, and there are some <a href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">great</a> <a href="https://news.ycombinator.com/item?id=5696451">blog</a> <a href="https://www.compose.io/articles/you-dont-have-big-data/">posts</a> all about <a href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">that</a>.</p> <p>So that's point number one: most people blabbing on about big data can fit their problem onto a big vertical machine and analyze it to their heart's content in R or something.</p> <p>Point number two is less frequently touched upon: sure, you have 2 trillion records, but why do you need to look at all of them? The whole point of an education in statistics is to learn how to reason about a population using a <b>random sample</b>. So why are all these alleged "data scientists" firing up massive compute clusters to summarize <b>every single record in their collections</b>?</p> <p><img border="0" src="http://2.bp.blogspot.com/-1V7vEO_LsZc/VjD7F3ZypDI/AAAAAAAAAjQ/piC2pyiUQ40/s320/350px-Normal_Distribution_PDF.svg.png" style="float:right; padding:10px;" />I'm guessing it's the usual reason: because they can. And because the current meme is that they should. They should stand up a 100 node cluster on AWS and bloody well count all 2 trillion of them. Because: CPUs.</p> <p>But honestly, if you want to know the age distribution of people buying red socks, draw a sample of a couple hundred thousand records, and find out to within a fraction of a percentage point 19-times-out-of-20. After all, you're a freaking "data scientist", right?</p>";}i:8;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-8526107745197894120";s:7:"pubdate";s:31:"Wed, 15 Jul 2015 23:05:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-07-15T19:03:15.583-07:00";}s:8:"category";s:42:"bcenterprisehpitoutsourcingpublic accounts";s:5:"title";s:25:"BC IT Outsourcing 2014/15";s:11:"description";s:3083:"<p>If what goes up must come down, nobody told BC's IT outsourcers, because they continue to gobble up a larger chunk of the government pie every year.</p> <p>The <a href="http://www.fin.gov.bc.ca/ocg/pa/14_15/Pa14_15.htm">BC Public Accounts</a> came out today, and I'm happy to say that the People Who Are Smarter Than You Are managed to book <b>another record year of billings</b>: a <b>$468,549,154</b> spend, up <b>8%</b> over last year.</p> <p><img src="https://docs.google.com/a/cleverelephant.ca/spreadsheet/oimg?key=0AsM7ePw4lyCDdEpEUXZlZWNSRXZtQXZmeVNVajhvRmc&oid=4&zx=3jhlqyn4bbus" width="600" /></p> <p>It's not a victory unless you beat someone else, so good news:</p> <ul><li>Overall government revenue, up <b>5.4%</b></li><li>Overall government spending, up <b>2.4%</b></li><li>Health spending, up <b>2.8%</b></li><li>Education spending, up <b>0%</b></li><li><b><i>IT services spending up 8%!!!!</i></b></li></ul> <p>Don't be sad, kids and sick people, IT services folks are <i>Adding Value</i> and <i>Finding Synergies</i> in ways that you just can't. In the long run, workshopping the new <i>Management Strategy Realignment Plan</i> is just a better investment than fixing your gimpy hip, or hiring a teaching assistant to help Angry Jimmy focus on his work.</p> <p><b>HP Advanced Solutions</b> continues to dominate the category, adding $20M in billings this year alone (How many teachers could that hire? <a href="http://www.theglobeandmail.com/news/british-columbia/battle-of-numbers-how-much-does-an-average-teacher-make/article17309702/">At least 200</a>. Or even more teaching assistants.) In fact, two thirds of the billing growth this year was just HP.<p> <p><img src="https://docs.google.com/a/cleverelephant.ca/spreadsheet/oimg?key=0AsM7ePw4lyCDdEpEUXZlZWNSRXZtQXZmeVNVajhvRmc&oid=2&zx=uj2istl3ypgt"  width="600"/></p> <p>There's also a new kid in the enterprise software vendor list to keep an eye on: <a href="http://salesforce.com">Salesforce.com</a> (SFDC) showed up with a wee $463,053 in billings this year. I expect that to increase mightily in coming years. However, the big money in SFDC work will not be earned by SFDC (even after locking up the entire BC government enterprise back-office, Oracle bills less than $10M a year in software maintenance), but by the consultants providing SFDC "implementation services" (Deloitte, CGI, HP). Watch for a SFDC goldrush as the government starts replacing expensive Oracle systems with... expensive SFDC systems in the cloud.</p> <p>The best part about hiring big public companies enterprise IT like HP, Oracle, Maximus, and CGI to create lots of important <i>Technology Process</i> (and occasionally a bit of <i>Product</i>) for us isn't the soothingly glacial pace of progress or the fantastic billing rates. It's knowing that at least 20% of every public dollar spent goes straight to the bottom line of those companies, ensuring that shareholders and institutional investors survive through another year without undue financial hardship.</p> <p>Until next year, keep on spending, British Columbia!</p>";s:4:"link";s:67:"http://blog.cleverelephant.ca/2015/07/bc-it-outsourcing-201415.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"7";}s:7:"summary";s:3083:"<p>If what goes up must come down, nobody told BC's IT outsourcers, because they continue to gobble up a larger chunk of the government pie every year.</p> <p>The <a href="http://www.fin.gov.bc.ca/ocg/pa/14_15/Pa14_15.htm">BC Public Accounts</a> came out today, and I'm happy to say that the People Who Are Smarter Than You Are managed to book <b>another record year of billings</b>: a <b>$468,549,154</b> spend, up <b>8%</b> over last year.</p> <p><img src="https://docs.google.com/a/cleverelephant.ca/spreadsheet/oimg?key=0AsM7ePw4lyCDdEpEUXZlZWNSRXZtQXZmeVNVajhvRmc&oid=4&zx=3jhlqyn4bbus" width="600" /></p> <p>It's not a victory unless you beat someone else, so good news:</p> <ul><li>Overall government revenue, up <b>5.4%</b></li><li>Overall government spending, up <b>2.4%</b></li><li>Health spending, up <b>2.8%</b></li><li>Education spending, up <b>0%</b></li><li><b><i>IT services spending up 8%!!!!</i></b></li></ul> <p>Don't be sad, kids and sick people, IT services folks are <i>Adding Value</i> and <i>Finding Synergies</i> in ways that you just can't. In the long run, workshopping the new <i>Management Strategy Realignment Plan</i> is just a better investment than fixing your gimpy hip, or hiring a teaching assistant to help Angry Jimmy focus on his work.</p> <p><b>HP Advanced Solutions</b> continues to dominate the category, adding $20M in billings this year alone (How many teachers could that hire? <a href="http://www.theglobeandmail.com/news/british-columbia/battle-of-numbers-how-much-does-an-average-teacher-make/article17309702/">At least 200</a>. Or even more teaching assistants.) In fact, two thirds of the billing growth this year was just HP.<p> <p><img src="https://docs.google.com/a/cleverelephant.ca/spreadsheet/oimg?key=0AsM7ePw4lyCDdEpEUXZlZWNSRXZtQXZmeVNVajhvRmc&oid=2&zx=uj2istl3ypgt"  width="600"/></p> <p>There's also a new kid in the enterprise software vendor list to keep an eye on: <a href="http://salesforce.com">Salesforce.com</a> (SFDC) showed up with a wee $463,053 in billings this year. I expect that to increase mightily in coming years. However, the big money in SFDC work will not be earned by SFDC (even after locking up the entire BC government enterprise back-office, Oracle bills less than $10M a year in software maintenance), but by the consultants providing SFDC "implementation services" (Deloitte, CGI, HP). Watch for a SFDC goldrush as the government starts replacing expensive Oracle systems with... expensive SFDC systems in the cloud.</p> <p>The best part about hiring big public companies enterprise IT like HP, Oracle, Maximus, and CGI to create lots of important <i>Technology Process</i> (and occasionally a bit of <i>Product</i>) for us isn't the soothingly glacial pace of progress or the fantastic billing rates. It's knowing that at least 20% of every public dollar spent goes straight to the bottom line of those companies, ensuring that shareholders and institutional investors survive through another year without undue financial hardship.</p> <p>Until next year, keep on spending, British Columbia!</p>";}i:9;a:10:{s:4:"guid";s:57:"tag:blogger.com,1999:blog-14903426.post-22106059530918359";s:7:"pubdate";s:31:"Mon, 27 Apr 2015 23:18:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-05-07T05:39:35.750-07:00";}s:8:"category";s:21:"bcbcpolimoneypolitics";s:5:"title";s:21:"More Speech for Money";s:11:"description";s:3250:"<p>The BC Liberal government is changing the Elections Act to allow <b>unlimited party and candidate spending within one month of election day</b> and meanwhile, as usual, the media are transfixed by the shiny object in the corner.</p> <img src="https://www.svncanada.com/gfx/layout/pages/affiliates-money-pile-canadian.jpg" style="float:right;"/><p>The political pundits are making a great deal of noise (see V. Palmer's <a href="http://www.vancouversun.com/news/Vaughn+Palmer+Ground+shifts+under+political+parties+voter+list/11002488/story.html">inside baseball assessment</a> if you care) about an amendment to the Elections Act that says that:</p> <blockquote>"the chief electoral officer must provide &hellip; to a registered political party, in respect of a general election &hellip; a list of voters that indicates which voters on the list voted in the general election"</blockquote> <p>At the same time, they are <b>ignoring</b> the BC Liberals fundamentally changing the money dynamic of the fixed election date by <b>eliminating the 60-day "pre-campaign" period</b>.</p> <blockquote>"Section 198 is amended (a) by repealing subsections (1) and (2) and substituting the following: (1) In respect of a general election, the total value of election expenses incurred by a registered political party during the campaign period must not exceed $4.4 million."</blockquote> <p>The Elections Act currently divides up the election period before a fixed election into two "halves": the 60 days before the official campaign, and the campaign period itself (about 28 days if I recall correctly). In the first 60 days, candidates can spend a maximum of $70,000 and parties a maximum of $1.1 million. In the campaign period, candidates can spend another $70,000 and parties as much as $4.4 million.</p> <p>The intent of the "pre-campaign" period is clearly to focus campaigning on the campaign period itself, by limiting the amount of early spending by parties. The "money density" of the pre-campaign period is about $18,000 / day in party spending; in the campaign period, it is almost $160,000 / day.</p> <p>This is all very public-spirited, and contributes to a nice focussed election period. But (<b>BUT!</b>) the BC Liberals currently have <a href="http://www.theprovince.com/business/Liberal+Party+raised+million+2014+official+records+show/10955807/story.html">more money than they know what to do with</a>, so it is in their interest to be able to focus all that money as close to the event as possible. And rather than simply raising the pre-campaign spending limit they went one better: they removed it all together. They can spend <b>unlimited amounts of money</b> as close as 28 days before election day, 21 days before the opening of advance polls. </p> <p>Let me repeat that: they can spend <b>unlimited amounts of money</b>.</p> <p>So in British Columbia now, it is legal to both <b>raise</b> unlimited amounts of money from corporations, unions and individuals in any amounts at all (and some individuals and corporations have donated to the BC Liberals, individually, over $100,000 a year), and it is legal to <b>spend</b> unlimited amounts of money, right up to within 28 days of the election day.</p> <p>See any problems with that?</p>";s:4:"link";s:64:"http://blog.cleverelephant.ca/2015/04/more-speech-for-money.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:7:"summary";s:3250:"<p>The BC Liberal government is changing the Elections Act to allow <b>unlimited party and candidate spending within one month of election day</b> and meanwhile, as usual, the media are transfixed by the shiny object in the corner.</p> <img src="https://www.svncanada.com/gfx/layout/pages/affiliates-money-pile-canadian.jpg" style="float:right;"/><p>The political pundits are making a great deal of noise (see V. Palmer's <a href="http://www.vancouversun.com/news/Vaughn+Palmer+Ground+shifts+under+political+parties+voter+list/11002488/story.html">inside baseball assessment</a> if you care) about an amendment to the Elections Act that says that:</p> <blockquote>"the chief electoral officer must provide &hellip; to a registered political party, in respect of a general election &hellip; a list of voters that indicates which voters on the list voted in the general election"</blockquote> <p>At the same time, they are <b>ignoring</b> the BC Liberals fundamentally changing the money dynamic of the fixed election date by <b>eliminating the 60-day "pre-campaign" period</b>.</p> <blockquote>"Section 198 is amended (a) by repealing subsections (1) and (2) and substituting the following: (1) In respect of a general election, the total value of election expenses incurred by a registered political party during the campaign period must not exceed $4.4 million."</blockquote> <p>The Elections Act currently divides up the election period before a fixed election into two "halves": the 60 days before the official campaign, and the campaign period itself (about 28 days if I recall correctly). In the first 60 days, candidates can spend a maximum of $70,000 and parties a maximum of $1.1 million. In the campaign period, candidates can spend another $70,000 and parties as much as $4.4 million.</p> <p>The intent of the "pre-campaign" period is clearly to focus campaigning on the campaign period itself, by limiting the amount of early spending by parties. The "money density" of the pre-campaign period is about $18,000 / day in party spending; in the campaign period, it is almost $160,000 / day.</p> <p>This is all very public-spirited, and contributes to a nice focussed election period. But (<b>BUT!</b>) the BC Liberals currently have <a href="http://www.theprovince.com/business/Liberal+Party+raised+million+2014+official+records+show/10955807/story.html">more money than they know what to do with</a>, so it is in their interest to be able to focus all that money as close to the event as possible. And rather than simply raising the pre-campaign spending limit they went one better: they removed it all together. They can spend <b>unlimited amounts of money</b> as close as 28 days before election day, 21 days before the opening of advance polls. </p> <p>Let me repeat that: they can spend <b>unlimited amounts of money</b>.</p> <p>So in British Columbia now, it is legal to both <b>raise</b> unlimited amounts of money from corporations, unions and individuals in any amounts at all (and some individuals and corporations have donated to the BC Liberals, individually, over $100,000 a year), and it is legal to <b>spend</b> unlimited amounts of money, right up to within 28 days of the election day.</p> <p>See any problems with that?</p>";}i:10;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-2661395897126409101";s:7:"pubdate";s:31:"Mon, 27 Apr 2015 16:17:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-04-27T09:17:32.962-07:00";}s:8:"category";s:7:"gisrant";s:5:"title";s:17:"GIS "Data Models"";s:11:"description";s:950:"<p>Most IT professionals have some expectation, having received a basic education on relational data modelling, that a model for a medium sized problem might look like this:</p> <img src="http://lecture.cs.buu.ac.th/~piya/StudyWork/E-Commerce/All_diagram/ER%20Diagram/ER%20Diagram.jpg" width=500 /> <p>Why is it, then, that production GIS data flows so consistently produce models that look like this:</p> <a href="http://1.bp.blogspot.com/-pfiwJjLhL1o/VT5gmjPW9EI/AAAAAAAAAfY/dX6WtAst4Z8/s1600/screenshot_269.png" imageanchor="1" ><img border="0" src="http://1.bp.blogspot.com/-pfiwJjLhL1o/VT5gmjPW9EI/AAAAAAAAAfY/dX6WtAst4Z8/s1600/screenshot_269.png" /></a> <p>What is wrong with us?!?? I bring up this rant only because I was just told that some users find the PostgreSQL 1600 column limit <b>constraining</b> since it makes it hard to import the Esri census data, which are "modelled" into tables that are presumably wider than they are long.</p>";s:4:"link";s:58:"http://blog.cleverelephant.ca/2015/04/gis-data-models.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"7";}s:7:"summary";s:950:"<p>Most IT professionals have some expectation, having received a basic education on relational data modelling, that a model for a medium sized problem might look like this:</p> <img src="http://lecture.cs.buu.ac.th/~piya/StudyWork/E-Commerce/All_diagram/ER%20Diagram/ER%20Diagram.jpg" width=500 /> <p>Why is it, then, that production GIS data flows so consistently produce models that look like this:</p> <a href="http://1.bp.blogspot.com/-pfiwJjLhL1o/VT5gmjPW9EI/AAAAAAAAAfY/dX6WtAst4Z8/s1600/screenshot_269.png" imageanchor="1" ><img border="0" src="http://1.bp.blogspot.com/-pfiwJjLhL1o/VT5gmjPW9EI/AAAAAAAAAfY/dX6WtAst4Z8/s1600/screenshot_269.png" /></a> <p>What is wrong with us?!?? I bring up this rant only because I was just told that some users find the PostgreSQL 1600 column limit <b>constraining</b> since it makes it hard to import the Esri census data, which are "modelled" into tables that are presumably wider than they are long.</p>";}i:11;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-6718080837604564215";s:7:"pubdate";s:31:"Sat, 21 Mar 2015 16:16:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-03-21T09:16:17.482-07:00";}s:8:"category";s:12:"postgisvideo";s:5:"title";s:15:"Magical PostGIS";s:11:"description";s:564:"<p>I did a new PostGIS talk for <a href="https://2015.foss4g-na.org/">FOSS4G North America 2015</a>, an exploration of some of the tidbits I've learned over the past six months about using <a href="http://postgresql.org">PostgreSQL</a> and <a href="http://postgis.net">PostGIS</a> together to make "magic" (any sufficiently advanced technology<a href="http://www.brainyquote.com/quotes/quotes/a/arthurccl101182.html">...</a>)</p><iframe width="420" height="315" src="https://www.youtube.com/embed/Y0SBkjcyXOc" frameborder="0" allowfullscreen></iframe><p>&nbsp;</p>";s:4:"link";s:58:"http://blog.cleverelephant.ca/2015/03/magical-postgis.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"3";}s:7:"summary";s:564:"<p>I did a new PostGIS talk for <a href="https://2015.foss4g-na.org/">FOSS4G North America 2015</a>, an exploration of some of the tidbits I've learned over the past six months about using <a href="http://postgresql.org">PostgreSQL</a> and <a href="http://postgis.net">PostGIS</a> together to make "magic" (any sufficiently advanced technology<a href="http://www.brainyquote.com/quotes/quotes/a/arthurccl101182.html">...</a>)</p><iframe width="420" height="315" src="https://www.youtube.com/embed/Y0SBkjcyXOc" frameborder="0" allowfullscreen></iframe><p>&nbsp;</p>";}i:12;a:10:{s:4:"guid";s:58:"tag:blogger.com,1999:blog-14903426.post-572071594835044806";s:7:"pubdate";s:31:"Fri, 20 Mar 2015 20:07:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-03-20T13:07:02.435-07:00";}s:8:"category";s:7:"postgis";s:5:"title";s:24:"Making Lines from Points";s:11:"description";s:1133:"<p>Somehow I've gotten through 10 years of SQL without ever learning this construction, which I found while proof-reading a <a href="https://twitter.com/sanderpick">colleague's</a> blog post and looked so unlikely that I had to test it before I believed it actually worked. Just goes to show, there's always something new to learn.</p> <p>Suppose you have a GPS location table:</p><ul><li><b>gps_id</b>: integer</li><li><b>geom</b>: geometry</li><li><b>gps_time</b>: timestamp</li><li><b>gps_track_id</b>: integer</li></ul><p>You can get a correct set of lines from this collection of points with just this SQL:</p><pre><br />SELECT <br />  gps_track_id, <br />  ST_MakeLine(geom ORDER BY gps_time ASC) AS geom <br />FROM gps_poinst<br />GROUP BY gps_track_id<br /></pre><p>Those of you who already knew about placing <code>ORDER BY</code> within an aggregate function are going "duh", and the rest of you are, like me, going "whaaaaaa?"</p><p>Prior to this, I would solve this problem by ordering all the groups in a CTE or sub-query first, and only then pass them to the aggregate make-line function. This, is, so, much, nicer.</p>";s:4:"link";s:67:"http://blog.cleverelephant.ca/2015/03/making-lines-from-points.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"2";}s:7:"summary";s:1133:"<p>Somehow I've gotten through 10 years of SQL without ever learning this construction, which I found while proof-reading a <a href="https://twitter.com/sanderpick">colleague's</a> blog post and looked so unlikely that I had to test it before I believed it actually worked. Just goes to show, there's always something new to learn.</p> <p>Suppose you have a GPS location table:</p><ul><li><b>gps_id</b>: integer</li><li><b>geom</b>: geometry</li><li><b>gps_time</b>: timestamp</li><li><b>gps_track_id</b>: integer</li></ul><p>You can get a correct set of lines from this collection of points with just this SQL:</p><pre><br />SELECT <br />  gps_track_id, <br />  ST_MakeLine(geom ORDER BY gps_time ASC) AS geom <br />FROM gps_poinst<br />GROUP BY gps_track_id<br /></pre><p>Those of you who already knew about placing <code>ORDER BY</code> within an aggregate function are going "duh", and the rest of you are, like me, going "whaaaaaa?"</p><p>Prior to this, I would solve this problem by ordering all the groups in a CTE or sub-query first, and only then pass them to the aggregate make-line function. This, is, so, much, nicer.</p>";}i:13;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-9202532208332066627";s:7:"pubdate";s:31:"Wed, 18 Mar 2015 22:35:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-03-19T11:43:08.148-07:00";}s:8:"category";s:41:"bcdeloitteenterprise ITnrs transformation";s:5:"title";s:21:"Deloitte's Second Act";s:11:"description";s:6189:"<p>Hot off their success <a href="/2012/06/more-icm.html">transforming the BC social services sector with "integrated case management"</a>, Deloitte is now heavily staffing the upcoming transformation of the IT systems that underpin our natural resource management ministries.</p> <p><b>Interlude:</b> I should briefly note here that Deloitte's work in social services involved building a $180,000,000 case management system that the people who use it generally <a href="http://www.theglobeandmail.com/news/british-columbia/bc-child-services-hampered-by-deeply-flawed-computer-system/article21472981/">do not like</a>, using software that <a href="http://www.vancouversun.com/life/alone+using+troubled+software+system+manage+child+welfare/10023345/story.html">nobody else uses</a> for social services, that <a href="http://www.vancouversun.com/technology/government+million+computer+system+just+work+with+video/9840193/story.html">went offline</a> for several consecutive days last year, and based on software that basically entered end-of-life almost <a href="http://en.wikipedia.org/wiki/Siebel_Systems#Key_dates">five years ago</a>. I'm sure that's not Deloitte's fault, they are only the <b>international experts hired to advise on the best ways to build the system and then actually build it</b>.</p> <div style="float: right; padding: 10px;"><p align="center"><img src="http://www.pwc.co.uk/en_uk/uk/assets/images/epm-decision-making.jpg"><br/><i>So many shiny arrows!<br/>Smells like management consultants...</i></p></div> <p>Anyhow...</p> <p>The brain trust has now decided that the thing we need on the land base is "integrated decision making", presumably because everything tastes better "integrated". A <a href="http://www.uvic.ca/hsd/publicadmin/graduate/index.php">UVic MPA</a> student has done a <a href="https://dspace.library.uvic.ca:8443/bitstream/handle/1828/5038/Lesley_Valour_MPA_2013.pdf">complete write-up</a> of the scheme&mdash;and I challenge you to find the hard centre inside this chewey mess of an idea&mdash;but here's a representative sample:</p><blockquote>The IDM initiative is an example of horizontal management because it is an initiative among non‐hierarchical ministries focused on gaining efficiencies by harmonizing regulations, IT systems and business processes for the betterment of the NRS as a whole. Horizontal management is premised on joint or consensual decision making rather than a more traditional vertical hierarchy.  Horizontal collaborations create links and share information, goodwill, resources, and power or capabilities by organizations in two or more sectors to achieve jointly what they cannot achieve individually.  </blockquote><p>Sounds great, right!?! Just the sort of thing I'd choose to manage billions of dollars in natural resources! (I jest.)</p><p>Of course, the brain trust really isn't all that interested in "horizontal management", what has them hot and bothered about "integrated decision making" is that it's an opportunity to spend money on "IT systems and business processes". Yay!</p><p>To that end, they carefully prepared a business case for Treasury Board, asking for well north of $100M to rewrite every land management system in government. Forests, lands, oil and gas, heritage, the whole kit and caboodle. The <a href="http://docs.openinfo.gov.bc.ca/D60134714A_Response_Package_FNR-2014-00169.PDF">business case</a> says:</p><blockquote>IDM will improve the ability of the six ministries and many agencies in the NRS to work together to provide seamless, high‐quality service to proponents and the public, to provide effective resource stewardship across the province, to effectively consult with First Nations in natural resource decisions, and to contribute to cross‐government priorities. </blockquote><p>Sounds ambitious! I wonder how they're going to accomplish this feat of re-engineering? Well, I'm going to keep on wondering, because <a href="/2014/11/my-tax-dollars-at-work.html">they redacted everything in the business case except the glowing hyperbole</a>.</p><p>However, even though we don't know <b>how</b>, or really <b>why</b>, they are embarking on this grand adventure, we can rest assured that they are now spending money at a rate of about $10M / year making it happen, much of it on <b>our good friends Deloitte.</b></p><ul><li>There are currently <b>80 consultants</b> billing on what has been christened the "Natural Resource Sector Transformation Secretariat".</li><p align="center"><img src="https://espn.go.com/i/sportscentury/inline/secretariat.jpg"><br/><i>Not that Secretariat...</i></p><li>Of those consultants <b>34 are (so far) from Deloitte</b>.</li><li>Coincidentally, <b>34 is also the number of government staff</b> working at the Secretariat.</li><li>So, 114 staff, of which 34 are government employees and the rest are contractors. How many government employees does it take to change a lightbulb? Let me take that to procurement and I'll get back to you.</li></ul><p>The FOI system charged me $120 (and only after I bargained down my request to a much less informative one) to <a href="http://docs.openinfo.gov.bc.ca/Response_Package_FNR-2014-50149.pdf">find the above out</a>, because they felt that the information did not meet the test of being "of public interest". If you feel it actually <b>is</b> in the public interest to learn where our $100M on IT services for natural resources are being spent, and you live in BC, please leave me a comment on this post.</p><p><b>Interlude:</b> The test for whether fees should be waived is double barrelled, but is (hilariously) decided by the public body itself (soooo unbiased). Here are the tests I think I pass (but they don't):</p><ol><li>Do the records show how the public body is allocating financial or other resources? </li><li>Is your primary purpose to disseminate information in a way that could reasonably be expected to benefit the public, or to serve a private interest?</ol><p>I'm still digging for more information (like, how is it that Deloitte can bill out 34 staff on this project when there hasn't been a major RFP for it yet?) so stay tuned and send me any hints if you have them.</p>";s:4:"link";s:63:"http://blog.cleverelephant.ca/2015/03/deloittes-second-act.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"6";}s:7:"summary";s:6189:"<p>Hot off their success <a href="/2012/06/more-icm.html">transforming the BC social services sector with "integrated case management"</a>, Deloitte is now heavily staffing the upcoming transformation of the IT systems that underpin our natural resource management ministries.</p> <p><b>Interlude:</b> I should briefly note here that Deloitte's work in social services involved building a $180,000,000 case management system that the people who use it generally <a href="http://www.theglobeandmail.com/news/british-columbia/bc-child-services-hampered-by-deeply-flawed-computer-system/article21472981/">do not like</a>, using software that <a href="http://www.vancouversun.com/life/alone+using+troubled+software+system+manage+child+welfare/10023345/story.html">nobody else uses</a> for social services, that <a href="http://www.vancouversun.com/technology/government+million+computer+system+just+work+with+video/9840193/story.html">went offline</a> for several consecutive days last year, and based on software that basically entered end-of-life almost <a href="http://en.wikipedia.org/wiki/Siebel_Systems#Key_dates">five years ago</a>. I'm sure that's not Deloitte's fault, they are only the <b>international experts hired to advise on the best ways to build the system and then actually build it</b>.</p> <div style="float: right; padding: 10px;"><p align="center"><img src="http://www.pwc.co.uk/en_uk/uk/assets/images/epm-decision-making.jpg"><br/><i>So many shiny arrows!<br/>Smells like management consultants...</i></p></div> <p>Anyhow...</p> <p>The brain trust has now decided that the thing we need on the land base is "integrated decision making", presumably because everything tastes better "integrated". A <a href="http://www.uvic.ca/hsd/publicadmin/graduate/index.php">UVic MPA</a> student has done a <a href="https://dspace.library.uvic.ca:8443/bitstream/handle/1828/5038/Lesley_Valour_MPA_2013.pdf">complete write-up</a> of the scheme&mdash;and I challenge you to find the hard centre inside this chewey mess of an idea&mdash;but here's a representative sample:</p><blockquote>The IDM initiative is an example of horizontal management because it is an initiative among non‐hierarchical ministries focused on gaining efficiencies by harmonizing regulations, IT systems and business processes for the betterment of the NRS as a whole. Horizontal management is premised on joint or consensual decision making rather than a more traditional vertical hierarchy.  Horizontal collaborations create links and share information, goodwill, resources, and power or capabilities by organizations in two or more sectors to achieve jointly what they cannot achieve individually.  </blockquote><p>Sounds great, right!?! Just the sort of thing I'd choose to manage billions of dollars in natural resources! (I jest.)</p><p>Of course, the brain trust really isn't all that interested in "horizontal management", what has them hot and bothered about "integrated decision making" is that it's an opportunity to spend money on "IT systems and business processes". Yay!</p><p>To that end, they carefully prepared a business case for Treasury Board, asking for well north of $100M to rewrite every land management system in government. Forests, lands, oil and gas, heritage, the whole kit and caboodle. The <a href="http://docs.openinfo.gov.bc.ca/D60134714A_Response_Package_FNR-2014-00169.PDF">business case</a> says:</p><blockquote>IDM will improve the ability of the six ministries and many agencies in the NRS to work together to provide seamless, high‐quality service to proponents and the public, to provide effective resource stewardship across the province, to effectively consult with First Nations in natural resource decisions, and to contribute to cross‐government priorities. </blockquote><p>Sounds ambitious! I wonder how they're going to accomplish this feat of re-engineering? Well, I'm going to keep on wondering, because <a href="/2014/11/my-tax-dollars-at-work.html">they redacted everything in the business case except the glowing hyperbole</a>.</p><p>However, even though we don't know <b>how</b>, or really <b>why</b>, they are embarking on this grand adventure, we can rest assured that they are now spending money at a rate of about $10M / year making it happen, much of it on <b>our good friends Deloitte.</b></p><ul><li>There are currently <b>80 consultants</b> billing on what has been christened the "Natural Resource Sector Transformation Secretariat".</li><p align="center"><img src="https://espn.go.com/i/sportscentury/inline/secretariat.jpg"><br/><i>Not that Secretariat...</i></p><li>Of those consultants <b>34 are (so far) from Deloitte</b>.</li><li>Coincidentally, <b>34 is also the number of government staff</b> working at the Secretariat.</li><li>So, 114 staff, of which 34 are government employees and the rest are contractors. How many government employees does it take to change a lightbulb? Let me take that to procurement and I'll get back to you.</li></ul><p>The FOI system charged me $120 (and only after I bargained down my request to a much less informative one) to <a href="http://docs.openinfo.gov.bc.ca/Response_Package_FNR-2014-50149.pdf">find the above out</a>, because they felt that the information did not meet the test of being "of public interest". If you feel it actually <b>is</b> in the public interest to learn where our $100M on IT services for natural resources are being spent, and you live in BC, please leave me a comment on this post.</p><p><b>Interlude:</b> The test for whether fees should be waived is double barrelled, but is (hilariously) decided by the public body itself (soooo unbiased). Here are the tests I think I pass (but they don't):</p><ol><li>Do the records show how the public body is allocating financial or other resources? </li><li>Is your primary purpose to disseminate information in a way that could reasonably be expected to benefit the public, or to serve a private interest?</ol><p>I'm still digging for more information (like, how is it that Deloitte can bill out 34 staff on this project when there hasn't been a major RFP for it yet?) so stay tuned and send me any hints if you have them.</p>";}i:14;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-7372174568774204581";s:7:"pubdate";s:31:"Thu, 19 Feb 2015 18:38:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-02-19T10:40:45.310-08:00";}s:8:"category";s:25:"gdalgeotiffgisimageryjpeg";s:5:"title";s:31:"GeoTiff Compression for Dummies";s:11:"description";s:5114:"<p>"What's the best image format for map serving?" they ask me, shortly after I tell them not to serve their images from inside a database. </p><p>"Is it MrSid? Or ECW? those are nice and small." Which indeed they are. Unfortunately, outside of proprietary image server software I've never seen them be <b>fast</b> and nice and small at the same time. Generally the decode step is incredibly CPU intensive, presumably because of the fancy wavelet math that makes them so small in the first place.</p><p>"So, what's the best image format for map serving?".</p><p>In my experience, the best format for image serving, using open source rendering engines (MapServer, GeoServer, Mapnik) is: <b>GeoTIFF, with JPEG compression, internally tiled, in the YCBCR color space, with internal overviews</b>. Unfortunately, GeoTiffs are almost never delivered this way, as I was reminded today while downloading a sample image from the <a href="http://www.kamloops.ca/maps/disclaimer.html">City of Kamloops</a> (But nonetheless, thanks for the great free imagery, Kamloops!)</p><p><code>5255C.zip [593M]</code></p><p>It came in a 593Mb ZIP file. "Hm, that's pretty big, I thought." I unzipped it.</p><p><code>5255C.tif [515M]</code></p><p>Unzipped it was a 515Mb TIF file. That's right, it was smaller "uncompressed". Why? Because internally it was already compressed, and applying the ZIP compression algorithm to already compressed data generally fluffs it up a little. Whoops.</p><p>The default TIFF compression is, unfortunately, "<a href="http://en.wikipedia.org/wiki/Huffman_coding">deflate</a>", the same as that used for ZIP. This is a lossless encoding, but not very good for imagery. We can make the image a whole lot smaller just by using a more appropriate compression, like JPEG. We'll also tile it internally while we're at it. Internal tiling allows renderers to quickly pick out and decompress just a small portion of the image, which is important once you've applied a more serious compression algorithm like JPEG.</p><pre>gdal_translate \<br />  -co COMPRESS=JPEG \<br />  -co TILED=YES \<br />  5255C.tif 5255C_JPEG.tif</pre><p>This is much better, now we have a vastly smaller file.</p><p><code>5255C_JPEG.tif [67M]</code></p><p>But we can still do better! For reasons that well pass my understanding, the JPEG algorithm is more effective against images that are stored in the <a href="http://en.wikipedia.org/wiki/YCbCr">YCBCR</a> color space. Mine is not to reason why, though.</p><pre>gdal_translate \<br />  -co COMPRESS=JPEG \<br />  -co PHOTOMETRIC=YCBCR \<br />  -co TILED=YES \<br />  5255C.tif 5255C_JPEG_YCBCR.tif</pre><p>Wow, now we're down to 1/20 the size of the original.</p><p><code>5255C_JPEG_YCBCR.tif [24M]</code></p><p>But, we've applied a "lossy" algorithm, JPEG, maybe we've ruined the data! Let's have a look.</p><table border="0"><tr><td><a href="http://2.bp.blogspot.com/-ViYOsz3kMUY/VOYqHnXIlDI/AAAAAAAAAdU/0OGl8u9TmMs/s1600/example_jpg.png" imageanchor="1" ><img border="0" height="300" src="http://2.bp.blogspot.com/-ViYOsz3kMUY/VOYqHnXIlDI/AAAAAAAAAdU/0OGl8u9TmMs/s1600/example_jpg.png" /></a></td><td><a href="http://4.bp.blogspot.com/-EqVHMQXxWy0/VOYqH15vBaI/AAAAAAAAAdY/XS5ffZl1mME/s1600/example_lzw.png" imageanchor="1" ><img border="0" height="300" src="http://4.bp.blogspot.com/-EqVHMQXxWy0/VOYqH15vBaI/AAAAAAAAAdY/XS5ffZl1mME/s1600/example_lzw.png" /></a></td></tr><tr><th>Original</th><th>After JPEG/YCBCR</th></tr></table><p>Can you see the difference? Me neither. Using a JPEG "quality" level of 75%, there are no visible artefacts. In general, JPEG is very good at compressing things so humans "can't see" the lost information. I'd never use it for compressing a DEM or a data raster, but for a visual image, I use JPEG with impunity, and with much lower quality settings too (for more space saved).</p><p>Finally, for high speed serving at more zoomed out scales, we need to add overviews to the image. We'll make sure the overviews use the same, high compression options as the base data.</p><pre>gdaladdo \<br />  --config COMPRESS_OVERVIEW JPEG \<br />  --config PHOTOMETRIC_OVERVIEW YCBCR \<br />  --config INTERLEAVE_OVERVIEW PIXEL \<br />  -r average \<br />  5255C_JPEG_YCBCR.tif \<br />  2 4 8 16</pre><p>For reasons passing understanding, <code>gdaladdo</code> uses a different set of command-line switches to pass the configuration info to the compressor than <code>gdal_translate</code> does, but as before, mine is not to reason why.</p><p>The final size, now <b>with</b> overviews as well as the original data, is still less that 1/10 the size of the original.</p><p><code>5255C_JPEG_YCBCR.tif [37M]</code></p><p>So, to sum up, your best format for image serving is:</p><ul><li>GeoTiff, so you can avoid proprietary image formats and nonsense, with</li><li>JPEG compression, for visually fine results with much space savings, and</li><li>YCBCR color, for even smaller size, and</li><li>internal tiling, for fast access of random squares of data, and</li><li>overviews, for fast access of zoomed out views of the data.</li></ul><p>Go forth and compress!</p>";s:4:"link";s:74:"http://blog.cleverelephant.ca/2015/02/geotiff-compression-for-dummies.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"7";}s:7:"summary";s:5114:"<p>"What's the best image format for map serving?" they ask me, shortly after I tell them not to serve their images from inside a database. </p><p>"Is it MrSid? Or ECW? those are nice and small." Which indeed they are. Unfortunately, outside of proprietary image server software I've never seen them be <b>fast</b> and nice and small at the same time. Generally the decode step is incredibly CPU intensive, presumably because of the fancy wavelet math that makes them so small in the first place.</p><p>"So, what's the best image format for map serving?".</p><p>In my experience, the best format for image serving, using open source rendering engines (MapServer, GeoServer, Mapnik) is: <b>GeoTIFF, with JPEG compression, internally tiled, in the YCBCR color space, with internal overviews</b>. Unfortunately, GeoTiffs are almost never delivered this way, as I was reminded today while downloading a sample image from the <a href="http://www.kamloops.ca/maps/disclaimer.html">City of Kamloops</a> (But nonetheless, thanks for the great free imagery, Kamloops!)</p><p><code>5255C.zip [593M]</code></p><p>It came in a 593Mb ZIP file. "Hm, that's pretty big, I thought." I unzipped it.</p><p><code>5255C.tif [515M]</code></p><p>Unzipped it was a 515Mb TIF file. That's right, it was smaller "uncompressed". Why? Because internally it was already compressed, and applying the ZIP compression algorithm to already compressed data generally fluffs it up a little. Whoops.</p><p>The default TIFF compression is, unfortunately, "<a href="http://en.wikipedia.org/wiki/Huffman_coding">deflate</a>", the same as that used for ZIP. This is a lossless encoding, but not very good for imagery. We can make the image a whole lot smaller just by using a more appropriate compression, like JPEG. We'll also tile it internally while we're at it. Internal tiling allows renderers to quickly pick out and decompress just a small portion of the image, which is important once you've applied a more serious compression algorithm like JPEG.</p><pre>gdal_translate \<br />  -co COMPRESS=JPEG \<br />  -co TILED=YES \<br />  5255C.tif 5255C_JPEG.tif</pre><p>This is much better, now we have a vastly smaller file.</p><p><code>5255C_JPEG.tif [67M]</code></p><p>But we can still do better! For reasons that well pass my understanding, the JPEG algorithm is more effective against images that are stored in the <a href="http://en.wikipedia.org/wiki/YCbCr">YCBCR</a> color space. Mine is not to reason why, though.</p><pre>gdal_translate \<br />  -co COMPRESS=JPEG \<br />  -co PHOTOMETRIC=YCBCR \<br />  -co TILED=YES \<br />  5255C.tif 5255C_JPEG_YCBCR.tif</pre><p>Wow, now we're down to 1/20 the size of the original.</p><p><code>5255C_JPEG_YCBCR.tif [24M]</code></p><p>But, we've applied a "lossy" algorithm, JPEG, maybe we've ruined the data! Let's have a look.</p><table border="0"><tr><td><a href="http://2.bp.blogspot.com/-ViYOsz3kMUY/VOYqHnXIlDI/AAAAAAAAAdU/0OGl8u9TmMs/s1600/example_jpg.png" imageanchor="1" ><img border="0" height="300" src="http://2.bp.blogspot.com/-ViYOsz3kMUY/VOYqHnXIlDI/AAAAAAAAAdU/0OGl8u9TmMs/s1600/example_jpg.png" /></a></td><td><a href="http://4.bp.blogspot.com/-EqVHMQXxWy0/VOYqH15vBaI/AAAAAAAAAdY/XS5ffZl1mME/s1600/example_lzw.png" imageanchor="1" ><img border="0" height="300" src="http://4.bp.blogspot.com/-EqVHMQXxWy0/VOYqH15vBaI/AAAAAAAAAdY/XS5ffZl1mME/s1600/example_lzw.png" /></a></td></tr><tr><th>Original</th><th>After JPEG/YCBCR</th></tr></table><p>Can you see the difference? Me neither. Using a JPEG "quality" level of 75%, there are no visible artefacts. In general, JPEG is very good at compressing things so humans "can't see" the lost information. I'd never use it for compressing a DEM or a data raster, but for a visual image, I use JPEG with impunity, and with much lower quality settings too (for more space saved).</p><p>Finally, for high speed serving at more zoomed out scales, we need to add overviews to the image. We'll make sure the overviews use the same, high compression options as the base data.</p><pre>gdaladdo \<br />  --config COMPRESS_OVERVIEW JPEG \<br />  --config PHOTOMETRIC_OVERVIEW YCBCR \<br />  --config INTERLEAVE_OVERVIEW PIXEL \<br />  -r average \<br />  5255C_JPEG_YCBCR.tif \<br />  2 4 8 16</pre><p>For reasons passing understanding, <code>gdaladdo</code> uses a different set of command-line switches to pass the configuration info to the compressor than <code>gdal_translate</code> does, but as before, mine is not to reason why.</p><p>The final size, now <b>with</b> overviews as well as the original data, is still less that 1/10 the size of the original.</p><p><code>5255C_JPEG_YCBCR.tif [37M]</code></p><p>So, to sum up, your best format for image serving is:</p><ul><li>GeoTiff, so you can avoid proprietary image formats and nonsense, with</li><li>JPEG compression, for visually fine results with much space savings, and</li><li>YCBCR color, for even smaller size, and</li><li>internal tiling, for fast access of random squares of data, and</li><li>overviews, for fast access of zoomed out views of the data.</li></ul><p>Go forth and compress!</p>";}i:15;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-6033266774773224546";s:7:"pubdate";s:31:"Fri, 06 Feb 2015 18:50:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-03-19T05:38:21.094-07:00";}s:8:"category";s:7:"postgis";s:5:"title";s:35:"Breaking a Linestring into Segments";s:11:"description";s:521:"<p>Like doing a sudoku, solving a "simple yet tricky" problem in spatial SQL can grab ones mind and hold it for a period. Someone on the PostGIS IRC channel was trying to "convert a linestring into a set of two-point segments", using an external C++ program, and I thought: "hm, I'm sure that's doable in SQL".</p><p>And sure enough, it is, though the syntax for referencing out the parts of the dump objects makes it look a little ugly.</p><script src="https://gist.github.com/pramsey/87f8d7cb0633282c37e5.js"></script> ";s:4:"link";s:76:"http://blog.cleverelephant.ca/2015/02/breaking-linestring-into-segments.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:7:"summary";s:521:"<p>Like doing a sudoku, solving a "simple yet tricky" problem in spatial SQL can grab ones mind and hold it for a period. Someone on the PostGIS IRC channel was trying to "convert a linestring into a set of two-point segments", using an external C++ program, and I thought: "hm, I'm sure that's doable in SQL".</p><p>And sure enough, it is, though the syntax for referencing out the parts of the dump objects makes it look a little ugly.</p><script src="https://gist.github.com/pramsey/87f8d7cb0633282c37e5.js"></script> ";}i:16;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-6370729313181481342";s:7:"pubdate";s:31:"Mon, 02 Feb 2015 16:00:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-02-02T08:55:22.148-08:00";}s:8:"category";s:29:"boundlesscareercartodbopengeo";s:5:"title";s:11:"The New Gig";s:11:"description";s:2860:"<p>I haven't had many jobs in my career, so changing jobs feels pretty momentous: two weeks ago I had my last day at <a href="http://boundlessgeo.com">Boundless</a>, and today will be my first at <a href="http://cartodb.com">CartoDB</a>.  </p><p>I started with Boundless back in 2009 when it was OpenGeo and still a part of the <a href="http://openplans.org">Open Planning Project</a>, a weird non-profit arm of a New York hedge fund millionaire's corporate archipelago. (The hedgie, <a href="http://en.wikipedia.org/wiki/Mark_Gorton">Mark Gorton</a>, is still going strong, despite the brief set-back he endured when <a href="http://en.wikipedia.org/wiki/LimeWire">LimeWire</a> was sued by <a href="https://www.riaa.com">RIAA</a>.) For that six year run, I was fortunate to have a lead role in articulating what it meant to "do open source" in the geospatial world, and to help to build OpenGeo into a self-supporting open source enterprise. We grew, spun out of the non-profit, gained lots of institutional customers, and I got to meet and work with lots of quality folks. After six years though, I feel like I need a change, an opportunity to learn some new things and meet some new people: To move from the enterprise space, to the consumer space. </p><p><div class="separator" style="clear: both; text-align: center;"><a href="http://cartodb.com" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://did14.datainnovationday.org/wp-content/uploads/2013/12/cartodb-logo1.png" width=200 /></a></div>So I was very lucky when a new opportunity came along: to work for a company that is reimagining what it means to be a spatial database in a software-as-a-service world. Under the covers, CartoDB uses my favorite open source spatial database, <a href="http://postgis.net">PostGIS</a>, to run their platform, and working for CartoDB gives me a chance to talk about and to work on something I like almost as much as (more than?) open source: spatial SQL! The team at CartoDB have done a great job with their platform, providing a simple entry-point into map making, while still leaving the power of SQL exposed and available, so that users can transition from beginner, to explorer, to power user. As someone who currently only knows a portion of their technology (the SQL bit), I'm looking forward to experiencing the rest of their platform as a beginner. I also know the platform folks will have lots of good questions for me on PostGIS internals, and we'll have many interesting conversations about how to keep pushing <a href="http://postgresql.org">PostgreSQL</a> and PostGIS to the limits. </p><p>My two week between-jobs break was refreshing, but sometimes a change is as good as rest too. I enjoyed my last six years with Boundless and I'm looking forward to the future with CartoDB. </p>";s:4:"link";s:54:"http://blog.cleverelephant.ca/2015/02/the-new-gig.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"4";}s:7:"summary";s:2860:"<p>I haven't had many jobs in my career, so changing jobs feels pretty momentous: two weeks ago I had my last day at <a href="http://boundlessgeo.com">Boundless</a>, and today will be my first at <a href="http://cartodb.com">CartoDB</a>.  </p><p>I started with Boundless back in 2009 when it was OpenGeo and still a part of the <a href="http://openplans.org">Open Planning Project</a>, a weird non-profit arm of a New York hedge fund millionaire's corporate archipelago. (The hedgie, <a href="http://en.wikipedia.org/wiki/Mark_Gorton">Mark Gorton</a>, is still going strong, despite the brief set-back he endured when <a href="http://en.wikipedia.org/wiki/LimeWire">LimeWire</a> was sued by <a href="https://www.riaa.com">RIAA</a>.) For that six year run, I was fortunate to have a lead role in articulating what it meant to "do open source" in the geospatial world, and to help to build OpenGeo into a self-supporting open source enterprise. We grew, spun out of the non-profit, gained lots of institutional customers, and I got to meet and work with lots of quality folks. After six years though, I feel like I need a change, an opportunity to learn some new things and meet some new people: To move from the enterprise space, to the consumer space. </p><p><div class="separator" style="clear: both; text-align: center;"><a href="http://cartodb.com" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://did14.datainnovationday.org/wp-content/uploads/2013/12/cartodb-logo1.png" width=200 /></a></div>So I was very lucky when a new opportunity came along: to work for a company that is reimagining what it means to be a spatial database in a software-as-a-service world. Under the covers, CartoDB uses my favorite open source spatial database, <a href="http://postgis.net">PostGIS</a>, to run their platform, and working for CartoDB gives me a chance to talk about and to work on something I like almost as much as (more than?) open source: spatial SQL! The team at CartoDB have done a great job with their platform, providing a simple entry-point into map making, while still leaving the power of SQL exposed and available, so that users can transition from beginner, to explorer, to power user. As someone who currently only knows a portion of their technology (the SQL bit), I'm looking forward to experiencing the rest of their platform as a beginner. I also know the platform folks will have lots of good questions for me on PostGIS internals, and we'll have many interesting conversations about how to keep pushing <a href="http://postgresql.org">PostgreSQL</a> and PostGIS to the limits. </p><p>My two week between-jobs break was refreshing, but sometimes a change is as good as rest too. I enjoyed my last six years with Boundless and I'm looking forward to the future with CartoDB. </p>";}i:17;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-6637254194859371132";s:7:"pubdate";s:31:"Thu, 08 Jan 2015 17:51:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2015-01-08T09:51:50.403-08:00";}s:8:"category";s:12:"enterpriseit";s:5:"title";s:25:"Can Procurement do Agile?";s:11:"description";s:1991:"<p>Some very interesting news out of the USA today, as the <a href="http://gsa.gov">GSA</a> (the biggest, baddest procurement agency in the world) has <a href="https://interact.gsa.gov/blog/gsa-releases-rfi-proposed-agile-delivery-services-bpa-–-industry-feedback-needed">released a Request For Information</a> (RFI) on <a href="http://en.wikipedia.org/wiki/Agile_software_development">agile technology delivery</a>. </p><blockquote>To shift the software procurement paradigm, GSA’s <a href="http://18f.gsa.gov/">18F Team</a> and the Office of Integrated Technology Services (ITS) is collaborating on the establishment of a BPA that will feature vendors who specialize in Agile Delivery Services.  The goal of the proposed BPA is to decrease software acquisition cycles to less than four weeks (from solicitation to contract) and expedite the delivery of a minimum viable product (MVP) within three months or less. </blockquote><p>In a wonderful "eat your own dogfood" move, the team working on building this new procurement vehicle are themselves adopting agile practices in their own process. Starting small with a pilot, working directly with the vendors who will be trying the new vehicle, etc. If the hidebound old GSA can develop a workable framework for agile procurement, then nobody else has an excuse.  </p><p>(The reason procurement agencies have found it hard to specify "<a href="http://en.wikipedia.org/wiki/Agile_software_development">agile</a>" is that agile by design does not define precise deliverables in advance, so it is damnably hard to fit into a "fixed cost bid" structure. In places where time-and-materials vehicles are already in place, lots of government organizations are already working with vendors in an agile way, but for the kinds of big, <a href="/2014/11/my-tax-dollars-at-work.html">boondoggle-prone capital investment projects</a> I write about, the <a href="http://en.wikipedia.org/wiki/Waterfall_model">waterfall model</a> still predominates.)<p>";s:4:"link";s:67:"http://blog.cleverelephant.ca/2015/01/can-procurement-do-agile.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"2";}s:7:"summary";s:1991:"<p>Some very interesting news out of the USA today, as the <a href="http://gsa.gov">GSA</a> (the biggest, baddest procurement agency in the world) has <a href="https://interact.gsa.gov/blog/gsa-releases-rfi-proposed-agile-delivery-services-bpa-–-industry-feedback-needed">released a Request For Information</a> (RFI) on <a href="http://en.wikipedia.org/wiki/Agile_software_development">agile technology delivery</a>. </p><blockquote>To shift the software procurement paradigm, GSA’s <a href="http://18f.gsa.gov/">18F Team</a> and the Office of Integrated Technology Services (ITS) is collaborating on the establishment of a BPA that will feature vendors who specialize in Agile Delivery Services.  The goal of the proposed BPA is to decrease software acquisition cycles to less than four weeks (from solicitation to contract) and expedite the delivery of a minimum viable product (MVP) within three months or less. </blockquote><p>In a wonderful "eat your own dogfood" move, the team working on building this new procurement vehicle are themselves adopting agile practices in their own process. Starting small with a pilot, working directly with the vendors who will be trying the new vehicle, etc. If the hidebound old GSA can develop a workable framework for agile procurement, then nobody else has an excuse.  </p><p>(The reason procurement agencies have found it hard to specify "<a href="http://en.wikipedia.org/wiki/Agile_software_development">agile</a>" is that agile by design does not define precise deliverables in advance, so it is damnably hard to fit into a "fixed cost bid" structure. In places where time-and-materials vehicles are already in place, lots of government organizations are already working with vendors in an agile way, but for the kinds of big, <a href="/2014/11/my-tax-dollars-at-work.html">boondoggle-prone capital investment projects</a> I write about, the <a href="http://en.wikipedia.org/wiki/Waterfall_model">waterfall model</a> still predominates.)<p>";}i:18;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-5842503071306732004";s:7:"pubdate";s:31:"Tue, 30 Dec 2014 19:34:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2014-12-30T11:34:18.498-08:00";}s:8:"category";s:25:"clientgisjavascriptserver";s:5:"title";s:43:"Give in, to the power of the Client Side...";s:11:"description";s:4492:"<img src="http://ecx.images-amazon.com/images/I/41IoEyPgbVL._SY300_.jpg" style="float:right; padding:10px" /><p>Brian Timoney <a href="http://mapbrief.com/2014/12/29/geo-in-the-browser-less-it-means-this-time-its-different/">called out</a> this <a href="http://www.macwright.org/">Tom Macwright</a> quote, and he's right, it deserves a little more discussion:</p><blockquote>…the client side will eat more of the server side stack.</blockquote><p>To understand what "more" there is left to eat, it's worth enumerating what's already been <b>eaten</b> (or, which is being consumed right now, as we watch):</p><ul><li><b>Interaction:</b> OK, so this was always on the client side, but it's worth noting that the impulse towards using a heavy-weight plug-in for interaction is now pretty much dead. The detritus of plug-in based solutions will be around for a long while, inching towards end-of-life, but not many new ones are being built. (I bet some are, though, in the bowels of organizations where IE remains an unbreakable corporate standard.</li><li><b>Single-Layer Rendering:</b> Go back almost 10 years and you'll find OpenLayers doing client-side rendering, though using some pretty gnarly hacks at the time. Given the restrictions in rendering performance, a common way to break down an app was a static, tiled base map with a single vector layer of interest on top. (Or, for the truly performance oriented, a single raster layer on top, only switching to vector for editing purposes.) With modern browser technology, and good implementations, rendering very large numbers of features on the client has become commonplace, to the extent that the new bottleneck is no longer the CPU, it's the network.</li><li><b>All-the-layers Rendering:</b> Already shown in-principle by Google Maps, tiled vector rendering is moving over the last 12 months rapidly from wow-wizzy-demo to oh-no-not-that-again status. Rather than rendering to raster on the server side, send a simplified version to the client for rendering there. For base maps there's not a *lot* of benefit over pre-rendered raster, but there's some: dynamic labelling means orientation is completely flexible, and also allows for multiple options for labelling; also, simplified vector tiles can serve a wider range of zoom levels while remaining attractively rendered, so the all-important network bandwidth issues can be addressed for mobile devices.</li><li><b>"GIS" operations:</b> While large scale analysis is not going to happen on a web page, a lot of visual effects that were otherwise hard to achieve can now be pushed to the client. Some of the GIS operations are actually in support of getting attractive client-side rendering: voronoi diagrams can be a great aid to label placement; buffers come in handy for cartography all the time.</li><li><b>Persistence:</b> Not really designed for long-term storage, but since any mobile application on a modern platform now has access to a storage area of pretty large size, there's nothing stopping these new "client" applications from wandering far and completely untethered from the server/cloud for long periods of time.</li></ul><p>Uh, what's left?</p><ul><li>Long term storage and coordination remain. If people are going to work together on data, they need a common place to store and access their information from.</li><li>Large scale data handling and analysis remain, thanks to those pesky narrow network pipes, for now.</li><li>Coordination between devices requires a central authority still. Although, not for long, with web sockets I'm sure some JavaScript wizard has already cooked up a browser-to-browser peer-to-peer scheme, so the age of fully distributed open street map will catch up to us eventually.</li></ul><p>Have I missed any?</p><p>Once all applications are written in 100% JavaScript we will have finally achieved the vision promised to me <a href="http://en.wikipedia.org/wiki/Java_(software_platform)#History">back in 1995</a>, a write-once, run-anywhere application development language, where applications are not installed but are downloaded as needed over the network (because "<a href="http://en.wikipedia.org/wiki/John_Gage">the network is the computer</a>"). Just turns out it took 20 years longer and the language and virtual machine are different (and there's this strange "document" cruft floating around, a <a href="http://en.wikipedia.org/wiki/Coccyx">coccyx</a>-like evolutionary remnant people will be wondering about for years).</p>";s:4:"link";s:74:"http://blog.cleverelephant.ca/2014/12/give-in-to-power-of-client-side.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"2";}s:7:"summary";s:4492:"<img src="http://ecx.images-amazon.com/images/I/41IoEyPgbVL._SY300_.jpg" style="float:right; padding:10px" /><p>Brian Timoney <a href="http://mapbrief.com/2014/12/29/geo-in-the-browser-less-it-means-this-time-its-different/">called out</a> this <a href="http://www.macwright.org/">Tom Macwright</a> quote, and he's right, it deserves a little more discussion:</p><blockquote>…the client side will eat more of the server side stack.</blockquote><p>To understand what "more" there is left to eat, it's worth enumerating what's already been <b>eaten</b> (or, which is being consumed right now, as we watch):</p><ul><li><b>Interaction:</b> OK, so this was always on the client side, but it's worth noting that the impulse towards using a heavy-weight plug-in for interaction is now pretty much dead. The detritus of plug-in based solutions will be around for a long while, inching towards end-of-life, but not many new ones are being built. (I bet some are, though, in the bowels of organizations where IE remains an unbreakable corporate standard.</li><li><b>Single-Layer Rendering:</b> Go back almost 10 years and you'll find OpenLayers doing client-side rendering, though using some pretty gnarly hacks at the time. Given the restrictions in rendering performance, a common way to break down an app was a static, tiled base map with a single vector layer of interest on top. (Or, for the truly performance oriented, a single raster layer on top, only switching to vector for editing purposes.) With modern browser technology, and good implementations, rendering very large numbers of features on the client has become commonplace, to the extent that the new bottleneck is no longer the CPU, it's the network.</li><li><b>All-the-layers Rendering:</b> Already shown in-principle by Google Maps, tiled vector rendering is moving over the last 12 months rapidly from wow-wizzy-demo to oh-no-not-that-again status. Rather than rendering to raster on the server side, send a simplified version to the client for rendering there. For base maps there's not a *lot* of benefit over pre-rendered raster, but there's some: dynamic labelling means orientation is completely flexible, and also allows for multiple options for labelling; also, simplified vector tiles can serve a wider range of zoom levels while remaining attractively rendered, so the all-important network bandwidth issues can be addressed for mobile devices.</li><li><b>"GIS" operations:</b> While large scale analysis is not going to happen on a web page, a lot of visual effects that were otherwise hard to achieve can now be pushed to the client. Some of the GIS operations are actually in support of getting attractive client-side rendering: voronoi diagrams can be a great aid to label placement; buffers come in handy for cartography all the time.</li><li><b>Persistence:</b> Not really designed for long-term storage, but since any mobile application on a modern platform now has access to a storage area of pretty large size, there's nothing stopping these new "client" applications from wandering far and completely untethered from the server/cloud for long periods of time.</li></ul><p>Uh, what's left?</p><ul><li>Long term storage and coordination remain. If people are going to work together on data, they need a common place to store and access their information from.</li><li>Large scale data handling and analysis remain, thanks to those pesky narrow network pipes, for now.</li><li>Coordination between devices requires a central authority still. Although, not for long, with web sockets I'm sure some JavaScript wizard has already cooked up a browser-to-browser peer-to-peer scheme, so the age of fully distributed open street map will catch up to us eventually.</li></ul><p>Have I missed any?</p><p>Once all applications are written in 100% JavaScript we will have finally achieved the vision promised to me <a href="http://en.wikipedia.org/wiki/Java_(software_platform)#History">back in 1995</a>, a write-once, run-anywhere application development language, where applications are not installed but are downloaded as needed over the network (because "<a href="http://en.wikipedia.org/wiki/John_Gage">the network is the computer</a>"). Just turns out it took 20 years longer and the language and virtual machine are different (and there's this strange "document" cruft floating around, a <a href="http://en.wikipedia.org/wiki/Coccyx">coccyx</a>-like evolutionary remnant people will be wondering about for years).</p>";}i:19;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-4092334853861029408";s:7:"pubdate";s:31:"Wed, 03 Dec 2014 18:33:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2014-12-07T11:45:03.844-08:00";}s:8:"category";s:17:"ccunitdevelopment";s:5:"title";s:26:"Building CUnit from Source";s:11:"description";s:1205:"<p>I haven't had to build CUnit myself for a while, because most of the systems I work with have it in their packaged software repositories, but for Solaris it's not there, and it turns out, it's quite painful to build! </p><ul><li>First, the latest version on sourceforge, 2.1.3, is incorrectly bundled, with files (config.h.in) missing, so you have to use 2.1.2. <li>Second, the directions in the README don't include all the details of what autotools commands need to be run.  </ul><p>Here's the commands I finally used to get a build. Note that you do need to run <code>libtoolize</code> to get some missing support scripts installed, and that you need to also run automake in "add missing" mode to get let more support scripts. Then and only then do you get a build. </p><pre>wget http://downloads.sourceforge.net/project/cunit/CUnit/2.1-2/CUnit-2.1-2-src.tar.bz2<br />tar xvfj CUnit-2.1-2.tar.bz2<br />cd CUnit-2.1-2<br />libtoolize -f -c -i \<br />&& aclocal \<br />&& autoconf \<br />&& automake --gnu --add-missing \<br />&& ./configure --prefix=/usr/local \<br />&& make \<br />&& make install</pre><p><b>Update:</b> The <code>bootstrap</code> file does provide the required autotools flags.</p>";s:4:"link";s:69:"http://blog.cleverelephant.ca/2014/12/building-cunit-from-source.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"2";}s:7:"summary";s:1205:"<p>I haven't had to build CUnit myself for a while, because most of the systems I work with have it in their packaged software repositories, but for Solaris it's not there, and it turns out, it's quite painful to build! </p><ul><li>First, the latest version on sourceforge, 2.1.3, is incorrectly bundled, with files (config.h.in) missing, so you have to use 2.1.2. <li>Second, the directions in the README don't include all the details of what autotools commands need to be run.  </ul><p>Here's the commands I finally used to get a build. Note that you do need to run <code>libtoolize</code> to get some missing support scripts installed, and that you need to also run automake in "add missing" mode to get let more support scripts. Then and only then do you get a build. </p><pre>wget http://downloads.sourceforge.net/project/cunit/CUnit/2.1-2/CUnit-2.1-2-src.tar.bz2<br />tar xvfj CUnit-2.1-2.tar.bz2<br />cd CUnit-2.1-2<br />libtoolize -f -c -i \<br />&& aclocal \<br />&& autoconf \<br />&& automake --gnu --add-missing \<br />&& ./configure --prefix=/usr/local \<br />&& make \<br />&& make install</pre><p><b>Update:</b> The <code>bootstrap</code> file does provide the required autotools flags.</p>";}i:20;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-6133333700312407616";s:7:"pubdate";s:31:"Tue, 02 Dec 2014 21:22:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2014-12-02T13:22:26.114-08:00";}s:8:"category";s:18:"environmentpostgis";s:5:"title";s:26:"The Tyranny of Environment";s:11:"description";s:2762:"<img src="http://i.technet.microsoft.com/cc162526.fig01(en-us).gif" style="float: right; padding: 10px"/><p>Most users of PostGIS are safely ensconsed in the world of Linux, and their build/deploy environments are pretty similar to the ones used by the developers, so any problems they might experience are quickly found and removed early in development.</p><p>Some users are on Windows, but they are our most numerous user base, so we at least test that platform preemptively before release and make sure it is as good as we can make it.</p> <p>And then there's the rest. We've had a passel of FreeBSD bugs lately, and I've found myself doing Solaris builds for customers, and don't get me started on the poor buggers running AIX. One of the annoyances of trying to fix a problem for a "weird platform" user is just getting the platform setup and running in the first place.</p> <p>So, having recently learned a bit about <a href="https://www.vagrantup.com/">vagrant</a>, and seeing that some of the "weird" platforms have boxes already, I thought I would whip off a couple vagrant configurations so it's easy in the future to throw up a Solaris or FreeBSD box, or even a quick Centos box for debugging purposes.</p> <p>I've just been setting up my <a href="https://github.com/pramsey/postgis-vagrant/blob/master/solaris/Vagrantfile">Solaris Vagrantfile</a> and using my favourite Solaris crutch: the <a href="http://opencsw.org/">OpenCSW</a> software repository. But as I use it, I'm not just adding the "things I need", I'm implicitly <b>choosing an environment</b>:</p><ul><li>my <code>libxml2</code> is from OpenCSV</li><li>so is my <code>gcc</code>, which is version 4, not version 3</li><li>so is my <code>postgres</code></li></ul><p>This is convenient for me, but what are the chances that it'll be the environment used by someone on Solaris having problems? They might be compiling against libraries from <code>/usr/sfw/bin</code>, or using the Solaris <code>gcc-3</code> package, or any number of other variants. At the end of the day, when testing on such a Solaris environment, will I be testing against a real situation, or a fantasyland of my own making?</p> <p>For platforms like Ubuntu (apt) or Red Hat (yum) or FreeBSD (port) where there is One True Way to get software, the difficulties are less, but even then there is no easy way to get a "standard environment", or to quickly replicate the combinations of versions a user might have run into that is causing problems (<code>libjson</code> is a current source of pain). <a href="http://en.wikipedia.org/wiki/DLL_Hell">DLL hell</a> has never really gone away, it has just found new ways to express itself.</p> <p>(I will send a psychic wedgie to anyone who says "docker", I'm not kidding.)</p>";s:4:"link";s:69:"http://blog.cleverelephant.ca/2014/12/the-tyranny-of-environment.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"1";}s:7:"summary";s:2762:"<img src="http://i.technet.microsoft.com/cc162526.fig01(en-us).gif" style="float: right; padding: 10px"/><p>Most users of PostGIS are safely ensconsed in the world of Linux, and their build/deploy environments are pretty similar to the ones used by the developers, so any problems they might experience are quickly found and removed early in development.</p><p>Some users are on Windows, but they are our most numerous user base, so we at least test that platform preemptively before release and make sure it is as good as we can make it.</p> <p>And then there's the rest. We've had a passel of FreeBSD bugs lately, and I've found myself doing Solaris builds for customers, and don't get me started on the poor buggers running AIX. One of the annoyances of trying to fix a problem for a "weird platform" user is just getting the platform setup and running in the first place.</p> <p>So, having recently learned a bit about <a href="https://www.vagrantup.com/">vagrant</a>, and seeing that some of the "weird" platforms have boxes already, I thought I would whip off a couple vagrant configurations so it's easy in the future to throw up a Solaris or FreeBSD box, or even a quick Centos box for debugging purposes.</p> <p>I've just been setting up my <a href="https://github.com/pramsey/postgis-vagrant/blob/master/solaris/Vagrantfile">Solaris Vagrantfile</a> and using my favourite Solaris crutch: the <a href="http://opencsw.org/">OpenCSW</a> software repository. But as I use it, I'm not just adding the "things I need", I'm implicitly <b>choosing an environment</b>:</p><ul><li>my <code>libxml2</code> is from OpenCSV</li><li>so is my <code>gcc</code>, which is version 4, not version 3</li><li>so is my <code>postgres</code></li></ul><p>This is convenient for me, but what are the chances that it'll be the environment used by someone on Solaris having problems? They might be compiling against libraries from <code>/usr/sfw/bin</code>, or using the Solaris <code>gcc-3</code> package, or any number of other variants. At the end of the day, when testing on such a Solaris environment, will I be testing against a real situation, or a fantasyland of my own making?</p> <p>For platforms like Ubuntu (apt) or Red Hat (yum) or FreeBSD (port) where there is One True Way to get software, the difficulties are less, but even then there is no easy way to get a "standard environment", or to quickly replicate the combinations of versions a user might have run into that is causing problems (<code>libjson</code> is a current source of pain). <a href="http://en.wikipedia.org/wiki/DLL_Hell">DLL hell</a> has never really gone away, it has just found new ways to express itself.</p> <p>(I will send a psychic wedgie to anyone who says "docker", I'm not kidding.)</p>";}i:21;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-4516137483036098034";s:7:"pubdate";s:31:"Fri, 21 Nov 2014 19:05:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2014-12-13T15:59:27.488-08:00";}s:8:"category";s:10:"bctaxiuber";s:5:"title";s:26:"What to do about Uber (BC)";s:11:"description";s:3806:"<p><b>Update:</b> New York and Chicago are exploring exactly this approach, <a href="http://bits.blogs.nytimes.com/2014/12/11/chicago-and-new-york-officials-look-to-build-uber-like-apps-for-taxis">as reported in the New York Times</a>: "Regulators in Chicago have approved a plan to create one or more applications that would allow users to hail taxis from any operators in the city, using a smartphone. In New York, a City Council member proposed a similar app on Monday that would let residents “e-hail” any of the 20,000 cabs that circulate in the city on a daily basis."</p><hr/> <p>Nick Denton has a <a href="http://nick.kinja.com/what-to-do-about-uber-1661643767/+nitasha">nice little article</a> on Kinja about Uber and how they are slowly taking over the local transportation market in cities they have been allowed to operate. </p><blockquote><div class="separator" style="clear: both; text-align: center;"><a href="http://fs01.androidpit.info/a/52/86/taxi-limo-ambulance-test-bc-5286f6-w192.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://fs01.androidpit.info/a/52/86/taxi-limo-ambulance-test-bc-5286f6-w192.png" /></a></div>it's increasingly clear that the fast-growing ride-hailing service is what economists would call a natural monopoly, with commensurate profitability... It's inevitable that one ride-sharing service will dominate in each major metropolitan area. Neither passengers nor drivers want to maintain accounts with multiple services. The latest numbers on [Uber], show a business likely to bring in nearly $1bn a month by this time next year, far ahead of any competitor</blockquote><p>BC has thus far <a href="http://www.cbc.ca/news/canada/british-columbia/uber-vancouver-to-get-undercover-government-checks-to-enforce-taxi-regulations-1.2821811">resisted the encroachment of Uber</a>, but that cannot last forever, and it shouldn't: users of taxis in Vancouver aren't getting great service, and that's why there's room in the market for Uber to muscle in.</p><p>Like Denton, I see Uber as a mixed bag: on the one hand, they've offered a streamlined experience which is qualitatively better than the old taxi service; on the other, in setting up an unregulated and exploitative market for drivers, they've sowed the seeds of chaos. The thing is, many of the positive aspects of Uber are easily duplicable by existing transportation providers: app-based dispatching and payment aren't rocket science by any stretch. <p>As an American, Denton naturally reaches for the American solution to the natural monopoly: regulated private enterprise. In the USA, monopolists (electric utilities, for example) are allowed to extract profits, but only at a regulated rate. As Canadians, we have an additional option: the <a href="http://en.wikipedia.org/wiki/Crown_corporations_of_Canada">Crown corporation</a>. Many of our natural monopolies, like electricity, are run by <a href="http://www.bchydro.com">government-owned corporations</a>.</p><p>Since most taxis are independently owned and operated anyways, all that a Crown taxi corporation would need to do is provide a central dispatching service, with enough ease-of-use to compete with Uber and its like. The experience of users would improve: one number to call, one app to use, no payment hassles, optimized routing, maybe even ride sharing. And the Crown corporation could use supply management to prevent a race to the bottom that would impoverish drivers and reduce safety on the roads.</p><p>There's nothing magical about what Uber is doing, they are arbitraging a currently inefficient system, but the system can save itself, and all its positive aspects, by recognizing and reforming now. Bring on our next Crown corporation, "BC Dispatching".</p>";s:4:"link";s:67:"http://blog.cleverelephant.ca/2014/11/what-to-do-about-uber-bc.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"3";}s:7:"summary";s:3806:"<p><b>Update:</b> New York and Chicago are exploring exactly this approach, <a href="http://bits.blogs.nytimes.com/2014/12/11/chicago-and-new-york-officials-look-to-build-uber-like-apps-for-taxis">as reported in the New York Times</a>: "Regulators in Chicago have approved a plan to create one or more applications that would allow users to hail taxis from any operators in the city, using a smartphone. In New York, a City Council member proposed a similar app on Monday that would let residents “e-hail” any of the 20,000 cabs that circulate in the city on a daily basis."</p><hr/> <p>Nick Denton has a <a href="http://nick.kinja.com/what-to-do-about-uber-1661643767/+nitasha">nice little article</a> on Kinja about Uber and how they are slowly taking over the local transportation market in cities they have been allowed to operate. </p><blockquote><div class="separator" style="clear: both; text-align: center;"><a href="http://fs01.androidpit.info/a/52/86/taxi-limo-ambulance-test-bc-5286f6-w192.png" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://fs01.androidpit.info/a/52/86/taxi-limo-ambulance-test-bc-5286f6-w192.png" /></a></div>it's increasingly clear that the fast-growing ride-hailing service is what economists would call a natural monopoly, with commensurate profitability... It's inevitable that one ride-sharing service will dominate in each major metropolitan area. Neither passengers nor drivers want to maintain accounts with multiple services. The latest numbers on [Uber], show a business likely to bring in nearly $1bn a month by this time next year, far ahead of any competitor</blockquote><p>BC has thus far <a href="http://www.cbc.ca/news/canada/british-columbia/uber-vancouver-to-get-undercover-government-checks-to-enforce-taxi-regulations-1.2821811">resisted the encroachment of Uber</a>, but that cannot last forever, and it shouldn't: users of taxis in Vancouver aren't getting great service, and that's why there's room in the market for Uber to muscle in.</p><p>Like Denton, I see Uber as a mixed bag: on the one hand, they've offered a streamlined experience which is qualitatively better than the old taxi service; on the other, in setting up an unregulated and exploitative market for drivers, they've sowed the seeds of chaos. The thing is, many of the positive aspects of Uber are easily duplicable by existing transportation providers: app-based dispatching and payment aren't rocket science by any stretch. <p>As an American, Denton naturally reaches for the American solution to the natural monopoly: regulated private enterprise. In the USA, monopolists (electric utilities, for example) are allowed to extract profits, but only at a regulated rate. As Canadians, we have an additional option: the <a href="http://en.wikipedia.org/wiki/Crown_corporations_of_Canada">Crown corporation</a>. Many of our natural monopolies, like electricity, are run by <a href="http://www.bchydro.com">government-owned corporations</a>.</p><p>Since most taxis are independently owned and operated anyways, all that a Crown taxi corporation would need to do is provide a central dispatching service, with enough ease-of-use to compete with Uber and its like. The experience of users would improve: one number to call, one app to use, no payment hassles, optimized routing, maybe even ride sharing. And the Crown corporation could use supply management to prevent a race to the bottom that would impoverish drivers and reduce safety on the roads.</p><p>There's nothing magical about what Uber is doing, they are arbitraging a currently inefficient system, but the system can save itself, and all its positive aspects, by recognizing and reforming now. Bring on our next Crown corporation, "BC Dispatching".</p>";}i:22;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-5668020613065501427";s:7:"pubdate";s:31:"Fri, 14 Nov 2014 22:04:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2014-11-14T14:04:24.756-08:00";}s:8:"category";s:20:"gisitspatial itvideo";s:5:"title";s:29:"Spatial IT and the Enterprise";s:11:"description";s:442:"<p>My latest, a webinar on how OpenGeo Suite and Spatial IT go together for enterprise geo.</p><iframe src="//player.vimeo.com/video/111882381" width="500" height="313" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> <p><a href="http://vimeo.com/111882381">Geospatial Open Source for the Enterprise</a> from <a href="http://vimeo.com/boundlessgeo">Boundless</a> on <a href="https://vimeo.com">Vimeo</a>.</p>";s:4:"link";s:68:"http://blog.cleverelephant.ca/2014/11/spatial-it-and-enterprise.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:7:"summary";s:442:"<p>My latest, a webinar on how OpenGeo Suite and Spatial IT go together for enterprise geo.</p><iframe src="//player.vimeo.com/video/111882381" width="500" height="313" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> <p><a href="http://vimeo.com/111882381">Geospatial Open Source for the Enterprise</a> from <a href="http://vimeo.com/boundlessgeo">Boundless</a> on <a href="https://vimeo.com">Vimeo</a>.</p>";}i:23;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-5030528820198561222";s:7:"pubdate";s:31:"Tue, 04 Nov 2014 18:41:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2014-11-04T12:38:00.651-08:00";}s:8:"category";s:24:"bccrockofshitfoifoippait";s:5:"title";s:23:"My Tax Dollars at Work?";s:11:"description";s:7355:"<p>Some days I feel like I'm king of the world, and some days I feel like the weak little fool that I am, and today is one of the latter. </p><p><div class="separator" style="clear: both; text-align: center;"><a href="http://novabrowningrutherford.com/wp-content/uploads/2013/10/sucker.jpg" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://novabrowningrutherford.com/wp-content/uploads/2013/10/sucker.jpg" width=150 /></a></div>I have <a href="http://blog.cleverelephant.ca/2013/07/natural-resource-sector-goes-over.html">written</a> <a href="http://blog.cleverelephant.ca/2013/06/bringing-icm-to-natural-resources-sector.html">previously</a> about the upcoming systems re-write in the British Columbia natural resources sector. This is a big one. Over $100M will be spent on IT services and software. Before you spend that kind of money, you should be clear about what the business value is, about why this big moon-shot of a project is the right thing to do, relative to the other options. </p><p>You should prepare a business plan. I wonder what that looks like? </p><p>I know something about data management in the natural resources sector, so I was actually interested more in the implementation plans than the business plans, I FOI'ed: </p><blockquote>Any business case documents or presentations; System development timelines or development plans; Software or product comparisons </blockquote><p>Potentially, this could be a lot of material, at least the FOI person thought so. They suggested I narrow my request: </p><blockquote style="font-color: #a00">...they’ve stated that the first option, “starting from the original request but limiting to only documents delivered to Doug Say”,  would be the best option for narrowing the request and making the fee smaller, as he would be the only one conducting the search and this would significantly reduce the amount of time that would be spent on searching for records... </blockquote><p>So, they were OK with my more limited request, that's great. Until they send back the <a href="http://s3.cleverelephant.ca/FNR-2014-00169-Fees.PDF">fee estimate</a>: </p><table><tr><td>28.5 hour(s) to locate and retrieve records @ $30.00/ hour =</td><td>$855.00</td></tr><tr><td>31.5 hours(s) to produce records @ $30.00/hour</td><td>$945.00</td></tr><tr><td>31.5 hour(s) to prepare and handle records for disclosure @ $30.00/hour = </td><td>$945.00</td></tr><tr><td>1 disc(s) @ $4.00/disc =</td><td>$4.00</td></tr><tr><td>2400 scanned electronic copies of paper records @ $0.10/page =</td><td>$240.00</td></tr><tr><th>Total</th><th>$2989.00</th></tr></table><p>Fortunately, they accept VISA, Mastercard and Amex. Actually, no. Chastened, I reduce my request yet further:</p><blockquote>I feel like I’m fishing with hand grenades here. Can I just have a copy of the business plan and delivery plan (project milestones and expected deliverables?) without an overview I’ll never figure out what to rationally ask for. </blockquote><p>So now we're down to 1-2 documents, no need for a big expensive search, the fee dogs are called off. No doubt, I'll get my documents shortly. The deadline rolls around: </p><blockquote style="font-color: #a00">I am writing to ask that you grant a 30 business day extension to the above-noted FOI request. The additional 30 days are required in order to obtain further information from the Ministry as to the status of the records in their submission to Cabinet. </blockquote><p>OK, I'm not a total a**hole, I can be patient here. And heck, they are making a good faith effort so ensure I can see the docs by checking them against cabinet submissions. What an awesome crew. 30 business days go by: </p><blockquote style="font-color: #a00">We are nearing the end of the review of the records for this request, however we will require some additional time to have it completed. I am writing to request that you grant a 10 business day extension so that we may provide a complete response to this request. </blockquote><p>Again, I'm a man of infinite patience, particularly when a "complete response" is in the offing! By all means, squire, take your 10 business days! 10 business days go by:</p><blockquote style="font-color: #a00">Please see the attached documents in response to your FOI request with the Ministry of Forests, Lands and Natural Resource Operations.  </blockquote><p>Oh, frabjous day! Calooh! Calay! <a href="http://s3.cleverelephant.ca/FNR-2014-00169.PDF">Let's open up our giftie and see what the FOI elves have brought!</a></p><ul><li>A signature page.</li><li>A contacts page.</li><li>The first 3 lines of the table of contents, the remainder redacted because it constitutes "policy advice or recommendations". That's right, the <em>table of contents</em> is policy advice.</li><li>The first page of the executive summary.</li><li>The first page of the "narrative" section (with the middle part of that page redacted for "policy advice" reasons).</li><li>The first page of the "investment context" section (with the end part of that page redacted for "policy advice" reasons).</li></ul><p><div class="separator" style="clear: both; text-align: center;"><a href="http://s3.cleverelephant.ca/FNR-2014-00169.PDF" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://3.bp.blogspot.com/-aTIcDDKVJ_Q/VFkr8pBMcDI/AAAAAAAAAO0/E-Vdql64D-U/s400/screenshot_144.png" /></a></div>That's it! <strong>The remaining 185 pages of this 204 page business case are withheld!</strong> You want to know the reasoning behind the expenditure of over $100M on IT services and software? Too f***ing bad, chump. </p><p>Yep, feeling totally played for the chump. They will spend that $100M, by god, and they will spend it without anyone outside the "yes, minister, yes deputy, yes director" echo chamber seeing the rationale or commenting on it.</p><p><b>Update:</b> I did vent a little back at the FOI officer (which, let's be clear, isn't really fair, he's just the messenger here):</p><blockquote>I'm concerned that this "process" is a farcical waste of your time. If you're going to redact the entire document, save yourself the effort and tell me ahead of time, "BTW, the whole thing will be redacted". Why even pretend, at this point? Who reviewed this document and decided on the redactions?  </blockquote><p>To which he patiently replied:</p><blockquote>I apologise for not making you aware ahead of time that very little information would be released. I was hoping by this time that more information could be provided to you, however the Ministry informed that <i>nothing has been implemented with regard to the initiatives, so the majority of the records are required to be withheld</i> [em. mine] at this time. Should you wish to request these records at a later time, please feel free to contact me before submitting your request to determine their status and whether or not more information would be made available at that time. </blockquote><p>I'm not sure how the implementation or not of things bears into their releasability, perhaps an FOI expert could drop some wisdom in the comments. It seems like the ultimate Catch-22, "we'll explain the reasoning behind our decisions once they are set in stone and cannot be changed".</p>";s:4:"link";s:65:"http://blog.cleverelephant.ca/2014/11/my-tax-dollars-at-work.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"0";}s:7:"summary";s:7355:"<p>Some days I feel like I'm king of the world, and some days I feel like the weak little fool that I am, and today is one of the latter. </p><p><div class="separator" style="clear: both; text-align: center;"><a href="http://novabrowningrutherford.com/wp-content/uploads/2013/10/sucker.jpg" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://novabrowningrutherford.com/wp-content/uploads/2013/10/sucker.jpg" width=150 /></a></div>I have <a href="http://blog.cleverelephant.ca/2013/07/natural-resource-sector-goes-over.html">written</a> <a href="http://blog.cleverelephant.ca/2013/06/bringing-icm-to-natural-resources-sector.html">previously</a> about the upcoming systems re-write in the British Columbia natural resources sector. This is a big one. Over $100M will be spent on IT services and software. Before you spend that kind of money, you should be clear about what the business value is, about why this big moon-shot of a project is the right thing to do, relative to the other options. </p><p>You should prepare a business plan. I wonder what that looks like? </p><p>I know something about data management in the natural resources sector, so I was actually interested more in the implementation plans than the business plans, I FOI'ed: </p><blockquote>Any business case documents or presentations; System development timelines or development plans; Software or product comparisons </blockquote><p>Potentially, this could be a lot of material, at least the FOI person thought so. They suggested I narrow my request: </p><blockquote style="font-color: #a00">...they’ve stated that the first option, “starting from the original request but limiting to only documents delivered to Doug Say”,  would be the best option for narrowing the request and making the fee smaller, as he would be the only one conducting the search and this would significantly reduce the amount of time that would be spent on searching for records... </blockquote><p>So, they were OK with my more limited request, that's great. Until they send back the <a href="http://s3.cleverelephant.ca/FNR-2014-00169-Fees.PDF">fee estimate</a>: </p><table><tr><td>28.5 hour(s) to locate and retrieve records @ $30.00/ hour =</td><td>$855.00</td></tr><tr><td>31.5 hours(s) to produce records @ $30.00/hour</td><td>$945.00</td></tr><tr><td>31.5 hour(s) to prepare and handle records for disclosure @ $30.00/hour = </td><td>$945.00</td></tr><tr><td>1 disc(s) @ $4.00/disc =</td><td>$4.00</td></tr><tr><td>2400 scanned electronic copies of paper records @ $0.10/page =</td><td>$240.00</td></tr><tr><th>Total</th><th>$2989.00</th></tr></table><p>Fortunately, they accept VISA, Mastercard and Amex. Actually, no. Chastened, I reduce my request yet further:</p><blockquote>I feel like I’m fishing with hand grenades here. Can I just have a copy of the business plan and delivery plan (project milestones and expected deliverables?) without an overview I’ll never figure out what to rationally ask for. </blockquote><p>So now we're down to 1-2 documents, no need for a big expensive search, the fee dogs are called off. No doubt, I'll get my documents shortly. The deadline rolls around: </p><blockquote style="font-color: #a00">I am writing to ask that you grant a 30 business day extension to the above-noted FOI request. The additional 30 days are required in order to obtain further information from the Ministry as to the status of the records in their submission to Cabinet. </blockquote><p>OK, I'm not a total a**hole, I can be patient here. And heck, they are making a good faith effort so ensure I can see the docs by checking them against cabinet submissions. What an awesome crew. 30 business days go by: </p><blockquote style="font-color: #a00">We are nearing the end of the review of the records for this request, however we will require some additional time to have it completed. I am writing to request that you grant a 10 business day extension so that we may provide a complete response to this request. </blockquote><p>Again, I'm a man of infinite patience, particularly when a "complete response" is in the offing! By all means, squire, take your 10 business days! 10 business days go by:</p><blockquote style="font-color: #a00">Please see the attached documents in response to your FOI request with the Ministry of Forests, Lands and Natural Resource Operations.  </blockquote><p>Oh, frabjous day! Calooh! Calay! <a href="http://s3.cleverelephant.ca/FNR-2014-00169.PDF">Let's open up our giftie and see what the FOI elves have brought!</a></p><ul><li>A signature page.</li><li>A contacts page.</li><li>The first 3 lines of the table of contents, the remainder redacted because it constitutes "policy advice or recommendations". That's right, the <em>table of contents</em> is policy advice.</li><li>The first page of the executive summary.</li><li>The first page of the "narrative" section (with the middle part of that page redacted for "policy advice" reasons).</li><li>The first page of the "investment context" section (with the end part of that page redacted for "policy advice" reasons).</li></ul><p><div class="separator" style="clear: both; text-align: center;"><a href="http://s3.cleverelephant.ca/FNR-2014-00169.PDF" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://3.bp.blogspot.com/-aTIcDDKVJ_Q/VFkr8pBMcDI/AAAAAAAAAO0/E-Vdql64D-U/s400/screenshot_144.png" /></a></div>That's it! <strong>The remaining 185 pages of this 204 page business case are withheld!</strong> You want to know the reasoning behind the expenditure of over $100M on IT services and software? Too f***ing bad, chump. </p><p>Yep, feeling totally played for the chump. They will spend that $100M, by god, and they will spend it without anyone outside the "yes, minister, yes deputy, yes director" echo chamber seeing the rationale or commenting on it.</p><p><b>Update:</b> I did vent a little back at the FOI officer (which, let's be clear, isn't really fair, he's just the messenger here):</p><blockquote>I'm concerned that this "process" is a farcical waste of your time. If you're going to redact the entire document, save yourself the effort and tell me ahead of time, "BTW, the whole thing will be redacted". Why even pretend, at this point? Who reviewed this document and decided on the redactions?  </blockquote><p>To which he patiently replied:</p><blockquote>I apologise for not making you aware ahead of time that very little information would be released. I was hoping by this time that more information could be provided to you, however the Ministry informed that <i>nothing has been implemented with regard to the initiatives, so the majority of the records are required to be withheld</i> [em. mine] at this time. Should you wish to request these records at a later time, please feel free to contact me before submitting your request to determine their status and whether or not more information would be made available at that time. </blockquote><p>I'm not sure how the implementation or not of things bears into their releasability, perhaps an FOI expert could drop some wisdom in the comments. It seems like the ultimate Catch-22, "we'll explain the reasoning behind our decisions once they are set in stone and cannot be changed".</p>";}i:24;a:10:{s:4:"guid";s:59:"tag:blogger.com,1999:blog-14903426.post-7163517542894164028";s:7:"pubdate";s:31:"Tue, 07 Oct 2014 16:14:00 +0000";s:4:"atom";a:1:{s:7:"updated";s:29:"2014-10-07T09:19:09.361-07:00";}s:8:"category";s:15:"bcenterprise IT";s:5:"title";s:30:"Failure never smelled so sweet";s:11:"description";s:2144:"<p>Politics/IT crossover moment! In his <a href="http://www.theglobeandmail.com/news/british-columbia/petronass-threats-to-abandon-lng-in-bc-are-not-for-naught/article20955266/">Globe &amp; Mail column today</a>, Gary Mason has this to say about the negotiations underway to bring LNG terminals to the BC coast: </p><p><small>Quick primer for the IT audience: in the last election the government promised 100s of thousands of jobs and 100s of billions of revenue (and amazingly, I'm not exaggerating those figures) should BC successfully seed an LNG "industry" on our coast. It was basically all they talked about in the campaign. Unsurprisingly, having placed all their political eggs in the LNG basket, the government is now at the mercy of the companies that are supposed to bring us this windfall, as Mason notes below.</small></p><blockquote>There is an enormous amount at stake for the Premier and her government. From the outside, it appears B.C. needs Petronas and the others more than they need B.C. Ms. Clark comes out the loser if those companies walk away and her LNG dream evaporates. <em>She can also lose if her government signs desperate deals [emphasis mine]</em> that are deemed to be so slanted in favour of the project promoter that the province becomes a global laughingstock.</blockquote><p>Actually no, as anyone on a failed IT project knows, after a desperate deal is signed, both the LNG proponent (vendor) and government (manager) will get together and sing the praises of the finished deal.</p><p>"It's a world class deal, and it was a tough negotiation, but we did it", the government will say.</p><p>"They drove a hard bargain, but we think we can make it work", the proponent will say.</p><p>The media at best will run a he said/she said with the government's claims against the opposition's analysis, and we'll all move on. The government won't be the loser in the case of a bad, desperate deal, <b>the people of BC will</b>.</p><p><small>Just as, when a vendor and a dependent manager deliver a shitty IT project and declare it the best thing since sliced chips, it's the users who suffer in the end.</small></p>";s:4:"link";s:73:"http://blog.cleverelephant.ca/2014/10/failure-never-smelled-so-sweet.html";s:6:"author";s:33:"noreply@blogger.com (Paul Ramsey)";s:3:"thr";a:1:{s:5:"total";s:1:"1";}s:7:"summary";s:2144:"<p>Politics/IT crossover moment! In his <a href="http://www.theglobeandmail.com/news/british-columbia/petronass-threats-to-abandon-lng-in-bc-are-not-for-naught/article20955266/">Globe &amp; Mail column today</a>, Gary Mason has this to say about the negotiations underway to bring LNG terminals to the BC coast: </p><p><small>Quick primer for the IT audience: in the last election the government promised 100s of thousands of jobs and 100s of billions of revenue (and amazingly, I'm not exaggerating those figures) should BC successfully seed an LNG "industry" on our coast. It was basically all they talked about in the campaign. Unsurprisingly, having placed all their political eggs in the LNG basket, the government is now at the mercy of the companies that are supposed to bring us this windfall, as Mason notes below.</small></p><blockquote>There is an enormous amount at stake for the Premier and her government. From the outside, it appears B.C. needs Petronas and the others more than they need B.C. Ms. Clark comes out the loser if those companies walk away and her LNG dream evaporates. <em>She can also lose if her government signs desperate deals [emphasis mine]</em> that are deemed to be so slanted in favour of the project promoter that the province becomes a global laughingstock.</blockquote><p>Actually no, as anyone on a failed IT project knows, after a desperate deal is signed, both the LNG proponent (vendor) and government (manager) will get together and sing the praises of the finished deal.</p><p>"It's a world class deal, and it was a tough negotiation, but we did it", the government will say.</p><p>"They drove a hard bargain, but we think we can make it work", the proponent will say.</p><p>The media at best will run a he said/she said with the government's claims against the opposition's analysis, and we'll all move on. The government won't be the loser in the case of a bad, desperate deal, <b>the people of BC will</b>.</p><p><small>Just as, when a vendor and a dependent manager deliver a shitty IT project and declare it the best thing since sliced chips, it's the users who suffer in the end.</small></p>";}}s:7:"channel";a:10:{s:4:"atom";a:1:{s:2:"id";s:34:"tag:blogger.com,1999:blog-14903426";}s:13:"lastbuilddate";s:31:"Sat, 16 Jan 2016 02:30:53 +0000";s:8:"category";s:1218:"bcitpostgisvideoenterprise ITicmgissprintfoiopen sourceosgeoenterpriseciofoippafoss4gmanagementpoliticsspatial itoutsourcingmapserverbcesisboundlessemailopengeooraclerantCOTSarchitecturecartodbdeloitteesrihpidmjavascriptnatural resourcesogcopen dataopenstudentosspostgresqltechnologyvendorweb1.4.0HRaccess to informationaccountingagilearchiveaspenbcpolibenchmarkbufferbuild vs buybusinessbusiness processccareercathedralclientcloudcodecommon senseconsultingcontractingcore reviewcrmcrockofshitcunitcustomdata sciencedata warehousedesigndevelopmentdigitalenvironmentessentialsevilexadatafcukfgdbfmefoocampfoss4g2007ftpgdalgdsgeocortexgeometrygeoservergeotiffgooglegoogle earthgovernmentgrasshadoopiaasicioimageryindustryinnovationintegrated case managementintroversionisoisssisvalidjpegjtslawyersmappingmcfdmediamicrosoftmoneymysqlnew itnosqlnrs transformationoipcopengisopenlayerspaaspgconfsvpiratespolicyportalproprietary softwarepublic accountsqgisrrdbmsrecursionredistributionregressionrfcright to informationsaassalesforcesardonicscandalseibelsermonserversiebelsnarkspatialstandardsstatisticssvrtaxitempesttexastiredtransittripledeletetwitteruberudigukuk gdsverbal culturevictoriawaterfallwfswherewith recursivewkb";s:5:"title";s:11:"Paul Ramsey";s:11:"description";s:96:"Open source geospatial opinions and techniques, seen from the trenches and from far far above...";s:4:"link";s:30:"http://blog.cleverelephant.ca/";s:14:"managingeditor";s:33:"noreply@blogger.com (Paul Ramsey)";s:9:"generator";s:7:"Blogger";s:10:"opensearch";a:3:{s:12:"totalresults";s:3:"454";s:10:"startindex";s:1:"1";s:12:"itemsperpage";s:2:"25";}s:7:"tagline";s:96:"Open source geospatial opinions and techniques, seen from the trenches and from far far above...";}s:9:"textinput";a:0:{}s:5:"image";a:0:{}s:9:"feed_type";s:3:"RSS";s:12:"feed_version";s:3:"2.0";s:5:"stack";a:0:{}s:9:"inchannel";b:0;s:6:"initem";b:0;s:9:"incontent";b:0;s:11:"intextinput";b:0;s:7:"inimage";b:0;s:13:"current_field";s:0:"";s:17:"current_namespace";b:0;s:5:"ERROR";s:0:"";s:19:"_CONTENT_CONSTRUCTS";a:6:{i:0;s:7:"content";i:1;s:7:"summary";i:2;s:4:"info";i:3;s:5:"title";i:4;s:7:"tagline";i:5;s:9:"copyright";}s:4:"etag";s:42:"W/"ad88eedd-8b86-4175-b03c-7309ae94a835"
";s:13:"last_modified";s:31:"Sat, 16 Jan 2016 02:30:53 GMT
";}