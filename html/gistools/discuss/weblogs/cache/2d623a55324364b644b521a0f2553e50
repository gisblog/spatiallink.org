O:9:"MagpieRSS":20:{s:6:"parser";i:0;s:12:"current_item";a:0:{}s:5:"items";a:30:{i:0;a:6:{s:5:"title";s:42:"Shaun M. Thomas: Foreign Keys are Not Free";s:4:"link";s:59:"http://bonesmoses.org/2014/05/14/foreign-keys-are-not-free/";s:11:"description";s:3742:"<p>PostgreSQL is a pretty good database, and I enjoy working with it. However, there is an implementation detail that not everyone knows about, which can drastically affect table performance. What is this mysterious feature? I am, of course, referring to foreign keys.</p>

<p>Foreign keys are normally a part of good database design, and for good reason. They inform about entity relationships, and they verify, enforce, and maintain those relationships. Yet all of this comes at a cost that might surprise you. In PostgreSQL, every foreign key is maintained with an invisible system-level trigger added to the <em>source</em> table in the reference. At least one trigger must go here, as operations that modify the source data must be checked that they do not violate the constraint.</p>

<p>This query is an easy way to see how many foreign keys are associated with every table in an entire PostgreSQL database:</p>

<pre><code>SELECT t.oid::regclass::text AS table_name, count(1) AS total
  FROM pg_constraint c
  JOIN pg_class t ON (t.oid = c.confrelid)
 GROUP BY table_name
 ORDER BY total DESC;
</code></pre>

<p>With this in mind, consider how much overhead each trigger incurs on the referenced table. We can actually calculate this overhead. Consider this function:</p>

<pre><code>CREATE OR REPLACE FUNCTION fnc_check_fk_overhead(key_count INT)
RETURNS VOID AS
$$
DECLARE
  i INT;
BEGIN
  CREATE TABLE test_fk
  (
    id   BIGINT PRIMARY KEY,
    junk VARCHAR
  );

  INSERT INTO test_fk
  SELECT generate_series(1, 100000), repeat(' ', 20);

  CLUSTER test_fk_pkey ON test_fk;

  FOR i IN 1..key_count LOOP
    EXECUTE 'CREATE TABLE test_fk_ref_' || i || 
            ' (test_fk_id BIGINT REFERENCES test_fk (id))';
  END LOOP;

  FOR i IN 1..100000 LOOP
    UPDATE test_fk SET junk = '                    '
     WHERE id = i;
  END LOOP;

  DROP TABLE test_fk CASCADE;

  FOR i IN 1..key_count LOOP
    EXECUTE 'DROP TABLE test_fk_ref_' || i;
  END LOOP;

END;
$$ LANGUAGE plpgsql VOLATILE;
</code></pre>

<p>The function is designed to create a simple two-column table, fill it with 100,000 records, and test how long it takes to update every record. This is purely meant to simulate a high-transaction load caused by multiple clients. I know no sane developer would actually update so many records this way.</p>

<p>The only parameter this function accepts, is the amount of tables it should create that reference this source table. Every referring table is empty, and has only one column for the reference to be valid. After the foreign key tables are created, it performs those 100,000 updates, and we can measure the output with our favorite SQL tool. Here is a quick test with <code>psql</code>:</p>

<pre><code>\timing
SELECT fnc_check_fk_overhead(0);
SELECT fnc_check_fk_overhead(5);
SELECT fnc_check_fk_overhead(10);
SELECT fnc_check_fk_overhead(15);
SELECT fnc_check_fk_overhead(20);
</code></pre>

<p>On our system, these timings were collected several times, and averaged 2961ms, 3805ms, 4606ms, 5089ms, and 5785ms after three runs each. As we can see, after merely five foreign keys, performance of our updates drops by 28.5%. By the time we have 20 foreign keys, the updates are 95% slower!</p>

<p>I don&#8217;t mention this to make you abandon foreign keys. However, if you are in charge of an extremely active OLTP system, you might consider removing any non-critical FK constraints. If the values are merely informative, or will not cause any integrity concerns, a foreign key is not required. Indeed, excessive foreign keys are actually detrimental to the database in a very tangible way.</p>

<p>I merely ask you keep this in mind when designing or revising schemas for yourself or developers you support.</p>";s:4:"guid";s:28:"http://bonesmoses.org/?p=906";s:7:"pubdate";s:29:"Wed, 14 May 2014 21:40:23 GMT";s:7:"summary";s:3742:"<p>PostgreSQL is a pretty good database, and I enjoy working with it. However, there is an implementation detail that not everyone knows about, which can drastically affect table performance. What is this mysterious feature? I am, of course, referring to foreign keys.</p>

<p>Foreign keys are normally a part of good database design, and for good reason. They inform about entity relationships, and they verify, enforce, and maintain those relationships. Yet all of this comes at a cost that might surprise you. In PostgreSQL, every foreign key is maintained with an invisible system-level trigger added to the <em>source</em> table in the reference. At least one trigger must go here, as operations that modify the source data must be checked that they do not violate the constraint.</p>

<p>This query is an easy way to see how many foreign keys are associated with every table in an entire PostgreSQL database:</p>

<pre><code>SELECT t.oid::regclass::text AS table_name, count(1) AS total
  FROM pg_constraint c
  JOIN pg_class t ON (t.oid = c.confrelid)
 GROUP BY table_name
 ORDER BY total DESC;
</code></pre>

<p>With this in mind, consider how much overhead each trigger incurs on the referenced table. We can actually calculate this overhead. Consider this function:</p>

<pre><code>CREATE OR REPLACE FUNCTION fnc_check_fk_overhead(key_count INT)
RETURNS VOID AS
$$
DECLARE
  i INT;
BEGIN
  CREATE TABLE test_fk
  (
    id   BIGINT PRIMARY KEY,
    junk VARCHAR
  );

  INSERT INTO test_fk
  SELECT generate_series(1, 100000), repeat(' ', 20);

  CLUSTER test_fk_pkey ON test_fk;

  FOR i IN 1..key_count LOOP
    EXECUTE 'CREATE TABLE test_fk_ref_' || i || 
            ' (test_fk_id BIGINT REFERENCES test_fk (id))';
  END LOOP;

  FOR i IN 1..100000 LOOP
    UPDATE test_fk SET junk = '                    '
     WHERE id = i;
  END LOOP;

  DROP TABLE test_fk CASCADE;

  FOR i IN 1..key_count LOOP
    EXECUTE 'DROP TABLE test_fk_ref_' || i;
  END LOOP;

END;
$$ LANGUAGE plpgsql VOLATILE;
</code></pre>

<p>The function is designed to create a simple two-column table, fill it with 100,000 records, and test how long it takes to update every record. This is purely meant to simulate a high-transaction load caused by multiple clients. I know no sane developer would actually update so many records this way.</p>

<p>The only parameter this function accepts, is the amount of tables it should create that reference this source table. Every referring table is empty, and has only one column for the reference to be valid. After the foreign key tables are created, it performs those 100,000 updates, and we can measure the output with our favorite SQL tool. Here is a quick test with <code>psql</code>:</p>

<pre><code>\timing
SELECT fnc_check_fk_overhead(0);
SELECT fnc_check_fk_overhead(5);
SELECT fnc_check_fk_overhead(10);
SELECT fnc_check_fk_overhead(15);
SELECT fnc_check_fk_overhead(20);
</code></pre>

<p>On our system, these timings were collected several times, and averaged 2961ms, 3805ms, 4606ms, 5089ms, and 5785ms after three runs each. As we can see, after merely five foreign keys, performance of our updates drops by 28.5%. By the time we have 20 foreign keys, the updates are 95% slower!</p>

<p>I don&#8217;t mention this to make you abandon foreign keys. However, if you are in charge of an extremely active OLTP system, you might consider removing any non-critical FK constraints. If the values are merely informative, or will not cause any integrity concerns, a foreign key is not required. Indeed, excessive foreign keys are actually detrimental to the database in a very tangible way.</p>

<p>I merely ask you keep this in mind when designing or revising schemas for yourself or developers you support.</p>";}i:1;a:6:{s:5:"title";s:49:"Dimitri Fontaine: Why is pgloader so much faster?";s:4:"link";s:59:"http://tapoueh.org/blog/2014/05/14-pgloader-got-faster.html";s:11:"description";s:7178:"<p><a href="http://pgloader.io/">pgloader</a> loads data into PostgreSQL. The new version is stable enough
nowadays that it's soon to be released, the last piece of the 
<code>3.1.0</code> puzzle
being full 
<a href="https://www.debian.org/">debian</a> packaging of the tool.
</p><center><img src="http://tapoueh.org/images/toy-loader.320.jpg" /></center><center><em>The pgloader logo is a loader truck, just because</em>.</center><p>As you might have noticed if you've read my blog before, I decided that
<a href="http://pgloader.io/">pgloader</a> needed a full rewrite in order for it to be able to enter the
current decade as a relevant tool. pgloader used to be written in the
<a href="https://www.python.org/">python programming language</a>, which is used by lots of people and generally
quite appreciated by its users.
</p><h2>Why changing</h2><p>Still, python is not without problems, the main ones I had to deal with
being 
<em>poor performances</em> and lack of threading capabilities. Also, the
pgloader setup design was pretty hard to maintain, and adding compatiblity
to other 
<em>loader</em> products from competitors was harder than it should.
</p><p>As I said in my 
<a href="http://tapoueh.org/confs/2014/05/05-ELS-2014">pgloader lightning talk</a> at the 
<a href="http://www.european-lisp-symposium.org/">7th European Lisp Symposium</a>
last week, in searching for a 
<em><strong>modern programming language</strong></em> the best candidate
I found was actually 
<a href="http://en.wikipedia.org/wiki/Common_Lisp">Common Lisp</a>.
</p><center><a href="http://tapoueh.org/images/confs/ELS_2014_pgloader.pdf"><img src="http://tapoueh.org/images/confs/ELS_2014_pgloader.png" /></a></center><p>After some basic performances checking as seen in my
<a href="https://github.com/dimitri/sudoku">Common Lisp Sudoku Solver</a> project where I did get up to 
<em><strong>ten times faster</strong></em>
code when compared to python, it felt like the amazing set of features of
the language could be put to good use here.
</p><h2>So, what about performances after rewrite?</h2><p>The main reason why I'm now writing this blog post is receiving emails from
pgloader users with strange feelings about the speedup. Let's see at the
numbers one user gave me, for some data point:
</p><pre><code>&#xA0;select rows, v2, v3,
        round((  extract(epoch from v2)
               / extract(epoch from v3))::numeric, 2) as speedup
   from timing;
        
  rows   |        v2         |       v3        | speedup 
---------+-------------------+-----------------+---------
 4768765 | @ 37 mins 10.878  | @ 1 min 26.917  |   25.67
 3115880 | @ 36 mins 5.881   | @ 1 min 10.994  |   30.51
 3865750 | @ 33 mins 40.233  | @ 1 min 15.33   |   26.82
 3994483 | @ 29 mins 30.028  | @ 1 min 18.484  |   22.55
(4 rows)
</code></pre><center><em>The raw numbers have been loaded into a PostgreSQL table</em></center><p>So what we see in this quite typical 
<a href="http://pgloader.io/howto/csv.html">CSV Loading</a> test case is a best case of
<em><strong>30 times faster</strong></em> import. Which brings some questions on the table, of course.
</p><h2>Wait, you're still using <code>COPY</code> right?</h2><p>The 
<a href="http://www.postgresql.org/docs/9.3/interactive/index.html">PostgreSQL</a> database system provides a really neat 
<a href="http://www.postgresql.org/docs/9.3/interactive/sql-copy.html">COPY</a> command, which in
turn is only exposing the 
<a href="http://www.postgresql.org/docs/9.3/static/protocol-flow.html#PROTOCOL-COPY">COPY Streaming Protocol</a>, that pgloader is using.
</p><p>So yes, 
<a href="http://pgloader.io/">pgloader</a> is still using 
<code>COPY</code>. This time the protocol implementation
is to be found in the Common Lisp 
<a href="http://marijnhaverbeke.nl/postmodern/">Postmodern</a> driver, which is really great.
Before that, back when pgloader was python code, it was using the very good
<a href="http://initd.org/psycopg/">psycopg</a> driver, which also exposes the COPY protocol.
</p><h2>So, what did happen here?</h2><p>Well it happens that pgloader is now built using Common Lisp technologies,
and those are really great, powerful and fast!
</p><p>Not only is Common Lisp code compiled to 
<em>machine code</em> when using most
<a href="http://cliki.net/Common%20Lisp%20implementation">Common Lisp Implementations</a> such as 
<a href="http://sbcl.org/">SBCL</a> or 
<a href="http://ccl.clozure.com/">Clozure Common Lisp</a>; it's also
possible to actually benfit from 
<em>parallel computing</em> and 
<em>threads</em> in Common
Lisp.
</p><center><img src="http://tapoueh.org/images/speedup.jpg" /></center><center><em>That's not how I did it!</em></center><p>In the 
<a href="http://pgloader.io/">pgloader</a> case I've been using the 
<a href="http://lparallel.org/">lparallel</a> utilities, in particular
its 
<a href="http://lparallel.org/api/queues/">queuing facility</a> to be able to implement 
<em>asynchronous IOs</em> where a thread
reads the source data and preprocess it, fills up a batch at a time in a
buffer that is then pushed down to the writer thread, that handles the 
<code>COPY</code>
protocol and operations.
</p><p>So my current analysis is that the new thread based architecture used with a
very powerful compiler for the Common Lisp high-level language are allowing
pgloader to enter a whole new field of 
<em>data loading performances</em>.
</p><h2>Conclusion</h2><p>Not only is pgloader so much faster now, it's also full of new capabilities
and supports several sources of data such as 
<a href="http://pgloader.io/howto/dBase.html">dBase files</a>,
<a href="http://pgloader.io/howto/sqlite.html">SQLite database files</a> or even 
<a href="http://pgloader.io/howto/mysql.html">MySQL live connections</a>.
</p><p>Rather than a configuration file, the way to use the new pgloader is using a
<em>command language</em> that has been designed to look as much like SQL as possible
in the pgloader context, to make it easy for its users. Implementation wise,
it should now be trivial enough to implement compatibility with other 
<em>data
load</em> software that some 
<a href="http://www.postgresql.org/">PostgreSQL</a> competitor products do have.
</p><p>Also, the new code base and feature set seems to attract way more users than
the previous implementation ever did, despite using a less popular
programming language.
</p><p>You can already 
<a href="http://pgloader.io/download.html">download pgloader binary packages</a> for 
<em>debian</em> based
distributions and 
<em>centos</em> based ones too, and you will even find a 
<em>Mac OS X</em>
package file (
<code>.pkg</code>) that will make 
<code>/usr/local/bin/pgloader</code> available for you
on the command line. If you need a windows binary, drop me an email.
</p><p>The first stable release of the new 
<a href="http://pgloader.io/">pgloader</a> utility is scheduled to be
named 
<code>3.1.0</code> and to happen quite soon. We are hard at work on packaging the
dependencies for 
<em>debian</em>, and you can have a look at the 
<a href="https://github.com/dimitri/ql-to-deb">Quicklisp to debian</a>
project if you want to help us get there!
</p>";s:4:"guid";s:59:"http://tapoueh.org/blog/2014/05/14-pgloader-got-faster.html";s:7:"pubdate";s:29:"Wed, 14 May 2014 12:59:00 GMT";s:7:"summary";s:7178:"<p><a href="http://pgloader.io/">pgloader</a> loads data into PostgreSQL. The new version is stable enough
nowadays that it's soon to be released, the last piece of the 
<code>3.1.0</code> puzzle
being full 
<a href="https://www.debian.org/">debian</a> packaging of the tool.
</p><center><img src="http://tapoueh.org/images/toy-loader.320.jpg" /></center><center><em>The pgloader logo is a loader truck, just because</em>.</center><p>As you might have noticed if you've read my blog before, I decided that
<a href="http://pgloader.io/">pgloader</a> needed a full rewrite in order for it to be able to enter the
current decade as a relevant tool. pgloader used to be written in the
<a href="https://www.python.org/">python programming language</a>, which is used by lots of people and generally
quite appreciated by its users.
</p><h2>Why changing</h2><p>Still, python is not without problems, the main ones I had to deal with
being 
<em>poor performances</em> and lack of threading capabilities. Also, the
pgloader setup design was pretty hard to maintain, and adding compatiblity
to other 
<em>loader</em> products from competitors was harder than it should.
</p><p>As I said in my 
<a href="http://tapoueh.org/confs/2014/05/05-ELS-2014">pgloader lightning talk</a> at the 
<a href="http://www.european-lisp-symposium.org/">7th European Lisp Symposium</a>
last week, in searching for a 
<em><strong>modern programming language</strong></em> the best candidate
I found was actually 
<a href="http://en.wikipedia.org/wiki/Common_Lisp">Common Lisp</a>.
</p><center><a href="http://tapoueh.org/images/confs/ELS_2014_pgloader.pdf"><img src="http://tapoueh.org/images/confs/ELS_2014_pgloader.png" /></a></center><p>After some basic performances checking as seen in my
<a href="https://github.com/dimitri/sudoku">Common Lisp Sudoku Solver</a> project where I did get up to 
<em><strong>ten times faster</strong></em>
code when compared to python, it felt like the amazing set of features of
the language could be put to good use here.
</p><h2>So, what about performances after rewrite?</h2><p>The main reason why I'm now writing this blog post is receiving emails from
pgloader users with strange feelings about the speedup. Let's see at the
numbers one user gave me, for some data point:
</p><pre><code>&#xA0;select rows, v2, v3,
        round((  extract(epoch from v2)
               / extract(epoch from v3))::numeric, 2) as speedup
   from timing;
        
  rows   |        v2         |       v3        | speedup 
---------+-------------------+-----------------+---------
 4768765 | @ 37 mins 10.878  | @ 1 min 26.917  |   25.67
 3115880 | @ 36 mins 5.881   | @ 1 min 10.994  |   30.51
 3865750 | @ 33 mins 40.233  | @ 1 min 15.33   |   26.82
 3994483 | @ 29 mins 30.028  | @ 1 min 18.484  |   22.55
(4 rows)
</code></pre><center><em>The raw numbers have been loaded into a PostgreSQL table</em></center><p>So what we see in this quite typical 
<a href="http://pgloader.io/howto/csv.html">CSV Loading</a> test case is a best case of
<em><strong>30 times faster</strong></em> import. Which brings some questions on the table, of course.
</p><h2>Wait, you're still using <code>COPY</code> right?</h2><p>The 
<a href="http://www.postgresql.org/docs/9.3/interactive/index.html">PostgreSQL</a> database system provides a really neat 
<a href="http://www.postgresql.org/docs/9.3/interactive/sql-copy.html">COPY</a> command, which in
turn is only exposing the 
<a href="http://www.postgresql.org/docs/9.3/static/protocol-flow.html#PROTOCOL-COPY">COPY Streaming Protocol</a>, that pgloader is using.
</p><p>So yes, 
<a href="http://pgloader.io/">pgloader</a> is still using 
<code>COPY</code>. This time the protocol implementation
is to be found in the Common Lisp 
<a href="http://marijnhaverbeke.nl/postmodern/">Postmodern</a> driver, which is really great.
Before that, back when pgloader was python code, it was using the very good
<a href="http://initd.org/psycopg/">psycopg</a> driver, which also exposes the COPY protocol.
</p><h2>So, what did happen here?</h2><p>Well it happens that pgloader is now built using Common Lisp technologies,
and those are really great, powerful and fast!
</p><p>Not only is Common Lisp code compiled to 
<em>machine code</em> when using most
<a href="http://cliki.net/Common%20Lisp%20implementation">Common Lisp Implementations</a> such as 
<a href="http://sbcl.org/">SBCL</a> or 
<a href="http://ccl.clozure.com/">Clozure Common Lisp</a>; it's also
possible to actually benfit from 
<em>parallel computing</em> and 
<em>threads</em> in Common
Lisp.
</p><center><img src="http://tapoueh.org/images/speedup.jpg" /></center><center><em>That's not how I did it!</em></center><p>In the 
<a href="http://pgloader.io/">pgloader</a> case I've been using the 
<a href="http://lparallel.org/">lparallel</a> utilities, in particular
its 
<a href="http://lparallel.org/api/queues/">queuing facility</a> to be able to implement 
<em>asynchronous IOs</em> where a thread
reads the source data and preprocess it, fills up a batch at a time in a
buffer that is then pushed down to the writer thread, that handles the 
<code>COPY</code>
protocol and operations.
</p><p>So my current analysis is that the new thread based architecture used with a
very powerful compiler for the Common Lisp high-level language are allowing
pgloader to enter a whole new field of 
<em>data loading performances</em>.
</p><h2>Conclusion</h2><p>Not only is pgloader so much faster now, it's also full of new capabilities
and supports several sources of data such as 
<a href="http://pgloader.io/howto/dBase.html">dBase files</a>,
<a href="http://pgloader.io/howto/sqlite.html">SQLite database files</a> or even 
<a href="http://pgloader.io/howto/mysql.html">MySQL live connections</a>.
</p><p>Rather than a configuration file, the way to use the new pgloader is using a
<em>command language</em> that has been designed to look as much like SQL as possible
in the pgloader context, to make it easy for its users. Implementation wise,
it should now be trivial enough to implement compatibility with other 
<em>data
load</em> software that some 
<a href="http://www.postgresql.org/">PostgreSQL</a> competitor products do have.
</p><p>Also, the new code base and feature set seems to attract way more users than
the previous implementation ever did, despite using a less popular
programming language.
</p><p>You can already 
<a href="http://pgloader.io/download.html">download pgloader binary packages</a> for 
<em>debian</em> based
distributions and 
<em>centos</em> based ones too, and you will even find a 
<em>Mac OS X</em>
package file (
<code>.pkg</code>) that will make 
<code>/usr/local/bin/pgloader</code> available for you
on the command line. If you need a windows binary, drop me an email.
</p><p>The first stable release of the new 
<a href="http://pgloader.io/">pgloader</a> utility is scheduled to be
named 
<code>3.1.0</code> and to happen quite soon. We are hard at work on packaging the
dependencies for 
<em>debian</em>, and you can have a look at the 
<a href="https://github.com/dimitri/ql-to-deb">Quicklisp to debian</a>
project if you want to help us get there!
</p>";}i:2;a:6:{s:5:"title";s:97:"Michael Paquier: Postgres 9.4 feature highlight: MSVC installer for client binaries and libraries";s:4:"link";s:92:"http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-msvc-client-installer/";s:11:"description";s:2435:"<p>Today here is a highlight of a new Postgres 9.4 feature interesting for
developers and companies doing packaging of Postgres on Windows as it
makes possible the installation of client-only binaries and libraries
using MSVC. It has been introduced by this commit:</p>
<div class="highlight"><pre><code class="text language-text">commit a7e5f7bf6890fdf14a6c6ecd0854ac3f5f308ccd
Author: Andrew Dunstan &lt;andrew@dunslane.net&gt;
Date:   Sun Jan 26 17:03:13 2014 -0500

Provide for client-only installs with MSVC.

MauMau.
</code></pre></div>
<p>Enters in the client package all the binaries used to interact directly
with the server (psql, pg_dump, pgbench) and the interface libraries
(libpq, ecpg).</p>

<p>Documentation precisely describes <a href="http://www.postgresql.org/docs/devel/static/install-windows-full.html">how to set up</a>
an environment to compile PostgreSQL on Windows, so in short here is
the new command that you can use from src/tools/msvc in for example a
Windows SDK command prompt:</p>
<div class="highlight"><pre><code class="text language-text">install c:\install\to\path client
</code></pre></div>
<p>The command &quot;install&quot; will install by default everything if no keyword
is specified. As a new behavior, the keyword &quot;all&quot; can be used to install
everything, meaning that the following commands are equivalent:</p>
<div class="highlight"><pre><code class="text language-text">install c:\install\to\path
install c:\install\to\path all
</code></pre></div>
<p>After the client installation, you will get the following things
installed:</p>
<div class="highlight"><pre><code class="text language-text">$ ls /c/install/to/path/bin/
clusterdb.exe   droplang.exe   oid2name.exe        pg_isready.exe      reindexdb.exe
createdb.exe    dropuser.exe   pg_basebackup.exe   pg_receivexlog.exe  vacuumdb.exe
createlang.exe  ecpg.exe       pg_config.exe       pg_restore.exe      vacuumlo.exe
createuser.exe  libpq.dll      pg_dump.exe         pgbench.exe
dropdb.exe      oid2name.exe   pg_dumpall.exe      psql.exe
$ ls /c/install/to/path/lib/
libecpg.dll  libecpg_compat.dll  libpgcommon.lib  libpgtypes.dll  libpq.dll  postgres.lib
libecpg.lib  libecpg_compat.lib  libpgport.lib    libpgtypes.lib  libpq.lib
</code></pre></div>
<p>This is going to simplify a bit more the life of Windows packagers who
have up to now needed custom scripts to install client-side things only.
So thanks MauMau!</p>";s:4:"guid";s:92:"http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-msvc-client-installer/";s:7:"pubdate";s:29:"Wed, 14 May 2014 12:41:17 GMT";s:7:"summary";s:2435:"<p>Today here is a highlight of a new Postgres 9.4 feature interesting for
developers and companies doing packaging of Postgres on Windows as it
makes possible the installation of client-only binaries and libraries
using MSVC. It has been introduced by this commit:</p>
<div class="highlight"><pre><code class="text language-text">commit a7e5f7bf6890fdf14a6c6ecd0854ac3f5f308ccd
Author: Andrew Dunstan &lt;andrew@dunslane.net&gt;
Date:   Sun Jan 26 17:03:13 2014 -0500

Provide for client-only installs with MSVC.

MauMau.
</code></pre></div>
<p>Enters in the client package all the binaries used to interact directly
with the server (psql, pg_dump, pgbench) and the interface libraries
(libpq, ecpg).</p>

<p>Documentation precisely describes <a href="http://www.postgresql.org/docs/devel/static/install-windows-full.html">how to set up</a>
an environment to compile PostgreSQL on Windows, so in short here is
the new command that you can use from src/tools/msvc in for example a
Windows SDK command prompt:</p>
<div class="highlight"><pre><code class="text language-text">install c:\install\to\path client
</code></pre></div>
<p>The command &quot;install&quot; will install by default everything if no keyword
is specified. As a new behavior, the keyword &quot;all&quot; can be used to install
everything, meaning that the following commands are equivalent:</p>
<div class="highlight"><pre><code class="text language-text">install c:\install\to\path
install c:\install\to\path all
</code></pre></div>
<p>After the client installation, you will get the following things
installed:</p>
<div class="highlight"><pre><code class="text language-text">$ ls /c/install/to/path/bin/
clusterdb.exe   droplang.exe   oid2name.exe        pg_isready.exe      reindexdb.exe
createdb.exe    dropuser.exe   pg_basebackup.exe   pg_receivexlog.exe  vacuumdb.exe
createlang.exe  ecpg.exe       pg_config.exe       pg_restore.exe      vacuumlo.exe
createuser.exe  libpq.dll      pg_dump.exe         pgbench.exe
dropdb.exe      oid2name.exe   pg_dumpall.exe      psql.exe
$ ls /c/install/to/path/lib/
libecpg.dll  libecpg_compat.dll  libpgcommon.lib  libpgtypes.dll  libpq.dll  postgres.lib
libecpg.lib  libecpg_compat.lib  libpgport.lib    libpgtypes.lib  libpq.lib
</code></pre></div>
<p>This is going to simplify a bit more the life of Windows packagers who
have up to now needed custom scripts to install client-side things only.
So thanks MauMau!</p>";}i:3;a:6:{s:5:"title";s:66:"Tim van der Linden: PostgreSQL: A full text search engine - Part 3";s:4:"link";s:64:"http://shisaa.jp/postset/postgresql-full-text-search-part-3.html";s:11:"description";s:78648:"<p>And so we arrive at the last part of the series.</p>
<p>If you have not done so, please read <a href="http://shisaa.jp/postset/postgresql-full-text-search-part-1.html" title="First chapter introducing the full text search capabilities of PostgreSQL.">part one</a> and <a href="http://shisaa.jp/postset/postgresql-full-text-search-part-2.html" title="Second chapter introducing the full text search capabilities of PostgreSQL.">part two</a> before embarking.</p>
<p>Today we will close up the introduction into PostgreSQL's full text capabilities by showing you a few aspects I have intentionally neglected in the previous parts. The most important ones being ranking and indexing.</p>
<p>So let us take off right away!</p>
<h3>Ranking</h3>
<p>Up until now you have seen what full text is, how to use it and how to do a full custom setup. What you have not yet seen is how to <em>rank</em> search results based on their relevance to the search query - a feature that most search engines offer and one that most users expect.</p>
<p>However, there is a problem when it comes to ranking, it something that is somewhat <em>undefined</em>. It is a gray area left wide open to interpretation. It is almost...personal.</p>
<p>In its core, ranking within full text means giving a document a place based on how many times certain words occur in a document, or how close these words are relevant to each other. So let us start there.</p>
<h4>Normal ranking</h4>
<p>The first case, ranking based on how many times certain words occur, has a accompanying function ready to be used: <em>ts_rank()</em>. It accepts a mandatory <em>tsvector</em> and a <em>tsquery</em> as its arguments and returns a float which represents how high the given document ranks. The function also accepts a <em>weights array</em> and <em>normalization integer</em>, but that is for later down the road.</p>
<p>Let us test out the basic functionality:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants'</span><span class="p">));</span>
</pre></div>


<p>This is an regular old 'on the fly' query where we feed a string which we convert to a tsvector and a <em>token</em> which is converted to a tsquery. The ranking result of this is:</p>
<div class="code"><pre>0.0607927
</pre></div>


<p>This does not say much, does it? Okay, let us throw a few more tokens in the mix:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins'</span><span class="p">));</span>
</pre></div>


<p>Now we want to query the two tokens <em>elephants</em> and <em>dolphins</em>. We chain them together in an AND (<em>&amp;</em>) formation. The ranking:</p>
<div class="code"><pre>0.0985009
</pre></div>


<p>Hmm, getting higher, good. More tokens please:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins &amp; habitat &amp; living'</span><span class="p">));</span>
</pre></div>


<p>Results in:</p>
<div class="code"><pre>0.414037
</pre></div>


<p>Oooh, that is quite nice. Notice the word <em>living</em>, the <em>tsquery</em> automatically stems it to match <em>live</em>, but that is, of course, all basic knowledge by now.</p>
<p>The idea here is simple, the more tokens match the string, the higher the ranking will be. You can use this float to later on sort your results.</p>
<h4>Normal ranking with weights</h4>
<p>Okay, let us spice things up a little bit, let us look at the <em>weights array</em> that could be set as an optional parameter.</p>
<p>Do you remember the weights we saw in chapter one? A quick rundown: You can optionally give weights to lexemes in a tsvector to group them together. This is, most of the time, used to reflect the original document structure within a tsvector. We also saw that, actually, all lexemes contain a standard weight of '<em>D</em>' unless specified otherwise.</p>
<p>Weights, when ranking, define importance of words. The <em>ts_rank()</em> function will automatically take these weights into account and use a <em>weights array</em> to influence the ranking float. Remember that there are only four possible weights: <em>A</em>, <em>B</em>, <em>C</em> and <em>D</em>. </p>
<p>The weights array has a default value of:</p>
<div class="code"><pre><span class="o">{</span>0.1, 0.2, 0.4, 1.0<span class="o">}</span>
</pre></div>


<p>These values correspond to the weight letters you can assign. Note that these are in reverse order, the array represents: {D,C,B,A}.</p>
<p>Let us test that out. We take the same query as before, but now using the <em>setweight()</em> function, we will apply a weight of <em>C</em> to all lexemes:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span><span class="s1">'C'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins &amp; habitat &amp; live'</span><span class="p">));</span>
</pre></div>


<p>The result:</p>
<div class="code"><pre>0.674972
</pre></div>


<p>Wow, that is a lot higher then our last ranking (which had an implicit, default weight of <em>D</em>).
The reason for this is that the floats in the weights array <em>influence</em> the ranking calculation.
Just for fun, you can override the default weights array, simply by passing it in as a first argument.
Let us put the weights all equal to the default of <em>D</em> being <em>0.1</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="nb">array</span><span class="p">[</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span><span class="s1">'C'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins &amp; habitat &amp; live'</span><span class="p">));</span>
</pre></div>


<p>And we get back:</p>
<div class="code"><pre>0.414037
</pre></div>


<p>You can see that this is now back to the value we had before we assigned weights, or in other words, when the implicit weight was <em>D</em>. You can thus influence what kind of an effect a certain weight has in you ranking. You can even reverse the lot and make a D have a more positive influence then an A, just to mess with peoples heads.</p>
<h4>Normal ranking, the fair way</h4>
<p>Not that what we have seen up until now was unfair, but is does not take into account the length of the documents searched through</p>
<p>Document length is also an important factor when judging the relevance. A short document which matches on four or five tokens has a different relevance than a three times as long document which matches on the same amount of tokens. The shorter one is probably more relevant then the longer one.</p>
<p>The same ranking function <em>ts_rank()</em> has an extra, final optional parameter that you can pass in called the <em>normalization integer</em>. This integer can have a combination of seven different values, they can be a single integer or mixed with a pipe (|) to pass in multiple values.</p>
<p>The default value is <em>0</em> - meaning that it will ignore document length all together, giving us the more "unfair" behavior. The next values you can give are <em>1</em>, <em>2</em>, <em>4</em>, <em>8</em>, <em>16</em> and <em>32</em> which stand for the following manipulations of the ranking float:</p>
<ul>
<li>1: It will divide the ranking float by the sum of 1 and the logarithmic number of the document length. The latter number is the ratio this document has compared to the other documents you wish to compare.</li>
<li>2: Simply divides the ranking float by the length of the document.</li>
<li>4: Divides the ranking float by the harmonic mean (the fair average) between matched tokens. This one is only uses by the other ranking function <em>ts_rank_cd</em>.</li>
<li>8: Divides the ranking float by the number of <em>unique</em> words that are found in the document. </li>
<li>16: Divides the ranking float by the sum of 1 and the logarithmic number of the number of <em>unique</em> words found in the document.</li>
<li>32: Simply divides the ranking float by <em>itself</em> and adds one to that.</li>
</ul>
<p>These are a lot of values and some of them are quite confusing. But all of these have only one purpose: to make ranking more "fair", based on various use cases.</p>
<p>Take, for example, <em>1</em> and <em>2</em>. These calculate document length by taking into account the amount of <em>words</em> present in the document.
The <em>words</em> here reference the amount of <em>pointers</em> that are present in the tsvector.</p>
<p>To illustrate, we will convert the sentence "These token are repeating on purpose. Bad tokens!" into a tsvector, resulting in:</p>
<div class="code"><pre><span class="s1">'bad'</span>:7 <span class="s1">'purpos'</span>:6 <span class="s1">'repeat'</span>:4 <span class="s1">'token'</span>:2,8
</pre></div>


<p>The <em>length</em> of this document is <em>5</em>, becuase we have <em>five pointers</em> in total.</p>
<p>If you now look at the integers <em>8</em> and <em>16</em>, they take the <em>uniqueness</em> to calculate document length.
What that means is they do not count the pointers, but the actual <em>lexemes</em>.
In the above tsvector and thus would result in a length of <em>4</em>.</p>
<p>All of these manipulations are just different ways of counting document length.
The ones summed up in the above integer list are mere educated guesses at what most people desire when ranking with a full text engine.
As I said in the beginning, it is a gray area, left open for interpretation.</p>
<p>Let us try to see the different effects that such an integer can have.</p>
<p>First we need to create a few documents (tsvectors) inside our famous phraseTable (from the previous chapters) that we will use throughout this chapter.
Connect to your phrase database, add a "title" column, truncate whatever we have stored there and insert a few variable length documents based on Edgar Allan Poe's "The Raven".
I have prepared the whole syntax below, this time you may copy-and-paste:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">phraseTable</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">title</span> <span class="nb">VARCHAR</span><span class="p">;</span>
<span class="k">TRUNCATE</span> <span class="n">phraseTable</span><span class="p">;</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more."'</span><span class="p">),</span> <span class="s1">'Tiny Allan'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore.'</span><span class="p">),</span> <span class="s1">'Small Allan'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."'</span><span class="p">),</span> <span class="s1">'Medium Allan'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."  Presently my soul grew stronger; hesitating then no longer, "Sir," said I, "or Madam, truly your forgiveness I implore; But the fact is I was napping, and so gently you came rapping, And so faintly you came tapping, tapping at my chamber door, That I scarce was sure I heard you"- here I opened wide the door; - Darkness there, and nothing more.'</span><span class="p">),</span> <span class="s1">'Big Allan'</span><span class="p">);</span>
</pre></div>


<p>Nothing better then some good old Edgar to demonstrate a full text search ranking. Here we have four different lengths of the same verse making for four documents of different lengths stored in our tsvector column. Now we would like to search through these documents and find the keywords <em>'door'</em> and <em>'gently'</em>, ranking them as we go.</p>
<p>For later reference, let us first count how many times our keywords occur in the sentence:</p>
<ul>
<li>Tiny Allan: "door" 2, "gently" 1</li>
<li>Small Allan: "door" 2, "gently" 1</li>
<li>Medium Allan: "door" 4, "gently" 1</li>
<li>Big Allan: "door" 6, "gently" 2</li>
</ul>
<p>First, let us simply rank the result with the default normalization of <em>0</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span><span class="p">;</span>
</pre></div>


<p>Before we go over the results, a little bit about this query for people who are not so familiar with this SQL syntax.
We do a simple <em>SELECT</em> from a data set using <em>FROM</em> filtering it with a <em>WHERE</em> clause.
Going over it line by line:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
</pre></div>


<p>We <em>SELECT</em> on the <em>title</em> column we just made and on a "on-the-fly" column we create for the result set named <em>rank</em> which contains the result of the <em>ts_rank()</em> function.</p>
<div class="code"><pre><span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
</pre></div>


<p>In the <em>FROM</em> clause you can put a series of statements that will deliver the data for the query. In this case we take our normal database <em>table</em> and the result of the <em>to_tsquery()</em> function which we name <em>keywords</em> so we can use it throughout the query itself.</p>
<div class="code"><pre><span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span><span class="p">;</span>
</pre></div>


<p>Here we filter the result set using the <em>WHERE</em> clause and the <em>matching</em> operator (@@). The @@ is a Boolean operator, meaning it will simply return <em>true</em> or <em>false</em>.
So in this case, we check if the result of the <em>to_tsquery()</em> function (named keywords and which will return lexemes) <em>match</em> the results of the phrase <em>column</em> from our table (which contains <em>tsvectors</em> and thus lexemes). We want to rank only those phrases that actually contain our keywords.</p>
<p>Now, back to our ranking. The result of this query will be:</p>
<div class="code"><pre>   title     |   rank    
--------------+-----------
Tiny Allan   | 0.0906565
Small Allan  | 0.0906565
Medium Allan | 0.0906565
Big Allan    |   0.10109
</pre></div>


<p>Let us order the results first, so the most relevant document is always on top:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>Result:</p>
<div class="code"><pre>   title     |   rank    
--------------+-----------
Big Allan    |   0.10109
Tiny Allan   | 0.0906565
Small Allan  | 0.0906565
Medium Allan | 0.0906565
</pre></div>


<p>"Big Allen" is on top, for it has more occurrences of the keywords "door" and "gently".
But to be fair, in ratio "Tiny Allan" has almost the same amount of occurrences of both keywords. Three times less, but it also is three times as small.</p>
<p>So let us take document length (based on <em>word count</em>) into account, setting our <em>normalization</em> to <em>1</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>You will get:</p>
<div class="code"><pre>   title     |   rank    
--------------+-----------
Tiny Allan   | 0.0181313
Small Allan  | 0.0151094
Big Allan    | 0.0145124
Medium Allan |  0.013831
</pre></div>


<p>This could be seen as a more fair ranking, "Tiny Allan" is now on top because, considering its <em>ratio</em>, it is the most relevant. "Medium Allan" falls all the way down because it is almost as big as "Big Allan", but contains lesser occurrences of the keywords. In total five keywords in contrast to "Big Allan" who has eight but is only slightly bigger.</p>
<p>Let us do the same, but count the document length based on the <em>unique</em> occurences using integer <em>8</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>The result:</p>
<div class="code"><pre>Tiny Allan   | 0.00335765
Small Allan  | 0.00161887
Medium Allan | 0.00119285
Big Allan    | 0.00105303
</pre></div>


<p>That is a very different result, but quite what you should expect.</p>
<p>We are searching for only <em>two</em> tokens here, and considering the fact that uniqueness is now adhered, all the extra occurrences of these words are ignored.
This means that for the ranking algorithm, all the documents we searched through (which all have at least one occurrence of each token) get normalized to only <em>2</em> matching tokens.
And in that case, the shortest document wins hands down, for it is seen as most relevant. As you can see in the result set, the documents are neatly ordered from tiny to big.</p>
<h4>Ranking with density</h4>
<p>Up until now we have seen the "normal" ranking function <em>ts_rank()</em>, which is the one you will probably use the most.</p>
<p>There is, however, one more function at our direct disposal called <em>ts_rank_cd()</em>. The <em>cd</em> stands for <em>Cover Density</em> and is simply yet another way of considering relevance.
This function has exactly the same required and optional arguments, it simply counts relevancy differently.
Very important for this function to work properly is that you do not let it operate on a <em>stripped</em> tsvector.</p>
<p>A stripped tsvector is one that has been undone of its pointer information. If you know that you do not need this pointer information - you just need to match tsqueries against the lexemes in you tsvector - you can strip these pointers and thus make for smaller footprints in your database.</p>
<p>In case of our cover density ranker, it needs this positional pointer information to see how <em>close</em> the search tokens are to each other.
It makes sense that this ranking function only works on multiple tokens, on single tokens it is kind of pointless.</p>
<p>In a way, this ranking function looks for <em>phrases</em> rather then single tokens; the closer lexemes are together, the more positive influence they will have on the resulting ranking float.</p>
<p>In our "Raven" examples this might be a little bit hard to see, so let me demonstrate this with a couple of new, on-the-fly queries.</p>
<p>We wish to search for the tokens <em>'token'</em> and <em>'count'</em>.</p>
<p>First, a sentence in which the searched for tokens are wide apart: "These tokens are very wide apart and therefor do not count as much.":</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'These tokens are very wide apart and do not count as much.'</span><span class="p">)</span> <span class="n">phrase</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'token &amp; count'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>Will have this tsvector:</p>
<div class="code"><pre><span class="s1">'apart'</span>:6 <span class="s1">'count'</span>:10 <span class="s1">'much'</span>:12 <span class="s1">'token'</span>:2 <span class="s1">'wide'</span>:5
</pre></div>


<p>And this result:</p>
<div class="code"><pre> 0.008624
</pre></div>


<p>Let us put these tokens closer together now: "These tokens count for much now that they are not so wide apart!":</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'These tokens count for much now that they are not so wide apart!'</span><span class="p">)</span> <span class="n">phrase</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'token &amp; count'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>The vector:</p>
<div class="code"><pre><span class="s1">'apart'</span>:13 <span class="s1">'count'</span>:3 <span class="s1">'much'</span>:5 <span class="s1">'token'</span>:2 <span class="s1">'wide'</span>:12
</pre></div>


<p>The result:</p>
<div class="code"><pre>0.0198206
</pre></div>


<p>You can see that both the vectors have <em>exactly</em> the same lexemes, but different pointer information.
In the second vector, the tokens we searched for are next to each other, which results in a ranking float that is more then double of the first result.</p>
<p>This demonstrates the working of this function. The same optional manipulations can be passed in (weights and normalization) and they will have roughly the same effect.</p>
<p>Pick the ranking function that is best fit for your use case.</p>
<p>It needs to be said that the two ranking functions we have seen so far are officially called <em>example functions</em> by the PostgreSQL community.
They are functions devised to be fitting for most purposes, but also to demonstrate how you could write your own.</p>
<p>If you have very specific use cases it is advised to write you own ranking functions to fit your exact needs.
But this is considered beyond the scope of this series (and maybe also beyond the scope of your needs).</p>
<h3>Highlight your results!</h3>
<p>The next interesting thing we can do with the results of our full text is to highlight the relevant words.</p>
<p>As is the case with many search engines, users want to skim over an excerpt of each result to see if it is what they are searching for.
For this PostgreSQL delivers us yet another function: <em>ts_headline()</em>.</p>
<p>To demonstrate its use, we first have to make our small database a little bit bigger by inserting the original text of the "Raven" next to our tsvectors.
So, again, copy and past this new set of queries (yes you may...):</p>
<div class="code"><pre><span class="k">TRUNCATE</span> <span class="n">phraseTable</span><span class="p">;</span>
<span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">phraseTable</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">article</span> <span class="nb">TEXT</span><span class="p">;</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more."'</span><span class="p">),</span> <span class="s1">'Tiny Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more."'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore.'</span><span class="p">),</span> <span class="s1">'Small Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore.'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."'</span><span class="p">),</span> <span class="s1">'Medium Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."  Presently my soul grew stronger; hesitating then no longer, "Sir," said I, "or Madam, truly your forgiveness I implore; But the fact is I was napping, and so gently you came rapping, And so faintly you came tapping, tapping at my chamber door, That I scarce was sure I heard you"- here I opened wide the door; - Darkness there, and nothing more.'</span><span class="p">),</span> <span class="s1">'Big Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."  Presently my soul grew stronger; hesitating then no longer, "Sir," said I, "or Madam, truly your forgiveness I implore; But the fact is I was napping, and so gently you came rapping, And so faintly you came tapping, tapping at my chamber door, That I scarce was sure I heard you"- here I opened wide the door; - Darkness there, and nothing more.'</span><span class="p">);</span>
</pre></div>


<p>Good, we now have the same data, but this time we stored the text of the original document alongside the "vectorized" version.</p>
<p>The reason for this being that this <em>ts_headline()</em> function searches in the original documents (being our <em>article</em> column) rather that in your ts_vector column.
Two arguments are mandatory: the original article and the ts_query. The optional arguments are the full text configuration you wish to use and a string of additional, comma separated options.</p>
<p>But first, let us take a look at its most basic usage:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Will give you:</p>
<div class="code"><pre>   title     |                                                      result                                                      
--------------+------------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
Small Allan  | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
Medium Allan | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
Big Allan    | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
</pre></div>


<p>As you can see, we get back a short excerpt of each verse with the tokens of interest surrounded with a HTML "&lt;b&gt;" tag.
That is actual all there is to this function, it return the results with the tokens highlighted.</p>
<p>However, there are some nice options you can set to alter this basic behavior.</p>
<p>The first one up is the HTML tag you wish to put around you highlighted words. For this you have two variables <em>StartSel</em> and <em>StopSel</em>.
If we wanted this to be a "&lt;em&gt;" tag instead, we could tell the function to change as follows:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'StartSel=&lt;em&gt;,StopSel=&lt;/em&gt;'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>And now we will get back an &lt;em&gt; instead of a &lt;b&gt; (including just one row this time):</p>
<div class="code"><pre>   title     |                                                        result                                                        
--------------+----------------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;em&gt;gently&lt;/em&gt; rapping, rapping at my chamber &lt;em&gt;door&lt;/em&gt;. "Tis some visitor," I muttered, "tapping at my chamber
</pre></div>


<p>In fact, it does not need to be HTML at all, you can put (almost) any string there:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'StartSel=foobar&gt;&gt;,StopSel=&lt;&lt;barfoo '</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Result:</p>
<div class="code"><pre>   title     |                                                               result                                                               
--------------+------------------------------------------------------------------------------------------------------------------------------------
Tiny Allan   | foobar&gt;&gt;gently&lt;&lt;barfoo rapping, rapping at my chamber foobar&gt;&gt;door&lt;&lt;barfoo. "Tis some visitor," I muttered, "tapping at my chamber
</pre></div>


<p>Quite awesome!</p>
<p>Another attribute you can tamper with is how many words should be included in the result set by using the <em>MaxWords</em> and <em>MinWords</em>: </p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'MaxWords=4,MinWords=1'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Which gives you:</p>
<div class="code"><pre>   title     |             result             
--------------+--------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping
</pre></div>


<p>To make the resulting headline a little bit more readable there is an attribute in this options string called <em>ShortWord</em> which tells the function which is the shortest word that may appear at the start or end of the headline. </p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'ShortWord=8'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Will give you:</p>
<div class="code"><pre>   title     |                                                                                   result                                                                                    
--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber &lt;b&gt;door&lt;/b&gt; - Only this, and nothing more."
Small Allan  | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber &lt;b&gt;door&lt;/b&gt; - Only this, and nothing more." Ah, distinctly
</pre></div>


<p>Now it will try and set word boundaries to words of minimal 8 letters. This time I included the second line of the result set. As you can see the engine could not find an 8 letter word at the remainder of the document, so it simply prints it until the end. The second row, "Small Allan" is a bit bigger and the word "distinctly" has more then 8 letters, so is set as the boundary,</p>
<p>So far the headline function has given us almost full sentences and not really fragments of text. This is because the optional <em>MaxFragments</em> defaults to 0. If we up this variable, it will start to include fragments and not sentences. Let us try it out:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'MaxFragments=2,MaxWords=8,MinWords=1'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Gives you</p>
<div class="code"><pre>   title     |                                                     result                                                      
--------------+-----------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;
...
Big Allan    | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt; ... chamber &lt;b&gt;door&lt;/b&gt; - This it is, and nothing more
</pre></div>


<p>I include only the first and last line of this result set. As you can see on the last line, the result is now fragmented, and we get back different pieces of our result.
If, for instance, four or five tokens match in our document, setting the <em>MaxFragments</em> to a higher value will show more of these matches glued together.</p>
<p>Accompanying this <em>MaxFragments</em> option is the <em>FragmentDelimiter</em> variable which is used to define, well, the delimiter between the fragments. Short demo:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'MaxFragments=2,FragmentDelimiter=;,MaxWords=8,MinWords=1'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>You will get:</p>
<div class="code"><pre>   title     |                                                   result                                                    
--------------+-------------------------------------------------------------------------------------------------------------
Big Allan    | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;;chamber &lt;b&gt;door&lt;/b&gt; - This it is, and nothing more
</pre></div>


<p>Including only the last line, you will see we now have a semicolon (;) instead of a ellipses (...). Neat.</p>
<p>A final, less common option for the <em>ts_headline()</em> function is to ignore all the word boundaries we set before and simply return the <em>whole</em> document and highlight all the words of relevance.
This variable is called <em>HighlightAll</em> and is a Boolean set to <em>false</em> by default:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'HighlightAll=true'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>The result would be too large to print here, but try it out. It will give you the whole text, but with the important tokens decorated with the element (or text) of choice.</p>
<h4>A big word of <em>caution</em></h4>
<p>It is very fun to play with highlighting your results, I will admit that. The only problem is, as you might have concluded yourself, this is a potential performance grinder.</p>
<p>The problem here is that this function cannot use any indexes and it can also not use your stored tsvector. It <em>needs</em> the original document text and it needs to not only parse the whole document text to a tsvector for matching, it also needs to parse the original document text a second time to find the substrings and <em>decorate</em> them with the characters you have set. And this whole process has to happen <em>for every single record</em> in your result set.</p>
<p>Highlighting, with this function, is a <em>very</em> expensive to do. </p>
<p>This does not mean that you have to avoid this function, if so I would have told you from the start and skipped this whole part. No, it is there to be used. But use it in a correct way.</p>
<p>A correct way often seen is to use the highlighting only on the top results you are interested in - the top results the user has on their screen at the moment.
This could be achieved in SQL with a so called <em>subquery</em>.</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span><span class="p">,</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">keywords</span><span class="p">,</span> <span class="n">article</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">phrase</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
          <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">AS</span> <span class="n">keywords</span>
          <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span>
          <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span>
          <span class="k">LIMIT</span> <span class="mi">2</span><span class="p">)</span> <span class="k">AS</span> <span class="k">alias</span><span class="p">;</span>
</pre></div>


<p>For those unfamiliar, a <em>subquery</em> is nothing more than a query within a query (queue Inception drums...sorry).</p>
<p>You evaluate the inner query and use the result set of that to perform the outer query. You can achieve the same with two queries, but that would prove not to be as elegant.
When PostgreSQL sees a subquery, it can plan and execute more efficiently then with separate queries, many times giving you a better performance.</p>
<p>The query you see above might look a bit frightening to beginning SQL folk, but simply see it as two separate ones and the beast becomes a tiny mouse.
Unless you are afraid of mice, let it become a...euhm...soft butterfly gliding on the wind instead.</p>
<p>In the inner query we perform the actual matching and ranking as we have seen before. This inner query then only returns <em>two</em> matching records, because of the <em>LIMIT</em> clause.
The outer query takes those results and performs the expensive operation of highlighting.</p>
<h3>Indexing</h3>
<p>Back to a more serious matter, the act of <em>indexing</em>.</p>
<p>If you do not know what an index is, you have to brush up real fast, for indexing is quite important for the performance of your queries.
In a very simplistic view, an index is like a chapter listing in a book. You can quickly skim over the chapters to find the page you are looking for, instead of having to flip over every single page.</p>
<p>You typically put indexes on tables which are consulted often and you build the index in a way that is in parallel with how you query them.</p>
<p>As indexing is a whole topic, or rather, a whole profession of its own, I will not go too deeply into the matter.
But I will try to give you some basic knowledge on the subject.</p>
<p>Note that I will go over this matter in lighting speed and thus have to greatly skim down on the amount of details.
A <em>very</em> good place to learn about indexes is Markus Winand's <a href="http://use-the-index-luke.com/" title="Use The Index, Luke series written by Markus Winand.">Use The Index, Luke</a> series.
I seriously suggest you read that stuff, it is golden knowledge for every serious developer working with databases.</p>
<h4>B-tree</h4>
<p>Before we can go to the PostgreSQL full text index types we first have to look at the most common index type, the <em>Binary tree</em> or <em>B-tree</em>.</p>
<p>The <em>B-tree</em> is a proven "computer science" concept that give us a way to search certain types of data, fast.</p>
<p>A <em>B-tree</em> is a tree structure with a root, nodes and leafs (inverse from a natural tree).
The data that is within your table rows will be ordered and chopped up to fit within the tree.</p>
<p>In database indexes we mostly use <em>balanced B-trees</em>, meaning that each level of the tree has the same amount of nodes.</p>
<p>Take this picture for example:</p>
<div class="code"><pre>            |root|
              |
       ----------------
       |               |
    |node|          |node|
       |               |
  ----------       --------- 
  |        |       |       |
|node|  |node|  |node|  |node|
</pre></div>


<p>In <em>B-tree</em> terms, we summarize this tree by saying:</p>
<ul>
<li>It has an <em>order</em> of 2, meaning that each node will contain two leaves only</li>
<li>It has a <em>depth</em> of 3, meaning it is three levels deep (including the root node)</li>
<li>It has 4 <em>leaves</em>, meaning that the amount of nodes that do not contain children is 4 (bottom row)</li>
</ul>
<p>If you set the <em>order</em> of your tree to a higher number, more nodes can fit onto a single row and you will end up with a lesser <em>depth</em>.</p>
<p>Now the actual power of an index comes from an I/O perspective. As you know (or will know now) the thing that will slow down a program/computer the most is I/O.
This can be network I/O, disk I/O, etc. In case of our database we will speak of disk I/O. </p>
<p>When a database has to go and <em>scan</em> your table without an index is has to plow through all your rows to find a match.
Database rows are almost always <em>not</em> I/O optimized, this means that they do not fit well in the blocks of your physical disks structure.
This, in short, means that there is a lot of overhead in reading through that physical data,</p>
<p>A <em>B-tree</em> on the other hand, is <em>very</em> optimized for I/O. Each level of a <em>B-tree</em> will try and fit perfectly within one physical block on your disk.
If all levels fit within one block each, walking over the tree will be very efficient and have almost no overhead.</p>
<p><em>B-trees</em> work with the most common data types such as TEXT, INT, VARCHAR, ... .</p>
<p>But because full text search in PostgreSQL is its own "thing" (using the @@ operator), all knowledge that you may have learned about regarding indexes does not apply (or not in full anyway) to full text search.</p>
<p>Full text search needs its own kind of indexing  for a <em>tsquery</em> to be able to use them. 
And as we will see in a moment, indexing on full text in PostgreSQL is a dance of trade-offs.
When it comes to this matter we have two types of indexes available: <em>GiST</em> and <em>GiN</em> which are both closely related to the <em>B-tree</em>.</p>
<h4>GiST</h4>
<p>GiST stands for <em>Generalized Search Tree</em> and can both be set on <em>tsvector</em> and <em>tsquery</em> column types, though most of the time you will use it on the former.</p>
<p>The <em>GiST</em> itself is not something that is unique to PostgreSQL, it is a project on its own and its concept is laid out in a C library called <em>libGist</em>.
You could go ahead and play around with <em>libGiist</em> to get a better understanding of how it works, it even comes shipped with some demo applications.</p>
<p>Over time there have come many new types of trees based on the <em>B-tree</em> concept, but most of them are limit in how they can match.
A <em>B-tree</em> and its direct descendants can only use basic match operators like "&lt;", "&gt;", "=", etc.
A <em>GiST</em> index, however, has more advanced matching capabilities like "intersect" and in case of PostgreSQL's implementation: the <em>"@@"</em> operator.</p>
<p>Another big advantage of the <em>GiST</em> is the fact that it can store arbitrary data types and therefor can be used in a wide area of conduct.
The trade off for the wide data type support is the fact that <em>GiST</em> will always return a <em>no</em> if there is no match or a <em>maybe</em> if there is.
There is no true <em>hit</em> with this kind of index.</p>
<p>Because of this behavior there is extra overhead in the case of full text search because PostgreSQL has to manually go and check all the <em>maybe</em>'s that return and see if they are an actual match.</p>
<p>The big advantages of <em>GiST</em> are the fact that the index builds faster and the update of such an index is less expensive then the next index type we will see.</p>
<h4>GiN</h4>
<p>The second index candidate we have at our disposal is the <em>Generalized Inverted Index</em> or <em>GiN</em> in short.</p>
<p>Same as we saw with <em>GiST</em>, <em>GiN</em> also allows for arbitrary data types to be indexes and allows for more matching operators to be used.
But as opposed to <em>GiST</em>, a <em>GiN</em> index is <em>deterministic</em> - it will always return a true match, cutting the checking overhead needed with <em>GiST</em>.</p>
<p>Well, unless you wish to use <em>weights</em> in your queries. A <em>GiN</em> index does <em>not</em> store lexeme weights. This means that, if weights need to be taken into account when querying, PostgreSQL still has to go and fetch the actual row(s) that return a true match, giving you somewhat of the same overhead as with a <em>GiST</em> index.</p>
<p><em>GiN</em> tries to improve the <em>B-tree</em> concept by minimizing the amount of redundancy within nodes and there leaves.
When you search for a number between 0 and 1000, it can be that your index has to go over 5 levels to find the desired entry.
This means that the four levels above the matching leaf could potentially contain an (implied) reference to the row id you wish to have fetched.
In a <em>GiN</em> index, this is generalized by storing a single entry of the duplicates into so-called <em>posting trees</em> and <em>posting lists</em> and pointing to those lists instead of drilling down multiple levels.</p>
<p>The downside of <em>GiN</em> is the fact that this kind of index will slow down the bigger it gets.</p>
<p>On a more positive note, <em>GiN</em> indexes are most of the time smaller on disk (because of it trying to reduce duplicates). And, as of PostgreSQL 9.4, they will be <em>even</em> smaller.
The soon-to-be version will introduce a so-called <em>varbyte</em> version of <em>GiN</em>. For now just take it from me that it will make these type of indexes <em>much</em> smaller, and even more efficient.</p>
<p>As you can see, there is no perfect index when it comes to full text. You will have to carefully look at what data you will save and how you wish to query the data.</p>
<p>If you do not update your database much but you have a lot of querying going on, <em>GiN</em> might be a better option for it is much faster with a lookup (if no weights are required).
If your data does not get read much, but is updated frequently, maybe a <em>GiST</em> is a better choice for it allows for faster updating.</p>
<h4>Making an index</h4>
<p>We have (very roughly) seen what an index is and what we have available for full text, but how do you actually build such an index?</p>
<p>Luckily for us, this too has been neatly abstracted and is very simple to do.</p>
<p>If we wanted our phraseTable to contain an index, we simply could go about and create it with the following syntax:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">prhasetable_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gin</span><span class="p">(</span><span class="n">phrase</span><span class="p">);</span>
</pre></div>


<p>This will create a <em>GiN</em> index called <em>phrasetable_ixd</em> on the column <em>phrase</em>.</p>
<p>Just like we did before, we will now re-populate our <em>phrase</em> column, but this time we will fill it with the data we want to have indexed: article and title.
Let me show you what I mean.</p>
<p>First, empty the four <em>phrase</em> columns in our tiny database:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">phraseTable</span> <span class="k">ALTER</span> <span class="n">phrase</span> <span class="k">DROP</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">;</span>
<span class="k">UPDATE</span> <span class="n">phraseTable</span> <span class="k">set</span> <span class="n">phrase</span> <span class="o">=</span> <span class="k">NULL</span><span class="p">;</span>
</pre></div>


<p>Notice that I removed the <em>NOT NULL</em> constraint.
Next we can populate it containing a tsvector of both the <em>title</em> and the <em>article</em> columns:</p>
<div class="code"><pre><span class="k">UPDATE</span> <span class="n">phraseTable</span> <span class="k">SET</span> <span class="n">phrase</span> <span class="o">=</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span> <span class="o">||</span> <span class="s1">' '</span> <span class="o">||</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">article</span><span class="p">,</span><span class="s1">''</span><span class="p">));</span>
</pre></div>


<p>The <em>coalesce</em> function may be something that you are unfamiliar with.
This functions simply returns the first argument which is <em>not</em> NULL.
In this case we use:</p>
<div class="code"><pre><span class="n">coalesce</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span>
</pre></div>


<p>Which means that if title would be NULL it will return the empty string <em>''</em> which never is NULL.
We use <em>coalesce</em> here to substitute a value for NULL, being the empty string.</p>
<p>If we would not substitute NULL then our <em>tsvector</em> generation would fail if either the title or article column would be NULL.</p>
<p>Next we can create an index on that newly filled column:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">phraseable_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gin</span><span class="p">(</span><span class="n">phrase</span><span class="p">);</span>
</pre></div>


<p>And we have magic, there now is a <em>GiN</em> index on that column which will be used during full text search.</p>
<p>To create a <em>GiST</em> index we could use exactly the same syntax:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">phrasetable_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gist</span><span class="p">(</span><span class="n">phrase</span><span class="p">);</span>
</pre></div>


<p>Now, the disk-space savvy readers will have noticed that our "phrase" column now contains some redundant information as we store the tsvector of the article and title column that was already in the database.
If you do not wish to have this extra column, you could created <em>expression</em> indexes (our on-the-fly queries we seen before).</p>
<p>The setup of such an expression index is trivial:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">phrasetable_exp_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gin</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span> <span class="o">||</span> <span class="s1">' '</span> <span class="o">||</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">article</span><span class="p">,</span><span class="s1">''</span><span class="p">)));</span>
</pre></div>


<p>Instead of having this extra tsvector column around, we now have created an on-the-fly index using the same syntax as we employed when we populated the <em>phrase</em> column a few lines back.</p>
<p>One important thing to note when you use <em>expression</em> indexes is the text search configuration you used. Here we specify that we wish the index to be created using the 'english' configuration set.
This results in an index which is configuration aware and will <em>only</em> work with a query which has the <em>same</em> configuration set fed to the tsquery function (well, the same name anyway).</p>
<p>You could omit the configuration which would then default to the one set in the "default_text_search_config" variable we saw in the last chapter. The problem you will have then is that the index is created using a configuration that <em>could</em> be altered <em>after</em> the index was created. If we later would query the database with the altered default, the index would be useless and will return inaccurate results. </p>
<p>Also note that we may save on disk space when we use the <em>expression</em> index, but we do not save on CPU. Now, instead of indexing data already parsed and ready in a column, the index has to compute the <em>to_tsvector</em> on every index match. Again, a world of trade-offs.</p>
<h3>Triggers</h3>
<p>A final, small topic I want to briefly touch on before I let you go free are <em>update triggers</em>. The way we have been populating our database so far does not need a trigger actually. Up until now we have been inserting records (or updating them) using the <em>ts_tsvector()</em> function. The negative aspect of going about it the way we did is that it is extremely <em>redundant</em>. </p>
<p>If we inserted a piece of Raven text into a record, we specified it <em>twice</em>, one time for the <em>article</em> column and one time for the <em>phrase</em> column which holds the tsvector result.</p>
<p>A better way to do this is to not let the insert query care about the tsvector <em>at all</em>. We simply insert the text we like and let the database do the converting  behind the curtains.</p>
<p>This is where a <em>trigger</em> comes in handy. Actually, PostgreSQL has a whole set of <em>trigger</em> functions available that will fire when certain conditions are met, but when it comes to full text we have two functions at our disposal.</p>
<h4>tsvector_update_trigger()</h4>
<p>The first, and most used one, is called <em>tsvector_update_trigger()</em> and fires whenever a new row is inserted into your table (in our case <em>phraseTable</em>).</p>
<p>To setup such a trigger, we could use the following SQL:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">TRIGGER</span> <span class="n">tsvectorupdate</span> <span class="k">BEFORE</span> <span class="k">INSERT</span> <span class="k">OR</span> <span class="k">UPDATE</span>
    <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">FOR</span> <span class="k">EACH</span> <span class="k">ROW</span> <span class="k">EXECUTE</span> <span class="k">PROCEDURE</span>
    <span class="n">tsvector_update_trigger</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="s1">'pg_catalog.english'</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">article</span><span class="p">);</span>
</pre></div>


<p>That is all you need to setup such a trigger. Let us see what we just did.</p>
<p>First, we have new syntax staring us in the face: <em>CREATE TRIGGER</em>. This will create a trigger on certain <em>events</em>. The events here are <em>BEFORE INSERT</em> and <em>BEFORE UPDATE</em> which are contracted to <em>BEFORE INSERT OR UPDATE</em>. Then we specify on which <em>table</em> this trigger has to act and for each <em>ROW</em>. Then we say we want to <em>EXECUTE PROCEDURE</em>, which, in our case, is the function <em>tsvector_update_trigger()</em>.</p>
<p>The function itself needs a bit of explaining as well. This version takes three required arguments: the tsvector column name, the full text configuration name and the original text column name.
The latter can be multiple columns to concatenate them together. This concatenation is done with <em>coalesce</em> under the hood, as we have seen before.</p>
<p>In our case, we create a trigger that takes the <em>phrase</em> tsvector column, the <em>enlgish</em> full text configuration and concatenates the text from both <em>title</em> and <em>article</em> to be normalized into lexemes.</p>
<p>Note that instead of <em>english</em> we say <em>pg_catalog.english</em> when providing this function with the full text configuration.
In case of this function (and the next) we have to provide the schema-qualified path to the configuration.</p>
<h4>tsvector_update_trigger_column()</h4>
<p>The other of the two full text trigger functions we have is called <em>tsvector_update_trigger_column()</em> and has only one difference to the former: the full text configuration used.
Here, the full text configuration can be read from a <em>column</em> instead of given directly as a string.</p>
<p>A possibility we have not seen in this series is one where you can have yet another column in your phraseTable where you store the name of the full text configuration you wish to use.
This way you can store multiple "languages" within the same table, specifying which configuration to use with each row.</p>
<p>This trigger functions can take into account these per-row differing configurations and is able to read them from the specified column.</p>
<p>But we have a trade-off once more. These two trigger functions, which are officially called <em>example functions</em> again (remember our ranking functions?), do <em>not</em> take into account weights.
If you have the need to store different weights in your tsvectors, you will have to write you own trigger function.</p>
<h3>The end</h3>
<p>Okay, I guess this covers the <em>basics</em> of full text within PostgreSQL.</p>
<p>We have covered the most important parts and touched some segments deeply, others just with a soft lovers glove.
As I always say at the end of such lengthy chapters: go out and explore.</p>
<p>I have tried to give you a solid, full text knowledge base to build further adventures on. I highly encourage you to pack your elephant, take your new ship for a maiden voyage, set high the sails and if certain blue wales try to swim next to your vessel, simply let the mammoth take a good relief down the ship's head, and let those turds float together with our squeaky finned friends!</p>
<p>And as always...thanks for reading!</p>
<!--  LocalWords:  PostgreSQL lexeme
 -->";s:4:"guid";s:64:"http://shisaa.jp/postset/postgresql-full-text-search-part-3.html";s:7:"pubdate";s:29:"Wed, 14 May 2014 09:00:00 GMT";s:7:"summary";s:78648:"<p>And so we arrive at the last part of the series.</p>
<p>If you have not done so, please read <a href="http://shisaa.jp/postset/postgresql-full-text-search-part-1.html" title="First chapter introducing the full text search capabilities of PostgreSQL.">part one</a> and <a href="http://shisaa.jp/postset/postgresql-full-text-search-part-2.html" title="Second chapter introducing the full text search capabilities of PostgreSQL.">part two</a> before embarking.</p>
<p>Today we will close up the introduction into PostgreSQL's full text capabilities by showing you a few aspects I have intentionally neglected in the previous parts. The most important ones being ranking and indexing.</p>
<p>So let us take off right away!</p>
<h3>Ranking</h3>
<p>Up until now you have seen what full text is, how to use it and how to do a full custom setup. What you have not yet seen is how to <em>rank</em> search results based on their relevance to the search query - a feature that most search engines offer and one that most users expect.</p>
<p>However, there is a problem when it comes to ranking, it something that is somewhat <em>undefined</em>. It is a gray area left wide open to interpretation. It is almost...personal.</p>
<p>In its core, ranking within full text means giving a document a place based on how many times certain words occur in a document, or how close these words are relevant to each other. So let us start there.</p>
<h4>Normal ranking</h4>
<p>The first case, ranking based on how many times certain words occur, has a accompanying function ready to be used: <em>ts_rank()</em>. It accepts a mandatory <em>tsvector</em> and a <em>tsquery</em> as its arguments and returns a float which represents how high the given document ranks. The function also accepts a <em>weights array</em> and <em>normalization integer</em>, but that is for later down the road.</p>
<p>Let us test out the basic functionality:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants'</span><span class="p">));</span>
</pre></div>


<p>This is an regular old 'on the fly' query where we feed a string which we convert to a tsvector and a <em>token</em> which is converted to a tsquery. The ranking result of this is:</p>
<div class="code"><pre>0.0607927
</pre></div>


<p>This does not say much, does it? Okay, let us throw a few more tokens in the mix:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins'</span><span class="p">));</span>
</pre></div>


<p>Now we want to query the two tokens <em>elephants</em> and <em>dolphins</em>. We chain them together in an AND (<em>&amp;</em>) formation. The ranking:</p>
<div class="code"><pre>0.0985009
</pre></div>


<p>Hmm, getting higher, good. More tokens please:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins &amp; habitat &amp; living'</span><span class="p">));</span>
</pre></div>


<p>Results in:</p>
<div class="code"><pre>0.414037
</pre></div>


<p>Oooh, that is quite nice. Notice the word <em>living</em>, the <em>tsquery</em> automatically stems it to match <em>live</em>, but that is, of course, all basic knowledge by now.</p>
<p>The idea here is simple, the more tokens match the string, the higher the ranking will be. You can use this float to later on sort your results.</p>
<h4>Normal ranking with weights</h4>
<p>Okay, let us spice things up a little bit, let us look at the <em>weights array</em> that could be set as an optional parameter.</p>
<p>Do you remember the weights we saw in chapter one? A quick rundown: You can optionally give weights to lexemes in a tsvector to group them together. This is, most of the time, used to reflect the original document structure within a tsvector. We also saw that, actually, all lexemes contain a standard weight of '<em>D</em>' unless specified otherwise.</p>
<p>Weights, when ranking, define importance of words. The <em>ts_rank()</em> function will automatically take these weights into account and use a <em>weights array</em> to influence the ranking float. Remember that there are only four possible weights: <em>A</em>, <em>B</em>, <em>C</em> and <em>D</em>. </p>
<p>The weights array has a default value of:</p>
<div class="code"><pre><span class="o">{</span>0.1, 0.2, 0.4, 1.0<span class="o">}</span>
</pre></div>


<p>These values correspond to the weight letters you can assign. Note that these are in reverse order, the array represents: {D,C,B,A}.</p>
<p>Let us test that out. We take the same query as before, but now using the <em>setweight()</em> function, we will apply a weight of <em>C</em> to all lexemes:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span><span class="s1">'C'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins &amp; habitat &amp; live'</span><span class="p">));</span>
</pre></div>


<p>The result:</p>
<div class="code"><pre>0.674972
</pre></div>


<p>Wow, that is a lot higher then our last ranking (which had an implicit, default weight of <em>D</em>).
The reason for this is that the floats in the weights array <em>influence</em> the ranking calculation.
Just for fun, you can override the default weights array, simply by passing it in as a first argument.
Let us put the weights all equal to the default of <em>D</em> being <em>0.1</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="nb">array</span><span class="p">[</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Elephants and dolphins do not live in the same habitat.'</span><span class="p">),</span><span class="s1">'C'</span><span class="p">),</span>
               <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; dolphins &amp; habitat &amp; live'</span><span class="p">));</span>
</pre></div>


<p>And we get back:</p>
<div class="code"><pre>0.414037
</pre></div>


<p>You can see that this is now back to the value we had before we assigned weights, or in other words, when the implicit weight was <em>D</em>. You can thus influence what kind of an effect a certain weight has in you ranking. You can even reverse the lot and make a D have a more positive influence then an A, just to mess with peoples heads.</p>
<h4>Normal ranking, the fair way</h4>
<p>Not that what we have seen up until now was unfair, but is does not take into account the length of the documents searched through</p>
<p>Document length is also an important factor when judging the relevance. A short document which matches on four or five tokens has a different relevance than a three times as long document which matches on the same amount of tokens. The shorter one is probably more relevant then the longer one.</p>
<p>The same ranking function <em>ts_rank()</em> has an extra, final optional parameter that you can pass in called the <em>normalization integer</em>. This integer can have a combination of seven different values, they can be a single integer or mixed with a pipe (|) to pass in multiple values.</p>
<p>The default value is <em>0</em> - meaning that it will ignore document length all together, giving us the more "unfair" behavior. The next values you can give are <em>1</em>, <em>2</em>, <em>4</em>, <em>8</em>, <em>16</em> and <em>32</em> which stand for the following manipulations of the ranking float:</p>
<ul>
<li>1: It will divide the ranking float by the sum of 1 and the logarithmic number of the document length. The latter number is the ratio this document has compared to the other documents you wish to compare.</li>
<li>2: Simply divides the ranking float by the length of the document.</li>
<li>4: Divides the ranking float by the harmonic mean (the fair average) between matched tokens. This one is only uses by the other ranking function <em>ts_rank_cd</em>.</li>
<li>8: Divides the ranking float by the number of <em>unique</em> words that are found in the document. </li>
<li>16: Divides the ranking float by the sum of 1 and the logarithmic number of the number of <em>unique</em> words found in the document.</li>
<li>32: Simply divides the ranking float by <em>itself</em> and adds one to that.</li>
</ul>
<p>These are a lot of values and some of them are quite confusing. But all of these have only one purpose: to make ranking more "fair", based on various use cases.</p>
<p>Take, for example, <em>1</em> and <em>2</em>. These calculate document length by taking into account the amount of <em>words</em> present in the document.
The <em>words</em> here reference the amount of <em>pointers</em> that are present in the tsvector.</p>
<p>To illustrate, we will convert the sentence "These token are repeating on purpose. Bad tokens!" into a tsvector, resulting in:</p>
<div class="code"><pre><span class="s1">'bad'</span>:7 <span class="s1">'purpos'</span>:6 <span class="s1">'repeat'</span>:4 <span class="s1">'token'</span>:2,8
</pre></div>


<p>The <em>length</em> of this document is <em>5</em>, becuase we have <em>five pointers</em> in total.</p>
<p>If you now look at the integers <em>8</em> and <em>16</em>, they take the <em>uniqueness</em> to calculate document length.
What that means is they do not count the pointers, but the actual <em>lexemes</em>.
In the above tsvector and thus would result in a length of <em>4</em>.</p>
<p>All of these manipulations are just different ways of counting document length.
The ones summed up in the above integer list are mere educated guesses at what most people desire when ranking with a full text engine.
As I said in the beginning, it is a gray area, left open for interpretation.</p>
<p>Let us try to see the different effects that such an integer can have.</p>
<p>First we need to create a few documents (tsvectors) inside our famous phraseTable (from the previous chapters) that we will use throughout this chapter.
Connect to your phrase database, add a "title" column, truncate whatever we have stored there and insert a few variable length documents based on Edgar Allan Poe's "The Raven".
I have prepared the whole syntax below, this time you may copy-and-paste:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">phraseTable</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">title</span> <span class="nb">VARCHAR</span><span class="p">;</span>
<span class="k">TRUNCATE</span> <span class="n">phraseTable</span><span class="p">;</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more."'</span><span class="p">),</span> <span class="s1">'Tiny Allan'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore.'</span><span class="p">),</span> <span class="s1">'Small Allan'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."'</span><span class="p">),</span> <span class="s1">'Medium Allan'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."  Presently my soul grew stronger; hesitating then no longer, "Sir," said I, "or Madam, truly your forgiveness I implore; But the fact is I was napping, and so gently you came rapping, And so faintly you came tapping, tapping at my chamber door, That I scarce was sure I heard you"- here I opened wide the door; - Darkness there, and nothing more.'</span><span class="p">),</span> <span class="s1">'Big Allan'</span><span class="p">);</span>
</pre></div>


<p>Nothing better then some good old Edgar to demonstrate a full text search ranking. Here we have four different lengths of the same verse making for four documents of different lengths stored in our tsvector column. Now we would like to search through these documents and find the keywords <em>'door'</em> and <em>'gently'</em>, ranking them as we go.</p>
<p>For later reference, let us first count how many times our keywords occur in the sentence:</p>
<ul>
<li>Tiny Allan: "door" 2, "gently" 1</li>
<li>Small Allan: "door" 2, "gently" 1</li>
<li>Medium Allan: "door" 4, "gently" 1</li>
<li>Big Allan: "door" 6, "gently" 2</li>
</ul>
<p>First, let us simply rank the result with the default normalization of <em>0</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span><span class="p">;</span>
</pre></div>


<p>Before we go over the results, a little bit about this query for people who are not so familiar with this SQL syntax.
We do a simple <em>SELECT</em> from a data set using <em>FROM</em> filtering it with a <em>WHERE</em> clause.
Going over it line by line:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
</pre></div>


<p>We <em>SELECT</em> on the <em>title</em> column we just made and on a "on-the-fly" column we create for the result set named <em>rank</em> which contains the result of the <em>ts_rank()</em> function.</p>
<div class="code"><pre><span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
</pre></div>


<p>In the <em>FROM</em> clause you can put a series of statements that will deliver the data for the query. In this case we take our normal database <em>table</em> and the result of the <em>to_tsquery()</em> function which we name <em>keywords</em> so we can use it throughout the query itself.</p>
<div class="code"><pre><span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span><span class="p">;</span>
</pre></div>


<p>Here we filter the result set using the <em>WHERE</em> clause and the <em>matching</em> operator (@@). The @@ is a Boolean operator, meaning it will simply return <em>true</em> or <em>false</em>.
So in this case, we check if the result of the <em>to_tsquery()</em> function (named keywords and which will return lexemes) <em>match</em> the results of the phrase <em>column</em> from our table (which contains <em>tsvectors</em> and thus lexemes). We want to rank only those phrases that actually contain our keywords.</p>
<p>Now, back to our ranking. The result of this query will be:</p>
<div class="code"><pre>   title     |   rank    
--------------+-----------
Tiny Allan   | 0.0906565
Small Allan  | 0.0906565
Medium Allan | 0.0906565
Big Allan    |   0.10109
</pre></div>


<p>Let us order the results first, so the most relevant document is always on top:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>Result:</p>
<div class="code"><pre>   title     |   rank    
--------------+-----------
Big Allan    |   0.10109
Tiny Allan   | 0.0906565
Small Allan  | 0.0906565
Medium Allan | 0.0906565
</pre></div>


<p>"Big Allen" is on top, for it has more occurrences of the keywords "door" and "gently".
But to be fair, in ratio "Tiny Allan" has almost the same amount of occurrences of both keywords. Three times less, but it also is three times as small.</p>
<p>So let us take document length (based on <em>word count</em>) into account, setting our <em>normalization</em> to <em>1</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>You will get:</p>
<div class="code"><pre>   title     |   rank    
--------------+-----------
Tiny Allan   | 0.0181313
Small Allan  | 0.0151094
Big Allan    | 0.0145124
Medium Allan |  0.013831
</pre></div>


<p>This could be seen as a more fair ranking, "Tiny Allan" is now on top because, considering its <em>ratio</em>, it is the most relevant. "Medium Allan" falls all the way down because it is almost as big as "Big Allan", but contains lesser occurrences of the keywords. In total five keywords in contrast to "Big Allan" who has eight but is only slightly bigger.</p>
<p>Let us do the same, but count the document length based on the <em>unique</em> occurences using integer <em>8</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>The result:</p>
<div class="code"><pre>Tiny Allan   | 0.00335765
Small Allan  | 0.00161887
Medium Allan | 0.00119285
Big Allan    | 0.00105303
</pre></div>


<p>That is a very different result, but quite what you should expect.</p>
<p>We are searching for only <em>two</em> tokens here, and considering the fact that uniqueness is now adhered, all the extra occurrences of these words are ignored.
This means that for the ranking algorithm, all the documents we searched through (which all have at least one occurrence of each token) get normalized to only <em>2</em> matching tokens.
And in that case, the shortest document wins hands down, for it is seen as most relevant. As you can see in the result set, the documents are neatly ordered from tiny to big.</p>
<h4>Ranking with density</h4>
<p>Up until now we have seen the "normal" ranking function <em>ts_rank()</em>, which is the one you will probably use the most.</p>
<p>There is, however, one more function at our direct disposal called <em>ts_rank_cd()</em>. The <em>cd</em> stands for <em>Cover Density</em> and is simply yet another way of considering relevance.
This function has exactly the same required and optional arguments, it simply counts relevancy differently.
Very important for this function to work properly is that you do not let it operate on a <em>stripped</em> tsvector.</p>
<p>A stripped tsvector is one that has been undone of its pointer information. If you know that you do not need this pointer information - you just need to match tsqueries against the lexemes in you tsvector - you can strip these pointers and thus make for smaller footprints in your database.</p>
<p>In case of our cover density ranker, it needs this positional pointer information to see how <em>close</em> the search tokens are to each other.
It makes sense that this ranking function only works on multiple tokens, on single tokens it is kind of pointless.</p>
<p>In a way, this ranking function looks for <em>phrases</em> rather then single tokens; the closer lexemes are together, the more positive influence they will have on the resulting ranking float.</p>
<p>In our "Raven" examples this might be a little bit hard to see, so let me demonstrate this with a couple of new, on-the-fly queries.</p>
<p>We wish to search for the tokens <em>'token'</em> and <em>'count'</em>.</p>
<p>First, a sentence in which the searched for tokens are wide apart: "These tokens are very wide apart and therefor do not count as much.":</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'These tokens are very wide apart and do not count as much.'</span><span class="p">)</span> <span class="n">phrase</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'token &amp; count'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>Will have this tsvector:</p>
<div class="code"><pre><span class="s1">'apart'</span>:6 <span class="s1">'count'</span>:10 <span class="s1">'much'</span>:12 <span class="s1">'token'</span>:2 <span class="s1">'wide'</span>:5
</pre></div>


<p>And this result:</p>
<div class="code"><pre> 0.008624
</pre></div>


<p>Let us put these tokens closer together now: "These tokens count for much now that they are not so wide apart!":</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'These tokens count for much now that they are not so wide apart!'</span><span class="p">)</span> <span class="n">phrase</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'token &amp; count'</span><span class="p">)</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">keywords</span> <span class="o">@@</span> <span class="n">phrase</span>
    <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">DESC</span><span class="p">;</span>
</pre></div>


<p>The vector:</p>
<div class="code"><pre><span class="s1">'apart'</span>:13 <span class="s1">'count'</span>:3 <span class="s1">'much'</span>:5 <span class="s1">'token'</span>:2 <span class="s1">'wide'</span>:12
</pre></div>


<p>The result:</p>
<div class="code"><pre>0.0198206
</pre></div>


<p>You can see that both the vectors have <em>exactly</em> the same lexemes, but different pointer information.
In the second vector, the tokens we searched for are next to each other, which results in a ranking float that is more then double of the first result.</p>
<p>This demonstrates the working of this function. The same optional manipulations can be passed in (weights and normalization) and they will have roughly the same effect.</p>
<p>Pick the ranking function that is best fit for your use case.</p>
<p>It needs to be said that the two ranking functions we have seen so far are officially called <em>example functions</em> by the PostgreSQL community.
They are functions devised to be fitting for most purposes, but also to demonstrate how you could write your own.</p>
<p>If you have very specific use cases it is advised to write you own ranking functions to fit your exact needs.
But this is considered beyond the scope of this series (and maybe also beyond the scope of your needs).</p>
<h3>Highlight your results!</h3>
<p>The next interesting thing we can do with the results of our full text is to highlight the relevant words.</p>
<p>As is the case with many search engines, users want to skim over an excerpt of each result to see if it is what they are searching for.
For this PostgreSQL delivers us yet another function: <em>ts_headline()</em>.</p>
<p>To demonstrate its use, we first have to make our small database a little bit bigger by inserting the original text of the "Raven" next to our tsvectors.
So, again, copy and past this new set of queries (yes you may...):</p>
<div class="code"><pre><span class="k">TRUNCATE</span> <span class="n">phraseTable</span><span class="p">;</span>
<span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">phraseTable</span> <span class="k">ADD</span> <span class="k">COLUMN</span> <span class="n">article</span> <span class="nb">TEXT</span><span class="p">;</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more."'</span><span class="p">),</span> <span class="s1">'Tiny Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more."'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore.'</span><span class="p">),</span> <span class="s1">'Small Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore.'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."'</span><span class="p">),</span> <span class="s1">'Medium Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more'</span><span class="p">);</span>
<span class="k">INSERT</span> <span class="k">into</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."  Presently my soul grew stronger; hesitating then no longer, "Sir," said I, "or Madam, truly your forgiveness I implore; But the fact is I was napping, and so gently you came rapping, And so faintly you came tapping, tapping at my chamber door, That I scarce was sure I heard you"- here I opened wide the door; - Darkness there, and nothing more.'</span><span class="p">),</span> <span class="s1">'Big Allan'</span><span class="p">,</span> <span class="s1">'Once upon a midnight dreary, while I pondered, weak and weary, Over many a quaint and curious volume of forgotten lore, While I nodded, nearly napping, suddenly there came a tapping, As of some one gently rapping, rapping at my chamber door. "Tis some visitor," I muttered, "tapping at my chamber door - Only this, and nothing more." Ah, distinctly I remember it was in the bleak December, And each separate dying ember wrought its ghost upon the floor. Eagerly I wished the morrow - vainly I had sought to borrow From my books surcease of sorrow - sorrow for the lost Lenore - For the rare and radiant maiden whom the angels name Lenore - Nameless here for evermore. And the silken sad uncertain rustling of each purple curtain Thrilled me - filled me with fantastic terrors never felt before; So that now, to still the beating of my heart, I stood repeating, "Tis some visitor entreating entrance at my chamber door - Some late visitor entreating entrance at my chamber door - This it is, and nothing more."  Presently my soul grew stronger; hesitating then no longer, "Sir," said I, "or Madam, truly your forgiveness I implore; But the fact is I was napping, and so gently you came rapping, And so faintly you came tapping, tapping at my chamber door, That I scarce was sure I heard you"- here I opened wide the door; - Darkness there, and nothing more.'</span><span class="p">);</span>
</pre></div>


<p>Good, we now have the same data, but this time we stored the text of the original document alongside the "vectorized" version.</p>
<p>The reason for this being that this <em>ts_headline()</em> function searches in the original documents (being our <em>article</em> column) rather that in your ts_vector column.
Two arguments are mandatory: the original article and the ts_query. The optional arguments are the full text configuration you wish to use and a string of additional, comma separated options.</p>
<p>But first, let us take a look at its most basic usage:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Will give you:</p>
<div class="code"><pre>   title     |                                                      result                                                      
--------------+------------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
Small Allan  | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
Medium Allan | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
Big Allan    | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber
</pre></div>


<p>As you can see, we get back a short excerpt of each verse with the tokens of interest surrounded with a HTML "&lt;b&gt;" tag.
That is actual all there is to this function, it return the results with the tokens highlighted.</p>
<p>However, there are some nice options you can set to alter this basic behavior.</p>
<p>The first one up is the HTML tag you wish to put around you highlighted words. For this you have two variables <em>StartSel</em> and <em>StopSel</em>.
If we wanted this to be a "&lt;em&gt;" tag instead, we could tell the function to change as follows:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'StartSel=&lt;em&gt;,StopSel=&lt;/em&gt;'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>And now we will get back an &lt;em&gt; instead of a &lt;b&gt; (including just one row this time):</p>
<div class="code"><pre>   title     |                                                        result                                                        
--------------+----------------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;em&gt;gently&lt;/em&gt; rapping, rapping at my chamber &lt;em&gt;door&lt;/em&gt;. "Tis some visitor," I muttered, "tapping at my chamber
</pre></div>


<p>In fact, it does not need to be HTML at all, you can put (almost) any string there:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'StartSel=foobar&gt;&gt;,StopSel=&lt;&lt;barfoo '</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Result:</p>
<div class="code"><pre>   title     |                                                               result                                                               
--------------+------------------------------------------------------------------------------------------------------------------------------------
Tiny Allan   | foobar&gt;&gt;gently&lt;&lt;barfoo rapping, rapping at my chamber foobar&gt;&gt;door&lt;&lt;barfoo. "Tis some visitor," I muttered, "tapping at my chamber
</pre></div>


<p>Quite awesome!</p>
<p>Another attribute you can tamper with is how many words should be included in the result set by using the <em>MaxWords</em> and <em>MinWords</em>: </p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'MaxWords=4,MinWords=1'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Which gives you:</p>
<div class="code"><pre>   title     |             result             
--------------+--------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping
</pre></div>


<p>To make the resulting headline a little bit more readable there is an attribute in this options string called <em>ShortWord</em> which tells the function which is the shortest word that may appear at the start or end of the headline. </p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'ShortWord=8'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Will give you:</p>
<div class="code"><pre>   title     |                                                                                   result                                                                                    
--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber &lt;b&gt;door&lt;/b&gt; - Only this, and nothing more."
Small Allan  | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;. "Tis some visitor," I muttered, "tapping at my chamber &lt;b&gt;door&lt;/b&gt; - Only this, and nothing more." Ah, distinctly
</pre></div>


<p>Now it will try and set word boundaries to words of minimal 8 letters. This time I included the second line of the result set. As you can see the engine could not find an 8 letter word at the remainder of the document, so it simply prints it until the end. The second row, "Small Allan" is a bit bigger and the word "distinctly" has more then 8 letters, so is set as the boundary,</p>
<p>So far the headline function has given us almost full sentences and not really fragments of text. This is because the optional <em>MaxFragments</em> defaults to 0. If we up this variable, it will start to include fragments and not sentences. Let us try it out:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'MaxFragments=2,MaxWords=8,MinWords=1'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>Gives you</p>
<div class="code"><pre>   title     |                                                     result                                                      
--------------+-----------------------------------------------------------------------------------------------------------------
Tiny Allan   | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;
...
Big Allan    | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt; ... chamber &lt;b&gt;door&lt;/b&gt; - This it is, and nothing more
</pre></div>


<p>I include only the first and last line of this result set. As you can see on the last line, the result is now fragmented, and we get back different pieces of our result.
If, for instance, four or five tokens match in our document, setting the <em>MaxFragments</em> to a higher value will show more of these matches glued together.</p>
<p>Accompanying this <em>MaxFragments</em> option is the <em>FragmentDelimiter</em> variable which is used to define, well, the delimiter between the fragments. Short demo:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'MaxFragments=2,FragmentDelimiter=;,MaxWords=8,MinWords=1'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>You will get:</p>
<div class="code"><pre>   title     |                                                   result                                                    
--------------+-------------------------------------------------------------------------------------------------------------
Big Allan    | &lt;b&gt;gently&lt;/b&gt; rapping, rapping at my chamber &lt;b&gt;door&lt;/b&gt;;chamber &lt;b&gt;door&lt;/b&gt; - This it is, and nothing more
</pre></div>


<p>Including only the last line, you will see we now have a semicolon (;) instead of a ellipses (...). Neat.</p>
<p>A final, less common option for the <em>ts_headline()</em> function is to ignore all the word boundaries we set before and simply return the <em>whole</em> document and highlight all the words of relevance.
This variable is called <em>HighlightAll</em> and is a Boolean set to <em>false</em> by default:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="s1">'HighlightAll=true'</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span>
    <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">as</span> <span class="n">keywords</span>
    <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span><span class="p">;</span>
</pre></div>


<p>The result would be too large to print here, but try it out. It will give you the whole text, but with the important tokens decorated with the element (or text) of choice.</p>
<h4>A big word of <em>caution</em></h4>
<p>It is very fun to play with highlighting your results, I will admit that. The only problem is, as you might have concluded yourself, this is a potential performance grinder.</p>
<p>The problem here is that this function cannot use any indexes and it can also not use your stored tsvector. It <em>needs</em> the original document text and it needs to not only parse the whole document text to a tsvector for matching, it also needs to parse the original document text a second time to find the substrings and <em>decorate</em> them with the characters you have set. And this whole process has to happen <em>for every single record</em> in your result set.</p>
<p>Highlighting, with this function, is a <em>very</em> expensive to do. </p>
<p>This does not mean that you have to avoid this function, if so I would have told you from the start and skipped this whole part. No, it is there to be used. But use it in a correct way.</p>
<p>A correct way often seen is to use the highlighting only on the top results you are interested in - the top results the user has on their screen at the moment.
This could be achieved in SQL with a so called <em>subquery</em>.</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">title</span><span class="p">,</span> <span class="n">ts_headline</span><span class="p">(</span><span class="n">article</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">as</span> <span class="k">result</span><span class="p">,</span> <span class="n">rank</span>
    <span class="k">FROM</span> <span class="p">(</span><span class="k">SELECT</span> <span class="n">keywords</span><span class="p">,</span> <span class="n">article</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">phrase</span><span class="p">,</span> <span class="n">ts_rank</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span> <span class="k">AS</span> <span class="n">rank</span>
          <span class="k">FROM</span> <span class="n">phraseTable</span><span class="p">,</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'door &amp; gently'</span><span class="p">)</span> <span class="k">AS</span> <span class="n">keywords</span>
          <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">keywords</span>
          <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span>
          <span class="k">LIMIT</span> <span class="mi">2</span><span class="p">)</span> <span class="k">AS</span> <span class="k">alias</span><span class="p">;</span>
</pre></div>


<p>For those unfamiliar, a <em>subquery</em> is nothing more than a query within a query (queue Inception drums...sorry).</p>
<p>You evaluate the inner query and use the result set of that to perform the outer query. You can achieve the same with two queries, but that would prove not to be as elegant.
When PostgreSQL sees a subquery, it can plan and execute more efficiently then with separate queries, many times giving you a better performance.</p>
<p>The query you see above might look a bit frightening to beginning SQL folk, but simply see it as two separate ones and the beast becomes a tiny mouse.
Unless you are afraid of mice, let it become a...euhm...soft butterfly gliding on the wind instead.</p>
<p>In the inner query we perform the actual matching and ranking as we have seen before. This inner query then only returns <em>two</em> matching records, because of the <em>LIMIT</em> clause.
The outer query takes those results and performs the expensive operation of highlighting.</p>
<h3>Indexing</h3>
<p>Back to a more serious matter, the act of <em>indexing</em>.</p>
<p>If you do not know what an index is, you have to brush up real fast, for indexing is quite important for the performance of your queries.
In a very simplistic view, an index is like a chapter listing in a book. You can quickly skim over the chapters to find the page you are looking for, instead of having to flip over every single page.</p>
<p>You typically put indexes on tables which are consulted often and you build the index in a way that is in parallel with how you query them.</p>
<p>As indexing is a whole topic, or rather, a whole profession of its own, I will not go too deeply into the matter.
But I will try to give you some basic knowledge on the subject.</p>
<p>Note that I will go over this matter in lighting speed and thus have to greatly skim down on the amount of details.
A <em>very</em> good place to learn about indexes is Markus Winand's <a href="http://use-the-index-luke.com/" title="Use The Index, Luke series written by Markus Winand.">Use The Index, Luke</a> series.
I seriously suggest you read that stuff, it is golden knowledge for every serious developer working with databases.</p>
<h4>B-tree</h4>
<p>Before we can go to the PostgreSQL full text index types we first have to look at the most common index type, the <em>Binary tree</em> or <em>B-tree</em>.</p>
<p>The <em>B-tree</em> is a proven "computer science" concept that give us a way to search certain types of data, fast.</p>
<p>A <em>B-tree</em> is a tree structure with a root, nodes and leafs (inverse from a natural tree).
The data that is within your table rows will be ordered and chopped up to fit within the tree.</p>
<p>In database indexes we mostly use <em>balanced B-trees</em>, meaning that each level of the tree has the same amount of nodes.</p>
<p>Take this picture for example:</p>
<div class="code"><pre>            |root|
              |
       ----------------
       |               |
    |node|          |node|
       |               |
  ----------       --------- 
  |        |       |       |
|node|  |node|  |node|  |node|
</pre></div>


<p>In <em>B-tree</em> terms, we summarize this tree by saying:</p>
<ul>
<li>It has an <em>order</em> of 2, meaning that each node will contain two leaves only</li>
<li>It has a <em>depth</em> of 3, meaning it is three levels deep (including the root node)</li>
<li>It has 4 <em>leaves</em>, meaning that the amount of nodes that do not contain children is 4 (bottom row)</li>
</ul>
<p>If you set the <em>order</em> of your tree to a higher number, more nodes can fit onto a single row and you will end up with a lesser <em>depth</em>.</p>
<p>Now the actual power of an index comes from an I/O perspective. As you know (or will know now) the thing that will slow down a program/computer the most is I/O.
This can be network I/O, disk I/O, etc. In case of our database we will speak of disk I/O. </p>
<p>When a database has to go and <em>scan</em> your table without an index is has to plow through all your rows to find a match.
Database rows are almost always <em>not</em> I/O optimized, this means that they do not fit well in the blocks of your physical disks structure.
This, in short, means that there is a lot of overhead in reading through that physical data,</p>
<p>A <em>B-tree</em> on the other hand, is <em>very</em> optimized for I/O. Each level of a <em>B-tree</em> will try and fit perfectly within one physical block on your disk.
If all levels fit within one block each, walking over the tree will be very efficient and have almost no overhead.</p>
<p><em>B-trees</em> work with the most common data types such as TEXT, INT, VARCHAR, ... .</p>
<p>But because full text search in PostgreSQL is its own "thing" (using the @@ operator), all knowledge that you may have learned about regarding indexes does not apply (or not in full anyway) to full text search.</p>
<p>Full text search needs its own kind of indexing  for a <em>tsquery</em> to be able to use them. 
And as we will see in a moment, indexing on full text in PostgreSQL is a dance of trade-offs.
When it comes to this matter we have two types of indexes available: <em>GiST</em> and <em>GiN</em> which are both closely related to the <em>B-tree</em>.</p>
<h4>GiST</h4>
<p>GiST stands for <em>Generalized Search Tree</em> and can both be set on <em>tsvector</em> and <em>tsquery</em> column types, though most of the time you will use it on the former.</p>
<p>The <em>GiST</em> itself is not something that is unique to PostgreSQL, it is a project on its own and its concept is laid out in a C library called <em>libGist</em>.
You could go ahead and play around with <em>libGiist</em> to get a better understanding of how it works, it even comes shipped with some demo applications.</p>
<p>Over time there have come many new types of trees based on the <em>B-tree</em> concept, but most of them are limit in how they can match.
A <em>B-tree</em> and its direct descendants can only use basic match operators like "&lt;", "&gt;", "=", etc.
A <em>GiST</em> index, however, has more advanced matching capabilities like "intersect" and in case of PostgreSQL's implementation: the <em>"@@"</em> operator.</p>
<p>Another big advantage of the <em>GiST</em> is the fact that it can store arbitrary data types and therefor can be used in a wide area of conduct.
The trade off for the wide data type support is the fact that <em>GiST</em> will always return a <em>no</em> if there is no match or a <em>maybe</em> if there is.
There is no true <em>hit</em> with this kind of index.</p>
<p>Because of this behavior there is extra overhead in the case of full text search because PostgreSQL has to manually go and check all the <em>maybe</em>'s that return and see if they are an actual match.</p>
<p>The big advantages of <em>GiST</em> are the fact that the index builds faster and the update of such an index is less expensive then the next index type we will see.</p>
<h4>GiN</h4>
<p>The second index candidate we have at our disposal is the <em>Generalized Inverted Index</em> or <em>GiN</em> in short.</p>
<p>Same as we saw with <em>GiST</em>, <em>GiN</em> also allows for arbitrary data types to be indexes and allows for more matching operators to be used.
But as opposed to <em>GiST</em>, a <em>GiN</em> index is <em>deterministic</em> - it will always return a true match, cutting the checking overhead needed with <em>GiST</em>.</p>
<p>Well, unless you wish to use <em>weights</em> in your queries. A <em>GiN</em> index does <em>not</em> store lexeme weights. This means that, if weights need to be taken into account when querying, PostgreSQL still has to go and fetch the actual row(s) that return a true match, giving you somewhat of the same overhead as with a <em>GiST</em> index.</p>
<p><em>GiN</em> tries to improve the <em>B-tree</em> concept by minimizing the amount of redundancy within nodes and there leaves.
When you search for a number between 0 and 1000, it can be that your index has to go over 5 levels to find the desired entry.
This means that the four levels above the matching leaf could potentially contain an (implied) reference to the row id you wish to have fetched.
In a <em>GiN</em> index, this is generalized by storing a single entry of the duplicates into so-called <em>posting trees</em> and <em>posting lists</em> and pointing to those lists instead of drilling down multiple levels.</p>
<p>The downside of <em>GiN</em> is the fact that this kind of index will slow down the bigger it gets.</p>
<p>On a more positive note, <em>GiN</em> indexes are most of the time smaller on disk (because of it trying to reduce duplicates). And, as of PostgreSQL 9.4, they will be <em>even</em> smaller.
The soon-to-be version will introduce a so-called <em>varbyte</em> version of <em>GiN</em>. For now just take it from me that it will make these type of indexes <em>much</em> smaller, and even more efficient.</p>
<p>As you can see, there is no perfect index when it comes to full text. You will have to carefully look at what data you will save and how you wish to query the data.</p>
<p>If you do not update your database much but you have a lot of querying going on, <em>GiN</em> might be a better option for it is much faster with a lookup (if no weights are required).
If your data does not get read much, but is updated frequently, maybe a <em>GiST</em> is a better choice for it allows for faster updating.</p>
<h4>Making an index</h4>
<p>We have (very roughly) seen what an index is and what we have available for full text, but how do you actually build such an index?</p>
<p>Luckily for us, this too has been neatly abstracted and is very simple to do.</p>
<p>If we wanted our phraseTable to contain an index, we simply could go about and create it with the following syntax:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">prhasetable_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gin</span><span class="p">(</span><span class="n">phrase</span><span class="p">);</span>
</pre></div>


<p>This will create a <em>GiN</em> index called <em>phrasetable_ixd</em> on the column <em>phrase</em>.</p>
<p>Just like we did before, we will now re-populate our <em>phrase</em> column, but this time we will fill it with the data we want to have indexed: article and title.
Let me show you what I mean.</p>
<p>First, empty the four <em>phrase</em> columns in our tiny database:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">phraseTable</span> <span class="k">ALTER</span> <span class="n">phrase</span> <span class="k">DROP</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">;</span>
<span class="k">UPDATE</span> <span class="n">phraseTable</span> <span class="k">set</span> <span class="n">phrase</span> <span class="o">=</span> <span class="k">NULL</span><span class="p">;</span>
</pre></div>


<p>Notice that I removed the <em>NOT NULL</em> constraint.
Next we can populate it containing a tsvector of both the <em>title</em> and the <em>article</em> columns:</p>
<div class="code"><pre><span class="k">UPDATE</span> <span class="n">phraseTable</span> <span class="k">SET</span> <span class="n">phrase</span> <span class="o">=</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span> <span class="o">||</span> <span class="s1">' '</span> <span class="o">||</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">article</span><span class="p">,</span><span class="s1">''</span><span class="p">));</span>
</pre></div>


<p>The <em>coalesce</em> function may be something that you are unfamiliar with.
This functions simply returns the first argument which is <em>not</em> NULL.
In this case we use:</p>
<div class="code"><pre><span class="n">coalesce</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span>
</pre></div>


<p>Which means that if title would be NULL it will return the empty string <em>''</em> which never is NULL.
We use <em>coalesce</em> here to substitute a value for NULL, being the empty string.</p>
<p>If we would not substitute NULL then our <em>tsvector</em> generation would fail if either the title or article column would be NULL.</p>
<p>Next we can create an index on that newly filled column:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">phraseable_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gin</span><span class="p">(</span><span class="n">phrase</span><span class="p">);</span>
</pre></div>


<p>And we have magic, there now is a <em>GiN</em> index on that column which will be used during full text search.</p>
<p>To create a <em>GiST</em> index we could use exactly the same syntax:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">phrasetable_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gist</span><span class="p">(</span><span class="n">phrase</span><span class="p">);</span>
</pre></div>


<p>Now, the disk-space savvy readers will have noticed that our "phrase" column now contains some redundant information as we store the tsvector of the article and title column that was already in the database.
If you do not wish to have this extra column, you could created <em>expression</em> indexes (our on-the-fly queries we seen before).</p>
<p>The setup of such an expression index is trivial:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">INDEX</span> <span class="n">phrasetable_exp_idx</span> <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">USING</span> <span class="n">gin</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="s1">''</span><span class="p">)</span> <span class="o">||</span> <span class="s1">' '</span> <span class="o">||</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">article</span><span class="p">,</span><span class="s1">''</span><span class="p">)));</span>
</pre></div>


<p>Instead of having this extra tsvector column around, we now have created an on-the-fly index using the same syntax as we employed when we populated the <em>phrase</em> column a few lines back.</p>
<p>One important thing to note when you use <em>expression</em> indexes is the text search configuration you used. Here we specify that we wish the index to be created using the 'english' configuration set.
This results in an index which is configuration aware and will <em>only</em> work with a query which has the <em>same</em> configuration set fed to the tsquery function (well, the same name anyway).</p>
<p>You could omit the configuration which would then default to the one set in the "default_text_search_config" variable we saw in the last chapter. The problem you will have then is that the index is created using a configuration that <em>could</em> be altered <em>after</em> the index was created. If we later would query the database with the altered default, the index would be useless and will return inaccurate results. </p>
<p>Also note that we may save on disk space when we use the <em>expression</em> index, but we do not save on CPU. Now, instead of indexing data already parsed and ready in a column, the index has to compute the <em>to_tsvector</em> on every index match. Again, a world of trade-offs.</p>
<h3>Triggers</h3>
<p>A final, small topic I want to briefly touch on before I let you go free are <em>update triggers</em>. The way we have been populating our database so far does not need a trigger actually. Up until now we have been inserting records (or updating them) using the <em>ts_tsvector()</em> function. The negative aspect of going about it the way we did is that it is extremely <em>redundant</em>. </p>
<p>If we inserted a piece of Raven text into a record, we specified it <em>twice</em>, one time for the <em>article</em> column and one time for the <em>phrase</em> column which holds the tsvector result.</p>
<p>A better way to do this is to not let the insert query care about the tsvector <em>at all</em>. We simply insert the text we like and let the database do the converting  behind the curtains.</p>
<p>This is where a <em>trigger</em> comes in handy. Actually, PostgreSQL has a whole set of <em>trigger</em> functions available that will fire when certain conditions are met, but when it comes to full text we have two functions at our disposal.</p>
<h4>tsvector_update_trigger()</h4>
<p>The first, and most used one, is called <em>tsvector_update_trigger()</em> and fires whenever a new row is inserted into your table (in our case <em>phraseTable</em>).</p>
<p>To setup such a trigger, we could use the following SQL:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">TRIGGER</span> <span class="n">tsvectorupdate</span> <span class="k">BEFORE</span> <span class="k">INSERT</span> <span class="k">OR</span> <span class="k">UPDATE</span>
    <span class="k">ON</span> <span class="n">phraseTable</span> <span class="k">FOR</span> <span class="k">EACH</span> <span class="k">ROW</span> <span class="k">EXECUTE</span> <span class="k">PROCEDURE</span>
    <span class="n">tsvector_update_trigger</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span> <span class="s1">'pg_catalog.english'</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">article</span><span class="p">);</span>
</pre></div>


<p>That is all you need to setup such a trigger. Let us see what we just did.</p>
<p>First, we have new syntax staring us in the face: <em>CREATE TRIGGER</em>. This will create a trigger on certain <em>events</em>. The events here are <em>BEFORE INSERT</em> and <em>BEFORE UPDATE</em> which are contracted to <em>BEFORE INSERT OR UPDATE</em>. Then we specify on which <em>table</em> this trigger has to act and for each <em>ROW</em>. Then we say we want to <em>EXECUTE PROCEDURE</em>, which, in our case, is the function <em>tsvector_update_trigger()</em>.</p>
<p>The function itself needs a bit of explaining as well. This version takes three required arguments: the tsvector column name, the full text configuration name and the original text column name.
The latter can be multiple columns to concatenate them together. This concatenation is done with <em>coalesce</em> under the hood, as we have seen before.</p>
<p>In our case, we create a trigger that takes the <em>phrase</em> tsvector column, the <em>enlgish</em> full text configuration and concatenates the text from both <em>title</em> and <em>article</em> to be normalized into lexemes.</p>
<p>Note that instead of <em>english</em> we say <em>pg_catalog.english</em> when providing this function with the full text configuration.
In case of this function (and the next) we have to provide the schema-qualified path to the configuration.</p>
<h4>tsvector_update_trigger_column()</h4>
<p>The other of the two full text trigger functions we have is called <em>tsvector_update_trigger_column()</em> and has only one difference to the former: the full text configuration used.
Here, the full text configuration can be read from a <em>column</em> instead of given directly as a string.</p>
<p>A possibility we have not seen in this series is one where you can have yet another column in your phraseTable where you store the name of the full text configuration you wish to use.
This way you can store multiple "languages" within the same table, specifying which configuration to use with each row.</p>
<p>This trigger functions can take into account these per-row differing configurations and is able to read them from the specified column.</p>
<p>But we have a trade-off once more. These two trigger functions, which are officially called <em>example functions</em> again (remember our ranking functions?), do <em>not</em> take into account weights.
If you have the need to store different weights in your tsvectors, you will have to write you own trigger function.</p>
<h3>The end</h3>
<p>Okay, I guess this covers the <em>basics</em> of full text within PostgreSQL.</p>
<p>We have covered the most important parts and touched some segments deeply, others just with a soft lovers glove.
As I always say at the end of such lengthy chapters: go out and explore.</p>
<p>I have tried to give you a solid, full text knowledge base to build further adventures on. I highly encourage you to pack your elephant, take your new ship for a maiden voyage, set high the sails and if certain blue wales try to swim next to your vessel, simply let the mammoth take a good relief down the ship's head, and let those turds float together with our squeaky finned friends!</p>
<p>And as always...thanks for reading!</p>
<!--  LocalWords:  PostgreSQL lexeme
 -->";}i:4;a:6:{s:5:"title";s:48:"Robert Haas: Troubleshooting Database Corruption";s:4:"link";s:74:"http://rhaas.blogspot.com/2014/05/troubleshooting-database-corruption.html";s:11:"description";s:667:"When your database gets corrupted, one of the most important things to do is <a href="http://rhaas.blogspot.com/2012/03/why-is-my-database-corrupted.html">figure out why that happened</a>, so that you can try to ensure that it doesn't happen again. After all, there's little point in going to a lot of trouble to restore a corrupt database from backup, or in attempting to repair the damage, if it's just going to get corrupted again. However, there are times when root cause analysis must take a back seat to getting your database back on line.<br /><br /><a href="http://rhaas.blogspot.com/2014/05/troubleshooting-database-corruption.html#more">Read more </a>";s:4:"guid";s:59:"tag:blogger.com,1999:blog-20038672.post-5801223975538369937";s:7:"pubdate";s:29:"Tue, 13 May 2014 17:34:00 GMT";s:7:"summary";s:667:"When your database gets corrupted, one of the most important things to do is <a href="http://rhaas.blogspot.com/2012/03/why-is-my-database-corrupted.html">figure out why that happened</a>, so that you can try to ensure that it doesn't happen again. After all, there's little point in going to a lot of trouble to restore a corrupt database from backup, or in attempting to repair the damage, if it's just going to get corrupted again. However, there are times when root cause analysis must take a back seat to getting your database back on line.<br /><br /><a href="http://rhaas.blogspot.com/2014/05/troubleshooting-database-corruption.html#more">Read more </a>";}i:5;a:6:{s:5:"title";s:66:"Joel Jacobson: psql \watch 1400000000 epoch time countdown counter";s:4:"link";s:83:"http://joelonsql.com/2014/05/13/psql-watch-1400000000-epoch-time-countdown-counter/";s:11:"description";s:822:"<pre class="brush: sql; title: ; notranslate">
SET TIMEZONE TO 'UTC';
\t
\a
\pset fieldsep ' '
SELECT
    (('epoch'::timestamptz + 14*10^8 * '1 s'::interval)-now())::interval(0),
    (14*10^8-extract(epoch from now()))::int,
    extract(epoch from now())::int
;
\watch 1

09:18:28 33508 1399966492
09:18:27 33507 1399966493
09:18:26 33506 1399966494
09:18:25 33505 1399966495
09:18:24 33504 1399966496
09:18:23 33503 1399966497
</pre><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/joelonsql.wordpress.com/220/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/joelonsql.wordpress.com/220/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=joelonsql.com&#038;blog=36282610&#038;post=220&#038;subd=joelonsql&#038;ref=&#038;feed=1" width="1" />";s:4:"guid";s:27:"http://joelonsql.com/?p=220";s:7:"pubdate";s:29:"Tue, 13 May 2014 07:35:54 GMT";s:7:"summary";s:822:"<pre class="brush: sql; title: ; notranslate">
SET TIMEZONE TO 'UTC';
\t
\a
\pset fieldsep ' '
SELECT
    (('epoch'::timestamptz + 14*10^8 * '1 s'::interval)-now())::interval(0),
    (14*10^8-extract(epoch from now()))::int,
    extract(epoch from now())::int
;
\watch 1

09:18:28 33508 1399966492
09:18:27 33507 1399966493
09:18:26 33506 1399966494
09:18:25 33505 1399966495
09:18:24 33504 1399966496
09:18:23 33503 1399966497
</pre><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/joelonsql.wordpress.com/220/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/joelonsql.wordpress.com/220/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=joelonsql.com&#038;blog=36282610&#038;post=220&#038;subd=joelonsql&#038;ref=&#038;feed=1" width="1" />";}i:6;a:6:{s:5:"title";s:36:"Josh Berkus: cstore_fdw and big data";s:4:"link";s:63:"http://www.databasesoup.com/2014/05/cstorefdw-and-big-data.html";s:11:"description";s:3705:"About a month ago, PostgreSQL fork vendor and Data Warehousing company CitusDB announced the availability of the <a href="http://citusdata.github.io/cstore_fdw/" target="_blank">open-source cstore_fdw</a>.&nbsp; This foreign data wrapper creates an external table with highly compressed data, which allows you to keep large amounts of archival data on your PostgreSQL server.<br /><br />You can find out more if you want to tune in <a href="http://goo.gl/QupyHO" target="_blank">tommorrow night, May 13th, around 7:15PM PDT</a>.&nbsp; Tomorrow night's even will be sponsored by CitusDB and hosted by Rackspace.<br /><br />First, the good stuff: compression:<br /><br /><span>phc=# select pg_size_pretty(pg_total_relation_size('postgres_log'));<br />&nbsp;pg_size_pretty <br />----------------<br />&nbsp;28 GB&nbsp;</span><br /><br /><span>ls -lh $PGDATA/base</span><br /><br /><span>-rw------- 1 postgres postgres 3.2G May 12 13:37 pglog.cstore<br />-rw------- 1 postgres postgres&nbsp; 12K May 12 13:37 pglog.cstore.footer</span><br /><br />So, the "postgres_log" table from this Performance Health Check database, which has 15 million records, takes up 28GB in postgres, and 3.2GB as a cstore table ... a space savings of about 89%.&nbsp; Not bad.&nbsp; Especially if you consider that the cstore table already has skip indexes on all indexable columns.<br /><br />Now, where this space savings becomes a real benefit is if the cstore table fits in memory and the Postgres table doesn't.&nbsp;&nbsp; I don't have a case like that, although the cstore still does show performance benefits if you have a wide table and you don't need all columns:<br /><br /><span>phc=# select count(1) from postgres_log where command_tag = 'UPDATE';<br />&nbsp;count&nbsp; <br />--------<br />&nbsp;986390<br />(1 row)<br /><br />Time: 23746.476 ms<br />phc=# select count(1) from c_pglog where command_tag = 'UPDATE';<br />&nbsp;count&nbsp; <br />--------<br />&nbsp;986390<br />(1 row)<br /><br />Time: 14059.405 ms</span><br /><br />And even better if you can apply a relatively restrictive filter:<br /><br /><span>phc=# select count(1) from postgres_log where command_tag = 'UPDATE' and log_time BETWEEN '2014-04-16 07:15:00' and '2014-04-16 07:20:00';<br />&nbsp;count <br />-------<br />&nbsp;84982<br />(1 row)<br /><br />Time: 19653.746 ms</span><br /><br /><span>phc=# select count(1) from c_pglog where command_tag = 'UPDATE' and  log_time BETWEEN '2014-04-16 07:15:00' and '2014-04-16 07:20:00';<br />&nbsp;count <br />-------<br />&nbsp;84982<br />(1 row)<br /><br />Time: 2260.891 ms</span><br /><br />One limitation is that currently, with FDWs not able to cleanly push down aggregation to the foreign data wrapper, the actual aggregation is still done on the postgres side.&nbsp; This means that large aggregates are about the same speed on cstore_fdw as they are for PostgreSQL tables:<br /><br /><span>phc=# select round((sum(duration)/1000)::numeric,2) from statements where command_tag = 'UPDATE'; round&nbsp; <br />--------<br />&nbsp;444.94<br />(1 row)<br /><br />Time: 2920.640 ms<br />phc=# select round((sum(duration)/1000)::numeric,2) from c_statements where command_tag = 'UPDATE';<br />&nbsp;round&nbsp; <br />--------<br />&nbsp;444.94<br />(1 row)<br /><br />Time: 3232.986 ms</span><br /><br />The project plans to fix this, but until then, cstore_fdw is useful mainly for searches across really large/wide tables.&nbsp; Or for seldom-touched archive tables where you want to save yourself GB or TB of disk space.<br /><br />There are a bunch of other features, and a bunch of other limitations; <a href="http://goo.gl/QupyHO" target="_blank">tune in to the SFPUG event to learn more</a>.";s:4:"guid";s:69:"tag:blogger.com,1999:blog-7476449567742726187.post-169463896466565879";s:7:"pubdate";s:29:"Mon, 12 May 2014 22:26:00 GMT";s:7:"summary";s:3705:"About a month ago, PostgreSQL fork vendor and Data Warehousing company CitusDB announced the availability of the <a href="http://citusdata.github.io/cstore_fdw/" target="_blank">open-source cstore_fdw</a>.&nbsp; This foreign data wrapper creates an external table with highly compressed data, which allows you to keep large amounts of archival data on your PostgreSQL server.<br /><br />You can find out more if you want to tune in <a href="http://goo.gl/QupyHO" target="_blank">tommorrow night, May 13th, around 7:15PM PDT</a>.&nbsp; Tomorrow night's even will be sponsored by CitusDB and hosted by Rackspace.<br /><br />First, the good stuff: compression:<br /><br /><span>phc=# select pg_size_pretty(pg_total_relation_size('postgres_log'));<br />&nbsp;pg_size_pretty <br />----------------<br />&nbsp;28 GB&nbsp;</span><br /><br /><span>ls -lh $PGDATA/base</span><br /><br /><span>-rw------- 1 postgres postgres 3.2G May 12 13:37 pglog.cstore<br />-rw------- 1 postgres postgres&nbsp; 12K May 12 13:37 pglog.cstore.footer</span><br /><br />So, the "postgres_log" table from this Performance Health Check database, which has 15 million records, takes up 28GB in postgres, and 3.2GB as a cstore table ... a space savings of about 89%.&nbsp; Not bad.&nbsp; Especially if you consider that the cstore table already has skip indexes on all indexable columns.<br /><br />Now, where this space savings becomes a real benefit is if the cstore table fits in memory and the Postgres table doesn't.&nbsp;&nbsp; I don't have a case like that, although the cstore still does show performance benefits if you have a wide table and you don't need all columns:<br /><br /><span>phc=# select count(1) from postgres_log where command_tag = 'UPDATE';<br />&nbsp;count&nbsp; <br />--------<br />&nbsp;986390<br />(1 row)<br /><br />Time: 23746.476 ms<br />phc=# select count(1) from c_pglog where command_tag = 'UPDATE';<br />&nbsp;count&nbsp; <br />--------<br />&nbsp;986390<br />(1 row)<br /><br />Time: 14059.405 ms</span><br /><br />And even better if you can apply a relatively restrictive filter:<br /><br /><span>phc=# select count(1) from postgres_log where command_tag = 'UPDATE' and log_time BETWEEN '2014-04-16 07:15:00' and '2014-04-16 07:20:00';<br />&nbsp;count <br />-------<br />&nbsp;84982<br />(1 row)<br /><br />Time: 19653.746 ms</span><br /><br /><span>phc=# select count(1) from c_pglog where command_tag = 'UPDATE' and  log_time BETWEEN '2014-04-16 07:15:00' and '2014-04-16 07:20:00';<br />&nbsp;count <br />-------<br />&nbsp;84982<br />(1 row)<br /><br />Time: 2260.891 ms</span><br /><br />One limitation is that currently, with FDWs not able to cleanly push down aggregation to the foreign data wrapper, the actual aggregation is still done on the postgres side.&nbsp; This means that large aggregates are about the same speed on cstore_fdw as they are for PostgreSQL tables:<br /><br /><span>phc=# select round((sum(duration)/1000)::numeric,2) from statements where command_tag = 'UPDATE'; round&nbsp; <br />--------<br />&nbsp;444.94<br />(1 row)<br /><br />Time: 2920.640 ms<br />phc=# select round((sum(duration)/1000)::numeric,2) from c_statements where command_tag = 'UPDATE';<br />&nbsp;round&nbsp; <br />--------<br />&nbsp;444.94<br />(1 row)<br /><br />Time: 3232.986 ms</span><br /><br />The project plans to fix this, but until then, cstore_fdw is useful mainly for searches across really large/wide tables.&nbsp; Or for seldom-touched archive tables where you want to save yourself GB or TB of disk space.<br /><br />There are a bunch of other features, and a bunch of other limitations; <a href="http://goo.gl/QupyHO" target="_blank">tune in to the SFPUG event to learn more</a>.";}i:7;a:6:{s:5:"title";s:63:"Hubert 'depesz' Lubaczewski: Joining BTree and GIN/GiST indexes";s:4:"link";s:67:"http://www.depesz.com/2014/05/12/joining-btree-and-gingist-indexes/";s:11:"description";s:301:"Today, I'd like to show you how you can use the same index for two different types of conditions. One that is using normal BTree indexing ( equal, less than, greater than ), and one that is using GIN/GiST index, for full text searching. Before I will go any further &#8211; I will be showing [&#8230;]";s:4:"guid";s:29:"http://www.depesz.com/?p=2865";s:7:"pubdate";s:29:"Mon, 12 May 2014 21:52:13 GMT";s:7:"summary";s:301:"Today, I'd like to show you how you can use the same index for two different types of conditions. One that is using normal BTree indexing ( equal, less than, greater than ), and one that is using GIN/GiST index, for full text searching. Before I will go any further &#8211; I will be showing [&#8230;]";}i:8;a:6:{s:5:"title";s:57:"Pavel Stehule: A speed of PL languages for atypical usage";s:4:"link";s:75:"http://okbob.blogspot.com/2014/05/a-speed-of-pl-languages-for-atypical.html";s:11:"description";s:5611:"A speed of PL languages for atypical usage<br /><br />A typical usage of PL languages should be a glue of SQL statements. But sometimes can be useful use these languages for PostgreSQL library enhancing.<br />I test a simple variadic function - function "least" that I can to compare with native C implementation (buildin). I was little bit surprised by speed of Lua - it is really fast and only one order slower than C implementation - PL/pgSQL is not bad - it is slower than PL/Lua - but only two times (it is relative very fast SQL glue).<br /><br /><pre>-- native implementation<br />postgres=# select count(*) filter (where a = least(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 55.776 ms<br /></pre>Table foo has about 100K rows.<br /><pre>create table foo(a int, b int, c int, d int, e int);<br />insert into foo select random()*100, random()*100, random()*100, random()*100, random()*100 from generate_series(1,100000);<br /><br />postgres=# select count(*) from foo;<br /> count  <br /><br /> 100000<br />(1 row)<br /><br />Time: 21.305 ms<br /></pre>I started with PL/pgSQL <br /><pre>CREATE OR REPLACE FUNCTION public.myleast1(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plpgsql IMMUTABLE STRICT<br />AS $function$<br />declare result int;<br />a int;<br />begin<br />  foreach a in array $1<br />  loop<br />    if result is null then <br />      result := a; <br />    elseif a < result then<br />      result := a;<br />    end if;<br />  end loop;<br />  return result;<br />end;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast1(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 996.684 ms<br /></pre>with small optimization (possible due result is not varlena type) it is about 3% faster <pre>CREATE OR REPLACE FUNCTION public.myleast1a(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plpgsql IMMUTABLE STRICT<br />AS $function$<br />declare result int;<br />a int;<br />begin<br />  foreach a in array $1<br />  loop<br />    if a < result then <br />      result := a; <br />    else<br />      result := coalesce(result, a);<br />    end if;<br />  end loop;<br />  return result;<br />end;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast1a(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 968.769 ms<br /></pre>Wrapping SQL in PL/pgSQL doesn't help <pre>CREATE OR REPLACE FUNCTION public.myleast2(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plpgsql IMMUTABLE STRICT<br />AS $function$<br />declare result int;<br />a int;<br />begin<br />  return (select min(v) from unnest($1) g(v));<br />end;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast2(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 1886.462 ms<br /></pre>Single line SQL functions is not faster than PL/pgSQL - the body of SQL function is not trivial, and Postgres cannot to inline function body effectively <pre>CREATE OR REPLACE FUNCTION public.myleast3(VARIADIC integer[])<br /> RETURNS integer LANGUAGE sql IMMUTABLE STRICT<br />AS $function$select min(v) from unnest($1) g(v)$function$<br /><br />postgres=# select count(*) filter (where a = myleast3(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 1238.185 ms<br /></pre>A winner of this test is implementation in PL/Lua - the code is readable and pretty fast. <pre>CREATE OR REPLACE FUNCTION public.myleast4(VARIADIC a integer[])<br /> RETURNS integer LANGUAGE pllua IMMUTABLE STRICT<br />AS $function$<br />local result;<br />for k,v in pairs(a) do <br />  if result == nil then <br />    result = v<br />  elseif v < result then <br />    result = v<br />  end; <br />end<br />return result;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast4(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 469.174 ms<br /></pre>By contrast I was surprised a slower speed of PL/Perl (and write code was little bit more difficult). Sometimes I used a perl for similar small functions and looks so Lua is better than Perl for these purposes. <pre>CREATE OR REPLACE FUNCTION public.myleast5(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plperl IMMUTABLE STRICT<br />AS $function$<br />for my $value (@{$_[0]} ) {<br />  if (! defined $result) {<br />    $result = $value;<br />  } elsif ( $value < $result ) {<br />    $result = $value; <br />  }<br />}<br />return $result;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast5(a,b,c,d,e)) from foo;<br /> count <br /><br />   535<br />(1 row)<br /><br />Time: 1591.802 ms<br /></pre><a href="http://pllua.projects.pgfoundry.org/">PL/Lua</a> is not well known development environment - although it looks so for similar use cases is second candidate after C language. Still Perl is best for other special use cases due unlimited support of available extensions on CPAN. <br /><br />Second note: this synthetic benchmark is not pretty fair - in really typical use case a real bottleneck is IO operations and the speed of similar functions should not be significant - this class of databases like Postgres, MSSQL, Oracle is hardly optimized on minimize IO operations. Numeric calculations are secondary target. Probably any IO waits clean differences between these implementations.<br />";s:4:"guid";s:70:"tag:blogger.com,1999:blog-8839574367290288724.post-6734059996886156268";s:7:"pubdate";s:29:"Sun, 11 May 2014 07:14:00 GMT";s:7:"summary";s:5611:"A speed of PL languages for atypical usage<br /><br />A typical usage of PL languages should be a glue of SQL statements. But sometimes can be useful use these languages for PostgreSQL library enhancing.<br />I test a simple variadic function - function "least" that I can to compare with native C implementation (buildin). I was little bit surprised by speed of Lua - it is really fast and only one order slower than C implementation - PL/pgSQL is not bad - it is slower than PL/Lua - but only two times (it is relative very fast SQL glue).<br /><br /><pre>-- native implementation<br />postgres=# select count(*) filter (where a = least(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 55.776 ms<br /></pre>Table foo has about 100K rows.<br /><pre>create table foo(a int, b int, c int, d int, e int);<br />insert into foo select random()*100, random()*100, random()*100, random()*100, random()*100 from generate_series(1,100000);<br /><br />postgres=# select count(*) from foo;<br /> count  <br /><br /> 100000<br />(1 row)<br /><br />Time: 21.305 ms<br /></pre>I started with PL/pgSQL <br /><pre>CREATE OR REPLACE FUNCTION public.myleast1(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plpgsql IMMUTABLE STRICT<br />AS $function$<br />declare result int;<br />a int;<br />begin<br />  foreach a in array $1<br />  loop<br />    if result is null then <br />      result := a; <br />    elseif a < result then<br />      result := a;<br />    end if;<br />  end loop;<br />  return result;<br />end;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast1(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 996.684 ms<br /></pre>with small optimization (possible due result is not varlena type) it is about 3% faster <pre>CREATE OR REPLACE FUNCTION public.myleast1a(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plpgsql IMMUTABLE STRICT<br />AS $function$<br />declare result int;<br />a int;<br />begin<br />  foreach a in array $1<br />  loop<br />    if a < result then <br />      result := a; <br />    else<br />      result := coalesce(result, a);<br />    end if;<br />  end loop;<br />  return result;<br />end;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast1a(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 968.769 ms<br /></pre>Wrapping SQL in PL/pgSQL doesn't help <pre>CREATE OR REPLACE FUNCTION public.myleast2(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plpgsql IMMUTABLE STRICT<br />AS $function$<br />declare result int;<br />a int;<br />begin<br />  return (select min(v) from unnest($1) g(v));<br />end;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast2(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 1886.462 ms<br /></pre>Single line SQL functions is not faster than PL/pgSQL - the body of SQL function is not trivial, and Postgres cannot to inline function body effectively <pre>CREATE OR REPLACE FUNCTION public.myleast3(VARIADIC integer[])<br /> RETURNS integer LANGUAGE sql IMMUTABLE STRICT<br />AS $function$select min(v) from unnest($1) g(v)$function$<br /><br />postgres=# select count(*) filter (where a = myleast3(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 1238.185 ms<br /></pre>A winner of this test is implementation in PL/Lua - the code is readable and pretty fast. <pre>CREATE OR REPLACE FUNCTION public.myleast4(VARIADIC a integer[])<br /> RETURNS integer LANGUAGE pllua IMMUTABLE STRICT<br />AS $function$<br />local result;<br />for k,v in pairs(a) do <br />  if result == nil then <br />    result = v<br />  elseif v < result then <br />    result = v<br />  end; <br />end<br />return result;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast4(a,b,c,d,e)) from foo;<br /> count <br /><br /> 20634<br />(1 row)<br /><br />Time: 469.174 ms<br /></pre>By contrast I was surprised a slower speed of PL/Perl (and write code was little bit more difficult). Sometimes I used a perl for similar small functions and looks so Lua is better than Perl for these purposes. <pre>CREATE OR REPLACE FUNCTION public.myleast5(VARIADIC integer[])<br /> RETURNS integer LANGUAGE plperl IMMUTABLE STRICT<br />AS $function$<br />for my $value (@{$_[0]} ) {<br />  if (! defined $result) {<br />    $result = $value;<br />  } elsif ( $value < $result ) {<br />    $result = $value; <br />  }<br />}<br />return $result;<br />$function$<br /><br />postgres=# select count(*) filter (where a = myleast5(a,b,c,d,e)) from foo;<br /> count <br /><br />   535<br />(1 row)<br /><br />Time: 1591.802 ms<br /></pre><a href="http://pllua.projects.pgfoundry.org/">PL/Lua</a> is not well known development environment - although it looks so for similar use cases is second candidate after C language. Still Perl is best for other special use cases due unlimited support of available extensions on CPAN. <br /><br />Second note: this synthetic benchmark is not pretty fair - in really typical use case a real bottleneck is IO operations and the speed of similar functions should not be significant - this class of databases like Postgres, MSSQL, Oracle is hardly optimized on minimize IO operations. Numeric calculations are secondary target. Probably any IO waits clean differences between these implementations.<br />";}i:9;a:6:{s:5:"title";s:57:"Michael Paquier: Make Postgres sing with MinGW on Windows";s:4:"link";s:64:"http://michael.otacoo.com/postgresql-2/make-postgres-sing-mingw/";s:11:"description";s:6278:"<p>Community usually lacks developers on Windows able to test and provide
feedback on patches that are being implemented. Actually, by seeing bug
reports from users on Windows on a daily basis (not meaning that each
report is really a bug occurring only on this platform but that there
are many users of it), having more people in the field would be great.</p>

<p>Doing development on a different platform does not usually mean a lot
of things:</p>

<ul>
<li>Compiling manually code with a patch and check if a feature is really
working as expected on a platform. This would be a more natural process
than waiting for the <a href="http://michael.otacoo.com/feeds/buildfarm.postgresql.org/cgi-bin/show_failures.pl">buildfarm</a>
to become red with a Windows-only failure.</li>
<li>Helping people with their new features. A patch could be rejected
because it proposes something that is not cross-platform.</li>
<li>Getting new buildfarm machines publishing results that developers could
use to stabilize code properly.</li>
</ul>

<p>Either way, jumping from a development platform to another (especially
with already years of experience doing Linux/Unix things) may not be that
straight-forward, and even quite painful if this platform has its code
closed as you may not be able to do everything you want with it to satisfy
your needs.</p>

<p>PostgreSQL provides many ways to be able to <a href="http://www.postgresql.org/docs/devel/static/install-windows.html">compile its code on Windows</a>
for quite a long time now, involving for example the Windows SDK and even
Visual C++ (MSVC), but as well another, more hacky way using MinGW. This
may be even an easier way to enter in the world of Windows development
if you are used to Unix-like OSes as you do not need to rely on some
externally-developped SDK and still need to type by yourself commands
like ./configure and make.</p>

<p>First of all, once you have your hands on a Windows box, perhaps the
best thing to do is to install <a href="https://code.google.com/p/msysgit/">msysGit</a>
that really helps to provide an experience of Git on Windows close to
what you can live on Unix/Linux platforms. The console provided is not
perfect (need to use the Edit-&gt;[Mark|Paste] instead of a plain Ctrl-[C|V]
for any copy paste operation), but this is better than the native Command
Prompt of Windows if you are not used to it. Also, one thing to not forget
is that the paths to each disk are not prefixed with &quot;C:\PATH&quot; but with
/c/$PATH.</p>

<p>Then, continue with the <a href="http://www.postgresql.org/docs/devel/static/installation-platform-notes.html#INSTALLATION-NOTES-MINGW">installation of MinGW</a>.
Simply download it and then install it in a custom folder like. Something
like 7-zip is helpful to extract the content from tarballs. You may as
well consider another option to get 64-bit binaries with for example
<a href="http://mingw-w64.sourceforge.net/">MinGW-w64</a>. Some extra instructions
on how to use it are available <a href="http://sourceforge.net/apps/trac/mingw-w64/wiki/GeneralUsageInstructions">here</a>.</p>

<p>Even after deploying a MinGW build, you may need a <a href="http://sourceforge.net/apps/trac/mingw-w64/wiki/Make">proper make command</a> as it may not be
available in what you downloaded (that's actually what I noticed with a
MinGW-w64 build). This can for example be taken from one of the stable
snapshots of MinGW after renaming what is present there properly (make
commands are renamed to not conflict with MSYS).</p>

<p>Note as well that the Postgres wiki has some <a href="https://wiki.postgresql.org/wiki/Building_With_MinGW">additional notes</a> you may
find helpful.</p>

<p>It is usually adviced to deploy MinGW in a path like &quot;C:\mingw&quot; but this
is up to you as long as its binary folder is included in PATH, resulting
in that with msysgit for example:</p>
<div class="highlight"><pre><code class="text language-text">export PATH=$PATH:/c/mingw/bin
</code></pre></div>
<p>Once you got that done, fetch the code from <a href="https://github.com/postgres/postgres">Postgres git repository</a> and begin the real work. Here
are a couple of things to know though when beginning that.</p>

<p>First, the configure command should enforce a couple of environment
to make the build work smoothly with msysGit. Also a value should be
provided to &quot;--host&quot; to be able to detect the compiler shipped with
MinGW. This results in a configure command similar to that:</p>
<div class="highlight"><pre><code class="text language-text">PERL=perl \
    BISON=bison \
    FLEX=flex \
    MKDIR_P=&quot;mkdir -p&quot; \
    configure --host=x86_64-w64-mingw32 --without-zlib
</code></pre></div>
<p>Note as well that zlib is disabled for simplicity.</p>

<p>Once compilation has been done, be sure to change as well the calls
of &quot;$(SHELL)&quot; to &quot;bash&quot; like that in src/Makefile.global:</p>
<div class="highlight"><pre><code class="text language-text">sed -i &quot;s#\$(SHELL)#bash#g&quot; src/Makefile.global
</code></pre></div>
<p>All those things do not actually require to modify Postgres core code,
so it is up to you to modify your build scripts depending on your needs.</p>

<p>There are as well a couple of things to be aware if you try to backport
scripts that you have been using in other environments (some home-made
scripts have needed patches in my case):</p>

<ul>
<li>Be sure to update any newline of the type &quot;\n&quot; with &quot;\r\n&quot;, this will
avoid parsing failures when inserting multiple lines at the same time inside
a single file like pg_hba.conf.</li>
<li>USER is not a valid environment variable, USERNAME is.</li>
<li>Servers cannot start if kicked by users having Administrator privileges</li>
<li>Compilation is slow... </li>
</ul>

<p>Once compilation works correctly, you will be able to get something like
that:</p>
<div class="highlight"><pre><code class="text language-text">=# SELECT substring(version(), 1, 73);
                                 substring
---------------------------------------------------------------------------
 PostgreSQL 9.4devel on x86_64-w64-mingw32, compiled by x86_64-w64-mingw32
(1 row)
</code></pre></div>
<p>Then you can congratulate yourself and enjoy a glass of wine.</p>";s:4:"guid";s:64:"http://michael.otacoo.com/postgresql-2/make-postgres-sing-mingw/";s:7:"pubdate";s:29:"Sat, 10 May 2014 04:58:23 GMT";s:7:"summary";s:6278:"<p>Community usually lacks developers on Windows able to test and provide
feedback on patches that are being implemented. Actually, by seeing bug
reports from users on Windows on a daily basis (not meaning that each
report is really a bug occurring only on this platform but that there
are many users of it), having more people in the field would be great.</p>

<p>Doing development on a different platform does not usually mean a lot
of things:</p>

<ul>
<li>Compiling manually code with a patch and check if a feature is really
working as expected on a platform. This would be a more natural process
than waiting for the <a href="http://michael.otacoo.com/feeds/buildfarm.postgresql.org/cgi-bin/show_failures.pl">buildfarm</a>
to become red with a Windows-only failure.</li>
<li>Helping people with their new features. A patch could be rejected
because it proposes something that is not cross-platform.</li>
<li>Getting new buildfarm machines publishing results that developers could
use to stabilize code properly.</li>
</ul>

<p>Either way, jumping from a development platform to another (especially
with already years of experience doing Linux/Unix things) may not be that
straight-forward, and even quite painful if this platform has its code
closed as you may not be able to do everything you want with it to satisfy
your needs.</p>

<p>PostgreSQL provides many ways to be able to <a href="http://www.postgresql.org/docs/devel/static/install-windows.html">compile its code on Windows</a>
for quite a long time now, involving for example the Windows SDK and even
Visual C++ (MSVC), but as well another, more hacky way using MinGW. This
may be even an easier way to enter in the world of Windows development
if you are used to Unix-like OSes as you do not need to rely on some
externally-developped SDK and still need to type by yourself commands
like ./configure and make.</p>

<p>First of all, once you have your hands on a Windows box, perhaps the
best thing to do is to install <a href="https://code.google.com/p/msysgit/">msysGit</a>
that really helps to provide an experience of Git on Windows close to
what you can live on Unix/Linux platforms. The console provided is not
perfect (need to use the Edit-&gt;[Mark|Paste] instead of a plain Ctrl-[C|V]
for any copy paste operation), but this is better than the native Command
Prompt of Windows if you are not used to it. Also, one thing to not forget
is that the paths to each disk are not prefixed with &quot;C:\PATH&quot; but with
/c/$PATH.</p>

<p>Then, continue with the <a href="http://www.postgresql.org/docs/devel/static/installation-platform-notes.html#INSTALLATION-NOTES-MINGW">installation of MinGW</a>.
Simply download it and then install it in a custom folder like. Something
like 7-zip is helpful to extract the content from tarballs. You may as
well consider another option to get 64-bit binaries with for example
<a href="http://mingw-w64.sourceforge.net/">MinGW-w64</a>. Some extra instructions
on how to use it are available <a href="http://sourceforge.net/apps/trac/mingw-w64/wiki/GeneralUsageInstructions">here</a>.</p>

<p>Even after deploying a MinGW build, you may need a <a href="http://sourceforge.net/apps/trac/mingw-w64/wiki/Make">proper make command</a> as it may not be
available in what you downloaded (that's actually what I noticed with a
MinGW-w64 build). This can for example be taken from one of the stable
snapshots of MinGW after renaming what is present there properly (make
commands are renamed to not conflict with MSYS).</p>

<p>Note as well that the Postgres wiki has some <a href="https://wiki.postgresql.org/wiki/Building_With_MinGW">additional notes</a> you may
find helpful.</p>

<p>It is usually adviced to deploy MinGW in a path like &quot;C:\mingw&quot; but this
is up to you as long as its binary folder is included in PATH, resulting
in that with msysgit for example:</p>
<div class="highlight"><pre><code class="text language-text">export PATH=$PATH:/c/mingw/bin
</code></pre></div>
<p>Once you got that done, fetch the code from <a href="https://github.com/postgres/postgres">Postgres git repository</a> and begin the real work. Here
are a couple of things to know though when beginning that.</p>

<p>First, the configure command should enforce a couple of environment
to make the build work smoothly with msysGit. Also a value should be
provided to &quot;--host&quot; to be able to detect the compiler shipped with
MinGW. This results in a configure command similar to that:</p>
<div class="highlight"><pre><code class="text language-text">PERL=perl \
    BISON=bison \
    FLEX=flex \
    MKDIR_P=&quot;mkdir -p&quot; \
    configure --host=x86_64-w64-mingw32 --without-zlib
</code></pre></div>
<p>Note as well that zlib is disabled for simplicity.</p>

<p>Once compilation has been done, be sure to change as well the calls
of &quot;$(SHELL)&quot; to &quot;bash&quot; like that in src/Makefile.global:</p>
<div class="highlight"><pre><code class="text language-text">sed -i &quot;s#\$(SHELL)#bash#g&quot; src/Makefile.global
</code></pre></div>
<p>All those things do not actually require to modify Postgres core code,
so it is up to you to modify your build scripts depending on your needs.</p>

<p>There are as well a couple of things to be aware if you try to backport
scripts that you have been using in other environments (some home-made
scripts have needed patches in my case):</p>

<ul>
<li>Be sure to update any newline of the type &quot;\n&quot; with &quot;\r\n&quot;, this will
avoid parsing failures when inserting multiple lines at the same time inside
a single file like pg_hba.conf.</li>
<li>USER is not a valid environment variable, USERNAME is.</li>
<li>Servers cannot start if kicked by users having Administrator privileges</li>
<li>Compilation is slow... </li>
</ul>

<p>Once compilation works correctly, you will be able to get something like
that:</p>
<div class="highlight"><pre><code class="text language-text">=# SELECT substring(version(), 1, 73);
                                 substring
---------------------------------------------------------------------------
 PostgreSQL 9.4devel on x86_64-w64-mingw32, compiled by x86_64-w64-mingw32
(1 row)
</code></pre></div>
<p>Then you can congratulate yourself and enjoy a glass of wine.</p>";}i:10;a:6:{s:5:"title";s:43:"Josh Berkus: Remastering without restarting";s:4:"link";s:71:"http://www.databasesoup.com/2014/05/remastering-without-restarting.html";s:11:"description";s:4825:"Thanks to Streaming-Only Remastering, PostgreSQL 9.3 has been a boon to high-availability setups and maintenance&nbsp; You can re-arrange your replicas however you like; remastered, in a tree, in a ring, whatever.&nbsp; However, there's been one wart on this free reconfiguration of Postgres replication clusters: if you want to change masters, you have to restart the replica.<br /><br />This doesn't sound like a big deal, until you think about a cluster with load balancing to 16 read-only replicas.&nbsp; Every one of those you restart breaks a bunch of application connections.&nbsp;&nbsp; Looking at how timeline switch works, it didn't seem like there was even a good reason for this; really<span id="goog_900881356"></span><span id="goog_900881357"></span>, the only thing which seemed to be blocking it was that primary_conninfo comes from recovery.conf, which only gets read on startup.&nbsp; I'd hoped that the <a href="https://commitfest.postgresql.org/action/patch_view?id=1293" target="_blank">merger of recovery.conf and postgresql.conf</a> would solve this, but that patch got punted to 9.5 due to conflicts with SET PERSISTENT.<br /><br />So, I set out to find a workaround, as well as proving that it was only the deficiencies of recovery.conf preventing us from doing no-restart remastering.&nbsp; And I found one, thanks to the question someone asked me at pgDay NYC.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-8_sD7jAhrlM/U21prckmHgI/AAAAAAAACuw/_XAFtnrqi74/s1600/proxy_remastering_1.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="213" src="http://1.bp.blogspot.com/-8_sD7jAhrlM/U21prckmHgI/AAAAAAAACuw/_XAFtnrqi74/s1600/proxy_remastering_1.png" width="320" /></a></div>So, in the diagram above, M1 is the current master.&nbsp; M2 is a replica which is the designated failover target.&nbsp; R1 and R2 are additional replicas.&nbsp; "proxy" is a simple TCP proxy; in fact, I used a <a href="http://voorloopnul.com/blog/a-python-proxy-in-less-than-100-lines-of-code/" target="_blank">python proxy written in 100 lines of code</a> for this test.&nbsp; You can't use a Postgres proxy like pgBouncer because it won't accept a replication connection.<br /><br />Remastering time!<br /><ol><li>Shut down M1</li><li>Promote M2</li><li>Restart the proxy, now pointing to M2</li></ol>And the new configuration:<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-zoI0LW3akkc/U21qB3yzszI/AAAAAAAACu8/vaevkjodPS0/s1600/proxy_remastering_2.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="213" src="http://3.bp.blogspot.com/-zoI0LW3akkc/U21qB3yzszI/AAAAAAAACu8/vaevkjodPS0/s1600/proxy_remastering_2.png" width="320" /></a>&nbsp;</div><div class="separator" style="clear: both; text-align: left;">But: what happened to R1 and R2?&nbsp; Did they remaster without restarting?&nbsp; Yes, indeedy, they did!</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;"><span>LOG:&nbsp; entering standby mode<br />LOG:&nbsp; redo starts at 0/21000028<br />LOG:&nbsp; consistent recovery state reached at 0/210000F0<br />LOG:&nbsp; database system is ready to accept read only connections<br />LOG:&nbsp; started streaming WAL from primary at 0/22000000 on timeline 6<br />LOG:&nbsp; replication terminated by primary server<br />DETAIL:&nbsp; End of WAL reached on timeline 6 at 0/2219C1F0.<br />FATAL:&nbsp; could not send end-of-streaming message to primary: no COPY in progress<br />FATAL:&nbsp; could not connect to the primary server: could not connect to server: Connection refused<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is the server running on host "172.31.11.254" and accepting<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TCP/IP connections on port 9999?<br /><br />LOG:&nbsp; fetching timeline history file for timeline 7 from primary server<br />LOG:&nbsp; started streaming WAL from primary at 0/22000000 on timeline 6<br />LOG:&nbsp; replication terminated by primary server<br />DETAIL:&nbsp; End of WAL reached on timeline 6 at 0/2219C1F0.<br />LOG:&nbsp; new target timeline is 7<br />LOG:&nbsp; restarted WAL streaming at 0/22000000 on timeline 7</span></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">Not only does this provide us a new remastering workaround for high-availability configurations on 9.3, it also shows us that as soon as we get around to merging recovery.conf with postgresql.conf, restarting to remaster can be eliminated.</div>";s:4:"guid";s:70:"tag:blogger.com,1999:blog-7476449567742726187.post-4751206607102342058";s:7:"pubdate";s:29:"Fri, 09 May 2014 23:58:00 GMT";s:7:"summary";s:4825:"Thanks to Streaming-Only Remastering, PostgreSQL 9.3 has been a boon to high-availability setups and maintenance&nbsp; You can re-arrange your replicas however you like; remastered, in a tree, in a ring, whatever.&nbsp; However, there's been one wart on this free reconfiguration of Postgres replication clusters: if you want to change masters, you have to restart the replica.<br /><br />This doesn't sound like a big deal, until you think about a cluster with load balancing to 16 read-only replicas.&nbsp; Every one of those you restart breaks a bunch of application connections.&nbsp;&nbsp; Looking at how timeline switch works, it didn't seem like there was even a good reason for this; really<span id="goog_900881356"></span><span id="goog_900881357"></span>, the only thing which seemed to be blocking it was that primary_conninfo comes from recovery.conf, which only gets read on startup.&nbsp; I'd hoped that the <a href="https://commitfest.postgresql.org/action/patch_view?id=1293" target="_blank">merger of recovery.conf and postgresql.conf</a> would solve this, but that patch got punted to 9.5 due to conflicts with SET PERSISTENT.<br /><br />So, I set out to find a workaround, as well as proving that it was only the deficiencies of recovery.conf preventing us from doing no-restart remastering.&nbsp; And I found one, thanks to the question someone asked me at pgDay NYC.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-8_sD7jAhrlM/U21prckmHgI/AAAAAAAACuw/_XAFtnrqi74/s1600/proxy_remastering_1.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="213" src="http://1.bp.blogspot.com/-8_sD7jAhrlM/U21prckmHgI/AAAAAAAACuw/_XAFtnrqi74/s1600/proxy_remastering_1.png" width="320" /></a></div>So, in the diagram above, M1 is the current master.&nbsp; M2 is a replica which is the designated failover target.&nbsp; R1 and R2 are additional replicas.&nbsp; "proxy" is a simple TCP proxy; in fact, I used a <a href="http://voorloopnul.com/blog/a-python-proxy-in-less-than-100-lines-of-code/" target="_blank">python proxy written in 100 lines of code</a> for this test.&nbsp; You can't use a Postgres proxy like pgBouncer because it won't accept a replication connection.<br /><br />Remastering time!<br /><ol><li>Shut down M1</li><li>Promote M2</li><li>Restart the proxy, now pointing to M2</li></ol>And the new configuration:<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-zoI0LW3akkc/U21qB3yzszI/AAAAAAAACu8/vaevkjodPS0/s1600/proxy_remastering_2.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="213" src="http://3.bp.blogspot.com/-zoI0LW3akkc/U21qB3yzszI/AAAAAAAACu8/vaevkjodPS0/s1600/proxy_remastering_2.png" width="320" /></a>&nbsp;</div><div class="separator" style="clear: both; text-align: left;">But: what happened to R1 and R2?&nbsp; Did they remaster without restarting?&nbsp; Yes, indeedy, they did!</div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;"><span>LOG:&nbsp; entering standby mode<br />LOG:&nbsp; redo starts at 0/21000028<br />LOG:&nbsp; consistent recovery state reached at 0/210000F0<br />LOG:&nbsp; database system is ready to accept read only connections<br />LOG:&nbsp; started streaming WAL from primary at 0/22000000 on timeline 6<br />LOG:&nbsp; replication terminated by primary server<br />DETAIL:&nbsp; End of WAL reached on timeline 6 at 0/2219C1F0.<br />FATAL:&nbsp; could not send end-of-streaming message to primary: no COPY in progress<br />FATAL:&nbsp; could not connect to the primary server: could not connect to server: Connection refused<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is the server running on host "172.31.11.254" and accepting<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TCP/IP connections on port 9999?<br /><br />LOG:&nbsp; fetching timeline history file for timeline 7 from primary server<br />LOG:&nbsp; started streaming WAL from primary at 0/22000000 on timeline 6<br />LOG:&nbsp; replication terminated by primary server<br />DETAIL:&nbsp; End of WAL reached on timeline 6 at 0/2219C1F0.<br />LOG:&nbsp; new target timeline is 7<br />LOG:&nbsp; restarted WAL streaming at 0/22000000 on timeline 7</span></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">Not only does this provide us a new remastering workaround for high-availability configurations on 9.3, it also shows us that as soon as we get around to merging recovery.conf with postgresql.conf, restarting to remaster can be eliminated.</div>";}i:11;a:6:{s:5:"title";s:44:"Hans-Juergen Schoenig: Casting integer to IP";s:4:"link";s:45:"http://www.cybertec.at/casting-integer-to-ip/";s:11:"description";s:290:"Once in a while you have to juggle around with IP addresses and store them / process them in an efficient way. To do so PostgreSQL provides us with two data types: cidr and inet. The beauty here is that those two types make sure that no bad data can be inserted into the database: [&#8230;]";s:4:"guid";s:30:"http://www.cybertec.at/?p=3180";s:7:"pubdate";s:29:"Fri, 09 May 2014 10:45:15 GMT";s:7:"summary";s:290:"Once in a while you have to juggle around with IP addresses and store them / process them in an efficient way. To do so PostgreSQL provides us with two data types: cidr and inet. The beauty here is that those two types make sure that no bad data can be inserted into the database: [&#8230;]";}i:12;a:6:{s:5:"title";s:46:"gabrielle roth: PDXPUG:  May meeting next week";s:4:"link";s:68:"http://pdxpug.wordpress.com/2014/05/08/pdxpug-may-meeting-next-week/";s:11:"description";s:2347:"<p><strong>When</strong>: 7-9pm Thu May 15, 2014<br />
<strong>Where</strong>: Iovation<br />
<strong>Who</strong>: Selena Deckelmann<br />
<strong>What</strong>: The Final Crontab</p>
<p><a href="https://github.com/mozilla/socorro/blob/master/docs/crontabber.rst" target="_blank">Crontabber</a> is a new open source utility that makes cron jobs automatically retriable, uses Postgres to store useful information like duration and failure reasons, and integrates easily with Nagios.  Come hear about the reasons Mozilla created this tool, and how it&#8217;s helped us make our environment more stable and reliable, and less prone to getting calls on the weekend.</p>
<p>Selena Deckelmann is a major contributor to PostgreSQL and a data architect at Mozilla. Shes been involved with free and open source software since 1995 and began running conferences for PostgreSQL in 2007. In 2012, she founded PyLadiesPDX, a portland chapter of PyLadies. She founded Open Source Bridge, Postgres Open and speaks internationally about open source, databases and community. You can find her on twitter (<a href="https://twitter.com/selenamarie" target="_blank">@selenamarie</a>) and on her blog. She also keeps chickens and gives a lot of technical talks.</p>
<p>&#8211;</p>
<p>We have a new meeting location while the Iovation offices are being renovated.  We&#8217;re still in the US Bancorp Tower at 111 SW 5th (5th &amp; Oak), but on the ground floor in the Training Room.  As you face the bank of elevators from the main lobby, take a good deep breath of drywall dust and head to your right to a hallway.  Follow the hallway as it turns to the left.  The Training Room is the 2nd door on your right and is labeled &#8220;Training Room&#8221;.  There is no room number.</p>
<p>The building is on the Green &amp; Yellow Max lines.  Underground bike parking is available in the parking garage;  outdoors all around the block in the usual spots.</p>
<p>See you there!</p><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/pdxpug.wordpress.com/359/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/pdxpug.wordpress.com/359/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=pdxpug.wordpress.com&#038;blog=30930172&#038;post=359&#038;subd=pdxpug&#038;ref=&#038;feed=1" width="1" />";s:4:"guid";s:34:"http://pdxpug.wordpress.com/?p=359";s:7:"pubdate";s:29:"Fri, 09 May 2014 01:33:31 GMT";s:7:"summary";s:2347:"<p><strong>When</strong>: 7-9pm Thu May 15, 2014<br />
<strong>Where</strong>: Iovation<br />
<strong>Who</strong>: Selena Deckelmann<br />
<strong>What</strong>: The Final Crontab</p>
<p><a href="https://github.com/mozilla/socorro/blob/master/docs/crontabber.rst" target="_blank">Crontabber</a> is a new open source utility that makes cron jobs automatically retriable, uses Postgres to store useful information like duration and failure reasons, and integrates easily with Nagios.  Come hear about the reasons Mozilla created this tool, and how it&#8217;s helped us make our environment more stable and reliable, and less prone to getting calls on the weekend.</p>
<p>Selena Deckelmann is a major contributor to PostgreSQL and a data architect at Mozilla. Shes been involved with free and open source software since 1995 and began running conferences for PostgreSQL in 2007. In 2012, she founded PyLadiesPDX, a portland chapter of PyLadies. She founded Open Source Bridge, Postgres Open and speaks internationally about open source, databases and community. You can find her on twitter (<a href="https://twitter.com/selenamarie" target="_blank">@selenamarie</a>) and on her blog. She also keeps chickens and gives a lot of technical talks.</p>
<p>&#8211;</p>
<p>We have a new meeting location while the Iovation offices are being renovated.  We&#8217;re still in the US Bancorp Tower at 111 SW 5th (5th &amp; Oak), but on the ground floor in the Training Room.  As you face the bank of elevators from the main lobby, take a good deep breath of drywall dust and head to your right to a hallway.  Follow the hallway as it turns to the left.  The Training Room is the 2nd door on your right and is labeled &#8220;Training Room&#8221;.  There is no room number.</p>
<p>The building is on the Green &amp; Yellow Max lines.  Underground bike parking is available in the parking garage;  outdoors all around the block in the usual spots.</p>
<p>See you there!</p><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/pdxpug.wordpress.com/359/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/pdxpug.wordpress.com/359/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=pdxpug.wordpress.com&#038;blog=30930172&#038;post=359&#038;subd=pdxpug&#038;ref=&#038;feed=1" width="1" />";}i:13;a:6:{s:5:"title";s:54:"Marko Tiikkaja: PostgreSQL gotcha of the week, week 19";s:4:"link";s:74:"http://johtopg.blogspot.com/2014/05/postgresql-gotcha-of-week-week-19.html";s:11:"description";s:397:"local:marko=# create table foo();
CREATE TABLE
local:marko=#* insert into foo default values;
INSERT 0 1
local:marko=#* create function getdata(foo)
returns table (a int, b int)
as $$
begin

if random() < 0.5 then
    a := 1; b := 1;
    return next;
else
    a := 2; b := 2;
    return next;
end if ;
end
$$ language plpgsql
;
CREATE FUNCTION
local:marko=#* select (getdata(foo)).* from foo;
 a |";s:4:"guid";s:69:"tag:blogger.com,1999:blog-265587150912543268.post-5052488810102426775";s:7:"pubdate";s:29:"Thu, 08 May 2014 18:19:00 GMT";s:7:"summary";s:397:"local:marko=# create table foo();
CREATE TABLE
local:marko=#* insert into foo default values;
INSERT 0 1
local:marko=#* create function getdata(foo)
returns table (a int, b int)
as $$
begin

if random() < 0.5 then
    a := 1; b := 1;
    return next;
else
    a := 2; b := 2;
    return next;
end if ;
end
$$ language plpgsql
;
CREATE FUNCTION
local:marko=#* select (getdata(foo)).* from foo;
 a |";}i:14;a:6:{s:5:"title";s:54:"Shaun M. Thomas: Trumping the PostgreSQL Query Planner";s:4:"link";s:71:"http://bonesmoses.org/2014/05/08/trumping-the-postgresql-query-planner/";s:11:"description";s:7578:"<p>With the release of PostgreSQL 8.4, the community gained the ability to use CTE syntax. As such, this is a fairly old feature, yet it&#8217;s still misunderstood in a lot of ways. At the same time, the query planner has been advancing incrementally since that time. Most recently, PostgreSQL has gained the ability to perform index-only scans, making it possible to fetch results straight from the index, without confirming rows with the table data.</p>

<p>Unfortunately, this still isn&#8217;t enough. There are still quite a few areas where the PostgreSQL query planner is extremely naive, despite the advances we&#8217;ve seen recently. For instance, PostgreSQL still can&#8217;t do a basic <a href="http://wiki.postgresql.org/wiki/Loose_indexscan">loose index scan</a> natively. It has to be tricked by using CTE syntax.</p>

<p>To demonstrate this further, imagine this relatively common scenario: an order processing system where clients can order products. What happens when we want to find the most recent order for all current customers? Boiled down to its minimum elements, this extremely simplified table will act as our order system.</p>

<pre><code>CREATE TABLE test_order
(
  client_id   INT        NOT NULL,
  order_date  TIMESTAMP  NOT NULL,
  filler      TEXT       NOT NULL
);
</code></pre>

<p>Now we need data to test with. We can simulate a relatively old order processing system by taking the current date and subtracting 1,000 days. We can also bootstrap with 10,000 clients, and make the assumption that newer clients will be more active. This allows us to represent clients that have left our services as time goes on. So we start with this test data:</p>

<pre><code>INSERT INTO test_order
SELECT s1.id,
       (CURRENT_DATE - INTERVAL '1000 days')::DATE 
           + generate_series(1, s1.id%1000),
       repeat(' ', 20)
  FROM generate_series(1, 10000) s1 (id);
</code></pre>

<p>The <code>generate_series</code> function is very handy for building fake data. We&#8217;re still not ready to use that data, however. Since we want to find the most recent order for all customers, we need an index that will combine the <code>client_id</code> and <code>order_date</code> columns in such a way that a single lookup will provide the value we want for any particular client. This index should do nicely:</p>

<pre><code>CREATE INDEX idx_test_order_client_id_order_date
    ON test_order (client_id, order_date DESC);
</code></pre>

<p>Finally, we analyze to make sure the PostgreSQL engine has the most recent stats for our table. Just to make everything easily repeatable, we also set the <code>default_statistics_target</code> to a higher value than default as well.</p>

<pre><code>SET default_statistics_target TO 500;
ANALYZE test_order;
</code></pre>

<p>Now we&#8217;ll start with the most obvious query. Here, we just use the <code>client_id</code> column and look for the max <code>order_date</code> for each:</p>

<pre><code>EXPLAIN ANALYZE
SELECT client_id, max(order_date)
  FROM test_order
 GROUP BY client_id;
</code></pre>

<p>The query plan is fairly straight-forward, and will probably include a sequence scan. On the virtual server we&#8217;re testing with, the total runtime for us ended up looking like this:</p>

<pre><code>Total runtime: 1117.408 ms
</code></pre>

<p>There is some variance, but the end result is just over one second per execution. We ran this query several times to ensure it was properly cached by PostgreSQL. Why didn&#8217;t the planner use the index we created? Let&#8217;s assume the planner doesn&#8217;t know what <code>max</code> does, and treats it like any other function. With that in mind, we can exploit a different type of syntax that should make the index much more usable. So let&#8217;s try <code>DISTINCT ON</code> with an explicit <code>ORDER</code> clause that matches the definition of our index:</p>

<p>EXPLAIN ANALYZE
SELECT DISTINCT ON (client_id)
       client_id, order_date
  FROM test_order
 ORDER BY client_id, order_date DESC;</p>

<p>Well, this time our test system used an index-only scan, and produced the results somewhat faster. Our new runtime looks like this:</p>

<pre><code>Total runtime: 923.300 ms
</code></pre>

<p>That&#8217;s almost 20% faster than the sequence scan. Depending on how much bigger the table is than the index, reading the index and producing these results can vary significantly. And while the query time improved, it&#8217;s still pretty bad. For systems with tens or hundreds of millions of orders, the performance of this query will continue to degrade along with the row count. We&#8217;re also not really using the index effectively.</p>

<p>Reading the index from top to bottom and pulling out the desired results is faster than reading the whole table. But why should we do that? Due to the way we built this index, the root node for each client should always represent the value we&#8217;re looking for. So why doesn&#8217;t the planner simply perform a shallow index scan along the root nodes? It doesn&#8217;t matter what the reason is, because we can <em>force</em> it to do so. This is going to be ugly, but this query will act just as we described:</p>

<pre><code>EXPLAIN ANALYZE
WITH RECURSIVE skip AS
(
  (SELECT client_id, order_date
    FROM test_order
   ORDER BY client_id, order_date DESC
   LIMIT 1)
  UNION ALL
  (SELECT (SELECT min(client_id)
             FROM test_order
            WHERE client_id &gt; skip.client_id
          ) AS client_id,
          (SELECT max(order_date)
             FROM test_order
            WHERE client_id = (
                    SELECT min(client_id)
                      FROM test_order
                     WHERE client_id &gt; skip.client_id
                  )
          ) AS order_date
    FROM skip
   WHERE skip.client_id IS NOT NULL)
)
SELECT *
  FROM skip;
</code></pre>

<p>The query plan for this is extremely convoluted, and we&#8217;re not even going to try to explain what it&#8217;s doing. But the final query execution time is hard to discount:</p>

<pre><code>Total runtime: 181.501 ms
</code></pre>

<p>So what happened here? How can the abusive and ugly CTE above outwit the PostgreSQL query planner? We use the same principle as described in the PostgreSQL wiki for <a href="http://wiki.postgresql.org/wiki/Loose_indexscan">loose index scans</a>. We start with the desired maximum order date for a single <code>client_id</code>, then recursively begin adding clients one by one until the index is exhausted. Due to limitations preventing us from using the recursive element in a sub-query, we have to use the <code>SELECT</code> clause to get the next client ID and the associated order date for that client.</p>

<p>This technique works universally for performing sparse index scans, and actually improves as cardinality (the number of unique values) <em>decreases</em>. As unlikely as that sounds, since we are only using the root nodes within the index tree, performance increases when there are less root nodes to check. This is the exact opposite to how indexes are normally used, so we can see why PostgreSQL doesn&#8217;t natively integrate this technique. Yet we would like to see it added eventually so query authors can use the first query example we wrote, instead of the excessively unintuitive version that actually produced good performance.</p>

<p>In any case, all PostgreSQL DBAs owe it to themselves and their clusters to learn CTEs. They provide a powerful override for the query planner, and helps solve the edge cases it doesn&#8217;t yet handle.</p>";s:4:"guid";s:28:"http://bonesmoses.org/?p=901";s:7:"pubdate";s:29:"Thu, 08 May 2014 17:24:45 GMT";s:7:"summary";s:7578:"<p>With the release of PostgreSQL 8.4, the community gained the ability to use CTE syntax. As such, this is a fairly old feature, yet it&#8217;s still misunderstood in a lot of ways. At the same time, the query planner has been advancing incrementally since that time. Most recently, PostgreSQL has gained the ability to perform index-only scans, making it possible to fetch results straight from the index, without confirming rows with the table data.</p>

<p>Unfortunately, this still isn&#8217;t enough. There are still quite a few areas where the PostgreSQL query planner is extremely naive, despite the advances we&#8217;ve seen recently. For instance, PostgreSQL still can&#8217;t do a basic <a href="http://wiki.postgresql.org/wiki/Loose_indexscan">loose index scan</a> natively. It has to be tricked by using CTE syntax.</p>

<p>To demonstrate this further, imagine this relatively common scenario: an order processing system where clients can order products. What happens when we want to find the most recent order for all current customers? Boiled down to its minimum elements, this extremely simplified table will act as our order system.</p>

<pre><code>CREATE TABLE test_order
(
  client_id   INT        NOT NULL,
  order_date  TIMESTAMP  NOT NULL,
  filler      TEXT       NOT NULL
);
</code></pre>

<p>Now we need data to test with. We can simulate a relatively old order processing system by taking the current date and subtracting 1,000 days. We can also bootstrap with 10,000 clients, and make the assumption that newer clients will be more active. This allows us to represent clients that have left our services as time goes on. So we start with this test data:</p>

<pre><code>INSERT INTO test_order
SELECT s1.id,
       (CURRENT_DATE - INTERVAL '1000 days')::DATE 
           + generate_series(1, s1.id%1000),
       repeat(' ', 20)
  FROM generate_series(1, 10000) s1 (id);
</code></pre>

<p>The <code>generate_series</code> function is very handy for building fake data. We&#8217;re still not ready to use that data, however. Since we want to find the most recent order for all customers, we need an index that will combine the <code>client_id</code> and <code>order_date</code> columns in such a way that a single lookup will provide the value we want for any particular client. This index should do nicely:</p>

<pre><code>CREATE INDEX idx_test_order_client_id_order_date
    ON test_order (client_id, order_date DESC);
</code></pre>

<p>Finally, we analyze to make sure the PostgreSQL engine has the most recent stats for our table. Just to make everything easily repeatable, we also set the <code>default_statistics_target</code> to a higher value than default as well.</p>

<pre><code>SET default_statistics_target TO 500;
ANALYZE test_order;
</code></pre>

<p>Now we&#8217;ll start with the most obvious query. Here, we just use the <code>client_id</code> column and look for the max <code>order_date</code> for each:</p>

<pre><code>EXPLAIN ANALYZE
SELECT client_id, max(order_date)
  FROM test_order
 GROUP BY client_id;
</code></pre>

<p>The query plan is fairly straight-forward, and will probably include a sequence scan. On the virtual server we&#8217;re testing with, the total runtime for us ended up looking like this:</p>

<pre><code>Total runtime: 1117.408 ms
</code></pre>

<p>There is some variance, but the end result is just over one second per execution. We ran this query several times to ensure it was properly cached by PostgreSQL. Why didn&#8217;t the planner use the index we created? Let&#8217;s assume the planner doesn&#8217;t know what <code>max</code> does, and treats it like any other function. With that in mind, we can exploit a different type of syntax that should make the index much more usable. So let&#8217;s try <code>DISTINCT ON</code> with an explicit <code>ORDER</code> clause that matches the definition of our index:</p>

<p>EXPLAIN ANALYZE
SELECT DISTINCT ON (client_id)
       client_id, order_date
  FROM test_order
 ORDER BY client_id, order_date DESC;</p>

<p>Well, this time our test system used an index-only scan, and produced the results somewhat faster. Our new runtime looks like this:</p>

<pre><code>Total runtime: 923.300 ms
</code></pre>

<p>That&#8217;s almost 20% faster than the sequence scan. Depending on how much bigger the table is than the index, reading the index and producing these results can vary significantly. And while the query time improved, it&#8217;s still pretty bad. For systems with tens or hundreds of millions of orders, the performance of this query will continue to degrade along with the row count. We&#8217;re also not really using the index effectively.</p>

<p>Reading the index from top to bottom and pulling out the desired results is faster than reading the whole table. But why should we do that? Due to the way we built this index, the root node for each client should always represent the value we&#8217;re looking for. So why doesn&#8217;t the planner simply perform a shallow index scan along the root nodes? It doesn&#8217;t matter what the reason is, because we can <em>force</em> it to do so. This is going to be ugly, but this query will act just as we described:</p>

<pre><code>EXPLAIN ANALYZE
WITH RECURSIVE skip AS
(
  (SELECT client_id, order_date
    FROM test_order
   ORDER BY client_id, order_date DESC
   LIMIT 1)
  UNION ALL
  (SELECT (SELECT min(client_id)
             FROM test_order
            WHERE client_id &gt; skip.client_id
          ) AS client_id,
          (SELECT max(order_date)
             FROM test_order
            WHERE client_id = (
                    SELECT min(client_id)
                      FROM test_order
                     WHERE client_id &gt; skip.client_id
                  )
          ) AS order_date
    FROM skip
   WHERE skip.client_id IS NOT NULL)
)
SELECT *
  FROM skip;
</code></pre>

<p>The query plan for this is extremely convoluted, and we&#8217;re not even going to try to explain what it&#8217;s doing. But the final query execution time is hard to discount:</p>

<pre><code>Total runtime: 181.501 ms
</code></pre>

<p>So what happened here? How can the abusive and ugly CTE above outwit the PostgreSQL query planner? We use the same principle as described in the PostgreSQL wiki for <a href="http://wiki.postgresql.org/wiki/Loose_indexscan">loose index scans</a>. We start with the desired maximum order date for a single <code>client_id</code>, then recursively begin adding clients one by one until the index is exhausted. Due to limitations preventing us from using the recursive element in a sub-query, we have to use the <code>SELECT</code> clause to get the next client ID and the associated order date for that client.</p>

<p>This technique works universally for performing sparse index scans, and actually improves as cardinality (the number of unique values) <em>decreases</em>. As unlikely as that sounds, since we are only using the root nodes within the index tree, performance increases when there are less root nodes to check. This is the exact opposite to how indexes are normally used, so we can see why PostgreSQL doesn&#8217;t natively integrate this technique. Yet we would like to see it added eventually so query authors can use the first query example we wrote, instead of the excessively unintuitive version that actually produced good performance.</p>

<p>In any case, all PostgreSQL DBAs owe it to themselves and their clusters to learn CTEs. They provide a powerful override for the query planner, and helps solve the edge cases it doesn&#8217;t yet handle.</p>";}i:15;a:6:{s:5:"title";s:48:"robert berry: Querying Time Series in Postgresql";s:4:"link";s:74:"http://feedproxy.google.com/~r/no0p/~3/exUHI88jtjU/timeseries-tips-pg.html";s:11:"description";s:11249:"<h1>Querying Time Series in Postgresql</h1>
<p class="meta">May 8, 2014 &#8211; Portland, OR</p>
<p>This post covers some of the features which make Postgresql a fun and effective database system for storing and analyzing time series: date functions, window functions, and series generating functions.</p>
<h2>What are time series?</h2>
<p>In a computation context, a time series is a sequence of measurements taken at discrete time intervals.  Measurements can be taken every second, hour, day, or other arbitrary interval.  The code below will generate some time series data on the activity in a Postgresql cluster by sampling pg_stat_activity every 10 seconds for 100 seconds.</p>
<pre><code>-- add a table to store some time series data
create table activity_tseries (measured_at timestamptz, activity_count int);</code>

<code>-- add a pl function to collect counts of active queries at 10 second intervals
create or replace function collect_activity() returns void AS $$
  begin
    for i in 1..10 loop
      insert into activity_tseries (measured_at, activity_count) values 
        (clock_timestamp(), (select count(*) from pg_stat_activity where state &lt;&gt; 'idle'));
      perform pg_sleep(10);
    end loop;
  end;
$$ language plpgsql;</code>

<code>-- collect the data
select collect_activity();</code></pre>
<p>Consider the output, a sequence of activity measurements at 10 second intervals.  Note the selection of a timestamp and an associated metric.</p>
<pre><code>postgres=# select measured_at, activity_count from activity_tseries;
          measured_at          | activity_count
-------------------------------+----------------
 2014-05-07 18:22:09.655861-07 |             11
 2014-05-07 18:22:19.664114-07 |             10
 2014-05-07 18:22:29.674501-07 |              3
 2014-05-07 18:22:39.676574-07 |              9
 2014-05-07 18:22:49.686977-07 |              5
 2014-05-07 18:22:59.697342-07 |              6
 2014-05-07 18:23:09.707722-07 |              4
 2014-05-07 18:23:19.70827-07  |              6
 2014-05-07 18:23:29.718338-07 |              4
 2014-05-07 18:23:39.719099-07 |              2
(10 rows)</code></pre>
<p></p>
<h2>Continuous, Discrete, and Granularity &#8212; Changes Over Time Domain</h2>
<p>It&#8217;s kind of difficult to look at a large number of data points and attribute meaning.  So it&#8217;s common to ask questions like &#8220;how many activities were observed by hour?&#8221;  Maybe that would highlight a trend or pattern in activity.  For this illustrative example we will consider answering the question how many activities were observed per minute.</p>
<p>In a database time series data is always similar to the above in that it has discrete intervals &#8212; it&#8217;s integers all the way down.  While there are <a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">some rules</a> that govern accuracy when changing the time domain, we&#8217;ll be safely changing the time domain to buckets of 1 minute.</p>
<p>This is where <a href="http://www.postgresql.org/docs/9.3/static/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC">date_trunc</a> comes into the picture.  date_trunc is arguably the most important time function for this type of query.  Used with a sum aggregate we can easily count activities by minutes.</p>
<pre><code>postgres=# select date_trunc('minute', measured_at) as mins, sum(activity_count)
              from activity_tseries
              group by date_trunc('minute', measured_at)
              order by date_trunc('minute', measured_at) asc;</code>

<code>          mins          | sum 
------------------------+-----
 2014-05-07 18:22:00-07 |  12
 2014-05-07 18:23:00-07 |   8</code></pre>
<p>Representations of the a theoretical continuous sample, actual samples, the aggregated samples from this section are pictured below.</p>
<center>
<table>
<tr>
      <td class="trimage"><img src="http://feeds.feedburner.com/images/continuous.png" width="200" /></td>
      <td class="trimage"><img src="http://feeds.feedburner.com/images/discrete.png" width="200" /></td>
      <td class="trimage"><img src="http://feeds.feedburner.com/images/aggregate_bin.png" width="200" /></td>
</tr>
</table>
</center>
<h2>Quick Practical Example</h2>
<p>Before moving on to some more interesting features, here&#8217;s a quick example that will be widely applicable and that will set the stage for the next section.  Lets consider counting the number of new user records by day based on a created_at timestamp.  These are some actual numbers from <a href="http://thedriftapp.com/">The Drift</a>, in case you are curious about the adoption of a free Android application in its first months:</p>
<pre><code>postgres=# select date_trunc('day', created_at) as day, count(*)
              from users
              group by date_trunc('day', created_at)
              order by date_trunc('day', created_at) asc;</code>

<code>     date_trunc      | count 
---------------------+-------
 2014-01-05 00:00:00 |     1
 2014-01-13 00:00:00 |     1
 2014-01-14 00:00:00 |     1
 2014-01-15 00:00:00 |     1
 2014-01-16 00:00:00 |     1
 2014-02-10 00:00:00 |     1
 2014-02-13 00:00:00 |     1
 2014-02-14 00:00:00 |     2
 2014-02-18 00:00:00 |     1
 2014-02-19 00:00:00 |     2
 2014-02-21 00:00:00 |     1
 2014-02-23 00:00:00 |     2
 2014-02-24 00:00:00 |     1
 2014-03-02 00:00:00 |     1
 2014-03-03 00:00:00 |     1
 2014-03-06 00:00:00 |     3
 2014-03-09 00:00:00 |     2
...</code></pre>
<p></p>
<h2>Interval Filling</h2>
<p>Let&#8217;s say we wanted to reason about the rate of adoption from the above result set, or plot this data in a simple plotting library.  We might have a problem.  There are numerous gaps in the data where there were no results, e.g. from January 5th and January 13th. The library may not support parsing date strings and managing the time axis properly.</p>
<p>A straightforward technique to solve this problem is to outer join a result set from the <a href="http://www.postgresql.org/docs/9.3/static/functions-srf.html#FUNCTIONS-SRF-SERIES">generate_series</a> function.</p>
<pre><code>postgres=# with filled_dates as (
  select day, 0 as blank_count from
    generate_series('2014-01-01 00:00'::timestamptz, current_date::timestamptz, '1 day') 
      as day
),
signup_counts as (
  select date_trunc('day', created_at) as day, count(*) as signups
    from users
  group by date_trunc('day', created_at)
)
select filled_dates.day, 
       coalesce(signup_counts.signups, filled_dates.blank_count) as signups
  from filled_dates
    left outer join signup_counts on signup_counts.day = filled_dates.day
  order by filled_dates.day;</code>

<code>          day           | signups 
------------------------+---------
 2014-01-01 00:00:00-07 |       0
 2014-01-02 00:00:00-07 |       0
 2014-01-03 00:00:00-07 |       0
 2014-01-04 00:00:00-07 |       0
 2014-01-05 00:00:00-07 |       1
 2014-01-06 00:00:00-07 |       0
 2014-01-07 00:00:00-07 |       0
 2014-01-08 00:00:00-07 |       0
 2014-01-09 00:00:00-07 |       0
 2014-01-10 00:00:00-07 |       0
 2014-01-11 00:00:00-07 |       0
 2014-01-12 00:00:00-07 |       0
 2014-01-13 00:00:00-07 |       1
 2014-01-14 00:00:00-07 |       1
 2014-01-15 00:00:00-07 |       1
 2014-01-16 00:00:00-07 |       1
 2014-01-17 00:00:00-07 |       0</code></pre>
<p></p>
<h2>Finite Difference (Discrete Derivative)</h2>
<p>Here&#8217;s an interesting case for estimating a time series for the transactions/second being processed by a Postgresql cluster.  This is a real problem that came up when building <a href="https://relsys.io">Relational Systems</a>.</p>
<p>We start by collecting familiar time series data into a metrics table.  In this case we collect a timestamp associated with the result of the <a href="http://www.postgresql.org/docs/9.3/static/functions-info.html#FUNCTIONS-TXID-SNAPSHOT">txid_current</a> function.</p>
<pre><code>          measured_at          | current_tx_id 
-------------------------------+---------------
 2014-05-03 13:20:46.797304-07 |       1732896
 2014-05-03 13:21:05.012321-07 |       1732923
 2014-05-03 13:21:20.05257-07  |       1732945
 2014-05-03 13:21:35.069332-07 |       1732962
 2014-05-03 13:21:50.102453-07 |       1732991
 2014-05-03 13:22:05.127961-07 |       1733002
 2014-05-03 13:22:20.162577-07 |       1733023
 2014-05-03 13:22:35.189161-07 |       1733034
 2014-05-03 13:22:50.21059-07  |       1733056
 2014-05-03 13:23:20.319999-07 |       1733070
 2014-05-03 13:23:47.909198-07 |       1734933</code></pre>
<p>Ignoring wraparound cases for simplicity, the goal is to query this data for a result set which represents the familiar formula:</p>
<p>$\frac{\text{txid_current}_{n} &#8211; \text{txid_current}_{n-1}}{\Delta t_{sec}} \approx \frac{transactions}{second}$</p>
<p>And, fortunately, this is simple with the amazing feature that is <a href="http://www.postgresql.org/docs/9.3/static/tutorial-window.html">window functions</a>.</p>
<pre><code>postgres=# select measured_at,
              (current_tx_id - coalesce(lag(current_tx_id, 1) over w, current_tx_id)) / 
                extract( epoch from  (measured_at - lag(measured_at, 1) over w))::numeric 
                  as tx_sec 
            from heartbeats
              window w as (order by measured_at) 
            order by measured_at desc;</code>

<code>          measured_at          |         tx_sec         
-------------------------------+------------------------
 2014-05-04 13:02:56.456229-07 |     1.4642749200123185
 2014-05-04 13:02:41.431728-07 | 0.79921937579501514887
 2014-05-04 13:02:26.417077-07 |     1.4637795079651562
 2014-05-04 13:02:11.387491-07 | 0.79917248353238499975
 2014-05-04 13:01:56.371959-07 |     1.4634836872125585
 2014-05-04 13:01:41.339335-07 | 0.79888332089405429091
 2014-05-04 13:01:26.318368-07 |     1.4639291192137370
 2014-05-04 13:01:11.290318-07 | 0.79880897581705676836
 2014-05-04 13:00:56.267953-07 |    26.7313182385336655
 2014-05-04 13:00:40.743092-07 | 0.49860550013721623364
 2014-05-04 13:00:10.659188-07 |     2.3291032160723028</code></pre>
<p>Window functions allow you to reference records in a window, which is a set of records which have a relationship to the current row.  In the example above, the query uses the <a href="http://www.postgresql.org/docs/9.3/static/functions-window.html#FUNCTIONS-WINDOW-TABLE">lag</a> function which returns a value from a row offset before the current row.  The window for this query is ordered by the timestamp measured at.  Because a lag of 1 references the previous sample of txid_current() the tx_sec field matches the desired formula.</p>
<p>Window functions are remarkably powerful as you can apply aggregates over windows.</p>
<p>Below is a plot based on this query showing what happens when running <code>pgbench -T 100 -c 20</code> on a commodity desktop.</p>
<center>
<p><img src="http://feeds.feedburner.com/images/tx_sec.png" /></p>
</center>
<h2>Final Remarks</h2>
<p>The examples illustrate how easy and effective Postgresql can be for querying time series data with date functions, window functions, and series generating functions.  There are many more great tools for this type of querying that I hope to explore in future posts.</p>";s:4:"guid";s:56:"http://rner.com/postgresql/2014/05/08/timeseries-tips-pg";s:7:"pubdate";s:29:"Thu, 08 May 2014 07:00:00 GMT";s:7:"summary";s:11249:"<h1>Querying Time Series in Postgresql</h1>
<p class="meta">May 8, 2014 &#8211; Portland, OR</p>
<p>This post covers some of the features which make Postgresql a fun and effective database system for storing and analyzing time series: date functions, window functions, and series generating functions.</p>
<h2>What are time series?</h2>
<p>In a computation context, a time series is a sequence of measurements taken at discrete time intervals.  Measurements can be taken every second, hour, day, or other arbitrary interval.  The code below will generate some time series data on the activity in a Postgresql cluster by sampling pg_stat_activity every 10 seconds for 100 seconds.</p>
<pre><code>-- add a table to store some time series data
create table activity_tseries (measured_at timestamptz, activity_count int);</code>

<code>-- add a pl function to collect counts of active queries at 10 second intervals
create or replace function collect_activity() returns void AS $$
  begin
    for i in 1..10 loop
      insert into activity_tseries (measured_at, activity_count) values 
        (clock_timestamp(), (select count(*) from pg_stat_activity where state &lt;&gt; 'idle'));
      perform pg_sleep(10);
    end loop;
  end;
$$ language plpgsql;</code>

<code>-- collect the data
select collect_activity();</code></pre>
<p>Consider the output, a sequence of activity measurements at 10 second intervals.  Note the selection of a timestamp and an associated metric.</p>
<pre><code>postgres=# select measured_at, activity_count from activity_tseries;
          measured_at          | activity_count
-------------------------------+----------------
 2014-05-07 18:22:09.655861-07 |             11
 2014-05-07 18:22:19.664114-07 |             10
 2014-05-07 18:22:29.674501-07 |              3
 2014-05-07 18:22:39.676574-07 |              9
 2014-05-07 18:22:49.686977-07 |              5
 2014-05-07 18:22:59.697342-07 |              6
 2014-05-07 18:23:09.707722-07 |              4
 2014-05-07 18:23:19.70827-07  |              6
 2014-05-07 18:23:29.718338-07 |              4
 2014-05-07 18:23:39.719099-07 |              2
(10 rows)</code></pre>
<p></p>
<h2>Continuous, Discrete, and Granularity &#8212; Changes Over Time Domain</h2>
<p>It&#8217;s kind of difficult to look at a large number of data points and attribute meaning.  So it&#8217;s common to ask questions like &#8220;how many activities were observed by hour?&#8221;  Maybe that would highlight a trend or pattern in activity.  For this illustrative example we will consider answering the question how many activities were observed per minute.</p>
<p>In a database time series data is always similar to the above in that it has discrete intervals &#8212; it&#8217;s integers all the way down.  While there are <a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">some rules</a> that govern accuracy when changing the time domain, we&#8217;ll be safely changing the time domain to buckets of 1 minute.</p>
<p>This is where <a href="http://www.postgresql.org/docs/9.3/static/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC">date_trunc</a> comes into the picture.  date_trunc is arguably the most important time function for this type of query.  Used with a sum aggregate we can easily count activities by minutes.</p>
<pre><code>postgres=# select date_trunc('minute', measured_at) as mins, sum(activity_count)
              from activity_tseries
              group by date_trunc('minute', measured_at)
              order by date_trunc('minute', measured_at) asc;</code>

<code>          mins          | sum 
------------------------+-----
 2014-05-07 18:22:00-07 |  12
 2014-05-07 18:23:00-07 |   8</code></pre>
<p>Representations of the a theoretical continuous sample, actual samples, the aggregated samples from this section are pictured below.</p>
<center>
<table>
<tr>
      <td class="trimage"><img src="http://feeds.feedburner.com/images/continuous.png" width="200" /></td>
      <td class="trimage"><img src="http://feeds.feedburner.com/images/discrete.png" width="200" /></td>
      <td class="trimage"><img src="http://feeds.feedburner.com/images/aggregate_bin.png" width="200" /></td>
</tr>
</table>
</center>
<h2>Quick Practical Example</h2>
<p>Before moving on to some more interesting features, here&#8217;s a quick example that will be widely applicable and that will set the stage for the next section.  Lets consider counting the number of new user records by day based on a created_at timestamp.  These are some actual numbers from <a href="http://thedriftapp.com/">The Drift</a>, in case you are curious about the adoption of a free Android application in its first months:</p>
<pre><code>postgres=# select date_trunc('day', created_at) as day, count(*)
              from users
              group by date_trunc('day', created_at)
              order by date_trunc('day', created_at) asc;</code>

<code>     date_trunc      | count 
---------------------+-------
 2014-01-05 00:00:00 |     1
 2014-01-13 00:00:00 |     1
 2014-01-14 00:00:00 |     1
 2014-01-15 00:00:00 |     1
 2014-01-16 00:00:00 |     1
 2014-02-10 00:00:00 |     1
 2014-02-13 00:00:00 |     1
 2014-02-14 00:00:00 |     2
 2014-02-18 00:00:00 |     1
 2014-02-19 00:00:00 |     2
 2014-02-21 00:00:00 |     1
 2014-02-23 00:00:00 |     2
 2014-02-24 00:00:00 |     1
 2014-03-02 00:00:00 |     1
 2014-03-03 00:00:00 |     1
 2014-03-06 00:00:00 |     3
 2014-03-09 00:00:00 |     2
...</code></pre>
<p></p>
<h2>Interval Filling</h2>
<p>Let&#8217;s say we wanted to reason about the rate of adoption from the above result set, or plot this data in a simple plotting library.  We might have a problem.  There are numerous gaps in the data where there were no results, e.g. from January 5th and January 13th. The library may not support parsing date strings and managing the time axis properly.</p>
<p>A straightforward technique to solve this problem is to outer join a result set from the <a href="http://www.postgresql.org/docs/9.3/static/functions-srf.html#FUNCTIONS-SRF-SERIES">generate_series</a> function.</p>
<pre><code>postgres=# with filled_dates as (
  select day, 0 as blank_count from
    generate_series('2014-01-01 00:00'::timestamptz, current_date::timestamptz, '1 day') 
      as day
),
signup_counts as (
  select date_trunc('day', created_at) as day, count(*) as signups
    from users
  group by date_trunc('day', created_at)
)
select filled_dates.day, 
       coalesce(signup_counts.signups, filled_dates.blank_count) as signups
  from filled_dates
    left outer join signup_counts on signup_counts.day = filled_dates.day
  order by filled_dates.day;</code>

<code>          day           | signups 
------------------------+---------
 2014-01-01 00:00:00-07 |       0
 2014-01-02 00:00:00-07 |       0
 2014-01-03 00:00:00-07 |       0
 2014-01-04 00:00:00-07 |       0
 2014-01-05 00:00:00-07 |       1
 2014-01-06 00:00:00-07 |       0
 2014-01-07 00:00:00-07 |       0
 2014-01-08 00:00:00-07 |       0
 2014-01-09 00:00:00-07 |       0
 2014-01-10 00:00:00-07 |       0
 2014-01-11 00:00:00-07 |       0
 2014-01-12 00:00:00-07 |       0
 2014-01-13 00:00:00-07 |       1
 2014-01-14 00:00:00-07 |       1
 2014-01-15 00:00:00-07 |       1
 2014-01-16 00:00:00-07 |       1
 2014-01-17 00:00:00-07 |       0</code></pre>
<p></p>
<h2>Finite Difference (Discrete Derivative)</h2>
<p>Here&#8217;s an interesting case for estimating a time series for the transactions/second being processed by a Postgresql cluster.  This is a real problem that came up when building <a href="https://relsys.io">Relational Systems</a>.</p>
<p>We start by collecting familiar time series data into a metrics table.  In this case we collect a timestamp associated with the result of the <a href="http://www.postgresql.org/docs/9.3/static/functions-info.html#FUNCTIONS-TXID-SNAPSHOT">txid_current</a> function.</p>
<pre><code>          measured_at          | current_tx_id 
-------------------------------+---------------
 2014-05-03 13:20:46.797304-07 |       1732896
 2014-05-03 13:21:05.012321-07 |       1732923
 2014-05-03 13:21:20.05257-07  |       1732945
 2014-05-03 13:21:35.069332-07 |       1732962
 2014-05-03 13:21:50.102453-07 |       1732991
 2014-05-03 13:22:05.127961-07 |       1733002
 2014-05-03 13:22:20.162577-07 |       1733023
 2014-05-03 13:22:35.189161-07 |       1733034
 2014-05-03 13:22:50.21059-07  |       1733056
 2014-05-03 13:23:20.319999-07 |       1733070
 2014-05-03 13:23:47.909198-07 |       1734933</code></pre>
<p>Ignoring wraparound cases for simplicity, the goal is to query this data for a result set which represents the familiar formula:</p>
<p>$\frac{\text{txid_current}_{n} &#8211; \text{txid_current}_{n-1}}{\Delta t_{sec}} \approx \frac{transactions}{second}$</p>
<p>And, fortunately, this is simple with the amazing feature that is <a href="http://www.postgresql.org/docs/9.3/static/tutorial-window.html">window functions</a>.</p>
<pre><code>postgres=# select measured_at,
              (current_tx_id - coalesce(lag(current_tx_id, 1) over w, current_tx_id)) / 
                extract( epoch from  (measured_at - lag(measured_at, 1) over w))::numeric 
                  as tx_sec 
            from heartbeats
              window w as (order by measured_at) 
            order by measured_at desc;</code>

<code>          measured_at          |         tx_sec         
-------------------------------+------------------------
 2014-05-04 13:02:56.456229-07 |     1.4642749200123185
 2014-05-04 13:02:41.431728-07 | 0.79921937579501514887
 2014-05-04 13:02:26.417077-07 |     1.4637795079651562
 2014-05-04 13:02:11.387491-07 | 0.79917248353238499975
 2014-05-04 13:01:56.371959-07 |     1.4634836872125585
 2014-05-04 13:01:41.339335-07 | 0.79888332089405429091
 2014-05-04 13:01:26.318368-07 |     1.4639291192137370
 2014-05-04 13:01:11.290318-07 | 0.79880897581705676836
 2014-05-04 13:00:56.267953-07 |    26.7313182385336655
 2014-05-04 13:00:40.743092-07 | 0.49860550013721623364
 2014-05-04 13:00:10.659188-07 |     2.3291032160723028</code></pre>
<p>Window functions allow you to reference records in a window, which is a set of records which have a relationship to the current row.  In the example above, the query uses the <a href="http://www.postgresql.org/docs/9.3/static/functions-window.html#FUNCTIONS-WINDOW-TABLE">lag</a> function which returns a value from a row offset before the current row.  The window for this query is ordered by the timestamp measured at.  Because a lag of 1 references the previous sample of txid_current() the tx_sec field matches the desired formula.</p>
<p>Window functions are remarkably powerful as you can apply aggregates over windows.</p>
<p>Below is a plot based on this query showing what happens when running <code>pgbench -T 100 -c 20</code> on a commodity desktop.</p>
<center>
<p><img src="http://feeds.feedburner.com/images/tx_sec.png" /></p>
</center>
<h2>Final Remarks</h2>
<p>The examples illustrate how easy and effective Postgresql can be for querying time series data with date functions, window functions, and series generating functions.  There are many more great tools for this type of querying that I hope to explore in future posts.</p>";}i:16;a:6:{s:5:"title";s:54:"Josh Berkus: Why you should always set temp_file_limit";s:4:"link";s:80:"http://www.databasesoup.com/2014/05/why-you-should-always-set-tempfilelimit.html";s:11:"description";s:1294:""The database is growing at 2GB a minute.&nbsp; We're 40 minutes away from running out of disk space."<br /><br />"Sounds like I should probably take a look."<br /><br />I looked at the database size, which was 160GB.&nbsp; But the database SAN share was up to 1.4GB used out of 1.6GB.&nbsp; WTF?<br /><br />Then I looked at the filesystem and did directory sizes.&nbsp; pg_tmp was over a terabyte.&nbsp; Oooooooohhh.<br /><br />Apparently they'd accidentally pushed a new report to the application which worked OK in testing, but with certain parameters created a 15 billion item sort.&nbsp; And since it was slow, users called it several times.&nbsp; Ooops.<br /><br />Enter <span>temp_file_limit</span>, a parameter added by Mark Kirkwood to PostgreSQL 9.2. &nbsp; This is a limit on per-session usage of temporary files for sorts, hashes, and similar operations.&nbsp; If a user goes over the limit, their query gets cancelled and they see an error.<br /><br />This is an excellent way to prevent a single user, or a bad application change, from DOSing your database server.&nbsp; Set it to something high; I'm using 10GB or 20GB, or 10% of available disks space, whichever is less.&nbsp; But even a high limit like that will save you from some unexpected downtime. <br /><br /><br /><br />";s:4:"guid";s:70:"tag:blogger.com,1999:blog-7476449567742726187.post-1964660394383521041";s:7:"pubdate";s:29:"Wed, 07 May 2014 20:14:00 GMT";s:7:"summary";s:1294:""The database is growing at 2GB a minute.&nbsp; We're 40 minutes away from running out of disk space."<br /><br />"Sounds like I should probably take a look."<br /><br />I looked at the database size, which was 160GB.&nbsp; But the database SAN share was up to 1.4GB used out of 1.6GB.&nbsp; WTF?<br /><br />Then I looked at the filesystem and did directory sizes.&nbsp; pg_tmp was over a terabyte.&nbsp; Oooooooohhh.<br /><br />Apparently they'd accidentally pushed a new report to the application which worked OK in testing, but with certain parameters created a 15 billion item sort.&nbsp; And since it was slow, users called it several times.&nbsp; Ooops.<br /><br />Enter <span>temp_file_limit</span>, a parameter added by Mark Kirkwood to PostgreSQL 9.2. &nbsp; This is a limit on per-session usage of temporary files for sorts, hashes, and similar operations.&nbsp; If a user goes over the limit, their query gets cancelled and they see an error.<br /><br />This is an excellent way to prevent a single user, or a bad application change, from DOSing your database server.&nbsp; Set it to something high; I'm using 10GB or 20GB, or 10% of available disks space, whichever is less.&nbsp; But even a high limit like that will save you from some unexpected downtime. <br /><br /><br /><br />";}i:17;a:6:{s:5:"title";s:32:"Jim Mlodgenski: Trigger Overhead";s:4:"link";s:48:"http://www.openscg.com/2014/05/trigger-overhead/";s:11:"description";s:4345:"<p>I recently had discussions with some folks about triggers in PostgreSQL. They had two main questions.</p>
<ol>
<li>What is the overhead of putting a trigger on a table?</li>
<li>Should a trigger function be generic with IF statements to do different things for INSERT, UPDATE and DELETE?</li>
</ol>
<p>So I created a simple test to verify some assumptions.</p>
<p>First, I created a simple table and made it UNLOGGED. I didn&#8217;t want the overhead of the WAL to possibly dwarf the timings of the triggers.</p>
<pre>CREATE UNLOGGED TABLE trigger_test (
 key serial primary key, 
 value varchar, 
 insert_ts timestamp, 
 update_ts timestamp
);</pre>
<p>I then create two scripts to push through pgbench and get some timings.</p>
<p>INSERTS.pgbench</p>
<p><span style="font-family: Consolas, Monaco, monospace; font-size: 12px; line-height: 18px;">INSERT INTO trigger_test (value) VALUES (&#8216;hello&#8217;);</span><br />
UPDATES.pgbench</p>
<pre>\set keys :scale
\setrandom key 1 :keys
UPDATE trigger_test SET value = 'HELLO' WHERE key = :key;</pre>
<p>I ran these with the following pgbench commands:<br />
pgbench -n -t 100000 -f INSERTS.pgbench postgres<br />
pgbench -n -s 100000 -t 10000 -f UPDATES.pgbench postgres</p>
<p>The result is that I created 100,000 rows in the test table and then randomly updated 10,000 of them. I ran these commands several times with dropping and recreating the test table between each iteration and the average tps values I was seeing were:<br />
Inserts: 4510 tps<br />
Updates: 4349 tps<br />
Then to get the overhead of a trigger, I created a trigger function that just returns. I then repeated the process of running the pgbench commands as before.</p>
<pre>CREATE FUNCTION empty_trigger() RETURNS trigger AS $$
BEGIN
 RETURN NEW;
END;
$$ LANGUAGE plpgsql;
CREATE TRIGGER empty_trigger BEFORE INSERT OR UPDATE ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE empty_trigger();</pre>
<p>The result with the empty trigger were:<br />
Inserts: 4296 tps<br />
Updates: 3988 tps</p>
<p>That results in a 4.8% overhead for inserts and an 8.3% overhead for updates. I didn&#8217;t dig further as to why is appears that the overhead for a trigger on an update is almost twice as high as on an insert. I&#8217;ll leave that to a follow-up when I have some more time. A 4%-8% overhead of placing a trigger on a table will likely not be noticed in most real-world applications, the overhead of what is executed inside the trigger function can be noticed, which led to the next topic.<br />
I then wanted to see the overhead of having a single trigger function versus having separate trigger functions for inserts and updates.</p>
<p>For a single trigger function, I used the following:</p>
<pre>CREATE FUNCTION single_trigger() RETURNS trigger AS $$
BEGIN
 IF (TG_OP = 'INSERT') THEN
 NEW.insert_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
 ELSIF (TG_OP = 'UPDATE') THEN
 NEW.update_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
 END IF;
 RETURN NULL; 
END;
$$ LANGUAGE plpgsql;
CREATE TRIGGER single_trigger BEFORE INSERT OR UPDATE ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE single_trigger();</pre>
<p>And for separate trigger functions, I used:</p>
<pre>CREATE FUNCTION insert_trigger() RETURNS trigger AS $$
BEGIN
 NEW.insert_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
END;
$$ LANGUAGE plpgsql;
CREATE FUNCTION update_trigger() RETURNS trigger AS $$
BEGIN
 NEW.update_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
END;
$$ LANGUAGE plpgsql;
CREATE TRIGGER insert_trigger BEFORE INSERT ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE insert_trigger();
CREATE TRIGGER update_trigger BEFORE UPDATE ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE update_trigger();</pre>
<p>I then reran the same process as before to see the overhead.</p>
<p>Single Trigger Inserts: 3569 tps<br />
Single Trigger Updates: 3450 tps</p>
<p>Separate Triggers Inserts: 3623 tps<br />
Separate Triggers Updates: 3870 tps</p>
<p>It turns out that splitting the trigger function into separate functions does make a difference. For the insert trigger, keeping things as a single trigger only added 1.5% to the overhead, but for the update trigger, a single trigger function added nearly 11% overhead. The is most likely due to the update case being handle second in the trigger function. That&#8217;s another thing to dig into when there is time.</p>
<p>&nbsp;</p>";s:4:"guid";s:30:"http://www.openscg.com/?p=3392";s:7:"pubdate";s:29:"Wed, 07 May 2014 16:38:18 GMT";s:7:"summary";s:4345:"<p>I recently had discussions with some folks about triggers in PostgreSQL. They had two main questions.</p>
<ol>
<li>What is the overhead of putting a trigger on a table?</li>
<li>Should a trigger function be generic with IF statements to do different things for INSERT, UPDATE and DELETE?</li>
</ol>
<p>So I created a simple test to verify some assumptions.</p>
<p>First, I created a simple table and made it UNLOGGED. I didn&#8217;t want the overhead of the WAL to possibly dwarf the timings of the triggers.</p>
<pre>CREATE UNLOGGED TABLE trigger_test (
 key serial primary key, 
 value varchar, 
 insert_ts timestamp, 
 update_ts timestamp
);</pre>
<p>I then create two scripts to push through pgbench and get some timings.</p>
<p>INSERTS.pgbench</p>
<p><span style="font-family: Consolas, Monaco, monospace; font-size: 12px; line-height: 18px;">INSERT INTO trigger_test (value) VALUES (&#8216;hello&#8217;);</span><br />
UPDATES.pgbench</p>
<pre>\set keys :scale
\setrandom key 1 :keys
UPDATE trigger_test SET value = 'HELLO' WHERE key = :key;</pre>
<p>I ran these with the following pgbench commands:<br />
pgbench -n -t 100000 -f INSERTS.pgbench postgres<br />
pgbench -n -s 100000 -t 10000 -f UPDATES.pgbench postgres</p>
<p>The result is that I created 100,000 rows in the test table and then randomly updated 10,000 of them. I ran these commands several times with dropping and recreating the test table between each iteration and the average tps values I was seeing were:<br />
Inserts: 4510 tps<br />
Updates: 4349 tps<br />
Then to get the overhead of a trigger, I created a trigger function that just returns. I then repeated the process of running the pgbench commands as before.</p>
<pre>CREATE FUNCTION empty_trigger() RETURNS trigger AS $$
BEGIN
 RETURN NEW;
END;
$$ LANGUAGE plpgsql;
CREATE TRIGGER empty_trigger BEFORE INSERT OR UPDATE ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE empty_trigger();</pre>
<p>The result with the empty trigger were:<br />
Inserts: 4296 tps<br />
Updates: 3988 tps</p>
<p>That results in a 4.8% overhead for inserts and an 8.3% overhead for updates. I didn&#8217;t dig further as to why is appears that the overhead for a trigger on an update is almost twice as high as on an insert. I&#8217;ll leave that to a follow-up when I have some more time. A 4%-8% overhead of placing a trigger on a table will likely not be noticed in most real-world applications, the overhead of what is executed inside the trigger function can be noticed, which led to the next topic.<br />
I then wanted to see the overhead of having a single trigger function versus having separate trigger functions for inserts and updates.</p>
<p>For a single trigger function, I used the following:</p>
<pre>CREATE FUNCTION single_trigger() RETURNS trigger AS $$
BEGIN
 IF (TG_OP = 'INSERT') THEN
 NEW.insert_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
 ELSIF (TG_OP = 'UPDATE') THEN
 NEW.update_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
 END IF;
 RETURN NULL; 
END;
$$ LANGUAGE plpgsql;
CREATE TRIGGER single_trigger BEFORE INSERT OR UPDATE ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE single_trigger();</pre>
<p>And for separate trigger functions, I used:</p>
<pre>CREATE FUNCTION insert_trigger() RETURNS trigger AS $$
BEGIN
 NEW.insert_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
END;
$$ LANGUAGE plpgsql;
CREATE FUNCTION update_trigger() RETURNS trigger AS $$
BEGIN
 NEW.update_ts = CURRENT_TIMESTAMP;
 RETURN NEW;
END;
$$ LANGUAGE plpgsql;
CREATE TRIGGER insert_trigger BEFORE INSERT ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE insert_trigger();
CREATE TRIGGER update_trigger BEFORE UPDATE ON trigger_test
 FOR EACH ROW EXECUTE PROCEDURE update_trigger();</pre>
<p>I then reran the same process as before to see the overhead.</p>
<p>Single Trigger Inserts: 3569 tps<br />
Single Trigger Updates: 3450 tps</p>
<p>Separate Triggers Inserts: 3623 tps<br />
Separate Triggers Updates: 3870 tps</p>
<p>It turns out that splitting the trigger function into separate functions does make a difference. For the insert trigger, keeping things as a single trigger only added 1.5% to the overhead, but for the update trigger, a single trigger function added nearly 11% overhead. The is most likely due to the update case being handle second in the trigger function. That&#8217;s another thing to dig into when there is time.</p>
<p>&nbsp;</p>";}i:18;a:6:{s:5:"title";s:66:"Tim van der Linden: PostgreSQL: A full text search engine - Part 2";s:4:"link";s:64:"http://shisaa.jp/postset/postgresql-full-text-search-part-2.html";s:11:"description";s:52311:"<p>Welcome to the second installment of our look into full text search within PostgreSQL.</p>
<p>If this is the first time you heard about full text search I highly encourage you to go and read <a href="http://shisaa.jp/postset/postgresql-full-text-search-part-1.html" title="First chapter introducing the full text search capabilities of PostgreSQL.">the first chapter</a> in this series before continuing. This chapter builds on what we have seen previously.</p>
<h3>A look back</h3>
<p>In short, the previous chapter introduced the general concept of full text search, regardless of the software being used. It looked at how the idea of full text search was brought to computer software by breaking it up into roughly three steps: <em>case removal</em>, <em>stop word removal</em>, normalizing with <em>synonyms</em> and <em>stemming</em>. </p>
<p>Next we delved into PostgreSQL's implementation and introduced the <em>tsvector</em> and the <em>tsquery</em> as two new data types together with a handful of new functions such as <em>to_tsvector()</em>, <em>to_tsquery()</em> and <em>plainto_tsquery()</em>, which all extend PostgreSQL to support full text search. </p>
<p>We saw how we could feed PostgreSQL a string of text which would then get <em>parsed</em> into <em>tokens</em> and processed even further into <em>lexemes</em> which in turn got <em>stored</em> into a <em>tsvector</em>. We then queried that <em>tsvector</em> using the <em>tsquery</em> data type and the <em>@@</em> matching operator.</p>
<p>In this chapter, I want to flesh out an important topic we touched on in previously: PostgreSQL's full text search <em>configurations</em>. </p>
<h3>Precaution</h3>
<p>Let me be very clear, in <em>most</em> cases the configurations shipped with PostgreSQL will suffice and you do not need to touch them at all, in which case this chapter could be considered a waste of time.</p>
<p><em>However</em>, I highly encourage you to read through this chapter and, as always, actually <em>run the queries</em> with me. You need to know how the tools you use work under the hood.</p>
<p>To be even more bold, someday you might even need to get your hands dirty and actually <em>build</em> you own configuration. Why? Because a customer wanting full text search for their application might have specific requirements, or even deliver you specific dictionaries to use in the parsing stage. Such use cases may arise in very specific areas of conduct where much official, technical lingo is used which is not covered in a general dictionary.</p>
<p>So, put on your favorite pants (or none if you like that better), turn down the lights, pull the computer close to you, open up a terminal window, put on some eery music and let us get started.</p>
<h3>Configuring PostgreSQL full text search</h3>
<p>In the last chapter we saw that PostgreSQL uses a couple of tools like a <em>stop word list</em> and <em>dictionaries</em> to perform its parsing. We also saw that we did not need to tell PostgreSQL about which of these tools to use. It turned out that full text search comes with a set of default configurations for several languages. We also found out that, if no configuration was given, the database assumes that the document or string to be parsed is English and uses a configuration called <em>'english'</em>. </p>
<p>Beware of localized packages of PostgreSQL though. As I noted in the previous chapter, there is a small possibility that the default configuration in your PostgreSQL installation is <em>not</em> set to 'english'.
If this is the case with your setup, be sure to include the 'english' configuration if not stated otherwise or <em>change</em> it to be 'english'. We will see how to do that in a minute.</p>
<p>Taking the small database we created last time, the syntax to feed a configuration set to PostgreSQL during parsing was the following:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>The string '<em>english</em>' represents the <em>name</em> of the configuration which we would like to use. As you know by now, this string can be omitted which will make the database use the default configuration. PostgreSQL knows this default because it is set in the general <em>postgresql.conf</em> configuration file. In that file you will find a variable called <em>default_text_search_config</em> which, in most cases, is set to <em>pg_catalog.english</em>. If you wish to have a own, custom configuration to be the default, that is the place to set it.</p>
<p>Before hacking away at your own configuration, it may be of interest to see what PostgreSQL has to offer. To see which shipped configuration files are available to you, use the <em>describe</em> command (\d) together with the full text flag (F):</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span>
</pre></div>


<p>This will <em>describe</em> the objects in the database that represent full text configurations. You see that by default you have quite a lot of language support. To see a different configuration in action, let us do a quick, fun test. </p>
<p>First, take the dutch string "Een blauwe olifant springt al dartelend over de kreupele dolfijn.", which is a rough translation of the "The blue elephant jumps over the crippled dolphin." example from the first chapter. If we would feed this to PostgreSQL, using the default (english) configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span> <span class="s1">'Een blauwe olifant springt al dartelend over de kreupele dolfijn'</span><span class="p">);</span>
</pre></div>


<p>We would get back:</p>
<div class="code"><pre> <span class="s1">'al'</span><span class="p">:</span><span class="mi">5</span> <span class="s1">'blauw'</span><span class="p">:</span><span class="mi">2</span> <span class="s1">'dartelend'</span><span class="p">:</span><span class="mi">6</span> <span class="s1">'de'</span><span class="p">:</span><span class="mi">8</span> <span class="s1">'dolfijn'</span><span class="p">:</span><span class="mi">10</span> <span class="s1">'een'</span><span class="p">:</span><span class="mi">1</span> <span class="s1">'kreupel'</span><span class="p">:</span><span class="mi">9</span> <span class="s1">'olif'</span><span class="p">:</span><span class="mi">3</span> <span class="s1">'springt'</span><span class="p">:</span><span class="mi">4</span>
</pre></div>


<p>It attempted to guess some words as you can see from the lexeme 'olif', but, to a dutch reader, this is <em>not</em> stemmed correctly. Neither are the stop words removed: 'de' and 'een' are articles which, in dutch, are considered of no value in a text search context. So let us try this again with the built-in <em>dutch</em> configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'dutch'</span><span class="p">,</span> <span class="s1">'Een blauwe olifant springt al dartelend over de kreupele dolfijn'</span><span class="p">);</span>
</pre></div>


<p>And we get:</p>
<div class="code"><pre> <span class="s1">'blauw'</span><span class="p">:</span><span class="mi">2</span> <span class="s1">'dartel'</span><span class="p">:</span><span class="mi">6</span> <span class="s1">'dolfijn'</span><span class="p">:</span><span class="mi">10</span> <span class="s1">'kreupel'</span><span class="p">:</span><span class="mi">9</span> <span class="s1">'olifant'</span><span class="p">:</span><span class="mi">3</span> <span class="s1">'springt'</span><span class="p">:</span><span class="mi">4</span>
</pre></div>


<p>Aha! That is much shorter then the previous result, and it is also more correct. As you can see, the words 'de' and 'een' are now removed and the stemming is done correctly on 'dartel', 'olifant' and 'kreupel'.
The target of this series, however, is not to show you the dutch language (for it will make you weep...), but you see the effect a different configuration set can have. </p>
<p>But what is such a configuration set made of? To answer that, we can simply use the same describe, but ask for more detailed information with the <em>+</em> flag:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span>
</pre></div>


<p>This will return a list of <em>all</em> the configurations and their details, so let us filter that and look at only the english version:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">english</span>
</pre></div>


<p>The following result will be returned:</p>
<div class="code"><pre> asciihword      | english_stem
 asciiword       | english_stem
 email           | simple
 file            | simple
 float           | simple
 host            | simple
 hword           | english_stem
 hword_asciipart | english_stem
 hword_numpart   | simple
 hword_part      | english_stem
 int             | simple
 numhword        | simple
 numword         | simple
 sfloat          | simple
 uint            | simple
 url             | simple
 url_path        | simple
 version         | simple
 word            | english_stem
</pre></div>


<p>All of these are <em>token categories</em> that target the different groups of words that the PostgreSQL full text parser recognizes.
 For each category there are one or more dictionaries defined which will receive the token and try to return a lexeme.
 We also call this overview a configuration map, for it maps a category to one or more dictionaries.</p>
<p>If the parser encounters a URL, for example, it will categorize it as a <em>url</em> or <em>url_path</em> token and as a result, PostgreSQL will consult the dictionaries <em>mapped</em> to this category to try and create a single lexeme containing a URL pointing to the same path. Example:</p>
<ul>
<li>example.com</li>
<li>example.com/index.html</li>
<li>example.com/foo/../index.html</li>
</ul>
<p>The URLs all result in the same document being served, so it makes sense to only save one variant as a lexeme in the resulting vector.
The same kind of <em>normalization</em> is done for file paths, version numbers, host names, units of measure, ... . A lot more then normal, English words.</p>
<p>There are 23 categories in total that the parser can recognize, ones not included here, for example, are <em>tag</em> for XML tags, <em>blank</em> for whitespace or punctuation, etc.</p>
<p>To see a description of the different token categories supported, use the 'p' flag together with '+' for more information:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dFp</span><span class="o">+</span>
</pre></div>


<p>When parsing, the chain of command goes as follows:</p>
<ul>
<li>A string is fed to PostgreSQL's full text</li>
<li>The parser crawls over the string and chops it into tokens of a certain type</li>
<li>For each token category a list of dictionaries (or a single dictionary) is consulted</li>
<li>If a dictionary list is used, the dictionaries are (generally) ordered from most precise (narrow) to most generic (wide)</li>
<li>As soon as a dictionary returns a lexeme (single or in the form of an array), the flow for that token stops</li>
<li>If no lexeme is proposed (a dictionary returns <em>NULL</em>) the token is given to the next dictionary in line or if a stop word list returns a match (returns <em>empty array</em>), the token is discarded</li>
</ul>
<h3>Dictionary templates and dictionaries</h3>
<p>In the list of token categories that where supported by the built-in 'english' configuration, you will find that only two <em>dictionaries</em> are used: <em>simple</em> and <em>english_stem</em>, which in turn come from the <em>simple</em> and <em>snowball</em> dictionary <em>templates</em> respectively.</p>
<p>So, what exactly is the difference between a <em>dictionary template</em> and a <em>dictionary</em>?</p>
<p>A <em>dictionary template</em> is the skeleton (hence template) of a dictionary. It defines the actual <em>C</em> functions that will do the heavy lifting.
A <em>dictionary</em> is an instantiation of that template - providing it with data to work with.</p>
<p>Let me try to clear any confusion on this. </p>
<p>Take, for example, the <em>simple</em> dictionary <em>template</em>. It does two things: it first checks a token against a <em>stop word</em> list. If it finds a match it returns an <em>empty array</em>, which will result in the token being discarded. If no match is found in the stop word list, the process will return the same token, but with <em>casing removed</em>.</p>
<p>All the checking and case removing is done by functions, under the hood. The stop word file, however, is something that the <em>dictionary</em> (the instantiation) provides.
The instantiation of the <em>simple</em> dictionary template, thus the <em>dictionary</em> itself, would be defined as follows:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="k">simple</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="k">simple</span><span class="p">,</span>
    <span class="n">stopwords</span> <span class="o">=</span> <span class="n">english</span>
<span class="p">);</span>
</pre></div>


<p>No need to run this SQL for PostgreSQL already comes shipped with the <em>simple</em> dictionary, but I wish to show you how you <em>could</em> create it.</p>
<p>First, you will see that we <em>have</em> to define the template, thus telling PostgreSQL which set of functions to use.
Next we feed it the data it is expecting, in case of <em>simple</em> it only expects a stop word list.</p>
<p>The reason for this separation is a safe guard one. Only a database user with <em>super user</em> privileges can write the actual template, because this template will contain functions that, if written incorrectly, could slow down or crash the database. You need someone who knows what they are doing and not your local script kiddy who has normal user access to your part of the database.</p>
<p>Notice that we only give the stopwords attribute the word <em>english</em> instead of a full file path.
This is because PostgreSQL has set a few standards in place for all dictionary types we will see in this chapter.</p>
<p>First, in case of a stop word list, the file <em>must</em> have the <em>.stop</em> extension.</p>
<p>Next, you can provide a full path to the file, anywhere on your system. 
However, if you do not provide a full path, PostgreSQL will search for it inside a directory called <em>tsearch_data</em> within PostgreSQL's portion of your system's user <em>shared</em> directory.</p>
<p>On a Debian system (using PostgreSQL 9.3) the path to this directory reads: "/usr/share/postgresql/9.3/tsearch_data".</p>
<p>A dictionary like the <em>simple</em> dictionary is one that is most of the time put at the beginning of a <em>dictionary list</em> to remove all the stop words before other dictionaries are being consulted. However, in all the cases where we see <em>simple</em> in the dictionary column of the token type list above, only this dictionary is used, meaning that only stop words are removed and all else is stripped of casing.</p>
<h3>Creating the "simple" dictionary</h3>
<p>Say that we wanted to setup our own <em>simple</em> dictionary based on the <em>simple</em> dictionary template, but feed it our own list of stop words. Before setting up this new dictionary, we would first have to write a stop word file. </p>
<p>Luckily for us, this is trivial. A stop word file is nothing more then a plain text file with one word on each line. Empty lines and trailing whitespace are ignored. We would then have to save this file with the <em>.stop</em> extension. Let us try just that.</p>
<p>Open up your editor and punch in the words "dolphin" and "the", both on their own line. Write the file out as "shisaa_stop.stop", preferably in PostgreSQL's shared directory.</p>
<p>Next we need to setup our dictionary. Connect to the "phrases" database from chapter one and run the following SQL:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_simple</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="k">simple</span><span class="p">,</span>
    <span class="n">stopwords</span> <span class="o">=</span> <span class="n">shisaa_stop</span>
<span class="p">);</span>
</pre></div>


<h3>Setting up a configuration</h3>
<p>Now, the dictionary by itself is not very helpful. As we have seen before, we need to map it to token categories before we can actually use it for parsing.
This means that we need to make our own configuration.</p>
<p>Let us setup an empty configuration (not based on an existing one like 'english'):</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span> <span class="p">(</span><span class="n">parser</span><span class="o">=</span><span class="s1">'default'</span><span class="p">);</span>
</pre></div>


<p>This statement will create a new configuration for us which is completely empty, it has no mappings. The argument we have to give here can be either <em>parser</em> or <em>copy</em>. With parser you define which parser to use and it will create an empty configuration. PostgreSQL has only one parser by default which is named...<em>default</em>. If you choose <em>copy</em> then you will have to provide an <em>existing</em> configuration name (like english) from which you would like to make a copy.</p>
<p>To verify that the configuration is empty, run our describe on it:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">shisaa</span>
</pre></div>


<p>And marvel at its emptiness.</p>
<p>Now, let us add the <em>shisaa_simple</em> dictionary we created before:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">;</span>
</pre></div>


<p>As you will see throughout this (and the next) chapter, full text extends not only the data types and functions we have available, but also extends PostgreSQL's SQL syntax with a handful of new statements.
I need to note that all of these statements are <em>not</em> SQL standard (for SQL has no full text standard) and thus cannot be easily ported to a different database.
But then again...what is this folly...who would even need a different database!</p>
<p>The new statements introduced here (and in the previous SQL blocks) are:</p>
<ul>
<li><em>CREATE TEXT SEARCH DICTIONARY</em></li>
<li><em>CREATE TEXT SEARCH CONFIGURATION</em></li>
<li><em>ALTER TEXT SEARCH CONFIGURATION</em></li>
</ul>
<p>Just remember that these are not part of the SQL standard (something which PostgreSQL holds very dear, in high contrast with many other database).</p>
<p>Did it work? Well, describe it:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">shisaa</span>
</pre></div>


<p>And get back:</p>
<div class="code"><pre> asciihword      | shisaa_simple
 asciiword       | shisaa_simple
 hword           | shisaa_simple
 hword_asciipart | shisaa_simple
 hword_part      | shisaa_simple
 word            | shisaa_simple
</pre></div>


<p>Perfect!</p>
<p>Here we mapped our fresh dictionary to the token groups "asciihword", "asciiword", "hword", "hword_asciipart", "hword_part", "word", because these will target most of a normal, English sentence.</p>
<p>It is time to try out this new search configuration! Punch in the same on-the-fly SQL as we had in the previous chapter, but this time with <em>our own</em> configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>And we get back:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippled'</span>:8 <span class="s1">'elephant'</span>:4 <span class="s1">'jumped'</span>:5 <span class="s1">'over'</span>:6
</pre></div>


<p>Ha! All squeaky flippers unite! The word <em>dolphin</em> is <em>removed</em>, because we defined it to be a stop word. A world as it should be.</p>
<p>We now have a basic full text configuration with a <em>simple</em> dictionary. To have a more real world full text search we will need more then just this dictionary though, we will at least need to take care of stemming.</p>
<h3>Extending the configuration: stemming with the Snowball</h3>
<p>Stemming, the process of reducing words to their basic form, is done by a special, dedicated kind of dictionary, the <em>Snowball</em> dictionary. </p>
<p>What?</p>
<p><em>Snowball</em> is a <em>very proven</em> string processing language specially designed for stemming purposes and supports a wide range of languages. It originated from the <em>Porter stemming algorithm</em> and uses a natural syntax to define stemming rules. </p>
<p>And luckily for us, PostgreSQL has a <em>Snowball</em> dictionary template ready to use. This template has the Snowball stemming rules embedded for a wide variety of languages. Let us create a <em>dictionary</em> for our shisaa configuration, shall we?</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_snowball</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="n">snowball</span><span class="p">,</span>
    <span class="k">language</span> <span class="o">=</span> <span class="n">english</span>
<span class="p">);</span>
</pre></div>


<p>Again, very easy to setup. The snowball dictionary <em>template</em> accepts two variables to be setup. The first, mandatory one is the language you wish to support. Without this, the template does not know which of the Snowball stemming rules to take.</p>
<p>The next, optional one is, again, a stop word list. But...why can we feed this dictionary a stop word list? Did we not already do that with the <em>simple</em> dictionary?</p>
<p>That is correct, we did setup the <em>simple</em> dictionary to remove stop words for us, but we are not required to use the <em>simple</em> and the <em>snowball</em> dictionary in tandem.
It is perfectly possible to <em>map</em> only the <em>snowball</em> dictionary for various token categories and ignore all other dictionaries.
If you would not tell the <em>snowball</em> dictionary to remove stop words, it could become messy for the Snowball stemmer will try and stem <em>all</em> words it finds.</p>
<p>This stop word list can be the exact same list we fed the <em>simple</em> dictionary.</p>
<p>Also, because a <em>snowball</em> dictionary will try and parse <em>all</em> the tokens it is being fed, it is consider to be a <em>wide</em> dictionary. Therefor, as we have seen earlier, it is a good idea when chaining dictionaries together to put this dictionary at the end of your chain.</p>
<p>We now have our own version of the <em>snowball</em> dictionary and need to extend our configuration and map this dictionary to the desired token categories:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Notice that in the <em>WITH</em> clause we are now chaining the <em>simple</em> and the <em>snowball</em> dictionary together. The order is, of course, important.
Describe our configuration once more:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">shisaa</span>
</pre></div>


<p>And get back:</p>
<div class="code"><pre>asciihword      | shisaa_simple,shisaa_snowball
asciiword       | shisaa_simple,shisaa_snowball
hword           | shisaa_simple,shisaa_snowball
hword_asciipart | shisaa_simple,shisaa_snowball
hword_part      | shisaa_simple,shisaa_snowball
word            | shisaa_simple,shisaa_snowball
</pre></div>


<p>Perfect, now the <em>simple</em> dictionary will be consulted first followed by the <em>snowball</em> dictionary.</p>
<p>Note that throughout this chapter I will chain together dictionaries in order. This will <em>not</em> always be the most smart or desired order, just an order to demonstrate <em>how</em> you can chain dictionaries.</p>
<p>To the test, throw a new query at it:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>And get back:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippled'</span>:8 <span class="s1">'elephant'</span>:4 <span class="s1">'jumped'</span>:5 <span class="s1">'over'</span>:6
</pre></div>


<p>Nice, that is very...oh wait. Something is not correct. I am getting back <em>exactly</em> the same result as before. The words "crippled" and "elephant" are not stemmed at all. Why?</p>
<p>Well, the <em>simple</em> dictionary, as we defined it earlier, is setup to be a bit greedy. In its current state it will return an unmatched token as a lexeme with casing removed.
It does not return <em>NULL</em>. And, as you know by now, <em>NULL</em> is needed to give other dictionaries a chance to examine the token.</p>
<p>So, we need to alter the <em>simple</em> dictionary's behavior. For this, we can use the <em>ALTER</em> syntax provided to us. And as it turns out, the <em>simple</em> dictionary <em>template</em> can accept one more variable: the <em>accept</em> variable. If this is set to false, then it will return <em>NULL</em> for every unmatched token. Let us alter that dictionary:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_simple</span> <span class="p">(</span> <span class="n">accept</span> <span class="o">=</span> <span class="k">false</span> <span class="p">);</span>
</pre></div>


<p>Run the ts_vector query again, and look at the results:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippl'</span>:8 <span class="s1">'eleph'</span>:4 <span class="s1">'jump'</span>:5 <span class="s1">'over'</span>:6
</pre></div>


<p>That is what we were looking for, nicely stemmed results!</p>
<h3>Extending the configuration: fun with synonyms</h3>
<p>By now we have seen the first and the last dictionary in our control chain, but at least one more important part is missing: synonyms are not removed.</p>
<p>Let us extend our favorite sentence and add a few synonyms to it: "The big blue elephant, joined by its enormous blue mammoth friend, jumped over the crippled blue dolphin while smiling at the orca."</p>
<p>Still perfectly possible.</p>
<p>In the light of (cue dark en deep Batman voice) "science" (end Batman voice), let us first see what we get when we run it through our current configuration:</p>
<div class="code"><pre><span class="s1">'at'</span>:20 <span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9,16 <span class="s1">'by'</span>:6 <span class="s1">'crippl'</span>:15 <span class="s1">'eleph'</span>:4 <span class="s1">'enorm'</span>:8 <span class="s1">'friend'</span>:11 <span class="s1">'it'</span>:7 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:10 <span class="s1">'orca'</span>:22 <span class="s1">'over'</span>:13 <span class="s1">'smile'</span>:19 <span class="s1">'while'</span>:18
</pre></div>


<p>That is one big result set. Maybe we should cut the blue dolphin a little bit of slack and feed a real stop word list to our <em>simple</em> dictionary before continuing by altering our <em>dictionary</em>:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_simple</span> <span class="p">(</span> <span class="n">stopwords</span> <span class="o">=</span> <span class="n">english</span> <span class="p">);</span>
</pre></div>


<p>As you see you can simply use the same <em>ALTER</em> syntax as before. The "english" here refers to the shipped "english.stop" stop word list.</p>
<p>Querying again, we will get back a better, short list (including our Dolphin friend):</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9,16 <span class="s1">'crippl'</span>:15 <span class="s1">'dolphin'</span>:17 <span class="s1">'eleph'</span>:4 <span class="s1">'enorm'</span>:8 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:10 <span class="s1">'orca'</span>:22 <span class="s1">'smile'</span>:19
</pre></div>


<p>Now we would like to reduce this result even further by compacting synonyms into one lexeme.</p>
<p>Enter the <em>synonym</em> dictionary <em>template</em>.</p>
<p>This template requires you to have a so-called "synonym" file; A file containing lists of words with the same meaning. For the sake of learning, let us create our own synonym file. This file has to end with the <em>.syn</em> extension.</p>
<p>Open up your editor again and write out a file called "shisaa_syn.syn" with the following contents:</p>
<div class="code"><pre>big enormous
elephant mammoth
dolphin orca
</pre></div>


<p>And let us setup the <em>dictionary</em>:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_synonym</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="n">synonym</span><span class="p">,</span>
    <span class="n">synonyms</span> <span class="o">=</span> <span class="n">shisaa_syn</span>
<span class="p">);</span>
</pre></div>


<p>And add the mapping for it:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_synonym</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Okay, time to test our big string again and see the results:</p>
<div class="code"><pre><span class="s1">'blue'</span>:3,9,16 <span class="s1">'crippl'</span>:15 <span class="s1">'enorm'</span>:8 <span class="s1">'enormous'</span>:2 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:4,10 <span class="s1">'orca'</span>:17,22 <span class="s1">'smile'</span>:19
</pre></div>


<p>Very neat. The words "elephant", "big" and "dolphin" are now removed and only their synonyms are kept.
Also notice that both "mammoth" and "orca" have two pointers each, one for every synonym.</p>
<p>But look at the words 'enorm' and 'enormous', why is this happening?</p>
<p>If you look at the pointers, you see that <em>enormous</em> points to the second word in the string, being <em>big</em>, while <em>enorm</em> points to the original <em>enormous</em> word.
The reason why this is happening is because our <em>synonym</em> dictionary has priority over our <em>snowball</em> one. The <em>synonym</em> dictionary emits a lexeme as a synonym for <em>big</em>, being <em>enormous</em>, simply because we told it to do so in our <em>synonym file</em>. Now, because it emits a lexeme, the original token, <em>big</em>, is not available anymore for the rest of the dictionary chain.</p>
<p>The token <em>enormous</em> itself has <em>no</em> synonym because we did not define it in our synonym file. It is ignored by the <em>synonym</em> dictionary and passed over to the <em>snowball</em> dictionary which then stems the token into a lexeme resulting in <em>enorm</em>.</p>
<p>If you wish to prevent this from happening, you could add a self pointing line to your synonym list:</p>
<div class="code"><pre>enormous enormous
</pre></div>


<p>Now load in the file on disk to pull the changes into PostgreSQL:</p>
<div class="code"><pre> <span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_synonym</span> <span class="p">(</span><span class="n">synonyms</span><span class="o">=</span><span class="n">shisaa_syn</span><span class="p">);</span>
</pre></div>


<p>And run the query again, the result should now read:</p>
<div class="code"><pre><span class="s1">'blue'</span>:3,9,16 <span class="s1">'crippl'</span>:15 <span class="s1">'enormous'</span>:2,8 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:4,10 <span class="s1">'orca'</span>:17,22 <span class="s1">'smile'</span>:19
</pre></div>


<p>Now <em>enorm</em> will be removed and both <em>big</em> and <em>enormous</em> are cast to the same lexeme. </p>
<p>PostgreSQL does not ship a synonym list, so you will have to compile your own just like we did above but hopefully a little bit more useful</p>
<h3>Extending the configuration: phrasing with a Thesaurus</h3>
<p>Next up is the <em>thesaurus</em> dictionary, which is quite close to the <em>synonym</em> dictionary, with one exception: <em>phrases</em>.</p>
<p>A <em>thesaurus</em> dictionary is used to recognize phrases and convert them into lexemes with the same meaning. Again, this dictionary relies on a file containing the phrase conversions.
This time, the file has the <em>.ths</em> extension. </p>
<p>Open up your editor and write out a file called "shisaa_thesaurus.ths" with the following contents:</p>
<div class="code"><pre>big blue elephant : PostgreSQL
crippled blue dolphin : MySQL
</pre></div>


<p>Before we can create the dictionary, there is one more required variable we have to set, the <em>subdictionary</em> the <em>thesaurus</em> dictionary can use.
This subdictionary will be <em>another</em> dictionary you have defined before. Most of the time a stemmer is fed to this variable to let the thesaurus stem the input before comparing it with its thesaurus file.</p>
<p>So let us feed it our <em>snowball</em> dictionary and set it up:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_thesaurus</span> <span class="p">(</span>
    <span class="k">TEMPLATE</span> <span class="o">=</span> <span class="n">thesaurus</span><span class="p">,</span>
    <span class="n">DICTFILE</span> <span class="o">=</span> <span class="n">shisaa_thesaurus</span><span class="p">,</span>
    <span class="k">DICTIONARY</span> <span class="o">=</span> <span class="n">shisaa_snowball</span>
<span class="p">);</span>
</pre></div>


<p>Map it:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_thesaurus</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Notice that I took out the <em>synonym</em> dictionary. If we chain up to many dictionaries, the results might turn out to be undesirable in our demonstration use case.</p>
<p>Querying will result in the following tsvector:</p>
<div class="code"><pre><span class="s1">'blue'</span>:7 <span class="s1">'enorm'</span>:6 <span class="s1">'friend'</span>:9 <span class="s1">'join'</span>:3 <span class="s1">'jump'</span>:10 <span class="s1">'mammoth'</span>:8 <span class="s1">'mysql'</span>:13 <span class="s1">'orca'</span>:18 <span class="s1">'postgresql'</span>:2 <span class="s1">'smile'</span>:15
</pre></div>


<p>That is quite awesome, it now recognizes "big blue elephant" as PostgreSQL and "crippled blue dolphin" as MySQL. We have created a <em>pun-aware</em> full text search configuration!</p>
<p>As you can see,  both the "MySQL" and "PostgreSQL" lexemes have <em>one</em> pointer each, pointing to the first word of the substring that got converted.</p>
<h3>Extending the configuration a last time: morphing with Ispell</h3>
<p>Okay, we are almost at the end of the dictionary <em>templates</em> that PostgreSQL supports.</p>
<p>This last one is a fun one too. Many Unix and Linux systems come shipped with a spell checker called <em>Ispell</em> or with the more modern variant called <em>HunSpell</em>.
Besides your average spell checking, these dictionaries are very good at morphological lookups, meaning that they can link all different writing structures of words together.</p>
<p>A synonym or thesaurus dictionary would not catch these, unless explicitly set with a huge amount of lines in the <em>.syn</em> or <em>.ths</em> files, which is error prone and inelegant. 
The Ispell or Hunspell dictionaries <em>will</em> capture these and try to make them into one lexeme.</p>
<p>Before setting up the <em>dictionary</em>, we first need to make sure that we have the Ispell or Hunspell dictionary files for the language we wish to support.
Normally you would want to download these files from the official OpenOffice page. These pages, however, seem to be confusing and the correct files very hard to find. I have found <a href="http://fmg-www.cs.ucla.edu/geoff/ispell-dictionaries.html" title="OpenOffice Extension page.">the following page</a> of great help to get the files you need for your desired language
.
Download the files for your desired language and place the <em>.dict</em> and the <em>.affix</em> files into the PostgreSQL shared directory.</p>
<p>For now, let us just take the basic <em>english</em> dict and affix files (named both <em>en_us</em> and already shipped with PostgreSQL) and feed them to the configuration:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_ispell</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="n">ispell</span><span class="p">,</span>
    <span class="n">DictFile</span> <span class="o">=</span> <span class="n">en_us</span><span class="p">,</span>
    <span class="n">AffFile</span> <span class="o">=</span> <span class="n">en_us</span><span class="p">,</span>
    <span class="n">StopWords</span> <span class="o">=</span> <span class="n">english</span>
<span class="p">);</span>
</pre></div>


<p>And chain it:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_ispell</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Notice again I took out the <em>thesaurus</em> dictionary, not to pile up too many dictionaries at once.</p>
<p>Query it once more, and look at what we get back:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9,16 <span class="s1">'cripple'</span>:15 <span class="s1">'dolphin'</span>:17 <span class="s1">'elephant'</span>:4 <span class="s1">'enormous'</span>:8 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'joined'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:10 <span class="s1">'orca'</span>:22 <span class="s1">'smile'</span>:19 <span class="s1">'smiling'</span>:19
</pre></div>


<p>Hmm, interesting. Notice that we now got <em>more</em> lexemes than before, <em>smile</em> and <em>smiling</em> for example, and <em>join</em> and <em>joined</em>. Also, both these cases have the <em>same</em> pointer. Why is that?</p>
<p>What is happening here is a feature of the Ispell dictionary called <em>morphology</em>, or as we seen above, <em>morphological lookups</em>.
One of the reasons why Ispell is such a powerful dictionary is because it can recognize and act upon the <em>structure</em> of a word. </p>
<p>In our case, Ispell recognizes <em>joined</em> (or <em>smiling</em>) and emits an array of <em>two</em> lexemes, the original token converted to a lexeme <em>and</em> the stemmed version of the token.</p>
<p>This concludes all the dictionaries that PostgreSQL ships with by default and the ones you will most likely ever need. What is next?</p>
<h3>Debugging</h3>
<p>Now that you have a good understanding of how to build your own configuration and setup your own dictionaries, I would like to introduce a few new functions that can come in handy when your configuration would produce seemingly strange results.</p>
<h4>ts_debug()</h4>
<p>The first function I want show you is a <em>very</em> handy one that is built to test your <em>whole</em> full text configuration. It helps you keep your mental condition to just mildly insane, so to speak.</p>
<p>The function <em>ts_debug()</em> accepts a configuration and a string of text you wish to test. As a result you will get back a set that contains an overview of how the parser chopped your string into tokens,  which category it picked for each token, which dictionary was consulted and which lexeme(s) where emitted. Oh boy, this is too much fun, let us just try it out! </p>
<p>Feed our original pun string and let us test the current <em>shisaa</em> configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_debug</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>Hmm, that may not be very readable, rather use the wildcard selector and a FROM clause to include column names into our result set (one of the few times you may use this selector without getting smacked):</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">ts_debug</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>Which will result in the following, huge set:</p>
<div class="code"><pre>  <span class="nb">alias</span>   |   description   |  token   |                 dictionaries                  |  dictionary   |  lexemes   
-----------+-----------------+----------+-----------------------------------------------+---------------+------------
asciiword | Word, all ASCII | The      | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_simple | <span class="o">{}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | big      | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>big<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | blue     | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>blue<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | elephant | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>elephant<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | jumped   | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>jump<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | over     | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_simple | <span class="o">{}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | the      | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_simple | <span class="o">{}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | crippled | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>cripple<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | blue     | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>blue<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | dolphin  | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>dolphin<span class="o">}</span>
blank     | Space symbols   | .        | <span class="o">{}</span>                                            |               |
</pre></div>


<p>You now have a complete overview of the flow from string to vector of lexemes. Let me go over some interesting facts of this result set.</p>
<p>First, notice how the tokens <em>the</em> and <em>over</em> got removed by the <em>simple</em> dictionary. They where a hit in the stop word list, so the dictionary returned an <em>empty array</em>.</p>
<p>Next you see the alias <em>blank</em> between each <em>asciiword</em>. <em>Blank</em> is a category used for spaces or punctuation. A <em>space</em> and a <em>.</em> (full stop) is considered a token, but is stripped out by the parser itself for it has no value in this context.</p>
<p>And last, see that our <em>snowball</em> dictionary was never consulted. This means that, in this string, the <em>shisaa_ispell</em> gobbled all the lexemes that <em>shisaa_simple</em> threw at it.</p>
<h4>ts_lexize()</h4>
<p>The second function is <em>ts_lexize()</em>. This little helper lets you test different <em>parts</em> of your whole setup. Take the unexpected result of our last dictionary, where we got back multiple lexemes. As it turned out it is normal behavior, but you may want to verify that the result is coming from the dictionary and not from a side effect of how you chained your dictionaries together.</p>
<p>To test our single, <em>shisaa_ispell</em> dictionary, we could feed it to this new function, together with <em>one token</em> we wish to test:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_lexize</span><span class="p">(</span><span class="s1">'shisaa_ispell'</span><span class="p">,</span><span class="s1">'joined'</span><span class="p">);</span>
</pre></div>


<p>This will return:</p>
<div class="code"><pre><span class="o">{</span>joined,join<span class="o">}</span>
</pre></div>


<p>Same as we had before, but now we know, for sure, that it is a feature of our Ispell dictionary. 
Notice that I stressed the fact that you can only feed this function <em>one token</em>, not a string of text and not multiple tokens.</p>
<p>You can use this function to test all your dictionaries individually, one token at a time.</p>
<p>Phew, that was a lot to take in for we covered a lot of ground here today. You can turn the lights back high and go get some fresh air.
In the next chapter, I will round up this introduction by introducing you to the following, new material:</p>
<ul>
<li>Ranking search results</li>
<li>Highlighting word inside search results</li>
<li>Creating special, full text search indexes</li>
<li>Setting up update triggers for ts_vector records</li>
</ul>
<p>And as always...thanks for reading!</p>
<!--  LocalWords:  instantiation PostgreSQL
 -->";s:4:"guid";s:64:"http://shisaa.jp/postset/postgresql-full-text-search-part-2.html";s:7:"pubdate";s:29:"Wed, 07 May 2014 13:00:00 GMT";s:7:"summary";s:52311:"<p>Welcome to the second installment of our look into full text search within PostgreSQL.</p>
<p>If this is the first time you heard about full text search I highly encourage you to go and read <a href="http://shisaa.jp/postset/postgresql-full-text-search-part-1.html" title="First chapter introducing the full text search capabilities of PostgreSQL.">the first chapter</a> in this series before continuing. This chapter builds on what we have seen previously.</p>
<h3>A look back</h3>
<p>In short, the previous chapter introduced the general concept of full text search, regardless of the software being used. It looked at how the idea of full text search was brought to computer software by breaking it up into roughly three steps: <em>case removal</em>, <em>stop word removal</em>, normalizing with <em>synonyms</em> and <em>stemming</em>. </p>
<p>Next we delved into PostgreSQL's implementation and introduced the <em>tsvector</em> and the <em>tsquery</em> as two new data types together with a handful of new functions such as <em>to_tsvector()</em>, <em>to_tsquery()</em> and <em>plainto_tsquery()</em>, which all extend PostgreSQL to support full text search. </p>
<p>We saw how we could feed PostgreSQL a string of text which would then get <em>parsed</em> into <em>tokens</em> and processed even further into <em>lexemes</em> which in turn got <em>stored</em> into a <em>tsvector</em>. We then queried that <em>tsvector</em> using the <em>tsquery</em> data type and the <em>@@</em> matching operator.</p>
<p>In this chapter, I want to flesh out an important topic we touched on in previously: PostgreSQL's full text search <em>configurations</em>. </p>
<h3>Precaution</h3>
<p>Let me be very clear, in <em>most</em> cases the configurations shipped with PostgreSQL will suffice and you do not need to touch them at all, in which case this chapter could be considered a waste of time.</p>
<p><em>However</em>, I highly encourage you to read through this chapter and, as always, actually <em>run the queries</em> with me. You need to know how the tools you use work under the hood.</p>
<p>To be even more bold, someday you might even need to get your hands dirty and actually <em>build</em> you own configuration. Why? Because a customer wanting full text search for their application might have specific requirements, or even deliver you specific dictionaries to use in the parsing stage. Such use cases may arise in very specific areas of conduct where much official, technical lingo is used which is not covered in a general dictionary.</p>
<p>So, put on your favorite pants (or none if you like that better), turn down the lights, pull the computer close to you, open up a terminal window, put on some eery music and let us get started.</p>
<h3>Configuring PostgreSQL full text search</h3>
<p>In the last chapter we saw that PostgreSQL uses a couple of tools like a <em>stop word list</em> and <em>dictionaries</em> to perform its parsing. We also saw that we did not need to tell PostgreSQL about which of these tools to use. It turned out that full text search comes with a set of default configurations for several languages. We also found out that, if no configuration was given, the database assumes that the document or string to be parsed is English and uses a configuration called <em>'english'</em>. </p>
<p>Beware of localized packages of PostgreSQL though. As I noted in the previous chapter, there is a small possibility that the default configuration in your PostgreSQL installation is <em>not</em> set to 'english'.
If this is the case with your setup, be sure to include the 'english' configuration if not stated otherwise or <em>change</em> it to be 'english'. We will see how to do that in a minute.</p>
<p>Taking the small database we created last time, the syntax to feed a configuration set to PostgreSQL during parsing was the following:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>The string '<em>english</em>' represents the <em>name</em> of the configuration which we would like to use. As you know by now, this string can be omitted which will make the database use the default configuration. PostgreSQL knows this default because it is set in the general <em>postgresql.conf</em> configuration file. In that file you will find a variable called <em>default_text_search_config</em> which, in most cases, is set to <em>pg_catalog.english</em>. If you wish to have a own, custom configuration to be the default, that is the place to set it.</p>
<p>Before hacking away at your own configuration, it may be of interest to see what PostgreSQL has to offer. To see which shipped configuration files are available to you, use the <em>describe</em> command (\d) together with the full text flag (F):</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span>
</pre></div>


<p>This will <em>describe</em> the objects in the database that represent full text configurations. You see that by default you have quite a lot of language support. To see a different configuration in action, let us do a quick, fun test. </p>
<p>First, take the dutch string "Een blauwe olifant springt al dartelend over de kreupele dolfijn.", which is a rough translation of the "The blue elephant jumps over the crippled dolphin." example from the first chapter. If we would feed this to PostgreSQL, using the default (english) configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span> <span class="s1">'Een blauwe olifant springt al dartelend over de kreupele dolfijn'</span><span class="p">);</span>
</pre></div>


<p>We would get back:</p>
<div class="code"><pre> <span class="s1">'al'</span><span class="p">:</span><span class="mi">5</span> <span class="s1">'blauw'</span><span class="p">:</span><span class="mi">2</span> <span class="s1">'dartelend'</span><span class="p">:</span><span class="mi">6</span> <span class="s1">'de'</span><span class="p">:</span><span class="mi">8</span> <span class="s1">'dolfijn'</span><span class="p">:</span><span class="mi">10</span> <span class="s1">'een'</span><span class="p">:</span><span class="mi">1</span> <span class="s1">'kreupel'</span><span class="p">:</span><span class="mi">9</span> <span class="s1">'olif'</span><span class="p">:</span><span class="mi">3</span> <span class="s1">'springt'</span><span class="p">:</span><span class="mi">4</span>
</pre></div>


<p>It attempted to guess some words as you can see from the lexeme 'olif', but, to a dutch reader, this is <em>not</em> stemmed correctly. Neither are the stop words removed: 'de' and 'een' are articles which, in dutch, are considered of no value in a text search context. So let us try this again with the built-in <em>dutch</em> configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'dutch'</span><span class="p">,</span> <span class="s1">'Een blauwe olifant springt al dartelend over de kreupele dolfijn'</span><span class="p">);</span>
</pre></div>


<p>And we get:</p>
<div class="code"><pre> <span class="s1">'blauw'</span><span class="p">:</span><span class="mi">2</span> <span class="s1">'dartel'</span><span class="p">:</span><span class="mi">6</span> <span class="s1">'dolfijn'</span><span class="p">:</span><span class="mi">10</span> <span class="s1">'kreupel'</span><span class="p">:</span><span class="mi">9</span> <span class="s1">'olifant'</span><span class="p">:</span><span class="mi">3</span> <span class="s1">'springt'</span><span class="p">:</span><span class="mi">4</span>
</pre></div>


<p>Aha! That is much shorter then the previous result, and it is also more correct. As you can see, the words 'de' and 'een' are now removed and the stemming is done correctly on 'dartel', 'olifant' and 'kreupel'.
The target of this series, however, is not to show you the dutch language (for it will make you weep...), but you see the effect a different configuration set can have. </p>
<p>But what is such a configuration set made of? To answer that, we can simply use the same describe, but ask for more detailed information with the <em>+</em> flag:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span>
</pre></div>


<p>This will return a list of <em>all</em> the configurations and their details, so let us filter that and look at only the english version:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">english</span>
</pre></div>


<p>The following result will be returned:</p>
<div class="code"><pre> asciihword      | english_stem
 asciiword       | english_stem
 email           | simple
 file            | simple
 float           | simple
 host            | simple
 hword           | english_stem
 hword_asciipart | english_stem
 hword_numpart   | simple
 hword_part      | english_stem
 int             | simple
 numhword        | simple
 numword         | simple
 sfloat          | simple
 uint            | simple
 url             | simple
 url_path        | simple
 version         | simple
 word            | english_stem
</pre></div>


<p>All of these are <em>token categories</em> that target the different groups of words that the PostgreSQL full text parser recognizes.
 For each category there are one or more dictionaries defined which will receive the token and try to return a lexeme.
 We also call this overview a configuration map, for it maps a category to one or more dictionaries.</p>
<p>If the parser encounters a URL, for example, it will categorize it as a <em>url</em> or <em>url_path</em> token and as a result, PostgreSQL will consult the dictionaries <em>mapped</em> to this category to try and create a single lexeme containing a URL pointing to the same path. Example:</p>
<ul>
<li>example.com</li>
<li>example.com/index.html</li>
<li>example.com/foo/../index.html</li>
</ul>
<p>The URLs all result in the same document being served, so it makes sense to only save one variant as a lexeme in the resulting vector.
The same kind of <em>normalization</em> is done for file paths, version numbers, host names, units of measure, ... . A lot more then normal, English words.</p>
<p>There are 23 categories in total that the parser can recognize, ones not included here, for example, are <em>tag</em> for XML tags, <em>blank</em> for whitespace or punctuation, etc.</p>
<p>To see a description of the different token categories supported, use the 'p' flag together with '+' for more information:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dFp</span><span class="o">+</span>
</pre></div>


<p>When parsing, the chain of command goes as follows:</p>
<ul>
<li>A string is fed to PostgreSQL's full text</li>
<li>The parser crawls over the string and chops it into tokens of a certain type</li>
<li>For each token category a list of dictionaries (or a single dictionary) is consulted</li>
<li>If a dictionary list is used, the dictionaries are (generally) ordered from most precise (narrow) to most generic (wide)</li>
<li>As soon as a dictionary returns a lexeme (single or in the form of an array), the flow for that token stops</li>
<li>If no lexeme is proposed (a dictionary returns <em>NULL</em>) the token is given to the next dictionary in line or if a stop word list returns a match (returns <em>empty array</em>), the token is discarded</li>
</ul>
<h3>Dictionary templates and dictionaries</h3>
<p>In the list of token categories that where supported by the built-in 'english' configuration, you will find that only two <em>dictionaries</em> are used: <em>simple</em> and <em>english_stem</em>, which in turn come from the <em>simple</em> and <em>snowball</em> dictionary <em>templates</em> respectively.</p>
<p>So, what exactly is the difference between a <em>dictionary template</em> and a <em>dictionary</em>?</p>
<p>A <em>dictionary template</em> is the skeleton (hence template) of a dictionary. It defines the actual <em>C</em> functions that will do the heavy lifting.
A <em>dictionary</em> is an instantiation of that template - providing it with data to work with.</p>
<p>Let me try to clear any confusion on this. </p>
<p>Take, for example, the <em>simple</em> dictionary <em>template</em>. It does two things: it first checks a token against a <em>stop word</em> list. If it finds a match it returns an <em>empty array</em>, which will result in the token being discarded. If no match is found in the stop word list, the process will return the same token, but with <em>casing removed</em>.</p>
<p>All the checking and case removing is done by functions, under the hood. The stop word file, however, is something that the <em>dictionary</em> (the instantiation) provides.
The instantiation of the <em>simple</em> dictionary template, thus the <em>dictionary</em> itself, would be defined as follows:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="k">simple</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="k">simple</span><span class="p">,</span>
    <span class="n">stopwords</span> <span class="o">=</span> <span class="n">english</span>
<span class="p">);</span>
</pre></div>


<p>No need to run this SQL for PostgreSQL already comes shipped with the <em>simple</em> dictionary, but I wish to show you how you <em>could</em> create it.</p>
<p>First, you will see that we <em>have</em> to define the template, thus telling PostgreSQL which set of functions to use.
Next we feed it the data it is expecting, in case of <em>simple</em> it only expects a stop word list.</p>
<p>The reason for this separation is a safe guard one. Only a database user with <em>super user</em> privileges can write the actual template, because this template will contain functions that, if written incorrectly, could slow down or crash the database. You need someone who knows what they are doing and not your local script kiddy who has normal user access to your part of the database.</p>
<p>Notice that we only give the stopwords attribute the word <em>english</em> instead of a full file path.
This is because PostgreSQL has set a few standards in place for all dictionary types we will see in this chapter.</p>
<p>First, in case of a stop word list, the file <em>must</em> have the <em>.stop</em> extension.</p>
<p>Next, you can provide a full path to the file, anywhere on your system. 
However, if you do not provide a full path, PostgreSQL will search for it inside a directory called <em>tsearch_data</em> within PostgreSQL's portion of your system's user <em>shared</em> directory.</p>
<p>On a Debian system (using PostgreSQL 9.3) the path to this directory reads: "/usr/share/postgresql/9.3/tsearch_data".</p>
<p>A dictionary like the <em>simple</em> dictionary is one that is most of the time put at the beginning of a <em>dictionary list</em> to remove all the stop words before other dictionaries are being consulted. However, in all the cases where we see <em>simple</em> in the dictionary column of the token type list above, only this dictionary is used, meaning that only stop words are removed and all else is stripped of casing.</p>
<h3>Creating the "simple" dictionary</h3>
<p>Say that we wanted to setup our own <em>simple</em> dictionary based on the <em>simple</em> dictionary template, but feed it our own list of stop words. Before setting up this new dictionary, we would first have to write a stop word file. </p>
<p>Luckily for us, this is trivial. A stop word file is nothing more then a plain text file with one word on each line. Empty lines and trailing whitespace are ignored. We would then have to save this file with the <em>.stop</em> extension. Let us try just that.</p>
<p>Open up your editor and punch in the words "dolphin" and "the", both on their own line. Write the file out as "shisaa_stop.stop", preferably in PostgreSQL's shared directory.</p>
<p>Next we need to setup our dictionary. Connect to the "phrases" database from chapter one and run the following SQL:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_simple</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="k">simple</span><span class="p">,</span>
    <span class="n">stopwords</span> <span class="o">=</span> <span class="n">shisaa_stop</span>
<span class="p">);</span>
</pre></div>


<h3>Setting up a configuration</h3>
<p>Now, the dictionary by itself is not very helpful. As we have seen before, we need to map it to token categories before we can actually use it for parsing.
This means that we need to make our own configuration.</p>
<p>Let us setup an empty configuration (not based on an existing one like 'english'):</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span> <span class="p">(</span><span class="n">parser</span><span class="o">=</span><span class="s1">'default'</span><span class="p">);</span>
</pre></div>


<p>This statement will create a new configuration for us which is completely empty, it has no mappings. The argument we have to give here can be either <em>parser</em> or <em>copy</em>. With parser you define which parser to use and it will create an empty configuration. PostgreSQL has only one parser by default which is named...<em>default</em>. If you choose <em>copy</em> then you will have to provide an <em>existing</em> configuration name (like english) from which you would like to make a copy.</p>
<p>To verify that the configuration is empty, run our describe on it:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">shisaa</span>
</pre></div>


<p>And marvel at its emptiness.</p>
<p>Now, let us add the <em>shisaa_simple</em> dictionary we created before:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">;</span>
</pre></div>


<p>As you will see throughout this (and the next) chapter, full text extends not only the data types and functions we have available, but also extends PostgreSQL's SQL syntax with a handful of new statements.
I need to note that all of these statements are <em>not</em> SQL standard (for SQL has no full text standard) and thus cannot be easily ported to a different database.
But then again...what is this folly...who would even need a different database!</p>
<p>The new statements introduced here (and in the previous SQL blocks) are:</p>
<ul>
<li><em>CREATE TEXT SEARCH DICTIONARY</em></li>
<li><em>CREATE TEXT SEARCH CONFIGURATION</em></li>
<li><em>ALTER TEXT SEARCH CONFIGURATION</em></li>
</ul>
<p>Just remember that these are not part of the SQL standard (something which PostgreSQL holds very dear, in high contrast with many other database).</p>
<p>Did it work? Well, describe it:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">shisaa</span>
</pre></div>


<p>And get back:</p>
<div class="code"><pre> asciihword      | shisaa_simple
 asciiword       | shisaa_simple
 hword           | shisaa_simple
 hword_asciipart | shisaa_simple
 hword_part      | shisaa_simple
 word            | shisaa_simple
</pre></div>


<p>Perfect!</p>
<p>Here we mapped our fresh dictionary to the token groups "asciihword", "asciiword", "hword", "hword_asciipart", "hword_part", "word", because these will target most of a normal, English sentence.</p>
<p>It is time to try out this new search configuration! Punch in the same on-the-fly SQL as we had in the previous chapter, but this time with <em>our own</em> configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>And we get back:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippled'</span>:8 <span class="s1">'elephant'</span>:4 <span class="s1">'jumped'</span>:5 <span class="s1">'over'</span>:6
</pre></div>


<p>Ha! All squeaky flippers unite! The word <em>dolphin</em> is <em>removed</em>, because we defined it to be a stop word. A world as it should be.</p>
<p>We now have a basic full text configuration with a <em>simple</em> dictionary. To have a more real world full text search we will need more then just this dictionary though, we will at least need to take care of stemming.</p>
<h3>Extending the configuration: stemming with the Snowball</h3>
<p>Stemming, the process of reducing words to their basic form, is done by a special, dedicated kind of dictionary, the <em>Snowball</em> dictionary. </p>
<p>What?</p>
<p><em>Snowball</em> is a <em>very proven</em> string processing language specially designed for stemming purposes and supports a wide range of languages. It originated from the <em>Porter stemming algorithm</em> and uses a natural syntax to define stemming rules. </p>
<p>And luckily for us, PostgreSQL has a <em>Snowball</em> dictionary template ready to use. This template has the Snowball stemming rules embedded for a wide variety of languages. Let us create a <em>dictionary</em> for our shisaa configuration, shall we?</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_snowball</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="n">snowball</span><span class="p">,</span>
    <span class="k">language</span> <span class="o">=</span> <span class="n">english</span>
<span class="p">);</span>
</pre></div>


<p>Again, very easy to setup. The snowball dictionary <em>template</em> accepts two variables to be setup. The first, mandatory one is the language you wish to support. Without this, the template does not know which of the Snowball stemming rules to take.</p>
<p>The next, optional one is, again, a stop word list. But...why can we feed this dictionary a stop word list? Did we not already do that with the <em>simple</em> dictionary?</p>
<p>That is correct, we did setup the <em>simple</em> dictionary to remove stop words for us, but we are not required to use the <em>simple</em> and the <em>snowball</em> dictionary in tandem.
It is perfectly possible to <em>map</em> only the <em>snowball</em> dictionary for various token categories and ignore all other dictionaries.
If you would not tell the <em>snowball</em> dictionary to remove stop words, it could become messy for the Snowball stemmer will try and stem <em>all</em> words it finds.</p>
<p>This stop word list can be the exact same list we fed the <em>simple</em> dictionary.</p>
<p>Also, because a <em>snowball</em> dictionary will try and parse <em>all</em> the tokens it is being fed, it is consider to be a <em>wide</em> dictionary. Therefor, as we have seen earlier, it is a good idea when chaining dictionaries together to put this dictionary at the end of your chain.</p>
<p>We now have our own version of the <em>snowball</em> dictionary and need to extend our configuration and map this dictionary to the desired token categories:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Notice that in the <em>WITH</em> clause we are now chaining the <em>simple</em> and the <em>snowball</em> dictionary together. The order is, of course, important.
Describe our configuration once more:</p>
<div class="code"><pre><span class="err">\</span><span class="n">dF</span><span class="o">+</span> <span class="n">shisaa</span>
</pre></div>


<p>And get back:</p>
<div class="code"><pre>asciihword      | shisaa_simple,shisaa_snowball
asciiword       | shisaa_simple,shisaa_snowball
hword           | shisaa_simple,shisaa_snowball
hword_asciipart | shisaa_simple,shisaa_snowball
hword_part      | shisaa_simple,shisaa_snowball
word            | shisaa_simple,shisaa_snowball
</pre></div>


<p>Perfect, now the <em>simple</em> dictionary will be consulted first followed by the <em>snowball</em> dictionary.</p>
<p>Note that throughout this chapter I will chain together dictionaries in order. This will <em>not</em> always be the most smart or desired order, just an order to demonstrate <em>how</em> you can chain dictionaries.</p>
<p>To the test, throw a new query at it:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>And get back:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippled'</span>:8 <span class="s1">'elephant'</span>:4 <span class="s1">'jumped'</span>:5 <span class="s1">'over'</span>:6
</pre></div>


<p>Nice, that is very...oh wait. Something is not correct. I am getting back <em>exactly</em> the same result as before. The words "crippled" and "elephant" are not stemmed at all. Why?</p>
<p>Well, the <em>simple</em> dictionary, as we defined it earlier, is setup to be a bit greedy. In its current state it will return an unmatched token as a lexeme with casing removed.
It does not return <em>NULL</em>. And, as you know by now, <em>NULL</em> is needed to give other dictionaries a chance to examine the token.</p>
<p>So, we need to alter the <em>simple</em> dictionary's behavior. For this, we can use the <em>ALTER</em> syntax provided to us. And as it turns out, the <em>simple</em> dictionary <em>template</em> can accept one more variable: the <em>accept</em> variable. If this is set to false, then it will return <em>NULL</em> for every unmatched token. Let us alter that dictionary:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_simple</span> <span class="p">(</span> <span class="n">accept</span> <span class="o">=</span> <span class="k">false</span> <span class="p">);</span>
</pre></div>


<p>Run the ts_vector query again, and look at the results:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippl'</span>:8 <span class="s1">'eleph'</span>:4 <span class="s1">'jump'</span>:5 <span class="s1">'over'</span>:6
</pre></div>


<p>That is what we were looking for, nicely stemmed results!</p>
<h3>Extending the configuration: fun with synonyms</h3>
<p>By now we have seen the first and the last dictionary in our control chain, but at least one more important part is missing: synonyms are not removed.</p>
<p>Let us extend our favorite sentence and add a few synonyms to it: "The big blue elephant, joined by its enormous blue mammoth friend, jumped over the crippled blue dolphin while smiling at the orca."</p>
<p>Still perfectly possible.</p>
<p>In the light of (cue dark en deep Batman voice) "science" (end Batman voice), let us first see what we get when we run it through our current configuration:</p>
<div class="code"><pre><span class="s1">'at'</span>:20 <span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9,16 <span class="s1">'by'</span>:6 <span class="s1">'crippl'</span>:15 <span class="s1">'eleph'</span>:4 <span class="s1">'enorm'</span>:8 <span class="s1">'friend'</span>:11 <span class="s1">'it'</span>:7 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:10 <span class="s1">'orca'</span>:22 <span class="s1">'over'</span>:13 <span class="s1">'smile'</span>:19 <span class="s1">'while'</span>:18
</pre></div>


<p>That is one big result set. Maybe we should cut the blue dolphin a little bit of slack and feed a real stop word list to our <em>simple</em> dictionary before continuing by altering our <em>dictionary</em>:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_simple</span> <span class="p">(</span> <span class="n">stopwords</span> <span class="o">=</span> <span class="n">english</span> <span class="p">);</span>
</pre></div>


<p>As you see you can simply use the same <em>ALTER</em> syntax as before. The "english" here refers to the shipped "english.stop" stop word list.</p>
<p>Querying again, we will get back a better, short list (including our Dolphin friend):</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9,16 <span class="s1">'crippl'</span>:15 <span class="s1">'dolphin'</span>:17 <span class="s1">'eleph'</span>:4 <span class="s1">'enorm'</span>:8 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:10 <span class="s1">'orca'</span>:22 <span class="s1">'smile'</span>:19
</pre></div>


<p>Now we would like to reduce this result even further by compacting synonyms into one lexeme.</p>
<p>Enter the <em>synonym</em> dictionary <em>template</em>.</p>
<p>This template requires you to have a so-called "synonym" file; A file containing lists of words with the same meaning. For the sake of learning, let us create our own synonym file. This file has to end with the <em>.syn</em> extension.</p>
<p>Open up your editor again and write out a file called "shisaa_syn.syn" with the following contents:</p>
<div class="code"><pre>big enormous
elephant mammoth
dolphin orca
</pre></div>


<p>And let us setup the <em>dictionary</em>:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_synonym</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="n">synonym</span><span class="p">,</span>
    <span class="n">synonyms</span> <span class="o">=</span> <span class="n">shisaa_syn</span>
<span class="p">);</span>
</pre></div>


<p>And add the mapping for it:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_synonym</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Okay, time to test our big string again and see the results:</p>
<div class="code"><pre><span class="s1">'blue'</span>:3,9,16 <span class="s1">'crippl'</span>:15 <span class="s1">'enorm'</span>:8 <span class="s1">'enormous'</span>:2 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:4,10 <span class="s1">'orca'</span>:17,22 <span class="s1">'smile'</span>:19
</pre></div>


<p>Very neat. The words "elephant", "big" and "dolphin" are now removed and only their synonyms are kept.
Also notice that both "mammoth" and "orca" have two pointers each, one for every synonym.</p>
<p>But look at the words 'enorm' and 'enormous', why is this happening?</p>
<p>If you look at the pointers, you see that <em>enormous</em> points to the second word in the string, being <em>big</em>, while <em>enorm</em> points to the original <em>enormous</em> word.
The reason why this is happening is because our <em>synonym</em> dictionary has priority over our <em>snowball</em> one. The <em>synonym</em> dictionary emits a lexeme as a synonym for <em>big</em>, being <em>enormous</em>, simply because we told it to do so in our <em>synonym file</em>. Now, because it emits a lexeme, the original token, <em>big</em>, is not available anymore for the rest of the dictionary chain.</p>
<p>The token <em>enormous</em> itself has <em>no</em> synonym because we did not define it in our synonym file. It is ignored by the <em>synonym</em> dictionary and passed over to the <em>snowball</em> dictionary which then stems the token into a lexeme resulting in <em>enorm</em>.</p>
<p>If you wish to prevent this from happening, you could add a self pointing line to your synonym list:</p>
<div class="code"><pre>enormous enormous
</pre></div>


<p>Now load in the file on disk to pull the changes into PostgreSQL:</p>
<div class="code"><pre> <span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_synonym</span> <span class="p">(</span><span class="n">synonyms</span><span class="o">=</span><span class="n">shisaa_syn</span><span class="p">);</span>
</pre></div>


<p>And run the query again, the result should now read:</p>
<div class="code"><pre><span class="s1">'blue'</span>:3,9,16 <span class="s1">'crippl'</span>:15 <span class="s1">'enormous'</span>:2,8 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:4,10 <span class="s1">'orca'</span>:17,22 <span class="s1">'smile'</span>:19
</pre></div>


<p>Now <em>enorm</em> will be removed and both <em>big</em> and <em>enormous</em> are cast to the same lexeme. </p>
<p>PostgreSQL does not ship a synonym list, so you will have to compile your own just like we did above but hopefully a little bit more useful</p>
<h3>Extending the configuration: phrasing with a Thesaurus</h3>
<p>Next up is the <em>thesaurus</em> dictionary, which is quite close to the <em>synonym</em> dictionary, with one exception: <em>phrases</em>.</p>
<p>A <em>thesaurus</em> dictionary is used to recognize phrases and convert them into lexemes with the same meaning. Again, this dictionary relies on a file containing the phrase conversions.
This time, the file has the <em>.ths</em> extension. </p>
<p>Open up your editor and write out a file called "shisaa_thesaurus.ths" with the following contents:</p>
<div class="code"><pre>big blue elephant : PostgreSQL
crippled blue dolphin : MySQL
</pre></div>


<p>Before we can create the dictionary, there is one more required variable we have to set, the <em>subdictionary</em> the <em>thesaurus</em> dictionary can use.
This subdictionary will be <em>another</em> dictionary you have defined before. Most of the time a stemmer is fed to this variable to let the thesaurus stem the input before comparing it with its thesaurus file.</p>
<p>So let us feed it our <em>snowball</em> dictionary and set it up:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_thesaurus</span> <span class="p">(</span>
    <span class="k">TEMPLATE</span> <span class="o">=</span> <span class="n">thesaurus</span><span class="p">,</span>
    <span class="n">DICTFILE</span> <span class="o">=</span> <span class="n">shisaa_thesaurus</span><span class="p">,</span>
    <span class="k">DICTIONARY</span> <span class="o">=</span> <span class="n">shisaa_snowball</span>
<span class="p">);</span>
</pre></div>


<p>Map it:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_thesaurus</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Notice that I took out the <em>synonym</em> dictionary. If we chain up to many dictionaries, the results might turn out to be undesirable in our demonstration use case.</p>
<p>Querying will result in the following tsvector:</p>
<div class="code"><pre><span class="s1">'blue'</span>:7 <span class="s1">'enorm'</span>:6 <span class="s1">'friend'</span>:9 <span class="s1">'join'</span>:3 <span class="s1">'jump'</span>:10 <span class="s1">'mammoth'</span>:8 <span class="s1">'mysql'</span>:13 <span class="s1">'orca'</span>:18 <span class="s1">'postgresql'</span>:2 <span class="s1">'smile'</span>:15
</pre></div>


<p>That is quite awesome, it now recognizes "big blue elephant" as PostgreSQL and "crippled blue dolphin" as MySQL. We have created a <em>pun-aware</em> full text search configuration!</p>
<p>As you can see,  both the "MySQL" and "PostgreSQL" lexemes have <em>one</em> pointer each, pointing to the first word of the substring that got converted.</p>
<h3>Extending the configuration a last time: morphing with Ispell</h3>
<p>Okay, we are almost at the end of the dictionary <em>templates</em> that PostgreSQL supports.</p>
<p>This last one is a fun one too. Many Unix and Linux systems come shipped with a spell checker called <em>Ispell</em> or with the more modern variant called <em>HunSpell</em>.
Besides your average spell checking, these dictionaries are very good at morphological lookups, meaning that they can link all different writing structures of words together.</p>
<p>A synonym or thesaurus dictionary would not catch these, unless explicitly set with a huge amount of lines in the <em>.syn</em> or <em>.ths</em> files, which is error prone and inelegant. 
The Ispell or Hunspell dictionaries <em>will</em> capture these and try to make them into one lexeme.</p>
<p>Before setting up the <em>dictionary</em>, we first need to make sure that we have the Ispell or Hunspell dictionary files for the language we wish to support.
Normally you would want to download these files from the official OpenOffice page. These pages, however, seem to be confusing and the correct files very hard to find. I have found <a href="http://fmg-www.cs.ucla.edu/geoff/ispell-dictionaries.html" title="OpenOffice Extension page.">the following page</a> of great help to get the files you need for your desired language
.
Download the files for your desired language and place the <em>.dict</em> and the <em>.affix</em> files into the PostgreSQL shared directory.</p>
<p>For now, let us just take the basic <em>english</em> dict and affix files (named both <em>en_us</em> and already shipped with PostgreSQL) and feed them to the configuration:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="k">DICTIONARY</span> <span class="n">shisaa_ispell</span> <span class="p">(</span>
    <span class="k">template</span> <span class="o">=</span> <span class="n">ispell</span><span class="p">,</span>
    <span class="n">DictFile</span> <span class="o">=</span> <span class="n">en_us</span><span class="p">,</span>
    <span class="n">AffFile</span> <span class="o">=</span> <span class="n">en_us</span><span class="p">,</span>
    <span class="n">StopWords</span> <span class="o">=</span> <span class="n">english</span>
<span class="p">);</span>
</pre></div>


<p>And chain it:</p>
<div class="code"><pre><span class="k">ALTER</span> <span class="nb">TEXT</span> <span class="k">SEARCH</span> <span class="n">CONFIGURATION</span> <span class="n">shisaa</span>
    <span class="k">ALTER</span> <span class="n">MAPPING</span> <span class="k">FOR</span> <span class="n">asciiword</span><span class="p">,</span> <span class="n">asciihword</span><span class="p">,</span> <span class="n">hword_asciipart</span><span class="p">,</span>
                  <span class="n">word</span><span class="p">,</span> <span class="n">hword</span><span class="p">,</span> <span class="n">hword_part</span>
    <span class="k">WITH</span> <span class="n">shisaa_simple</span><span class="p">,</span> <span class="n">shisaa_ispell</span><span class="p">,</span> <span class="n">shisaa_snowball</span><span class="p">;</span>
</pre></div>


<p>Notice again I took out the <em>thesaurus</em> dictionary, not to pile up too many dictionaries at once.</p>
<p>Query it once more, and look at what we get back:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9,16 <span class="s1">'cripple'</span>:15 <span class="s1">'dolphin'</span>:17 <span class="s1">'elephant'</span>:4 <span class="s1">'enormous'</span>:8 <span class="s1">'friend'</span>:11 <span class="s1">'join'</span>:5 <span class="s1">'joined'</span>:5 <span class="s1">'jump'</span>:12 <span class="s1">'mammoth'</span>:10 <span class="s1">'orca'</span>:22 <span class="s1">'smile'</span>:19 <span class="s1">'smiling'</span>:19
</pre></div>


<p>Hmm, interesting. Notice that we now got <em>more</em> lexemes than before, <em>smile</em> and <em>smiling</em> for example, and <em>join</em> and <em>joined</em>. Also, both these cases have the <em>same</em> pointer. Why is that?</p>
<p>What is happening here is a feature of the Ispell dictionary called <em>morphology</em>, or as we seen above, <em>morphological lookups</em>.
One of the reasons why Ispell is such a powerful dictionary is because it can recognize and act upon the <em>structure</em> of a word. </p>
<p>In our case, Ispell recognizes <em>joined</em> (or <em>smiling</em>) and emits an array of <em>two</em> lexemes, the original token converted to a lexeme <em>and</em> the stemmed version of the token.</p>
<p>This concludes all the dictionaries that PostgreSQL ships with by default and the ones you will most likely ever need. What is next?</p>
<h3>Debugging</h3>
<p>Now that you have a good understanding of how to build your own configuration and setup your own dictionaries, I would like to introduce a few new functions that can come in handy when your configuration would produce seemingly strange results.</p>
<h4>ts_debug()</h4>
<p>The first function I want show you is a <em>very</em> handy one that is built to test your <em>whole</em> full text configuration. It helps you keep your mental condition to just mildly insane, so to speak.</p>
<p>The function <em>ts_debug()</em> accepts a configuration and a string of text you wish to test. As a result you will get back a set that contains an overview of how the parser chopped your string into tokens,  which category it picked for each token, which dictionary was consulted and which lexeme(s) where emitted. Oh boy, this is too much fun, let us just try it out! </p>
<p>Feed our original pun string and let us test the current <em>shisaa</em> configuration:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_debug</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>Hmm, that may not be very readable, rather use the wildcard selector and a FROM clause to include column names into our result set (one of the few times you may use this selector without getting smacked):</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">ts_debug</span><span class="p">(</span><span class="s1">'shisaa'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">);</span>
</pre></div>


<p>Which will result in the following, huge set:</p>
<div class="code"><pre>  <span class="nb">alias</span>   |   description   |  token   |                 dictionaries                  |  dictionary   |  lexemes   
-----------+-----------------+----------+-----------------------------------------------+---------------+------------
asciiword | Word, all ASCII | The      | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_simple | <span class="o">{}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | big      | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>big<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | blue     | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>blue<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | elephant | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>elephant<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | jumped   | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>jump<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | over     | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_simple | <span class="o">{}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | the      | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_simple | <span class="o">{}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | crippled | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>cripple<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | blue     | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>blue<span class="o">}</span>
blank     | Space symbols   |          | <span class="o">{}</span>                                            |               | 
asciiword | Word, all ASCII | dolphin  | <span class="o">{</span>shisaa_simple,shisaa_ispell,shisaa_snowball<span class="o">}</span> | shisaa_ispell | <span class="o">{</span>dolphin<span class="o">}</span>
blank     | Space symbols   | .        | <span class="o">{}</span>                                            |               |
</pre></div>


<p>You now have a complete overview of the flow from string to vector of lexemes. Let me go over some interesting facts of this result set.</p>
<p>First, notice how the tokens <em>the</em> and <em>over</em> got removed by the <em>simple</em> dictionary. They where a hit in the stop word list, so the dictionary returned an <em>empty array</em>.</p>
<p>Next you see the alias <em>blank</em> between each <em>asciiword</em>. <em>Blank</em> is a category used for spaces or punctuation. A <em>space</em> and a <em>.</em> (full stop) is considered a token, but is stripped out by the parser itself for it has no value in this context.</p>
<p>And last, see that our <em>snowball</em> dictionary was never consulted. This means that, in this string, the <em>shisaa_ispell</em> gobbled all the lexemes that <em>shisaa_simple</em> threw at it.</p>
<h4>ts_lexize()</h4>
<p>The second function is <em>ts_lexize()</em>. This little helper lets you test different <em>parts</em> of your whole setup. Take the unexpected result of our last dictionary, where we got back multiple lexemes. As it turned out it is normal behavior, but you may want to verify that the result is coming from the dictionary and not from a side effect of how you chained your dictionaries together.</p>
<p>To test our single, <em>shisaa_ispell</em> dictionary, we could feed it to this new function, together with <em>one token</em> we wish to test:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">ts_lexize</span><span class="p">(</span><span class="s1">'shisaa_ispell'</span><span class="p">,</span><span class="s1">'joined'</span><span class="p">);</span>
</pre></div>


<p>This will return:</p>
<div class="code"><pre><span class="o">{</span>joined,join<span class="o">}</span>
</pre></div>


<p>Same as we had before, but now we know, for sure, that it is a feature of our Ispell dictionary. 
Notice that I stressed the fact that you can only feed this function <em>one token</em>, not a string of text and not multiple tokens.</p>
<p>You can use this function to test all your dictionaries individually, one token at a time.</p>
<p>Phew, that was a lot to take in for we covered a lot of ground here today. You can turn the lights back high and go get some fresh air.
In the next chapter, I will round up this introduction by introducing you to the following, new material:</p>
<ul>
<li>Ranking search results</li>
<li>Highlighting word inside search results</li>
<li>Creating special, full text search indexes</li>
<li>Setting up update triggers for ts_vector records</li>
</ul>
<p>And as always...thanks for reading!</p>
<!--  LocalWords:  instantiation PostgreSQL
 -->";}i:19;a:6:{s:5:"title";s:66:"Craig Kerstiens: Postgres Datatypes  The ones you're not using.";s:4:"link";s:85:"http://www.craigkerstiens.com/2014/05/07/Postgres-datatypes-the-ones-youre-not-using/";s:11:"description";s:5801:"<p><img src="http://cl.ly/image/0V3d2A0w2V0N/Naws__Its_all_just_1_s_and_0_s.png" style="float: right;" />Postgres has a variety of datatypes, in fact quite a few more than most other databases. Most commonly applications take advantage of the standard ones  integers, text, numeric, etc. Almost every application needs these basic types, the rarer ones may be needed less frequently. And while not needed on every application when you do need them they can be an extremely handy. So without further adieu let&rsquo;s look at some of these rarer but awesome types.</p>

<h3>hstore</h3>

<p>Yes, I&rsquo;ve talked about <a href="http://www.craigkerstiens.com/2013/07/03/hstore-vs-json/">this one before</a>, yet still not enough people are using it. Of this list of datatypes this is one that could also have benefit for most if not all applications. <!--more-->Hstore is a key-value store directly within Postgres. This means you can easily add new keys and values <em>(optionally)</em>, without haveing to run a migration to setup new columns. Further you can still get great performance by using Gin and GiST indexes with them, which automatically index all keys and values for hstore.</p>

<p><em>It&rsquo;s of note that hstore is an extension and not enabled by default. If you want the ins and outs of getting hands on with it, give the article on <a href="http://postgresguide.com/sexy/hstore.html">Postgres Guide</a> a read.</em></p>

<h3>Range types</h3>

<p>If there is ever a time where you have two columns in your database with one being a from, another being a to, you probably want to be using <a href="http://www.postgresql.org/docs/9.2/static/rangetypes.html">range types</a>. Range types are just that a set of ranges. A super common use of them is when doing anything with calendaring. The place where they really become useful is in their ability to apply constraints on those ranges. This means you can make sure you don&rsquo;t have overlapping time issues, and don&rsquo;t have to rebuild heavy application logic to accomplish it.</p>

<h3>Timestamp with Timezone</h3>

<p>Timestamps are annoying, plain and simple. If you&rsquo;ve re-invented handling different timezones within your application you&rsquo;ve wasted plenty of time and likely done it wrong. If you&rsquo;re using plain timestamps within your application further there&rsquo;s a good chance they dont even mean what you think they mean. Timestamps with timezone or timestamptz automatically includes the timezone with the timestamp. This makes it easy to convert between timezones, know exactly what you&rsquo;re dealing with, and will in short save you a ton of time. There&rsquo;s seldom a case you shouldn&rsquo;t be using these.</p>

<h3>UUID</h3>

<p>Integers are primary keys aren&rsquo;t great. Sure if you&rsquo;re running a small blog they work fine, but if you&rsquo;re application has to scale to a large size integers can create problems. First you can run out of them, second it can make other details such as sharding a little more annoying. At the same time they are super readable. However, using the actual UUID datatype and extension to automatically generate them can be incredibly handy if you have to scale an application.</p>

<p><em>Similar to hstore, there&rsquo;s an <a href="http://www.postgresql.org/docs/9.3/static/uuid-ossp.html">extension</a> that makes the UUID much more useful.</em></p>

<h3>Binary JSON</h3>

<p>This isn&rsquo;t available yet, but will be in Postgres 9.4. <a href="http://www.craigkerstiens.com/2014/03/24/Postgres-9.4-Looking-up/">Binary JSON</a> is of course JSON directly within your database, but also lets you add Gin indexes directly onto JSON. This means a much simpler setup in not only inserting JSON, but having fast reads. If you want to learn a bit more about this, <a href="http://www.craigkerstiens.com/training/index.html">sign up</a> to <a href="http://www.craigkerstiens.com/training/index.html">get notified</a> of training regarding the upcoming PostgreSQL 9.4 release.</p>

<h3>Money</h3>

<p>Please don&rsquo;t use this&hellip; The money datatype assumes a single currency type, and generally brings with it more caveats than simply using a numeric type.</p>

<h3>In conclusion</h3>

<p>What&rsquo;d I miss? What are you&rsquo;re favorite types? Let me know <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a>, or sign-up below to updates on Postgres content and first access to training.</p>

<!-- Begin MailChimp Signup Form -->


<p></p>



<div id="mc_embed_signup">
<form action="http://craigkerstiens.us5.list-manage.com/subscribe/post?u=0bb2ad96ec10236507971efdc&amp;id=dacc2c6d9a" class="validate" id="mc-embedded-subscribe-form" method="post" name="mc-embedded-subscribe-form" target="_blank">
    <h2>Sign up to get weekly advice and content on Postgres</h2>
<div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
    <input class="required email" id="mce-EMAIL" name="EMAIL" type="email" value="" />
</div>
    <div class="clear" id="mce-responses">
        <div class="response" id="mce-error-response" style="display: none;"></div>
        <div class="response" id="mce-success-response" style="display: none;"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div><input name="b_0bb2ad96ec10236507971efdc_dacc2c6d9a" tabindex="-1" type="text" value="" /></div>
    <div class="clear"><input class="button" id="mc-embedded-subscribe" name="subscribe" type="submit" value="Subscribe" /></div>
</form>
</div>


&amp;&amp;<div id="'+err_id+'"></div>


<!--End mc_embed_signup-->";s:4:"guid";s:84:"http://www.craigkerstiens.com/2014/05/07/Postgres-datatypes-the-ones-youre-not-using";s:7:"pubdate";s:29:"Wed, 07 May 2014 07:00:00 GMT";s:7:"summary";s:5801:"<p><img src="http://cl.ly/image/0V3d2A0w2V0N/Naws__Its_all_just_1_s_and_0_s.png" style="float: right;" />Postgres has a variety of datatypes, in fact quite a few more than most other databases. Most commonly applications take advantage of the standard ones  integers, text, numeric, etc. Almost every application needs these basic types, the rarer ones may be needed less frequently. And while not needed on every application when you do need them they can be an extremely handy. So without further adieu let&rsquo;s look at some of these rarer but awesome types.</p>

<h3>hstore</h3>

<p>Yes, I&rsquo;ve talked about <a href="http://www.craigkerstiens.com/2013/07/03/hstore-vs-json/">this one before</a>, yet still not enough people are using it. Of this list of datatypes this is one that could also have benefit for most if not all applications. <!--more-->Hstore is a key-value store directly within Postgres. This means you can easily add new keys and values <em>(optionally)</em>, without haveing to run a migration to setup new columns. Further you can still get great performance by using Gin and GiST indexes with them, which automatically index all keys and values for hstore.</p>

<p><em>It&rsquo;s of note that hstore is an extension and not enabled by default. If you want the ins and outs of getting hands on with it, give the article on <a href="http://postgresguide.com/sexy/hstore.html">Postgres Guide</a> a read.</em></p>

<h3>Range types</h3>

<p>If there is ever a time where you have two columns in your database with one being a from, another being a to, you probably want to be using <a href="http://www.postgresql.org/docs/9.2/static/rangetypes.html">range types</a>. Range types are just that a set of ranges. A super common use of them is when doing anything with calendaring. The place where they really become useful is in their ability to apply constraints on those ranges. This means you can make sure you don&rsquo;t have overlapping time issues, and don&rsquo;t have to rebuild heavy application logic to accomplish it.</p>

<h3>Timestamp with Timezone</h3>

<p>Timestamps are annoying, plain and simple. If you&rsquo;ve re-invented handling different timezones within your application you&rsquo;ve wasted plenty of time and likely done it wrong. If you&rsquo;re using plain timestamps within your application further there&rsquo;s a good chance they dont even mean what you think they mean. Timestamps with timezone or timestamptz automatically includes the timezone with the timestamp. This makes it easy to convert between timezones, know exactly what you&rsquo;re dealing with, and will in short save you a ton of time. There&rsquo;s seldom a case you shouldn&rsquo;t be using these.</p>

<h3>UUID</h3>

<p>Integers are primary keys aren&rsquo;t great. Sure if you&rsquo;re running a small blog they work fine, but if you&rsquo;re application has to scale to a large size integers can create problems. First you can run out of them, second it can make other details such as sharding a little more annoying. At the same time they are super readable. However, using the actual UUID datatype and extension to automatically generate them can be incredibly handy if you have to scale an application.</p>

<p><em>Similar to hstore, there&rsquo;s an <a href="http://www.postgresql.org/docs/9.3/static/uuid-ossp.html">extension</a> that makes the UUID much more useful.</em></p>

<h3>Binary JSON</h3>

<p>This isn&rsquo;t available yet, but will be in Postgres 9.4. <a href="http://www.craigkerstiens.com/2014/03/24/Postgres-9.4-Looking-up/">Binary JSON</a> is of course JSON directly within your database, but also lets you add Gin indexes directly onto JSON. This means a much simpler setup in not only inserting JSON, but having fast reads. If you want to learn a bit more about this, <a href="http://www.craigkerstiens.com/training/index.html">sign up</a> to <a href="http://www.craigkerstiens.com/training/index.html">get notified</a> of training regarding the upcoming PostgreSQL 9.4 release.</p>

<h3>Money</h3>

<p>Please don&rsquo;t use this&hellip; The money datatype assumes a single currency type, and generally brings with it more caveats than simply using a numeric type.</p>

<h3>In conclusion</h3>

<p>What&rsquo;d I miss? What are you&rsquo;re favorite types? Let me know <a href="http://www.twitter.com/craigkerstiens">@craigkerstiens</a>, or sign-up below to updates on Postgres content and first access to training.</p>

<!-- Begin MailChimp Signup Form -->


<p></p>



<div id="mc_embed_signup">
<form action="http://craigkerstiens.us5.list-manage.com/subscribe/post?u=0bb2ad96ec10236507971efdc&amp;id=dacc2c6d9a" class="validate" id="mc-embedded-subscribe-form" method="post" name="mc-embedded-subscribe-form" target="_blank">
    <h2>Sign up to get weekly advice and content on Postgres</h2>
<div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
    <label for="mce-EMAIL">Email Address  <span class="asterisk">*</span>
</label>
    <input class="required email" id="mce-EMAIL" name="EMAIL" type="email" value="" />
</div>
    <div class="clear" id="mce-responses">
        <div class="response" id="mce-error-response" style="display: none;"></div>
        <div class="response" id="mce-success-response" style="display: none;"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div><input name="b_0bb2ad96ec10236507971efdc_dacc2c6d9a" tabindex="-1" type="text" value="" /></div>
    <div class="clear"><input class="button" id="mc-embedded-subscribe" name="subscribe" type="submit" value="Subscribe" /></div>
</form>
</div>


&amp;&amp;<div id="'+err_id+'"></div>


<!--End mc_embed_signup-->";}i:20;a:6:{s:5:"title";s:106:"Hubert 'depesz' Lubaczewski: Waiting for 9.4  Add support for wrapping to psqls extended mode.";s:4:"link";s:97:"http://www.depesz.com/2014/05/05/waiting-for-9-4-add-support-for-wrapping-to-psqls-extended-mode/";s:11:"description";s:339:"On 28th of April, Greg Stark committed patch: Add support for wrapping to psql's "extended" mode. This makes it very &#160; feasible to display tables that have both many columns and some large data in some columns (such as pg_stats). &#160; Emre Hasegeli with review and rewriting from Sergey Muraviov and reviewed by Greg Stark [&#8230;]";s:4:"guid";s:29:"http://www.depesz.com/?p=2849";s:7:"pubdate";s:29:"Mon, 05 May 2014 18:39:06 GMT";s:7:"summary";s:339:"On 28th of April, Greg Stark committed patch: Add support for wrapping to psql's "extended" mode. This makes it very &#160; feasible to display tables that have both many columns and some large data in some columns (such as pg_stats). &#160; Emre Hasegeli with review and rewriting from Sergey Muraviov and reviewed by Greg Stark [&#8230;]";}i:21;a:6:{s:5:"title";s:38:"Andrew Dunstan: pgbouncer enhancements";s:4:"link";s:64:"http://adpgtech.blogspot.com/2014/05/pgbouncer-enhancements.html";s:11:"description";s:1726:"A couple of customers have recently asked for enhancements of pgbouncer, and I have provided them.<br /><br />One that's been working for a while now, puts the address and port of the actual client (i.e. the program that connects to the proxy) into the session's application_name setting. That means that if you want to see where the client is that's running some query that's gone rogue, it's no longer hidden from you by the fact that all connections appear to be coming from the pgbouncer host.You can see it appearing in places like pg_stat_activity.<br /><br />It only works when a client connects, so if the client itself sets application_name then the setting gets overridden. But few clients do this, and the original requester has found it useful. I've submitted this to the upsteam repo, as can be seen at&nbsp; <a href="https://github.com/markokr/pgbouncer-dev/pull/23">https://github.com/markokr/pgbouncer-dev/pull/23</a>.<br /><br />The other enhancement is the ability to include files in the config file. This actually involves a modification to the library pgbouncer uses as a git submodule, libusual. With this enhancement, a line that has "%include filename" causes the contents of that file to be included in place of the directive. Includes can be nested up to 10 deep. The pull request for this is at&nbsp; <a href="https://github.com/markokr/libusual/pull/7">https://github.com/markokr/libusual/pull/7</a>. This one too sems to be working happily at the client's site.<br /><br />There is one more enhancement on the horizon, which involves adding in host based authentication control similar to that used by Postgres. That's a rather larger bit of work, but I hope to get to it in the next month or two.";s:4:"guid";s:70:"tag:blogger.com,1999:blog-2356137376934964551.post-3400032009414945992";s:7:"pubdate";s:29:"Fri, 02 May 2014 14:20:00 GMT";s:7:"summary";s:1726:"A couple of customers have recently asked for enhancements of pgbouncer, and I have provided them.<br /><br />One that's been working for a while now, puts the address and port of the actual client (i.e. the program that connects to the proxy) into the session's application_name setting. That means that if you want to see where the client is that's running some query that's gone rogue, it's no longer hidden from you by the fact that all connections appear to be coming from the pgbouncer host.You can see it appearing in places like pg_stat_activity.<br /><br />It only works when a client connects, so if the client itself sets application_name then the setting gets overridden. But few clients do this, and the original requester has found it useful. I've submitted this to the upsteam repo, as can be seen at&nbsp; <a href="https://github.com/markokr/pgbouncer-dev/pull/23">https://github.com/markokr/pgbouncer-dev/pull/23</a>.<br /><br />The other enhancement is the ability to include files in the config file. This actually involves a modification to the library pgbouncer uses as a git submodule, libusual. With this enhancement, a line that has "%include filename" causes the contents of that file to be included in place of the directive. Includes can be nested up to 10 deep. The pull request for this is at&nbsp; <a href="https://github.com/markokr/libusual/pull/7">https://github.com/markokr/libusual/pull/7</a>. This one too sems to be working happily at the client's site.<br /><br />There is one more enhancement on the horizon, which involves adding in host based authentication control similar to that used by Postgres. That's a rather larger bit of work, but I hope to get to it in the next month or two.";}i:22;a:6:{s:5:"title";s:45:"Josh Berkus: New Finding Unused Indexes Query";s:4:"link";s:73:"http://www.databasesoup.com/2014/05/new-finding-unused-indexes-query.html";s:11:"description";s:2294:"As long as we're <a href="http://www.databasesoup.com/2014/04/new-new-index-bloat-query.html" target="_blank">overhauling standard monitoring queries for PostgreSQL</a>, here's another one.&nbsp; <a href="https://gist.github.com/jberkus/6b1bcaf7724dfc2a54f3" target="_blank">This query helps you find indexes which are relatively unused</a>, and as a result could probably be dropped.&nbsp; I wrote a more complex (yes, really) version for our internal Performance Health Check suite, but the linked version is usable by anyone.<br /><br />The query is also an example of why CTEs, otherwise known as "WITH statements", are a life-saver for working with complex queries.&nbsp; I've only tested it on 9.2 and 9.3; I don't know if it'll work on older versions.<br /><br />Before you use it, you need to check how long you've been collecting data into pg_stat_user_indexes and friends.&nbsp; The default is since you created the database, but some people reset stats on a daily or monthly basis.&nbsp; So it's important to know what you're looking at.&nbsp;&nbsp; Don't make the mistake of dropping the indexes which are needed for the month-end reports!<br /><br />The query divides seldom-used indexes into four groups:<br /><br /><b>Indexes Which Aren't Scanned At All</b>: these indexes have no scans during the stats period.&nbsp; These pretty much certainly can be dropped, except those on really small tables which you expect to grow later.<br /><br /><b>Seldom Used Indexes on Heavily Written Tables</b>:&nbsp; as a general rule, if you're not using an index twice as often as it's written to, you should probably drop it.&nbsp; This query is a little more conservative than that.<br /><br /><b>Really Large, Seldom-Used Indexes</b>: these indexes get used, but not that often, and they're taking up a lot of RAM and storage space.&nbsp;&nbsp; Consider dropping them after some thought about why they were added.<br /><br /><b>Large Non-BTree Indexes On Busy Tables</b>:&nbsp; as a rule, non-BTree indexes like GiST and GIN don't accurately report usage stats.&nbsp; As a result, we can't check how often they're used, just how big they are and if they're attached to tables which get a lot of writes.&nbsp; This list of indexes should be very judiciously pruned.<br /><br />Happy pruning!";s:4:"guid";s:70:"tag:blogger.com,1999:blog-7476449567742726187.post-7909169834652469560";s:7:"pubdate";s:29:"Fri, 02 May 2014 01:07:00 GMT";s:7:"summary";s:2294:"As long as we're <a href="http://www.databasesoup.com/2014/04/new-new-index-bloat-query.html" target="_blank">overhauling standard monitoring queries for PostgreSQL</a>, here's another one.&nbsp; <a href="https://gist.github.com/jberkus/6b1bcaf7724dfc2a54f3" target="_blank">This query helps you find indexes which are relatively unused</a>, and as a result could probably be dropped.&nbsp; I wrote a more complex (yes, really) version for our internal Performance Health Check suite, but the linked version is usable by anyone.<br /><br />The query is also an example of why CTEs, otherwise known as "WITH statements", are a life-saver for working with complex queries.&nbsp; I've only tested it on 9.2 and 9.3; I don't know if it'll work on older versions.<br /><br />Before you use it, you need to check how long you've been collecting data into pg_stat_user_indexes and friends.&nbsp; The default is since you created the database, but some people reset stats on a daily or monthly basis.&nbsp; So it's important to know what you're looking at.&nbsp;&nbsp; Don't make the mistake of dropping the indexes which are needed for the month-end reports!<br /><br />The query divides seldom-used indexes into four groups:<br /><br /><b>Indexes Which Aren't Scanned At All</b>: these indexes have no scans during the stats period.&nbsp; These pretty much certainly can be dropped, except those on really small tables which you expect to grow later.<br /><br /><b>Seldom Used Indexes on Heavily Written Tables</b>:&nbsp; as a general rule, if you're not using an index twice as often as it's written to, you should probably drop it.&nbsp; This query is a little more conservative than that.<br /><br /><b>Really Large, Seldom-Used Indexes</b>: these indexes get used, but not that often, and they're taking up a lot of RAM and storage space.&nbsp;&nbsp; Consider dropping them after some thought about why they were added.<br /><br /><b>Large Non-BTree Indexes On Busy Tables</b>:&nbsp; as a rule, non-BTree indexes like GiST and GIN don't accurately report usage stats.&nbsp; As a result, we can't check how often they're used, just how big they are and if they're attached to tables which get a lot of writes.&nbsp; This list of indexes should be very judiciously pruned.<br /><br />Happy pruning!";}i:23;a:6:{s:5:"title";s:74:"Andreas Scherbaum: GSoC 2014: Implementing clustering algorithms in MADlib";s:4:"link";s:105:"http://andreas.scherbaum.la/blog/archives/882-GSoC-2014-Implementing-clustering-algorithms-in-MADlib.html";s:11:"description";s:1120:"<div class="serendipity_authorpic"><img alt="Author" src="http://andreas.scherbaum.la/blog/templates/default/img/Andreas__ads__Scherbaum.jpg" title="Andreas 'ads' Scherbaum" /><br /><span>Andreas 'ads' Scherbaum</span></div><p>Happy to announce that Maxence Ahlouche has been accepted by both the <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9995&amp;entry_id=882" title="http://www.postgresql.org">PostgreSQL Project</a> and by <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9996&amp;entry_id=882" title="https://www.google-melange.com/gsoc/homepage/google/gsoc2014">Google</a> to implement clustering algorithms in <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9997&amp;entry_id=882" title="http://www.madlib.net/">MADlib</a> during the Google Summer of Code 2014.</p>

<p>This project is mentored by Atri Sharma (former GSoC student) and me, with technical help from <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9998&amp;entry_id=882" title="http://www.gopivotal.com/">Pivotal</a> (product owner of MADlib).</p>

<p>Looking forward to another successful GSoC year!</p>";s:4:"guid";s:55:"http://andreas.scherbaum.la/blog/archives/882-guid.html";s:7:"pubdate";s:29:"Thu, 01 May 2014 23:22:14 GMT";s:7:"summary";s:1120:"<div class="serendipity_authorpic"><img alt="Author" src="http://andreas.scherbaum.la/blog/templates/default/img/Andreas__ads__Scherbaum.jpg" title="Andreas 'ads' Scherbaum" /><br /><span>Andreas 'ads' Scherbaum</span></div><p>Happy to announce that Maxence Ahlouche has been accepted by both the <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9995&amp;entry_id=882" title="http://www.postgresql.org">PostgreSQL Project</a> and by <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9996&amp;entry_id=882" title="https://www.google-melange.com/gsoc/homepage/google/gsoc2014">Google</a> to implement clustering algorithms in <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9997&amp;entry_id=882" title="http://www.madlib.net/">MADlib</a> during the Google Summer of Code 2014.</p>

<p>This project is mentored by Atri Sharma (former GSoC student) and me, with technical help from <a href="http://andreas.scherbaum.la/blog/exit.php?url_id=9998&amp;entry_id=882" title="http://www.gopivotal.com/">Pivotal</a> (product owner of MADlib).</p>

<p>Looking forward to another successful GSoC year!</p>";}i:24;a:6:{s:5:"title";s:44:"Marko Tiikkaja: Why I consider USING harmful";s:4:"link";s:69:"http://johtopg.blogspot.com/2014/05/why-i-consider-using-harmful.html";s:11:"description";s:397:"Often on the topic of the different JOIN syntaxes in SQL I mention that I consider ON the only reasonable way to specify JOIN clauses in production code, and I get asked for details. It always takes me a while to recall the specific problem and to come up with a plausible scenario, so I figured it would be easier to document this once.

Suppose you have a database containing information about";s:4:"guid";s:69:"tag:blogger.com,1999:blog-265587150912543268.post-2799419518282455946";s:7:"pubdate";s:29:"Thu, 01 May 2014 00:31:00 GMT";s:7:"summary";s:397:"Often on the topic of the different JOIN syntaxes in SQL I mention that I consider ON the only reasonable way to specify JOIN clauses in production code, and I get asked for details. It always takes me a while to recall the specific problem and to come up with a plausible scenario, so I figured it would be easier to document this once.

Suppose you have a database containing information about";}i:25;a:6:{s:5:"title";s:70:"Gurjeet Singh: Postgres Hibernator: Reduce Planned Database Down Times";s:4:"link";s:95:"http://gurjeet.singh.im/blog/2014/04/30/postgres-hibernator-reduce-planned-database-down-times/";s:11:"description";s:6257:"<p><em>TL;DR</em>: Reduce planned database down times by about 97%, by using <a href="https://github.com/gurjeet/pg_hibernator">Postgres Hibernator</a>.</p>

<p>DBAs are often faced with the task of performing some maintenance on their database server(s) which requires shutting down the database. The maintenance may involve anything from a database minor-version upgrade, to a hardware upgrade. One ugly side-effect of restarting the database server/service is that all the data currently in database server&rsquo;s memory will be all lost, which was painstakingly fetched from disk and put there in response to application queries over time. And this data will have to be rebuilt as applications start querying database again. The query response times will be very high until all the &ldquo;hot&rdquo; data is fetched from disk and put back in memory again.</p>

<p>People employ a few tricks to get around this ugly truth, which range from running a <code>select * from app_table;</code>, to <code>dd if=table_file ...</code>, to using specialized utilities like <a href="https://github.com/klando/pgfincore">pgfincore</a> to prefetch data files into OS cache. Wouldn&rsquo;t it be ideal if the database itself could save and restore its memory contents across restarts!</p>

<p>The <a href="https://github.com/gurjeet/pg_hibernator">Postgres Hibernator</a> extension for <a href="http://www.postgresql.org">Postgres</a> performs the automatic save and restore of database buffers, integrated with database shutdown and startup, hence reducing the durations of database maintenance windows, in effect increasing the uptime of your applications.</p>

<p>Postgres Hibernator automatically saves the list of shared buffers to the disk on database shutdown, and automatically restores the buffers on database startup. This acts pretty much like your Operating System&rsquo;s hibernate feature, except, instead of saving the contents of the memory to disk, Postgres Hibernator saves just a list of block identifiers. And it uses that list after startup to restore the blocks from data directory into Postgres' <a href="http://www.postgresql.org/docs/current/static/runtime-config-resource.html#GUC-SHARED-BUFFERS">shared buffers</a>.</p>

<p>As explained in my <a href="http://gurjeet.singh.im/blog/2014/02/03/introducing-postgres-hibernator/">earlier post</a>, this extension is a set-it-and-forget-it solution, so, to get the benefits of this extension there&rsquo;s not much a DBA has to do, except install it.</p>

<p>Ideal database installations that would benefit from this extension would be the ones with a high cache-hit ratio. With Postgres Hibernator enabled, your database would start cranking pre-maintenance TPS (Transactions Per Second) within first couple of minutes after a restart.</p>

<p>As can be seen in the chart below, the database ramp-up time drops dramatically when Postgres Hibernator is enabled. The sooner the database TPS can reach the steady state, the faster your applications can start performing at full throttle.</p>

<p>The ramp-up time is even shorter if you wait for the Postgres Hibernator processes to end, before starting your applications.</p>

<h2>Sample Runs</h2>

<p><img alt="Postgres Hibernator Comparison" src="http://gurjeet.singh.im/../images/pg_hibernator_comparison.png" /></p>

<p>As is quite evident, waiting for Postgres Hibernator to finish loading the data blocks before starting the application yeilds a 97% impprovement in database ramp-up time (2300 seconds to get to 122k TPS without Postgres Hibernator vs. 70 seconds).</p>

<h3>Details</h3>

<p>Please note that this is not a real benchmark, just something I developed to showcase this extension at its sweet spot.</p>

<p>The full source of this mini benchmark is available with the source code of the Postgres Hibernator, at its <a href="https://github.com/gurjeet/pg_hibernator">Git repo</a>.</p>

<p><code>
Hardware: MacBook Pro 9,1
OS Distribution: Ubuntu 12.04 Desktop
OS Kernel: Linux 3.11.0-19-generic
RAM: 8 GB
Physical CPU: 1
CPU Count: 4
Core Count: 8
pgbench scale: 260 (~ 4 GB database)
</code></p>

<p>Before every test run, except the last (&lsquo;DB-only restart; No Hibernator&rsquo;), the Linux OS caches are dropped to simulate an OS restart.</p>

<p>In &lsquo;First Run&rsquo;, the Postgres Hibernator is enabled, but since this is the first ever run of the database, Postgres Hibernator doesn&rsquo;t kick in until shutdown, to save the buffer list.</p>

<p>In &lsquo;Hibernator w/ App&rsquo;, the application (pgbench) is started right after database restart. The Postgres Hibernator is restoring the data blocks to shared buffers while the application is also querying the database.</p>

<p>In the &lsquo;App after Hibernator&rsquo;, the application is started <em>after</em> the Postgres Hibernator has finished reading database blocks. This took 70 seconds for reading the ~4 GB database.</p>

<p>In &lsquo;DB-only restart; No Hibernator` run, the OS caches are not dropped, but just the database service is restarted. This simulates database minor version upgrades, etc.</p>

<p>It&rsquo;s interesting to monitor the <code>bi</code> column in <code>vmstat 10</code> output, while these tests are running. In &lsquo;First Run&rsquo; and &lsquo;DB-only restart&rsquo; cases this column&rsquo;s values stayed between 2000 and 5000 until all data was in shared buffers, and then it dropped to zero (meaning that all data has been read from disk into shared buffers). In &lsquo;Hibernator w/ app&rsquo; case, this column&rsquo;s value ranges from 15,000 to 65,000, with an average around 25,000. it demonstrates that Postgres Hibernator&rsquo;s <code>Block Reader</code> process is aggressively reading the blocks from data directory into the shared buffers, but apparently not fast enough because the applicaion&rsquo;s queries are causing random reads from disk, which interfere with the sequential scans that Postgres Hibernator is trying to perform.</p>

<p>And finally, in &lsquo;App after Hibernator&rsquo; case, this column consistently shows values between 60,000 and 65,000, implying that in absence of simultaneous application load, the <code>Block Reader</code> can read data into shared buffers much faster.</p>";s:4:"guid";s:94:"http://gurjeet.singh.im/blog/2014/04/30/postgres-hibernator-reduce-planned-database-down-times";s:7:"pubdate";s:29:"Wed, 30 Apr 2014 17:23:57 GMT";s:7:"summary";s:6257:"<p><em>TL;DR</em>: Reduce planned database down times by about 97%, by using <a href="https://github.com/gurjeet/pg_hibernator">Postgres Hibernator</a>.</p>

<p>DBAs are often faced with the task of performing some maintenance on their database server(s) which requires shutting down the database. The maintenance may involve anything from a database minor-version upgrade, to a hardware upgrade. One ugly side-effect of restarting the database server/service is that all the data currently in database server&rsquo;s memory will be all lost, which was painstakingly fetched from disk and put there in response to application queries over time. And this data will have to be rebuilt as applications start querying database again. The query response times will be very high until all the &ldquo;hot&rdquo; data is fetched from disk and put back in memory again.</p>

<p>People employ a few tricks to get around this ugly truth, which range from running a <code>select * from app_table;</code>, to <code>dd if=table_file ...</code>, to using specialized utilities like <a href="https://github.com/klando/pgfincore">pgfincore</a> to prefetch data files into OS cache. Wouldn&rsquo;t it be ideal if the database itself could save and restore its memory contents across restarts!</p>

<p>The <a href="https://github.com/gurjeet/pg_hibernator">Postgres Hibernator</a> extension for <a href="http://www.postgresql.org">Postgres</a> performs the automatic save and restore of database buffers, integrated with database shutdown and startup, hence reducing the durations of database maintenance windows, in effect increasing the uptime of your applications.</p>

<p>Postgres Hibernator automatically saves the list of shared buffers to the disk on database shutdown, and automatically restores the buffers on database startup. This acts pretty much like your Operating System&rsquo;s hibernate feature, except, instead of saving the contents of the memory to disk, Postgres Hibernator saves just a list of block identifiers. And it uses that list after startup to restore the blocks from data directory into Postgres' <a href="http://www.postgresql.org/docs/current/static/runtime-config-resource.html#GUC-SHARED-BUFFERS">shared buffers</a>.</p>

<p>As explained in my <a href="http://gurjeet.singh.im/blog/2014/02/03/introducing-postgres-hibernator/">earlier post</a>, this extension is a set-it-and-forget-it solution, so, to get the benefits of this extension there&rsquo;s not much a DBA has to do, except install it.</p>

<p>Ideal database installations that would benefit from this extension would be the ones with a high cache-hit ratio. With Postgres Hibernator enabled, your database would start cranking pre-maintenance TPS (Transactions Per Second) within first couple of minutes after a restart.</p>

<p>As can be seen in the chart below, the database ramp-up time drops dramatically when Postgres Hibernator is enabled. The sooner the database TPS can reach the steady state, the faster your applications can start performing at full throttle.</p>

<p>The ramp-up time is even shorter if you wait for the Postgres Hibernator processes to end, before starting your applications.</p>

<h2>Sample Runs</h2>

<p><img alt="Postgres Hibernator Comparison" src="http://gurjeet.singh.im/../images/pg_hibernator_comparison.png" /></p>

<p>As is quite evident, waiting for Postgres Hibernator to finish loading the data blocks before starting the application yeilds a 97% impprovement in database ramp-up time (2300 seconds to get to 122k TPS without Postgres Hibernator vs. 70 seconds).</p>

<h3>Details</h3>

<p>Please note that this is not a real benchmark, just something I developed to showcase this extension at its sweet spot.</p>

<p>The full source of this mini benchmark is available with the source code of the Postgres Hibernator, at its <a href="https://github.com/gurjeet/pg_hibernator">Git repo</a>.</p>

<p><code>
Hardware: MacBook Pro 9,1
OS Distribution: Ubuntu 12.04 Desktop
OS Kernel: Linux 3.11.0-19-generic
RAM: 8 GB
Physical CPU: 1
CPU Count: 4
Core Count: 8
pgbench scale: 260 (~ 4 GB database)
</code></p>

<p>Before every test run, except the last (&lsquo;DB-only restart; No Hibernator&rsquo;), the Linux OS caches are dropped to simulate an OS restart.</p>

<p>In &lsquo;First Run&rsquo;, the Postgres Hibernator is enabled, but since this is the first ever run of the database, Postgres Hibernator doesn&rsquo;t kick in until shutdown, to save the buffer list.</p>

<p>In &lsquo;Hibernator w/ App&rsquo;, the application (pgbench) is started right after database restart. The Postgres Hibernator is restoring the data blocks to shared buffers while the application is also querying the database.</p>

<p>In the &lsquo;App after Hibernator&rsquo;, the application is started <em>after</em> the Postgres Hibernator has finished reading database blocks. This took 70 seconds for reading the ~4 GB database.</p>

<p>In &lsquo;DB-only restart; No Hibernator` run, the OS caches are not dropped, but just the database service is restarted. This simulates database minor version upgrades, etc.</p>

<p>It&rsquo;s interesting to monitor the <code>bi</code> column in <code>vmstat 10</code> output, while these tests are running. In &lsquo;First Run&rsquo; and &lsquo;DB-only restart&rsquo; cases this column&rsquo;s values stayed between 2000 and 5000 until all data was in shared buffers, and then it dropped to zero (meaning that all data has been read from disk into shared buffers). In &lsquo;Hibernator w/ app&rsquo; case, this column&rsquo;s value ranges from 15,000 to 65,000, with an average around 25,000. it demonstrates that Postgres Hibernator&rsquo;s <code>Block Reader</code> process is aggressively reading the blocks from data directory into the shared buffers, but apparently not fast enough because the applicaion&rsquo;s queries are causing random reads from disk, which interfere with the sequential scans that Postgres Hibernator is trying to perform.</p>

<p>And finally, in &lsquo;App after Hibernator&rsquo; case, this column consistently shows values between 60,000 and 65,000, implying that in absence of simultaneous application load, the <code>Block Reader</code> can read data into shared buffers much faster.</p>";}i:26;a:6:{s:5:"title";s:66:"Tim van der Linden: PostgreSQL: A full text search engine - Part 1";s:4:"link";s:64:"http://shisaa.jp/postset/postgresql-full-text-search-part-1.html";s:11:"description";s:35467:"<h3>Preface</h3>
<p>PostgreSQL, the database of miracles, the RDBMS of wonders.</p>
<p>People who have read my stuff before know that I am a fan of the blue-ish elephant and I greatly entrust it with my data. 
For reasons why, I invite you to read the "Dolphin ass-whopping" part of the <a href="http://shisaa.jp/postset/mailserver-2.html" title="Second chapter of the mail setup series.">second chapter</a> of my mail server setup series.</p>
<p>But what some of you may not know is that PostgreSQL is capable of much more then simply storing and retrieving your data.
Well, that is actually not entirely correct...you are <em>always</em> storing and retrieving data.
A more correct way to say it is that PostgreSQL is capable of storing all <em>kinds</em> of data and gives you all <em>kinds</em> of ways to retrieve it.
It is not limited to <em>storing</em> boring stuff like "VARCHAR" or "INT". Neither is it limited to retrieving and <em>comparing</em> with boring
operators like "=", "ILIKE" or "~". </p>
<p>For instance, are you familiar with PostgreSQL's <em>"tsvector"</em> data type? Or the <em>"tsquery"</em> type? Or what these two represent? No?
Well, diddlydangeroo, then by all means, keep reading, because that is exactly what this series is all about!</p>
<p>In the following three chapters I would like to show you how you can configure PostgreSQL to be a batteries included, blazing fast, competition crunching, full text search engine.</p>
<h3>But, I can already search strings of text with PostgreSQL!</h3>
<p>Hmm, that is very correct. But the basic operators you have at your disposal are limited. </p>
<p>Let me demonstrate.</p>
<p>Imagine we would have a table, called "phraseTable" containing thousands of strings, all saved in a regular, old VARCHAR column named "phrase".
Now we would like to find the string <em>"An elephant a day keeps the dolphins at bay."</em>.
We do not fully remember the above string, but we do remember it had the word "elephant" in it.
With regular SQL you could use the "LIKE" operator to try and find a matching substring. The resulting query would look something like this:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">LIKE</span> <span class="s1">'%elephant%'</span><span class="p">;</span>
</pre></div>


<p>It would work, you render any index on the table mute when using front <em>and</em> back wildcards, but it would work.
Now imagine a humble user would like to find the same string but their memory is bad, they thought the word elephant was capitalized, because it may refer to PostgreSQL, of course.
The query would become this:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">LIKE</span> <span class="s1">'%Elephant%'</span><span class="p">;</span>
</pre></div>


<p>And as a result, you get back zero records.</p>
<p>"But wait!", you shout, "I am a smart ass, there is a solution to this!". And you are correct: the ILIKE operator.
The "I" stands for Insensitive...as in <em>Case Insensitive</em>. So you change the query:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">ILIKE</span> <span class="s1">'%Elephant%'</span><span class="p">;</span>
</pre></div>


<p>And now you will get back a result. Good for you.</p>
<p>A day goes by and the same user comes back and wishes to find this string again. But, his memory still being bad and all, he thought there where multiple elephants keeping the dolphins at bay, because, you know, pun. So the query, you altered yesterday, now reads:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">ILIKE</span> <span class="s1">'%Elephants%'</span><span class="p">;</span>
</pre></div>


<p>And...now the query will return zero results.</p>
<p>"Ha!", you shout in my general direction. "I am a master of Regular Expressions! I shall fix thay query!".</p>
<p>No, you shall <em>not</em> fix my query. Never, ever go smart on my derrire by throwing a regular expression in the mix to solve a database lookup problem. It is unreadable, un-scalable and fits only one solution perfectly-ish. And, not to forget, is <em>slow as hell</em> for it not only ignores any index you have set, it also asks more of the database engine then a LIKE or ILIKE.</p>
<p>Let me put an end to this and tell you that I am afraid there are no more (scalable) smart ass tricks left to perform and the possibilities to search text with regular, build-in operators are exhausted.</p>
<p>You agree? Yes? Good! So, enter "<em>full text search</em>"!</p>
<h3>Full text search?</h3>
<p>But before we delve into the details of the PostgreSQL implementation, let us take a step back and first see what exactly a full text search engine is.</p>
<p>Short version: A full text search engine is a system that can retrieve documents, or parts of documents, based on natural language searching.</p>
<p><em>Natural language</em> means the living, breathing language we humans use. And as you know, human language can be complex and above all <em>ambiguous</em>.</p>
<p>Consider yourself in the situation where you knew, for sure, that you have read an interesting article about elephants in the latest edition of "Your Favorite Elephant Magazine".
You liked it so much that you want to show it to your best friend, who happens to be an elephant lover too.
The only bummer is, you cannot remember the title, but you do remember it has an interesting sentence in it.</p>
<p>So what do you do? First you quote the sentence in your mind: "The best elephants have a blue skin color.".
Next, you pick up the latest edition and you start <em>searching</em>, flipping through the pages, skimming for that sentence.</p>
<p>After a minute or two you shout: "Dumbo!, I have found the article!". You read the sentence out loud: "The best Elephants bear a blue skin tone.".
You are happy with yourself, call up your friend and tell him that you will be over right away to show him that specific article.</p>
<p>One thing you forgot to notice was that the sentence in your head, and the sentence that was actually printed where <em>different</em>, but your brain (which is trained in this natural stuff), sees them as the same.
How did that work? Well, your brain used its internal <em>synonym</em> list and <em>thesaurus</em> to link the different words together, making them the same thing, just written differently:</p>
<ul>
<li>"elephants" is the same as "Elephants"</li>
<li>"have" is the same as "bear"</li>
<li>"skin color" is the same "skin tone"</li>
</ul>
<p>Without noticing it, you have just completed a full text search using natural language algorithms, your magazine as the database, your brain as the engine.</p>
<h3>But how does such a natural language lookup work...on an unnatural computer?</h3>
<p>What a perfectly timed question, I was just getting to that.</p>
<p>Now that you have a basic understanding of what natural language searching is, how does one port this idea to a stupid, binary ticking tin box?
By dissecting the process we do in our brains, lay it out in several programmable steps and concepts. 
Such a process, run by computers, will never be as good on the <em>natural</em> part as our brains are, but it is certainly a lot faster with flipping and skimming through the magazine pages.</p>
<p>Let us look at how a computer, regardless of which program, platform or engine you use, would go about being "natural" when searching for strings of text.</p>
<p>To speed up the search process, a full text search engine will never search through the actual document itself.
That is how we humans would do it, and that is slow and (for our eyes) error prone. Before a document can be searched through with a full text search engine, it has to be parsed into a list of words first.
The parsing is where the magic happens, this is our attempt at programming the natural language process. Once the document is parsed, the parsed state is saved. Depending on your database model, you can save the parsed state together with a reference to the original document for later retrieval.</p>
<p>Note that a document, in this context, is simply a big collection of words contained within a file. The engine does not care, and most of the time does not know, about what kind of file (plain text, LibreOffice document, HTML file, ...) it is handling or what the files structure is. It simply looks at all the readable words inside of the file.</p>
<p>So how does the parsing work? Parsing, in this regard, is all about compressing the text found in a document. Cutting down the word count to the least possible, so later, when a user searches, the engine has to plow through fewer words. This compressing, in most engines, is done in roughly three steps.</p>
<h4>Eliminate casing</h4>
<p>The first step in the compression process is the elimination of casing - keeping only the lower case versions of a word.
If you would keep a search case sensitive, then "The ElEphAnt" would not match "the elephant", but generally you do want a match to happen.
The user will many times not care (or not know) about casing in a full text search.</p>
<h4>Remove stop words</h4>
<p>The following step is the removal of words that do not add any searchable value to the text and are seldom searched for.
These words are known as "stop words", a term first coined by Hans Peter Luhn, a renowned IBM computer scientist who specialized in the retrieval and indexing of information stored in computer systems.</p>
<p>The list of stop words is not limited to simply ones like "and" or "the". There is an extensive list of hundreds and hundreds of words which are generally considered to be of little value in a search context.
A (very) short list of stop words: her, him, the, also, each, was, we, after, been, they, would, up, from, only, you, while, ... .</p>
<h4>Remove synonyms, employing a thesaurus and perform stemming</h4>
<p>The last part in the compacting of our to-be-indexed document is removing words that have the same meaning and perform stemming.
Synonym lookups are used for removing <em>words</em> of the same meaning where as thesaurus lookups are used to compact whole <em>phrases</em> with similar meaning.</p>
<p>Only one instance of all the synonyms, thesaurus phrases and case eliminations is stored, the surviving word is referred to as a <em>lexeme</em>, the smallest, meaningful word.
The lexemes that are stored usually (depending on the engine you use) get an accompanying list of (alpha)numeric values stored alongside. Two types of (alpha)numeric values can be stored in case of PostgreSQL:</p>
<ul>
<li>The first type are pure numerical and represent pointer(s) to where the word occurs in the original document.</li>
<li>The second type is pure alphabetical (actually only capital A,B,C,D) and represent the weight a certain lexeme has. </li>
</ul>
<p>Do not worry to much about these two (alpha)numerical values for now, we will get to that later.</p>
<p>Next, let us get practical and start to actually use PostgreSQL to see how all of this works. </p>
<h3>The tsvector</h3>
<p>As PostgreSQL is an <em>extendable</em> database engine, two new data types where added to make full text search possible, as you have seen in the beginning.
One of them is called <em>tsvector</em>, "ts" for <em>t</em>ext <em>s</em>earch and "vector", which is analogous with the generic programming data type "vector".
It is the container in which the result of the parsing is eventually stored.</p>
<p>Let me show you an example of such a tsvector, as presented by PostgreSQL on querying.
Imagine a document with the following string of text inside: <em>"The big blue elephant jumped over the crippled blue dolphin."</em>.
A perfectly normal sentence, elephants jump over dolphins all the time.</p>
<p>Without bothering about how to do it, if we let PostgreSQL parse this string, we will get the following tsvector stored in our record:</p>
<div class="code"><pre><span class="s1">'big'</span><span class="p">:</span><span class="mi">2</span> <span class="s1">'blue'</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span><span class="mi">9</span> <span class="s1">'crippl'</span><span class="p">:</span><span class="mi">8</span> <span class="s1">'dolphin'</span><span class="p">:</span><span class="mi">10</span> <span class="s1">'eleph'</span><span class="p">:</span><span class="mi">4</span> <span class="s1">'jump'</span><span class="p">:</span><span class="mi">5</span>
</pre></div>


<p>You will notice a few things about this vector, let me go over them one by one.</p>
<ul>
<li>First, you recognize the structure of a vector-ish data type. Hence the name "tsvector".</li>
<li>Next, the numbers behind the lexemes themselves, like I said before, represent the pointer(s) to that word. Notice the word "blue" in particular, it has two pointers for the two occurrences in the string.</li>
<li>And last, notice how some lexemes do not even look like English words at all. The lexeme "crippl" or "eleph" do not mean anything, to us humans anyway. These are the surviving lexemes of "cripple" and "elephant". PostgreSQL has stemmed and reduced the words to match all possible variants. The lexeme "crippl", for example, matches "cripple", "crippled", "crippling", "cripples", ... .</li>
</ul>
<p>Note that the above example is the simplest of full text search parsing results, we did not add any weights nor did we employ a thesaurus (or an advanced dictionary) to get back a more efficient compressing.</p>
<p>Now that we are dwelling inside of PostgreSQL, I can elaborate a bit more about how the parsing works exactly.
As we have seen above, it happens in roughly three steps. But I intentionally neglected to say that with PostgreSQL, there is an intermediate state between the word and the resulting lexeme.</p>
<p>When PostgreSQL parses the string of text it goes over them and first <em>categorizes</em> each word into sections like "stop words", "plural words", "synonyms", ... .
Once the words are broken down into categories, we refer to them as <em>tokens</em>. This is the intermediate state.
For a token to become a lexeme, PostgreSQL will consult a set of defined <em>dictionaries</em> for each category to try and find a match.
If a match is found, the dictionary will propose a lexeme. This lexeme is the one that will finally be put in the vector as the parsed result.</p>
<p>If the dictionaries did not find a match, the word is discarded. The one exception to this are the "stop words", if a word matches a stop word, it will be discarded instead of kept.</p>
<p>Let us now get our hands dirty and setup a quick testing database and rig it up with the phraseTable table we have been using in our journey so far.
But instead of a varchar column, this table will contain a tsvector type for we will unleash to power of Full Text Search!</p>
<p>Note: I am assuming you have at least PostgreSQL 9.1 or higher. This post was written with PostgreSQL 9.3 in mind.</p>
<p>So connect to your PostgreSQL install and create the database:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">DATABASE</span> <span class="n">phrases</span><span class="p">;</span>
</pre></div>


<p>Do not worry to much about the ownership of this database nor the ownership of its tables, you can discard it whole later.
Now, switch over to the database:</p>
<div class="code"><pre><span class="err">\</span><span class="k">c</span> <span class="n">phrases</span>
</pre></div>


<p>And create the phraseTable table:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">phrases</span> <span class="p">(</span><span class="n">phrase</span> <span class="n">tsvector</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">);</span>
</pre></div>


<p>Okay, simple enough. We now have a tiny database, with a table containing one column of type <em>tsvector</em>.</p>
<p>Let us insert a parsed vector into the table.
Again, without employing a thesaurus or any other tools, we only use the built-in, default configuration to parse a string and save it as a vector.</p>
<p>Let us insert the vector, shall we?</p>
<div class="code"><pre><span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">));</span>
</pre></div>


<p>That was easy enough. Most of what you see is simple, regular SQL with one new kid on the block: "<em>to_tsvector</em>".
The latter is a <em>function</em> that is shipped with PostgreSQL's Full Text Search extension and it does what its name suggests: it takes a string of text and converts it into a <em>tsvector</em>.</p>
<p>As a first argument to this function you can optionally input the full text search <em>configuration</em> you wish the parser to use. The default is <em>"english"</em>, so I could have omitted it from the argument list.
This configuration holds everything that PostgreSQL will employ to do all of the parsing, including a basic dictionary, stop word list, ... .
PostgreSQL has some default settings, which many times are good enough. The 'english' configuration is such an example.</p>
<p>In the next chapter we will delve <em>deep</em> into creating our own configuration, for now just take it for granted.</p>
<p>If we query the result, with a simple select, it will return our newly created vector:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">from</span> <span class="n">phraseTable</span>
</pre></div>


<p>Will return:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippl'</span>:8 <span class="s1">'dolphin'</span>:10 <span class="s1">'eleph'</span>:4 <span class="s1">'jump'</span>:5
</pre></div>


<p>Now remember that I talked about the second kind of value we could store alongside the numeric pointers, the <em>weights</em>? Let us take a deeper look into that now.</p>
<p>First, weights are not mandatory and only give you an extra tool for ranking the results afterwards.
They are nothing more then a label you can put on a lexeme to group it together. With weights you could, for example, reflect the structure the original document had.
You may wish to put a higher weight on lexemes that come from a title element and a lower weight on those from the body text.</p>
<p>PostgreSQL knows four weight labels <em>A</em>, <em>B</em>, <em>C</em>, <em>D</em>. The lowest in rank being <em>D</em>. In fact, if you do not define any weights to the lexemes inside a tsvector, all of them will implicitly get a <em>D</em> assigned.
If all the lexemes in a tsvector carry a <em>D</em>, it is omitted from display when printing the tsvector, simply for readability.
The above query result could thus also be written as:</p>
<div class="code"><pre><span class="s1">'big'</span>:2D <span class="s1">'blue'</span>:3D,9D <span class="s1">'crippl'</span>:8D <span class="s1">'dolphin'</span>:10D <span class="s1">'eleph'</span>:4D <span class="s1">'jump'</span>:5D
</pre></div>


<p>It is <em>exactly</em> the same result, but unnecessarily verbose.</p>
<p>I told you, in the very beginning, that a full text engine does not know or care about the structure of a document, it only sees the words.
So how can it then put labels on lexemes based on a document structure that it does not know?</p>
<p>It cannot.</p>
<p>It is your job to provide PostgreSQL with label information when building the tsvector.
Up until now we have been working with simple text strings, which contain no hierarchy. 
If you wish to reflect your original document structure by using weights, you will have to preprocess the document and construct your <em>to_tsvector</em> query manually.</p>
<p>Just for demonstration purposes, we could, of course, assign weights to the lexemes inside a simple text string.
The process of weight assignment is trivial. PostgreSQL gives you the appropriately named <em>setweight</em> function for this.
This function accepts a tsvector as the first argument and a weight label as the second.</p>
<p>To demonstrate, let me update our record and give all the lexemes in our famous sentence a <em>A</em> weight label:</p>
<div class="code"><pre><span class="k">UPDATE</span> <span class="n">phraseTable</span> <span class="k">SET</span> <span class="n">phrase</span> <span class="o">=</span> <span class="n">setweight</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span><span class="s1">'A'</span><span class="p">);</span>
</pre></div>


<p>If we now query this table, the result will be this:</p>
<div class="code"><pre><span class="s1">'big'</span>:2A <span class="s1">'blue'</span>:3A,9A <span class="s1">'crippl'</span>:8A <span class="s1">'dolphin'</span>:10A <span class="s1">'eleph'</span>:4A <span class="s1">'jump'</span>:5A
</pre></div>


<p>Simple, right?</p>
<p>One more for fun. What if you wanted to assign different weights to the lexemes?
For this, you have to concatenate several <em>setweight</em> functions together.
An example query would look something like this:</p>
<div class="code"><pre><span class="k">update</span> <span class="n">phraseTable</span> <span class="k">set</span> <span class="n">phrase</span><span class="o">=</span>
<span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'the big blue elephant'</span><span class="p">),</span> <span class="s1">'A'</span><span class="p">)</span> <span class="o">||</span>
<span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'jumped over the'</span><span class="p">),</span> <span class="s1">'B'</span><span class="p">)</span> <span class="o">||</span>
<span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'crippled blue dolphin.'</span><span class="p">),</span> <span class="s1">'C'</span><span class="p">);</span>
</pre></div>


<p>The result:</p>
<div class="code"><pre><span class="s1">'big'</span>:2A <span class="s1">'blue'</span>:3A,7C <span class="s1">'crippl'</span>:6C <span class="s1">'dolphin'</span>:8C <span class="s1">'eleph'</span>:4A <span class="s1">'jump'</span>:5B
</pre></div>


<p>Not very usefull, but it demonstrates the principle.</p>
<p>If the documents you wish to index have a fixed structure, many times the table that will hold the tsvectors for these documents will reflect that structure with appropriately named columns.
For example, if your document would always have a title, body text and a footer, you could create a table which contains three tsvector type columns, named after each structure type.
When you parse the document and construct the query, you could assign all lexemes that will be stored in the title column with an <em>A</em> label, in the body column with a <em>B</em> and in the footer column with a <em>C</em>.</p>
<p>Okay, that is enough about weights. Simply remember that they give you extra power to influence the search result ranking, if needed.</p>
<p>We now have a table with a decent tsvector inside. The data is in, so to speak. But what can we do with it now?</p>
<p>Well, let us try to retrieve and compare it, shall we!</p>
<h3>The tsquery</h3>
<p>You could, of course, simply retrieve the results stored in a <em>tsvector</em> by doing a <em>SELECT</em> on the column. However, you have no way of filtering out the results using the operators we have seen before (LIKE, ILIKE). Even if you could use them, you would still run into the same kind of problems as before. You would still have a user who will search for a synonym or search for a plural form of the stemmed lexeme actually stored in the vector.</p>
<p>So how do we query it?</p>
<p>Step in <em>tsquery</em>. What is it? It is a data type that gives us extra tools to <em>query</em> the full <em>text search</em> vector.</p>
<p>Pay attention to the fact that we do not call <em>tsquery</em> a set of extra <em>operators</em> but we call it a <em>data type</em>. This is very important to understand.
With <em>tsquery</em> we can construct search <em>predicates</em>, which can search through a <em>tsvector</em> type and can employ specially designed indexes to speed up the process. </p>
<p>Let me throw a query at you that will try to find the word "elephants" in our favorite string using <em>tsquery</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants'</span><span class="p">);</span>
</pre></div>


<p>Try it out, this will give you back the same result set we had before. Let me explain what just happened.</p>
<p>As you can see, there is again a new function introduced: <em>to_tsquery</em> and it is almost identical to its <em>to_tsvector</em> counterpart.
The function <em>to_tsquery</em> takes one argument, a string containing the <em>tokens</em> (not the words, not the lexemes) you wish to search for.</p>
<p>Let us first look a bit more at this one. Say, for instance, you wish to find two tokens of the sentence inside your database.
Your first instinct would be to query this the following way:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants blue'</span><span class="p">);</span>
</pre></div>


<p>Unfortunately, this will throw an error stating there is a syntax error. Why? Because the string your provided as an argument is malformed.
The <em>to_tsquery</em> helper function does not accept a simple string of text, it needs a string of tokens <em>separated by operators</em>.
The operators at your disposal are the regular <em>&amp;</em> (AND), <em>|</em> (OR) and <em>!</em> (NOT). Note that the <em>!</em> operator <em>needs</em> the <em>&amp;</em> or the <em>|</em> operator.</p>
<p>It then goes and creates a true <em>tsquery</em> to retrieve the results. Let us try this query again, but with correct syntax this time:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>Perfect! This will return, once more, the same result as before. You could even use parenthesis inside your string argument to enforce grouping if desired.
Like I said before, what this helper function does is translate its input (the tokens in the string) into actual lexemes. After that, it tries to match this result with the lexemes present in the tsvector.</p>
<p>We still have a problem if we would let a user type her or his search string into an interface search box and feed it to <em>to_tsquery</em>, for a user does not know about the operators they need to use.
Luckily for us, there is another help function, the <em>plainto_tsquery</em> which takes care of exactly that problem: convert an arbitrary string of text into lexemes.</p>
<p>Let me demonstrate:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">plainto_tsquery</span><span class="p">(</span><span class="s1">'elephants blue'</span><span class="p">);</span>
</pre></div>


<p>Notice we did not separate the words with operators, now it is a simple search string. In fact, <em>plainto_tsquery</em> converts it to a list of lexemes separated by an <em>&amp;</em> (AND) operator.
The only drawback is that this function can only separate the lexemes with an <em>&amp;</em> operator.
If you wish to have something other then the <em>&amp;</em> operator, you will have to stick to <em>to_tsquery</em>.</p>
<p>A word of caution though, the <em>plainto_tsvector</em> may seem interesting, but is most of the time not a general solution for building a higher level search interface. When you are building, say, a web application that contains a full text search box, there are a few more steps between the string entered in that box, and the final query that will be preformed. </p>
<p>Building a web application and safely handling user input that travels to the database is a separate story and <em>way</em> beyond the scope of this post, but you will have to build your own parser that sits between the user input and the query.</p>
<p>If you would play dumb and accept the fact that your interface would only allow to enter a string in the search box (no operators, no grouping, ...) then you still need to send over the user input using query parameters <em>and</em> you need to make sure that the parameter sent over is a string. This, of course, is not really a parser, this is more basic, sane database handling on the web. </p>
<p>As tempting (and simple) it might seem to be to build a query like that, it will probably frustrate your end users. The reason why is because as I mentioned before, the <em>plainto_tsquery</em> accepts a string, but will chop the string into separate lexemes and put the <em>&amp;</em> operator between them. This means that <em>all</em> the words entered by the user (or at least their resulting lexemes) must be found in the string.</p>
<p>Many times, this may not be what you want. Users expect to have their search string interpreted as <em>|</em> (OR) separated lexemes, or users may want the ability to define these operators themselves on the interface.</p>
<p>So, one way or the other, you will have to write your own parser if you want a non-database user to work with your application. This parser looks at the options your present on your search form and will crawl over the user entered string to interpret certain characters not as words to search but as operators or grouping tokens to build your final query.</p>
<p>But enough about web applications, that is not our focus now. Let us continue.</p>
<p>The next, new item you will see in the last few queries is the <em>@@</em> operator. This operator (also referred to as text-search-matching operator) is also specific to a full text search context. It allows you to <em>match</em> a <em>ts_vector</em> against the results of a <em>ts_query</em>. In our queries we matched the result of a <em>ts_query</em> against a column, but you could also match against a <em>ts_vector</em> on the fly:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'The blue elephant.'</span><span class="p">)</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'blue &amp; the'</span><span class="p">);</span>
</pre></div>


<p>A nice little detail about the <em>@@</em> operator is that it can also match against a <em>TEXT</em> or <em>VARCHAR</em> data type, giving you a poor-mans full text capability. Let me demonstrate:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="s1">'The blue elephant.'</span> <span class="p">::</span> <span class="nb">VARCHAR</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'blue &amp; the'</span><span class="p">);</span>
</pre></div>


<p>This 'on-the-fly' query will generate a <em>VARCHAR</em> string (by using the <em>::</em> or <em>cast</em> operator) and try to match the tokens <em>blue</em> and <em>the</em>. The result will be <em>t</em>, meaning that a match is found.</p>
<p>Before I continue, it is nice to know that you can always test the result of a <em>ts_query</em>, meaning, test the output of what it will use to find lexemes in the <em>ts_vector</em>.
To see that output, you simply call it with the helper function, the same way we called the <em>to_tsvector</em> a while ago:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>This will result in:</p>
<div class="code"><pre> <span class="s1">'eleph'</span> <span class="o">&amp;</span> <span class="s1">'blue'</span>
</pre></div>


<p>It is also important to note that <em>to_tsquery</em> (and <em>plainto_tsquery</em>) too uses a configuration of the same kind <em>to_tsvector</em> uses, for it too has to do the same parsing to find the lexemes of the string or tokens you feed it. So the first, optional argument to <em>to_tsquery</em> is the configuration, this also defaults to "english". This means we could rewrite the query as such:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>And we would get back the same results.</p>
<p>Okay, I think this is enough to take in for now. You have got a basic understanding of what full text search means, you know how to construct a vector containing lexemes, pointers and weights. You also know how to build a query data type and perform basic matching to retrieve the text you desire.</p>
<p>In part 2 we will look at how we can dig deeper and setup our own full text search configuration.
We will cover fun stuff like:</p>
<ul>
<li>Looking deeper into PostgreSQL's guts</li>
<li>Defining dictionaries</li>
<li>Building Stop word lists</li>
<li>Mapping token categories to our dictionaries</li>
<li>Defining our own, super awesome full text configuration</li>
<li>And, of course, more dolphin pun...</li>
</ul>
<p>In the last part we will break open yet another can of full text search goodness and look at:</p>
<ul>
<li>Creating special, full text search indexes</li>
<li>Ranking search results</li>
<li>Highlighting word inside search results</li>
<li>Setting up update triggers for ts_vector records</li>
</ul>
<p>Hang in there!</p>
<p>And as always...thanks for reading!</p>";s:4:"guid";s:64:"http://shisaa.jp/postset/postgresql-full-text-search-part-1.html";s:7:"pubdate";s:29:"Wed, 30 Apr 2014 14:00:00 GMT";s:7:"summary";s:35467:"<h3>Preface</h3>
<p>PostgreSQL, the database of miracles, the RDBMS of wonders.</p>
<p>People who have read my stuff before know that I am a fan of the blue-ish elephant and I greatly entrust it with my data. 
For reasons why, I invite you to read the "Dolphin ass-whopping" part of the <a href="http://shisaa.jp/postset/mailserver-2.html" title="Second chapter of the mail setup series.">second chapter</a> of my mail server setup series.</p>
<p>But what some of you may not know is that PostgreSQL is capable of much more then simply storing and retrieving your data.
Well, that is actually not entirely correct...you are <em>always</em> storing and retrieving data.
A more correct way to say it is that PostgreSQL is capable of storing all <em>kinds</em> of data and gives you all <em>kinds</em> of ways to retrieve it.
It is not limited to <em>storing</em> boring stuff like "VARCHAR" or "INT". Neither is it limited to retrieving and <em>comparing</em> with boring
operators like "=", "ILIKE" or "~". </p>
<p>For instance, are you familiar with PostgreSQL's <em>"tsvector"</em> data type? Or the <em>"tsquery"</em> type? Or what these two represent? No?
Well, diddlydangeroo, then by all means, keep reading, because that is exactly what this series is all about!</p>
<p>In the following three chapters I would like to show you how you can configure PostgreSQL to be a batteries included, blazing fast, competition crunching, full text search engine.</p>
<h3>But, I can already search strings of text with PostgreSQL!</h3>
<p>Hmm, that is very correct. But the basic operators you have at your disposal are limited. </p>
<p>Let me demonstrate.</p>
<p>Imagine we would have a table, called "phraseTable" containing thousands of strings, all saved in a regular, old VARCHAR column named "phrase".
Now we would like to find the string <em>"An elephant a day keeps the dolphins at bay."</em>.
We do not fully remember the above string, but we do remember it had the word "elephant" in it.
With regular SQL you could use the "LIKE" operator to try and find a matching substring. The resulting query would look something like this:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">LIKE</span> <span class="s1">'%elephant%'</span><span class="p">;</span>
</pre></div>


<p>It would work, you render any index on the table mute when using front <em>and</em> back wildcards, but it would work.
Now imagine a humble user would like to find the same string but their memory is bad, they thought the word elephant was capitalized, because it may refer to PostgreSQL, of course.
The query would become this:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">LIKE</span> <span class="s1">'%Elephant%'</span><span class="p">;</span>
</pre></div>


<p>And as a result, you get back zero records.</p>
<p>"But wait!", you shout, "I am a smart ass, there is a solution to this!". And you are correct: the ILIKE operator.
The "I" stands for Insensitive...as in <em>Case Insensitive</em>. So you change the query:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">ILIKE</span> <span class="s1">'%Elephant%'</span><span class="p">;</span>
</pre></div>


<p>And now you will get back a result. Good for you.</p>
<p>A day goes by and the same user comes back and wishes to find this string again. But, his memory still being bad and all, he thought there where multiple elephants keeping the dolphins at bay, because, you know, pun. So the query, you altered yesterday, now reads:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="k">ILIKE</span> <span class="s1">'%Elephants%'</span><span class="p">;</span>
</pre></div>


<p>And...now the query will return zero results.</p>
<p>"Ha!", you shout in my general direction. "I am a master of Regular Expressions! I shall fix thay query!".</p>
<p>No, you shall <em>not</em> fix my query. Never, ever go smart on my derrire by throwing a regular expression in the mix to solve a database lookup problem. It is unreadable, un-scalable and fits only one solution perfectly-ish. And, not to forget, is <em>slow as hell</em> for it not only ignores any index you have set, it also asks more of the database engine then a LIKE or ILIKE.</p>
<p>Let me put an end to this and tell you that I am afraid there are no more (scalable) smart ass tricks left to perform and the possibilities to search text with regular, build-in operators are exhausted.</p>
<p>You agree? Yes? Good! So, enter "<em>full text search</em>"!</p>
<h3>Full text search?</h3>
<p>But before we delve into the details of the PostgreSQL implementation, let us take a step back and first see what exactly a full text search engine is.</p>
<p>Short version: A full text search engine is a system that can retrieve documents, or parts of documents, based on natural language searching.</p>
<p><em>Natural language</em> means the living, breathing language we humans use. And as you know, human language can be complex and above all <em>ambiguous</em>.</p>
<p>Consider yourself in the situation where you knew, for sure, that you have read an interesting article about elephants in the latest edition of "Your Favorite Elephant Magazine".
You liked it so much that you want to show it to your best friend, who happens to be an elephant lover too.
The only bummer is, you cannot remember the title, but you do remember it has an interesting sentence in it.</p>
<p>So what do you do? First you quote the sentence in your mind: "The best elephants have a blue skin color.".
Next, you pick up the latest edition and you start <em>searching</em>, flipping through the pages, skimming for that sentence.</p>
<p>After a minute or two you shout: "Dumbo!, I have found the article!". You read the sentence out loud: "The best Elephants bear a blue skin tone.".
You are happy with yourself, call up your friend and tell him that you will be over right away to show him that specific article.</p>
<p>One thing you forgot to notice was that the sentence in your head, and the sentence that was actually printed where <em>different</em>, but your brain (which is trained in this natural stuff), sees them as the same.
How did that work? Well, your brain used its internal <em>synonym</em> list and <em>thesaurus</em> to link the different words together, making them the same thing, just written differently:</p>
<ul>
<li>"elephants" is the same as "Elephants"</li>
<li>"have" is the same as "bear"</li>
<li>"skin color" is the same "skin tone"</li>
</ul>
<p>Without noticing it, you have just completed a full text search using natural language algorithms, your magazine as the database, your brain as the engine.</p>
<h3>But how does such a natural language lookup work...on an unnatural computer?</h3>
<p>What a perfectly timed question, I was just getting to that.</p>
<p>Now that you have a basic understanding of what natural language searching is, how does one port this idea to a stupid, binary ticking tin box?
By dissecting the process we do in our brains, lay it out in several programmable steps and concepts. 
Such a process, run by computers, will never be as good on the <em>natural</em> part as our brains are, but it is certainly a lot faster with flipping and skimming through the magazine pages.</p>
<p>Let us look at how a computer, regardless of which program, platform or engine you use, would go about being "natural" when searching for strings of text.</p>
<p>To speed up the search process, a full text search engine will never search through the actual document itself.
That is how we humans would do it, and that is slow and (for our eyes) error prone. Before a document can be searched through with a full text search engine, it has to be parsed into a list of words first.
The parsing is where the magic happens, this is our attempt at programming the natural language process. Once the document is parsed, the parsed state is saved. Depending on your database model, you can save the parsed state together with a reference to the original document for later retrieval.</p>
<p>Note that a document, in this context, is simply a big collection of words contained within a file. The engine does not care, and most of the time does not know, about what kind of file (plain text, LibreOffice document, HTML file, ...) it is handling or what the files structure is. It simply looks at all the readable words inside of the file.</p>
<p>So how does the parsing work? Parsing, in this regard, is all about compressing the text found in a document. Cutting down the word count to the least possible, so later, when a user searches, the engine has to plow through fewer words. This compressing, in most engines, is done in roughly three steps.</p>
<h4>Eliminate casing</h4>
<p>The first step in the compression process is the elimination of casing - keeping only the lower case versions of a word.
If you would keep a search case sensitive, then "The ElEphAnt" would not match "the elephant", but generally you do want a match to happen.
The user will many times not care (or not know) about casing in a full text search.</p>
<h4>Remove stop words</h4>
<p>The following step is the removal of words that do not add any searchable value to the text and are seldom searched for.
These words are known as "stop words", a term first coined by Hans Peter Luhn, a renowned IBM computer scientist who specialized in the retrieval and indexing of information stored in computer systems.</p>
<p>The list of stop words is not limited to simply ones like "and" or "the". There is an extensive list of hundreds and hundreds of words which are generally considered to be of little value in a search context.
A (very) short list of stop words: her, him, the, also, each, was, we, after, been, they, would, up, from, only, you, while, ... .</p>
<h4>Remove synonyms, employing a thesaurus and perform stemming</h4>
<p>The last part in the compacting of our to-be-indexed document is removing words that have the same meaning and perform stemming.
Synonym lookups are used for removing <em>words</em> of the same meaning where as thesaurus lookups are used to compact whole <em>phrases</em> with similar meaning.</p>
<p>Only one instance of all the synonyms, thesaurus phrases and case eliminations is stored, the surviving word is referred to as a <em>lexeme</em>, the smallest, meaningful word.
The lexemes that are stored usually (depending on the engine you use) get an accompanying list of (alpha)numeric values stored alongside. Two types of (alpha)numeric values can be stored in case of PostgreSQL:</p>
<ul>
<li>The first type are pure numerical and represent pointer(s) to where the word occurs in the original document.</li>
<li>The second type is pure alphabetical (actually only capital A,B,C,D) and represent the weight a certain lexeme has. </li>
</ul>
<p>Do not worry to much about these two (alpha)numerical values for now, we will get to that later.</p>
<p>Next, let us get practical and start to actually use PostgreSQL to see how all of this works. </p>
<h3>The tsvector</h3>
<p>As PostgreSQL is an <em>extendable</em> database engine, two new data types where added to make full text search possible, as you have seen in the beginning.
One of them is called <em>tsvector</em>, "ts" for <em>t</em>ext <em>s</em>earch and "vector", which is analogous with the generic programming data type "vector".
It is the container in which the result of the parsing is eventually stored.</p>
<p>Let me show you an example of such a tsvector, as presented by PostgreSQL on querying.
Imagine a document with the following string of text inside: <em>"The big blue elephant jumped over the crippled blue dolphin."</em>.
A perfectly normal sentence, elephants jump over dolphins all the time.</p>
<p>Without bothering about how to do it, if we let PostgreSQL parse this string, we will get the following tsvector stored in our record:</p>
<div class="code"><pre><span class="s1">'big'</span><span class="p">:</span><span class="mi">2</span> <span class="s1">'blue'</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span><span class="mi">9</span> <span class="s1">'crippl'</span><span class="p">:</span><span class="mi">8</span> <span class="s1">'dolphin'</span><span class="p">:</span><span class="mi">10</span> <span class="s1">'eleph'</span><span class="p">:</span><span class="mi">4</span> <span class="s1">'jump'</span><span class="p">:</span><span class="mi">5</span>
</pre></div>


<p>You will notice a few things about this vector, let me go over them one by one.</p>
<ul>
<li>First, you recognize the structure of a vector-ish data type. Hence the name "tsvector".</li>
<li>Next, the numbers behind the lexemes themselves, like I said before, represent the pointer(s) to that word. Notice the word "blue" in particular, it has two pointers for the two occurrences in the string.</li>
<li>And last, notice how some lexemes do not even look like English words at all. The lexeme "crippl" or "eleph" do not mean anything, to us humans anyway. These are the surviving lexemes of "cripple" and "elephant". PostgreSQL has stemmed and reduced the words to match all possible variants. The lexeme "crippl", for example, matches "cripple", "crippled", "crippling", "cripples", ... .</li>
</ul>
<p>Note that the above example is the simplest of full text search parsing results, we did not add any weights nor did we employ a thesaurus (or an advanced dictionary) to get back a more efficient compressing.</p>
<p>Now that we are dwelling inside of PostgreSQL, I can elaborate a bit more about how the parsing works exactly.
As we have seen above, it happens in roughly three steps. But I intentionally neglected to say that with PostgreSQL, there is an intermediate state between the word and the resulting lexeme.</p>
<p>When PostgreSQL parses the string of text it goes over them and first <em>categorizes</em> each word into sections like "stop words", "plural words", "synonyms", ... .
Once the words are broken down into categories, we refer to them as <em>tokens</em>. This is the intermediate state.
For a token to become a lexeme, PostgreSQL will consult a set of defined <em>dictionaries</em> for each category to try and find a match.
If a match is found, the dictionary will propose a lexeme. This lexeme is the one that will finally be put in the vector as the parsed result.</p>
<p>If the dictionaries did not find a match, the word is discarded. The one exception to this are the "stop words", if a word matches a stop word, it will be discarded instead of kept.</p>
<p>Let us now get our hands dirty and setup a quick testing database and rig it up with the phraseTable table we have been using in our journey so far.
But instead of a varchar column, this table will contain a tsvector type for we will unleash to power of Full Text Search!</p>
<p>Note: I am assuming you have at least PostgreSQL 9.1 or higher. This post was written with PostgreSQL 9.3 in mind.</p>
<p>So connect to your PostgreSQL install and create the database:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">DATABASE</span> <span class="n">phrases</span><span class="p">;</span>
</pre></div>


<p>Do not worry to much about the ownership of this database nor the ownership of its tables, you can discard it whole later.
Now, switch over to the database:</p>
<div class="code"><pre><span class="err">\</span><span class="k">c</span> <span class="n">phrases</span>
</pre></div>


<p>And create the phraseTable table:</p>
<div class="code"><pre><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">phrases</span> <span class="p">(</span><span class="n">phrase</span> <span class="n">tsvector</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">);</span>
</pre></div>


<p>Okay, simple enough. We now have a tiny database, with a table containing one column of type <em>tsvector</em>.</p>
<p>Let us insert a parsed vector into the table.
Again, without employing a thesaurus or any other tools, we only use the built-in, default configuration to parse a string and save it as a vector.</p>
<p>Let us insert the vector, shall we?</p>
<div class="code"><pre><span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">phraseTable</span> <span class="k">VALUES</span> <span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span><span class="s1">'The big blue elephant jumped over the crippled blue dolphin.'</span><span class="p">));</span>
</pre></div>


<p>That was easy enough. Most of what you see is simple, regular SQL with one new kid on the block: "<em>to_tsvector</em>".
The latter is a <em>function</em> that is shipped with PostgreSQL's Full Text Search extension and it does what its name suggests: it takes a string of text and converts it into a <em>tsvector</em>.</p>
<p>As a first argument to this function you can optionally input the full text search <em>configuration</em> you wish the parser to use. The default is <em>"english"</em>, so I could have omitted it from the argument list.
This configuration holds everything that PostgreSQL will employ to do all of the parsing, including a basic dictionary, stop word list, ... .
PostgreSQL has some default settings, which many times are good enough. The 'english' configuration is such an example.</p>
<p>In the next chapter we will delve <em>deep</em> into creating our own configuration, for now just take it for granted.</p>
<p>If we query the result, with a simple select, it will return our newly created vector:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">from</span> <span class="n">phraseTable</span>
</pre></div>


<p>Will return:</p>
<div class="code"><pre><span class="s1">'big'</span>:2 <span class="s1">'blue'</span>:3,9 <span class="s1">'crippl'</span>:8 <span class="s1">'dolphin'</span>:10 <span class="s1">'eleph'</span>:4 <span class="s1">'jump'</span>:5
</pre></div>


<p>Now remember that I talked about the second kind of value we could store alongside the numeric pointers, the <em>weights</em>? Let us take a deeper look into that now.</p>
<p>First, weights are not mandatory and only give you an extra tool for ranking the results afterwards.
They are nothing more then a label you can put on a lexeme to group it together. With weights you could, for example, reflect the structure the original document had.
You may wish to put a higher weight on lexemes that come from a title element and a lower weight on those from the body text.</p>
<p>PostgreSQL knows four weight labels <em>A</em>, <em>B</em>, <em>C</em>, <em>D</em>. The lowest in rank being <em>D</em>. In fact, if you do not define any weights to the lexemes inside a tsvector, all of them will implicitly get a <em>D</em> assigned.
If all the lexemes in a tsvector carry a <em>D</em>, it is omitted from display when printing the tsvector, simply for readability.
The above query result could thus also be written as:</p>
<div class="code"><pre><span class="s1">'big'</span>:2D <span class="s1">'blue'</span>:3D,9D <span class="s1">'crippl'</span>:8D <span class="s1">'dolphin'</span>:10D <span class="s1">'eleph'</span>:4D <span class="s1">'jump'</span>:5D
</pre></div>


<p>It is <em>exactly</em> the same result, but unnecessarily verbose.</p>
<p>I told you, in the very beginning, that a full text engine does not know or care about the structure of a document, it only sees the words.
So how can it then put labels on lexemes based on a document structure that it does not know?</p>
<p>It cannot.</p>
<p>It is your job to provide PostgreSQL with label information when building the tsvector.
Up until now we have been working with simple text strings, which contain no hierarchy. 
If you wish to reflect your original document structure by using weights, you will have to preprocess the document and construct your <em>to_tsvector</em> query manually.</p>
<p>Just for demonstration purposes, we could, of course, assign weights to the lexemes inside a simple text string.
The process of weight assignment is trivial. PostgreSQL gives you the appropriately named <em>setweight</em> function for this.
This function accepts a tsvector as the first argument and a weight label as the second.</p>
<p>To demonstrate, let me update our record and give all the lexemes in our famous sentence a <em>A</em> weight label:</p>
<div class="code"><pre><span class="k">UPDATE</span> <span class="n">phraseTable</span> <span class="k">SET</span> <span class="n">phrase</span> <span class="o">=</span> <span class="n">setweight</span><span class="p">(</span><span class="n">phrase</span><span class="p">,</span><span class="s1">'A'</span><span class="p">);</span>
</pre></div>


<p>If we now query this table, the result will be this:</p>
<div class="code"><pre><span class="s1">'big'</span>:2A <span class="s1">'blue'</span>:3A,9A <span class="s1">'crippl'</span>:8A <span class="s1">'dolphin'</span>:10A <span class="s1">'eleph'</span>:4A <span class="s1">'jump'</span>:5A
</pre></div>


<p>Simple, right?</p>
<p>One more for fun. What if you wanted to assign different weights to the lexemes?
For this, you have to concatenate several <em>setweight</em> functions together.
An example query would look something like this:</p>
<div class="code"><pre><span class="k">update</span> <span class="n">phraseTable</span> <span class="k">set</span> <span class="n">phrase</span><span class="o">=</span>
<span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'the big blue elephant'</span><span class="p">),</span> <span class="s1">'A'</span><span class="p">)</span> <span class="o">||</span>
<span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'jumped over the'</span><span class="p">),</span> <span class="s1">'B'</span><span class="p">)</span> <span class="o">||</span>
<span class="n">setweight</span><span class="p">(</span><span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'crippled blue dolphin.'</span><span class="p">),</span> <span class="s1">'C'</span><span class="p">);</span>
</pre></div>


<p>The result:</p>
<div class="code"><pre><span class="s1">'big'</span>:2A <span class="s1">'blue'</span>:3A,7C <span class="s1">'crippl'</span>:6C <span class="s1">'dolphin'</span>:8C <span class="s1">'eleph'</span>:4A <span class="s1">'jump'</span>:5B
</pre></div>


<p>Not very usefull, but it demonstrates the principle.</p>
<p>If the documents you wish to index have a fixed structure, many times the table that will hold the tsvectors for these documents will reflect that structure with appropriately named columns.
For example, if your document would always have a title, body text and a footer, you could create a table which contains three tsvector type columns, named after each structure type.
When you parse the document and construct the query, you could assign all lexemes that will be stored in the title column with an <em>A</em> label, in the body column with a <em>B</em> and in the footer column with a <em>C</em>.</p>
<p>Okay, that is enough about weights. Simply remember that they give you extra power to influence the search result ranking, if needed.</p>
<p>We now have a table with a decent tsvector inside. The data is in, so to speak. But what can we do with it now?</p>
<p>Well, let us try to retrieve and compare it, shall we!</p>
<h3>The tsquery</h3>
<p>You could, of course, simply retrieve the results stored in a <em>tsvector</em> by doing a <em>SELECT</em> on the column. However, you have no way of filtering out the results using the operators we have seen before (LIKE, ILIKE). Even if you could use them, you would still run into the same kind of problems as before. You would still have a user who will search for a synonym or search for a plural form of the stemmed lexeme actually stored in the vector.</p>
<p>So how do we query it?</p>
<p>Step in <em>tsquery</em>. What is it? It is a data type that gives us extra tools to <em>query</em> the full <em>text search</em> vector.</p>
<p>Pay attention to the fact that we do not call <em>tsquery</em> a set of extra <em>operators</em> but we call it a <em>data type</em>. This is very important to understand.
With <em>tsquery</em> we can construct search <em>predicates</em>, which can search through a <em>tsvector</em> type and can employ specially designed indexes to speed up the process. </p>
<p>Let me throw a query at you that will try to find the word "elephants" in our favorite string using <em>tsquery</em>:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants'</span><span class="p">);</span>
</pre></div>


<p>Try it out, this will give you back the same result set we had before. Let me explain what just happened.</p>
<p>As you can see, there is again a new function introduced: <em>to_tsquery</em> and it is almost identical to its <em>to_tsvector</em> counterpart.
The function <em>to_tsquery</em> takes one argument, a string containing the <em>tokens</em> (not the words, not the lexemes) you wish to search for.</p>
<p>Let us first look a bit more at this one. Say, for instance, you wish to find two tokens of the sentence inside your database.
Your first instinct would be to query this the following way:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants blue'</span><span class="p">);</span>
</pre></div>


<p>Unfortunately, this will throw an error stating there is a syntax error. Why? Because the string your provided as an argument is malformed.
The <em>to_tsquery</em> helper function does not accept a simple string of text, it needs a string of tokens <em>separated by operators</em>.
The operators at your disposal are the regular <em>&amp;</em> (AND), <em>|</em> (OR) and <em>!</em> (NOT). Note that the <em>!</em> operator <em>needs</em> the <em>&amp;</em> or the <em>|</em> operator.</p>
<p>It then goes and creates a true <em>tsquery</em> to retrieve the results. Let us try this query again, but with correct syntax this time:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>Perfect! This will return, once more, the same result as before. You could even use parenthesis inside your string argument to enforce grouping if desired.
Like I said before, what this helper function does is translate its input (the tokens in the string) into actual lexemes. After that, it tries to match this result with the lexemes present in the tsvector.</p>
<p>We still have a problem if we would let a user type her or his search string into an interface search box and feed it to <em>to_tsquery</em>, for a user does not know about the operators they need to use.
Luckily for us, there is another help function, the <em>plainto_tsquery</em> which takes care of exactly that problem: convert an arbitrary string of text into lexemes.</p>
<p>Let me demonstrate:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">phrase</span> <span class="k">FROM</span> <span class="n">phraseTable</span> <span class="k">WHERE</span> <span class="n">phrase</span> <span class="o">@@</span> <span class="n">plainto_tsquery</span><span class="p">(</span><span class="s1">'elephants blue'</span><span class="p">);</span>
</pre></div>


<p>Notice we did not separate the words with operators, now it is a simple search string. In fact, <em>plainto_tsquery</em> converts it to a list of lexemes separated by an <em>&amp;</em> (AND) operator.
The only drawback is that this function can only separate the lexemes with an <em>&amp;</em> operator.
If you wish to have something other then the <em>&amp;</em> operator, you will have to stick to <em>to_tsquery</em>.</p>
<p>A word of caution though, the <em>plainto_tsvector</em> may seem interesting, but is most of the time not a general solution for building a higher level search interface. When you are building, say, a web application that contains a full text search box, there are a few more steps between the string entered in that box, and the final query that will be preformed. </p>
<p>Building a web application and safely handling user input that travels to the database is a separate story and <em>way</em> beyond the scope of this post, but you will have to build your own parser that sits between the user input and the query.</p>
<p>If you would play dumb and accept the fact that your interface would only allow to enter a string in the search box (no operators, no grouping, ...) then you still need to send over the user input using query parameters <em>and</em> you need to make sure that the parameter sent over is a string. This, of course, is not really a parser, this is more basic, sane database handling on the web. </p>
<p>As tempting (and simple) it might seem to be to build a query like that, it will probably frustrate your end users. The reason why is because as I mentioned before, the <em>plainto_tsquery</em> accepts a string, but will chop the string into separate lexemes and put the <em>&amp;</em> operator between them. This means that <em>all</em> the words entered by the user (or at least their resulting lexemes) must be found in the string.</p>
<p>Many times, this may not be what you want. Users expect to have their search string interpreted as <em>|</em> (OR) separated lexemes, or users may want the ability to define these operators themselves on the interface.</p>
<p>So, one way or the other, you will have to write your own parser if you want a non-database user to work with your application. This parser looks at the options your present on your search form and will crawl over the user entered string to interpret certain characters not as words to search but as operators or grouping tokens to build your final query.</p>
<p>But enough about web applications, that is not our focus now. Let us continue.</p>
<p>The next, new item you will see in the last few queries is the <em>@@</em> operator. This operator (also referred to as text-search-matching operator) is also specific to a full text search context. It allows you to <em>match</em> a <em>ts_vector</em> against the results of a <em>ts_query</em>. In our queries we matched the result of a <em>ts_query</em> against a column, but you could also match against a <em>ts_vector</em> on the fly:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsvector</span><span class="p">(</span><span class="s1">'The blue elephant.'</span><span class="p">)</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'blue &amp; the'</span><span class="p">);</span>
</pre></div>


<p>A nice little detail about the <em>@@</em> operator is that it can also match against a <em>TEXT</em> or <em>VARCHAR</em> data type, giving you a poor-mans full text capability. Let me demonstrate:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="s1">'The blue elephant.'</span> <span class="p">::</span> <span class="nb">VARCHAR</span> <span class="o">@@</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'blue &amp; the'</span><span class="p">);</span>
</pre></div>


<p>This 'on-the-fly' query will generate a <em>VARCHAR</em> string (by using the <em>::</em> or <em>cast</em> operator) and try to match the tokens <em>blue</em> and <em>the</em>. The result will be <em>t</em>, meaning that a match is found.</p>
<p>Before I continue, it is nice to know that you can always test the result of a <em>ts_query</em>, meaning, test the output of what it will use to find lexemes in the <em>ts_vector</em>.
To see that output, you simply call it with the helper function, the same way we called the <em>to_tsvector</em> a while ago:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>This will result in:</p>
<div class="code"><pre> <span class="s1">'eleph'</span> <span class="o">&amp;</span> <span class="s1">'blue'</span>
</pre></div>


<p>It is also important to note that <em>to_tsquery</em> (and <em>plainto_tsquery</em>) too uses a configuration of the same kind <em>to_tsvector</em> uses, for it too has to do the same parsing to find the lexemes of the string or tokens you feed it. So the first, optional argument to <em>to_tsquery</em> is the configuration, this also defaults to "english". This means we could rewrite the query as such:</p>
<div class="code"><pre><span class="k">SELECT</span> <span class="n">to_tsquery</span><span class="p">(</span><span class="s1">'english'</span><span class="p">,</span><span class="s1">'elephants &amp; blue'</span><span class="p">);</span>
</pre></div>


<p>And we would get back the same results.</p>
<p>Okay, I think this is enough to take in for now. You have got a basic understanding of what full text search means, you know how to construct a vector containing lexemes, pointers and weights. You also know how to build a query data type and perform basic matching to retrieve the text you desire.</p>
<p>In part 2 we will look at how we can dig deeper and setup our own full text search configuration.
We will cover fun stuff like:</p>
<ul>
<li>Looking deeper into PostgreSQL's guts</li>
<li>Defining dictionaries</li>
<li>Building Stop word lists</li>
<li>Mapping token categories to our dictionaries</li>
<li>Defining our own, super awesome full text configuration</li>
<li>And, of course, more dolphin pun...</li>
</ul>
<p>In the last part we will break open yet another can of full text search goodness and look at:</p>
<ul>
<li>Creating special, full text search indexes</li>
<li>Ranking search results</li>
<li>Highlighting word inside search results</li>
<li>Setting up update triggers for ts_vector records</li>
</ul>
<p>Hang in there!</p>
<p>And as always...thanks for reading!</p>";}i:27;a:6:{s:5:"title";s:98:"Michael Paquier: Postgres 9.4 feature highlight: Creating an output plugin for logical replication";s:4:"link";s:104:"http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-output-plugin-logical-replication/";s:11:"description";s:8609:"<p>Continuing on the series of posts about logical replication and after
looking at <a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-basics-logical-decoding/">some basics</a>
and the concept of <a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-replica-identity-logical-replication/">REPLICA IDENTITY</a>
for a table, now is time to begin more serious things by having a look
at how to develop an output plugin for the logical decoding facility.</p>

<p>Do you remember? Logical decoding is made of roughly two major parts:</p>

<ul>
<li>A decoder generating the database changes, that reads and translate
WAL files into a format given by a custom plugin that can be defined when
a logical replication slot is created. An example of that in core is the
contribution module called <a href="http://www.postgresql.org/docs/devel/static/test-decoding.html">test_decoding</a>. This is
linked with a replication slot.</li>
<li>A receiver, that can consume the changes decoder has created. 
<a href="http://www.postgresql.org/docs/devel/static/app-pgrecvlogical.html">pg_recvlogical</a>
as well as the SQL functions pg_logical_slot_[get|peek]_changes
as good examples of that.</li>
</ul>

<p>In short, in PostgreSQL architecture a logical decoder is a set of callback
functions that are loaded from a library listed in shared_preload_libraries
when server starts. Those callbacks functions need to be defined through
a function called _PG_output_plugin_init and here is their list:</p>

<ul>
<li>startup_cb, called when a replication slot is created or when changes
are requested. Put in here things like dedicated memory contexts or option
management for example.</li>
<li>shutdown_cb, called when a replication slot is not used anymore. You
should do here the necessary clean-up actions for things created at startup
phase.</li>
<li>begin_cb and commit_cb, called when respectively a transaction BEGIN or
COMMIT has been decoded. Transaction context is available here so you use
this data in the decoding stream as well.</li>
<li>change_cb, called when a change on database has occurred. By far this
is the most important one to be aware of in my opinion. With this callback
is available information about the relation changed in the shape of a Relation
entry and data: the old and new tuple as some HeapTupleData and
HeapTupleHeaderData entries.</li>
</ul>

<p>The role of the decoder plugin is to use all the data available and put it
in a shape that will be useful for a receiver when it is streamed. Its
flexibility is actually what makes this facility really powerful and useful
already for many things like rolling upgrades, audit functions, replication,
etc.</p>

<p>Note as well that a decoder can also use _PG_init to perform initialization
actions when it is loaded. By the way, when looking at implementing your
own decoder, the best advice givable is to base your implementation on
the existing plugin in PostgreSQL core called test_decoding. This module
does not do much in itself, but it is a gold mine for beginners and will save
you hours and a lot of tears. It is however preferable to have some knowledge
of the APIs of PostgreSQL before jumping in the wild. And this is actually
what I did to implement my own decoder, called <a href="https://github.com/michaelpq/pg_plugins/tree/master/decoder_raw">decoder_raw</a> available
in a personal repository on github called <a href="https://github.com/michaelpq/pg_plugins">pg_plugins</a> (that's actually the repository
I used to create a set of templates for backgroud workers when studying them).
This plugin is able to decode WAL entries and generate raw SQL queries that
stream receivers can use as-is. The code is a bit longer than test_decoding
as there is some logic in the WHERE clause of UPDATE and DELETE queries
caused by the logic of REPLICA IDENTITY for the DEFAULT and INDEX cases
to enforce tuple selectivity on a table's primary key (DEFAULT case) or a
not null unique index (INDEX case).</p>

<p>Here is for example how the callback functions are defined:</p>
<div class="highlight"><pre><code class="text language-text">void
_PG_output_plugin_init(OutputPluginCallbacks *cb)
{
    AssertVariableIsOfType(&amp;_PG_output_plugin_init, LogicalOutputPluginInit);

    cb-&gt;startup_cb = decoder_raw_startup;
    cb-&gt;begin_cb = decoder_raw_begin_txn;
    cb-&gt;change_cb = decoder_raw_change;
    cb-&gt;commit_cb = decoder_raw_commit_txn;
    cb-&gt;shutdown_cb = decoder_raw_shutdown;
}
</code></pre></div>
<p>Then, without entering in the details, here a couple of things that you need
to be aware of even if implementation is based on the structure of
test_decoding.</p>

<p>First, the relation information in structure Relation contains a new OID field
called rd_replidindex that defines the OID if the REPLICA IDENTITY index if
it exists. This is either a PRIMARY KEY or a user-defined index. Use and
abuse of it! It is really helpful. A common usage of rd_replidindex is to
open an index relation on it and then scan indnatts to find the list of
column attributes the index refers to. Useful to define a list of keys
that can uniquely define a tuple on a remote source for an UPDATE or DELETE
change. Here is a way to do that:</p>
<div class="highlight"><pre><code class="text language-text">int key;
Relation indexRel = index_open(relation-&gt;rd_replidindex, ShareLock);

for (key = 0; key &lt; indexRel-&gt;rd_index-&gt;indnatts; key++)
{
    int relattr = indexRel-&gt;rd_index-&gt;indkey.values[key - 1];        
    /*
     * Perform an action with the attribute number of parent relation 
     * and tuple data.
     */
    do_action_using_attribute_number(relattr, tupledata);
}
index_close(indexRel, NoLock);
</code></pre></div>
<p>Then, use a common way to generate the relation name for all the change
types (INSERT, UPDATE, DELETE). Something like that will generate
a complete relation name with both namespace and table name:</p>
<div class="highlight"><pre><code class="text language-text">Relation rel = blabla;
Form_pg_class   class_form = RelationGetForm(rel);

appendStringInfoString(s,
quote_qualified_identifier(
            get_namespace_name(
                       get_rel_namespace(RelationGetRelid(rel))),
        NameStr(class_form-&gt;relname)));
</code></pre></div>
<p>Now let's see how this works, with for example the following SQL sequence:</p>
<div class="highlight"><pre><code class="text language-text">=# -- Create slot
=# SELECT slotname
   FROM pg_create_logical_replication_slot('custom_slot', 'decoder_raw');
  slotname   
-------------
 custom_slot
(1 row)
=# -- A table using DEFAULT as REPLICA IDENTITY and some operations
=# CREATE TABLE aa (a int primary key, b text);
CREATE TABLE
=# INSERT INTO aa VALUES (1, 'aa'), (2, 'bb');
INSERT 0 2
=# UPDATE aa SET b = 'cc' WHERE a = 1;
UPDATE 1
=# DELETE FROM aa WHERE a = 1;
DELETE 1
</code></pre></div>
<p>And the following output is generated by the plugin. </p>
<div class="highlight"><pre><code class="text language-text">=# SELECT data
   FROM pg_logical_slot_peek_changes('custom_slot', 
             NULL, NULL, 'include-transaction', 'on');
                        data                        
----------------------------------------------------
 BEGIN;
 COMMIT;
 BEGIN;
 INSERT INTO public.aa (a, b) VALUES (1, 'aa');
 INSERT INTO public.aa (a, b) VALUES (2, 'bb');
 COMMIT;
 BEGIN;
 UPDATE public.aa SET a = 1, b = 'cc' WHERE a = 1 ;
 COMMIT;
 BEGIN;
 DELETE FROM public.aa WHERE a = 1 ;
 COMMIT;
(12 rows)
</code></pre></div>
<p>Pretty handy, no? Note that the rows of UPDATE and DELETE queries are
identified using the primary key of relation.</p>

<p>Consuming such changes shapped like that is straight-forward: simply
use them on another PostgreSQL node that has schema and data consistent
with the node decoding the changes before replication slot was marked
as active. Here is for example a command using pg_logical_slot_get_changes
running periodically that is able to replicate all the changes on an other
node listening to port 5433:</p>
<div class="highlight"><pre><code class="text language-text">psql -At -c &quot;SELECT pg_logical_slot_get_changes('custom_slot', NULL, NULL)&quot; | \
    psql -p 5433
</code></pre></div>
<p>This way, the second node replicates all the changes occuring on first node
doing the decoding effort. The code of decoder_raw has been released under
the PostgreSQL license and is available <a href="https://github.com/michaelpq/pg_plugins/tree/master/decoder_raw">here</a>. Feel
free to use it, feedback is welcome as well.</p>";s:4:"guid";s:104:"http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-output-plugin-logical-replication/";s:7:"pubdate";s:29:"Wed, 30 Apr 2014 13:15:43 GMT";s:7:"summary";s:8609:"<p>Continuing on the series of posts about logical replication and after
looking at <a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-basics-logical-decoding/">some basics</a>
and the concept of <a href="http://michael.otacoo.com/postgresql-2/postgres-9-4-feature-highlight-replica-identity-logical-replication/">REPLICA IDENTITY</a>
for a table, now is time to begin more serious things by having a look
at how to develop an output plugin for the logical decoding facility.</p>

<p>Do you remember? Logical decoding is made of roughly two major parts:</p>

<ul>
<li>A decoder generating the database changes, that reads and translate
WAL files into a format given by a custom plugin that can be defined when
a logical replication slot is created. An example of that in core is the
contribution module called <a href="http://www.postgresql.org/docs/devel/static/test-decoding.html">test_decoding</a>. This is
linked with a replication slot.</li>
<li>A receiver, that can consume the changes decoder has created. 
<a href="http://www.postgresql.org/docs/devel/static/app-pgrecvlogical.html">pg_recvlogical</a>
as well as the SQL functions pg_logical_slot_[get|peek]_changes
as good examples of that.</li>
</ul>

<p>In short, in PostgreSQL architecture a logical decoder is a set of callback
functions that are loaded from a library listed in shared_preload_libraries
when server starts. Those callbacks functions need to be defined through
a function called _PG_output_plugin_init and here is their list:</p>

<ul>
<li>startup_cb, called when a replication slot is created or when changes
are requested. Put in here things like dedicated memory contexts or option
management for example.</li>
<li>shutdown_cb, called when a replication slot is not used anymore. You
should do here the necessary clean-up actions for things created at startup
phase.</li>
<li>begin_cb and commit_cb, called when respectively a transaction BEGIN or
COMMIT has been decoded. Transaction context is available here so you use
this data in the decoding stream as well.</li>
<li>change_cb, called when a change on database has occurred. By far this
is the most important one to be aware of in my opinion. With this callback
is available information about the relation changed in the shape of a Relation
entry and data: the old and new tuple as some HeapTupleData and
HeapTupleHeaderData entries.</li>
</ul>

<p>The role of the decoder plugin is to use all the data available and put it
in a shape that will be useful for a receiver when it is streamed. Its
flexibility is actually what makes this facility really powerful and useful
already for many things like rolling upgrades, audit functions, replication,
etc.</p>

<p>Note as well that a decoder can also use _PG_init to perform initialization
actions when it is loaded. By the way, when looking at implementing your
own decoder, the best advice givable is to base your implementation on
the existing plugin in PostgreSQL core called test_decoding. This module
does not do much in itself, but it is a gold mine for beginners and will save
you hours and a lot of tears. It is however preferable to have some knowledge
of the APIs of PostgreSQL before jumping in the wild. And this is actually
what I did to implement my own decoder, called <a href="https://github.com/michaelpq/pg_plugins/tree/master/decoder_raw">decoder_raw</a> available
in a personal repository on github called <a href="https://github.com/michaelpq/pg_plugins">pg_plugins</a> (that's actually the repository
I used to create a set of templates for backgroud workers when studying them).
This plugin is able to decode WAL entries and generate raw SQL queries that
stream receivers can use as-is. The code is a bit longer than test_decoding
as there is some logic in the WHERE clause of UPDATE and DELETE queries
caused by the logic of REPLICA IDENTITY for the DEFAULT and INDEX cases
to enforce tuple selectivity on a table's primary key (DEFAULT case) or a
not null unique index (INDEX case).</p>

<p>Here is for example how the callback functions are defined:</p>
<div class="highlight"><pre><code class="text language-text">void
_PG_output_plugin_init(OutputPluginCallbacks *cb)
{
    AssertVariableIsOfType(&amp;_PG_output_plugin_init, LogicalOutputPluginInit);

    cb-&gt;startup_cb = decoder_raw_startup;
    cb-&gt;begin_cb = decoder_raw_begin_txn;
    cb-&gt;change_cb = decoder_raw_change;
    cb-&gt;commit_cb = decoder_raw_commit_txn;
    cb-&gt;shutdown_cb = decoder_raw_shutdown;
}
</code></pre></div>
<p>Then, without entering in the details, here a couple of things that you need
to be aware of even if implementation is based on the structure of
test_decoding.</p>

<p>First, the relation information in structure Relation contains a new OID field
called rd_replidindex that defines the OID if the REPLICA IDENTITY index if
it exists. This is either a PRIMARY KEY or a user-defined index. Use and
abuse of it! It is really helpful. A common usage of rd_replidindex is to
open an index relation on it and then scan indnatts to find the list of
column attributes the index refers to. Useful to define a list of keys
that can uniquely define a tuple on a remote source for an UPDATE or DELETE
change. Here is a way to do that:</p>
<div class="highlight"><pre><code class="text language-text">int key;
Relation indexRel = index_open(relation-&gt;rd_replidindex, ShareLock);

for (key = 0; key &lt; indexRel-&gt;rd_index-&gt;indnatts; key++)
{
    int relattr = indexRel-&gt;rd_index-&gt;indkey.values[key - 1];        
    /*
     * Perform an action with the attribute number of parent relation 
     * and tuple data.
     */
    do_action_using_attribute_number(relattr, tupledata);
}
index_close(indexRel, NoLock);
</code></pre></div>
<p>Then, use a common way to generate the relation name for all the change
types (INSERT, UPDATE, DELETE). Something like that will generate
a complete relation name with both namespace and table name:</p>
<div class="highlight"><pre><code class="text language-text">Relation rel = blabla;
Form_pg_class   class_form = RelationGetForm(rel);

appendStringInfoString(s,
quote_qualified_identifier(
            get_namespace_name(
                       get_rel_namespace(RelationGetRelid(rel))),
        NameStr(class_form-&gt;relname)));
</code></pre></div>
<p>Now let's see how this works, with for example the following SQL sequence:</p>
<div class="highlight"><pre><code class="text language-text">=# -- Create slot
=# SELECT slotname
   FROM pg_create_logical_replication_slot('custom_slot', 'decoder_raw');
  slotname   
-------------
 custom_slot
(1 row)
=# -- A table using DEFAULT as REPLICA IDENTITY and some operations
=# CREATE TABLE aa (a int primary key, b text);
CREATE TABLE
=# INSERT INTO aa VALUES (1, 'aa'), (2, 'bb');
INSERT 0 2
=# UPDATE aa SET b = 'cc' WHERE a = 1;
UPDATE 1
=# DELETE FROM aa WHERE a = 1;
DELETE 1
</code></pre></div>
<p>And the following output is generated by the plugin. </p>
<div class="highlight"><pre><code class="text language-text">=# SELECT data
   FROM pg_logical_slot_peek_changes('custom_slot', 
             NULL, NULL, 'include-transaction', 'on');
                        data                        
----------------------------------------------------
 BEGIN;
 COMMIT;
 BEGIN;
 INSERT INTO public.aa (a, b) VALUES (1, 'aa');
 INSERT INTO public.aa (a, b) VALUES (2, 'bb');
 COMMIT;
 BEGIN;
 UPDATE public.aa SET a = 1, b = 'cc' WHERE a = 1 ;
 COMMIT;
 BEGIN;
 DELETE FROM public.aa WHERE a = 1 ;
 COMMIT;
(12 rows)
</code></pre></div>
<p>Pretty handy, no? Note that the rows of UPDATE and DELETE queries are
identified using the primary key of relation.</p>

<p>Consuming such changes shapped like that is straight-forward: simply
use them on another PostgreSQL node that has schema and data consistent
with the node decoding the changes before replication slot was marked
as active. Here is for example a command using pg_logical_slot_get_changes
running periodically that is able to replicate all the changes on an other
node listening to port 5433:</p>
<div class="highlight"><pre><code class="text language-text">psql -At -c &quot;SELECT pg_logical_slot_get_changes('custom_slot', NULL, NULL)&quot; | \
    psql -p 5433
</code></pre></div>
<p>This way, the second node replicates all the changes occuring on first node
doing the decoding effort. The code of decoder_raw has been released under
the PostgreSQL license and is available <a href="https://github.com/michaelpq/pg_plugins/tree/master/decoder_raw">here</a>. Feel
free to use it, feedback is welcome as well.</p>";}i:28;a:6:{s:5:"title";s:73:"Joel Jacobson: Garbage Collection of Unused PostgreSQL Tables and Columns";s:4:"link";s:91:"http://joelonsql.com/2014/04/30/garbage-collection-of-unused-postgresql-tables-and-columns/";s:11:"description";s:4368:"<p>Over the last five years, our database at <a href="http://trustly.com" title="Trustly">Trustly</a> have kept growing in number of tables, columns and functions, where some of the tables and columns are being used by any database functions any longer. Getting rid of them is important, as otherwise people working with the database will be confused and annoyed. Database developers should always be able to rely on the data model being relevant and up to date.</p>
<p>In our system, no applications access the database tables directly, instead everything goes through stored procedures.</p>
<p>This means, if a table or column name is not present anywhere in any function&#8217;s source code, it&#8217;s very likely the table/column is not being used by anything. The only exception is if you have dynamically crafted queries executed using EXECUTE, where the table/column names are constructed from different parts. In our system, we thankfully only have a few such cases.</p>
<pre class="brush: sql; title: ; notranslate">
SELECT
Tables.TableName
FROM (
SELECT DISTINCT
regexp_replace(pg_catalog.pg_class.relname,'s$','')
FROM pg_catalog.pg_class
INNER JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace
WHERE pg_catalog.pg_class.relkind = 'r'
AND pg_catalog.pg_namespace.nspname NOT IN ('information_schema','pg_catalog')
) Tables(TableName)
WHERE NOT EXISTS (
SELECT 1 FROM pg_catalog.pg_proc
WHERE pg_catalog.pg_proc.prosrc ~* Tables.TableName
)
ORDER BY Tables.TableName
</pre>
<p>This query returned quite a lot of table names with about half of them being false positives,<br />
but still a managable list to go through manually.</p>
<p>50 minutes of manual work later:</p>
<pre class="brush: sql; title: ; notranslate">
92 files changed, 1114 deletions(-)
DROP TABLE: 16
DROP VIEW: 6
DROP SEQUENCES: 7
DROP FK_CONSTRAINTS: 5
DROP CONSTRAINTS: 17
False positives: 14
</pre>
<p>Then I moved on to the task of finding unused table columns.<br />
The query below excludes any false positives found in the previous query.</p>
<pre class="brush: sql; title: ; notranslate">
SELECT DISTINCT
    Columns.ColumnName
FROM (
    SELECT
        regexp_replace(pg_catalog.pg_attribute.attname,'id$','')
    FROM pg_catalog.pg_class
    INNER JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace
    INNER JOIN pg_catalog.pg_attribute ON pg_catalog.pg_attribute.attrelid = pg_catalog.pg_class.oid
    WHERE pg_catalog.pg_class.relkind = 'r'
    AND pg_catalog.pg_attribute.attnum &gt; 0
    AND NOT pg_catalog.pg_attribute.attisdropped
    AND pg_catalog.pg_namespace.nspname NOT IN ('information_schema','pg_catalog')
    -- Exclude columns in tables we know are unused by the stored procedures, but we want to keep around anyway:
    AND pg_catalog.pg_class.relname !~* '^(alexacountrie|bankersworldonline|bindgroup|clearinghousecurrencie|dailystat|geoiporganization|hourlystat|usercommitment|polishbank|polishbanknumber|swedishpostalcode|testbasedata|testperlmodule|useraccesslogarchive)'
) Columns(ColumnName)
WHERE NOT EXISTS (
    SELECT 1 FROM pg_catalog.pg_proc
    WHERE pg_catalog.pg_proc.prosrc ~* Columns.ColumnName
)
ORDER BY Columns.ColumnName
</pre>
<p>It took me two hours to go through all code. It was an interesting journey in time with lots of memories.</p>
<pre class="brush: sql; title: ; notranslate">
50 files changed, 5396 insertions(+), 5917 deletions(-)
ALTER TABLE DROP COLUMN: 30
False positives: 87
</pre>
<p>The reason why there were so many insertions and deletions, was because the dropped columns affected some of the base tables with reference data, which had to be regenerated, thus affecting all lines in those files.</p>
<p>In summary, the ROI on those three hours of time invested is enormous. Developers can now feel confident all tables and columns fulfill a purpose in the system. This exercise will of course need to be repeated in the future though.</p><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/joelonsql.wordpress.com/214/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/joelonsql.wordpress.com/214/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=joelonsql.com&#038;blog=36282610&#038;post=214&#038;subd=joelonsql&#038;ref=&#038;feed=1" width="1" />";s:4:"guid";s:27:"http://joelonsql.com/?p=214";s:7:"pubdate";s:29:"Wed, 30 Apr 2014 09:11:33 GMT";s:7:"summary";s:4368:"<p>Over the last five years, our database at <a href="http://trustly.com" title="Trustly">Trustly</a> have kept growing in number of tables, columns and functions, where some of the tables and columns are being used by any database functions any longer. Getting rid of them is important, as otherwise people working with the database will be confused and annoyed. Database developers should always be able to rely on the data model being relevant and up to date.</p>
<p>In our system, no applications access the database tables directly, instead everything goes through stored procedures.</p>
<p>This means, if a table or column name is not present anywhere in any function&#8217;s source code, it&#8217;s very likely the table/column is not being used by anything. The only exception is if you have dynamically crafted queries executed using EXECUTE, where the table/column names are constructed from different parts. In our system, we thankfully only have a few such cases.</p>
<pre class="brush: sql; title: ; notranslate">
SELECT
Tables.TableName
FROM (
SELECT DISTINCT
regexp_replace(pg_catalog.pg_class.relname,'s$','')
FROM pg_catalog.pg_class
INNER JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace
WHERE pg_catalog.pg_class.relkind = 'r'
AND pg_catalog.pg_namespace.nspname NOT IN ('information_schema','pg_catalog')
) Tables(TableName)
WHERE NOT EXISTS (
SELECT 1 FROM pg_catalog.pg_proc
WHERE pg_catalog.pg_proc.prosrc ~* Tables.TableName
)
ORDER BY Tables.TableName
</pre>
<p>This query returned quite a lot of table names with about half of them being false positives,<br />
but still a managable list to go through manually.</p>
<p>50 minutes of manual work later:</p>
<pre class="brush: sql; title: ; notranslate">
92 files changed, 1114 deletions(-)
DROP TABLE: 16
DROP VIEW: 6
DROP SEQUENCES: 7
DROP FK_CONSTRAINTS: 5
DROP CONSTRAINTS: 17
False positives: 14
</pre>
<p>Then I moved on to the task of finding unused table columns.<br />
The query below excludes any false positives found in the previous query.</p>
<pre class="brush: sql; title: ; notranslate">
SELECT DISTINCT
    Columns.ColumnName
FROM (
    SELECT
        regexp_replace(pg_catalog.pg_attribute.attname,'id$','')
    FROM pg_catalog.pg_class
    INNER JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace
    INNER JOIN pg_catalog.pg_attribute ON pg_catalog.pg_attribute.attrelid = pg_catalog.pg_class.oid
    WHERE pg_catalog.pg_class.relkind = 'r'
    AND pg_catalog.pg_attribute.attnum &gt; 0
    AND NOT pg_catalog.pg_attribute.attisdropped
    AND pg_catalog.pg_namespace.nspname NOT IN ('information_schema','pg_catalog')
    -- Exclude columns in tables we know are unused by the stored procedures, but we want to keep around anyway:
    AND pg_catalog.pg_class.relname !~* '^(alexacountrie|bankersworldonline|bindgroup|clearinghousecurrencie|dailystat|geoiporganization|hourlystat|usercommitment|polishbank|polishbanknumber|swedishpostalcode|testbasedata|testperlmodule|useraccesslogarchive)'
) Columns(ColumnName)
WHERE NOT EXISTS (
    SELECT 1 FROM pg_catalog.pg_proc
    WHERE pg_catalog.pg_proc.prosrc ~* Columns.ColumnName
)
ORDER BY Columns.ColumnName
</pre>
<p>It took me two hours to go through all code. It was an interesting journey in time with lots of memories.</p>
<pre class="brush: sql; title: ; notranslate">
50 files changed, 5396 insertions(+), 5917 deletions(-)
ALTER TABLE DROP COLUMN: 30
False positives: 87
</pre>
<p>The reason why there were so many insertions and deletions, was because the dropped columns affected some of the base tables with reference data, which had to be regenerated, thus affecting all lines in those files.</p>
<p>In summary, the ROI on those three hours of time invested is enormous. Developers can now feel confident all tables and columns fulfill a purpose in the system. This exercise will of course need to be repeated in the future though.</p><br />  <a href="http://feeds.wordpress.com/1.0/gocomments/joelonsql.wordpress.com/214/" rel="nofollow"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/joelonsql.wordpress.com/214/" /></a> <img alt="" border="0" height="1" src="http://stats.wordpress.com/b.gif?host=joelonsql.com&#038;blog=36282610&#038;post=214&#038;subd=joelonsql&#038;ref=&#038;feed=1" width="1" />";}i:29;a:6:{s:5:"title";s:62:"Jeff Frost: How to start PostgreSQL the Debian way with pg_ctl";s:4:"link";s:88:"http://frosty-postgres.blogspot.com/2014/04/how-to-start-postgresql-debian-way-with.html";s:11:"description";s:2211:"Sometimes I find myself wanting to start a PostgreSQL cluster with pg_ctl on a Debian or Ubuntu system.&nbsp; If you've ever tried this, you know that something like this doesn't work:<br /><br /><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:/vagrant$ sudo -u postgres /usr/lib/postgresql/9.3/bin/pg_ctl -D /var/lib/postgresql/9.3/main/ start<br />server starting<br />vagrant@vagrant-ubuntu-trusty-64:/vagrant$ postgres cannot access the server configuration file "/var/lib/postgresql/9.3/main/postgresql.conf": No such file or directory</span></span><br /><br />because the Debian packages place the config files in /etc/postgresql/&lt;version&gt;/&lt;cluster name&gt;<br /><br />So, I used to think that this was the way to start it:<br /><br /><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:/vagrant$ sudo -u postgres /usr/lib/postgresql/9.3/bin/pg_ctl -D /etc/postgresql/9.3/main/ start<br />server starting<br />vagrant@vagrant-ubuntu-trusty-64:/vagrant$ 2014-04-30 00:59:32 UTC LOG:&nbsp; database system was shut down at 2014-04-30 00:57:49 UTC<br />2014-04-30 00:59:32 UTC LOG:&nbsp; database system is ready to accept connections<br />2014-04-30 00:59:32 UTC LOG:&nbsp; autovacuum launcher started</span></span><br /><br />And that does generally work, but the proper way to do it is this:<br /><span><br /></span><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:/vagrant$ sudo -u postgres /usr/lib/postgresql/9.3/bin/pg_ctl -D /var/lib/postgresql/9.3/main/ -o "-c config_file=/etc/postgresql/9.3/main/postgresql.conf" start<br />server starting<br />vagrant@vagrant-ubuntu-trusty-64:/vagrant$ 2014-04-30 01:00:22 UTC LOG:&nbsp; database system was shut down at 2014-04-30 01:00:16 UTC<br />2014-04-30 01:00:22 UTC LOG:&nbsp; database system is ready to accept connections<br />2014-04-30 01:00:22 UTC LOG:&nbsp; autovacuum launcher started</span></span><br /><br />The real Debian way is to use pg_ctlcluster like so:<br /><br /><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:~$ sudo -u postgres pg_ctlcluster 9.3 main start<br />vagrant@vagrant-ubuntu-trusty-64:~$</span></span><br /><br /><br />";s:4:"guid";s:70:"tag:blogger.com,1999:blog-2239123977960506445.post-2123541037884802988";s:7:"pubdate";s:29:"Wed, 30 Apr 2014 01:04:00 GMT";s:7:"summary";s:2211:"Sometimes I find myself wanting to start a PostgreSQL cluster with pg_ctl on a Debian or Ubuntu system.&nbsp; If you've ever tried this, you know that something like this doesn't work:<br /><br /><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:/vagrant$ sudo -u postgres /usr/lib/postgresql/9.3/bin/pg_ctl -D /var/lib/postgresql/9.3/main/ start<br />server starting<br />vagrant@vagrant-ubuntu-trusty-64:/vagrant$ postgres cannot access the server configuration file "/var/lib/postgresql/9.3/main/postgresql.conf": No such file or directory</span></span><br /><br />because the Debian packages place the config files in /etc/postgresql/&lt;version&gt;/&lt;cluster name&gt;<br /><br />So, I used to think that this was the way to start it:<br /><br /><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:/vagrant$ sudo -u postgres /usr/lib/postgresql/9.3/bin/pg_ctl -D /etc/postgresql/9.3/main/ start<br />server starting<br />vagrant@vagrant-ubuntu-trusty-64:/vagrant$ 2014-04-30 00:59:32 UTC LOG:&nbsp; database system was shut down at 2014-04-30 00:57:49 UTC<br />2014-04-30 00:59:32 UTC LOG:&nbsp; database system is ready to accept connections<br />2014-04-30 00:59:32 UTC LOG:&nbsp; autovacuum launcher started</span></span><br /><br />And that does generally work, but the proper way to do it is this:<br /><span><br /></span><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:/vagrant$ sudo -u postgres /usr/lib/postgresql/9.3/bin/pg_ctl -D /var/lib/postgresql/9.3/main/ -o "-c config_file=/etc/postgresql/9.3/main/postgresql.conf" start<br />server starting<br />vagrant@vagrant-ubuntu-trusty-64:/vagrant$ 2014-04-30 01:00:22 UTC LOG:&nbsp; database system was shut down at 2014-04-30 01:00:16 UTC<br />2014-04-30 01:00:22 UTC LOG:&nbsp; database system is ready to accept connections<br />2014-04-30 01:00:22 UTC LOG:&nbsp; autovacuum launcher started</span></span><br /><br />The real Debian way is to use pg_ctlcluster like so:<br /><br /><span style="font-size: x-small;"><span>vagrant@vagrant-ubuntu-trusty-64:~$ sudo -u postgres pg_ctlcluster 9.3 main start<br />vagrant@vagrant-ubuntu-trusty-64:~$</span></span><br /><br /><br />";}}s:7:"channel";a:7:{s:5:"title";s:17:"Planet PostgreSQL";s:4:"link";s:28:"http://planet.postgresql.org";s:11:"description";s:17:"Planet PostgreSQL";s:13:"lastbuilddate";s:29:"Thu, 15 May 2014 16:48:28 GMT";s:9:"generator";s:17:"Planet PostgreSQL";s:4:"docs";s:37:"http://blogs.law.harvard.edu/tech/rss";s:7:"tagline";s:17:"Planet PostgreSQL";}s:9:"textinput";a:0:{}s:5:"image";a:0:{}s:9:"feed_type";s:3:"RSS";s:12:"feed_version";s:3:"2.0";s:5:"stack";a:0:{}s:9:"inchannel";b:0;s:6:"initem";b:0;s:9:"incontent";b:0;s:11:"intextinput";b:0;s:7:"inimage";b:0;s:13:"current_field";s:0:"";s:17:"current_namespace";b:0;s:5:"ERROR";s:0:"";s:19:"_CONTENT_CONSTRUCTS";a:6:{i:0;s:7:"content";i:1;s:7:"summary";i:2;s:4:"info";i:3;s:5:"title";i:4;s:7:"tagline";i:5;s:9:"copyright";}s:4:"etag";s:13:""443105687"
";s:13:"last_modified";s:31:"Thu, 15 May 2014 16:48:28 GMT
";}