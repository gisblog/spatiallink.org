O:9:"MagpieRSS":19:{s:6:"parser";i:0;s:12:"current_item";a:0:{}s:5:"items";a:50:{i:0;a:10:{s:5:"title";s:32:"“Toy” database on mainframes";s:4:"guid";s:41:"https://www.flamingspork.com/blog/?p=4044";s:4:"link";s:72:"https://www.flamingspork.com/blog/2016/04/29/toy-database-on-mainframes/";s:11:"description";s:525:"While much less common than 10 or 15 (err&#8230; even 20) years ago, you still sometimes here MySQL being called a &#8220;toy&#8221; database. The good news is, when somebody says that, they&#8217;re admitting ignorance and you can help educate them!
Recently, a fellow IBMer submitted a pull request (and bug) to start having MySQL support on Z Series (s390x).
Generally speaking, when there&#8217;s effort being spent on getting something to run on Z, it is in no way considered a toy by those who&#8217;ll end up using it.";s:7:"content";a:1:{s:7:"encoded";s:873:"<p>While much less common than 10 or 15 (err&#8230; even 20) years ago, you still sometimes here MySQL being called a &#8220;toy&#8221; database. The good news is, when somebody says that, they&#8217;re admitting ignorance and you can help educate them!</p>
<p>Recently, a fellow IBMer submitted a <a href="https://github.com/mysql/mysql-server/pull/58">pull request</a> (and <a href="https://bugs.mysql.com/bug.php?id=80907">bug</a>) to start having MySQL support on Z Series (s390x).</p>
<p>Generally speaking, when there&#8217;s effort being spent on getting something to run on Z, it is in no way considered a toy by those who&#8217;ll end up using it.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995338&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995338&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Fri, 29 Apr 2016 04:34:05 +0000";s:2:"dc";a:1:{s:7:"creator";s:13:"Stewart Smith";}s:8:"category";s:14:"IBMmysqls390xz";s:7:"summary";s:525:"While much less common than 10 or 15 (err&#8230; even 20) years ago, you still sometimes here MySQL being called a &#8220;toy&#8221; database. The good news is, when somebody says that, they&#8217;re admitting ignorance and you can help educate them!
Recently, a fellow IBMer submitted a pull request (and bug) to start having MySQL support on Z Series (s390x).
Generally speaking, when there&#8217;s effort being spent on getting something to run on Z, it is in no way considered a toy by those who&#8217;ll end up using it.";s:12:"atom_content";s:873:"<p>While much less common than 10 or 15 (err&#8230; even 20) years ago, you still sometimes here MySQL being called a &#8220;toy&#8221; database. The good news is, when somebody says that, they&#8217;re admitting ignorance and you can help educate them!</p>
<p>Recently, a fellow IBMer submitted a <a href="https://github.com/mysql/mysql-server/pull/58">pull request</a> (and <a href="https://bugs.mysql.com/bug.php?id=80907">bug</a>) to start having MySQL support on Z Series (s390x).</p>
<p>Generally speaking, when there&#8217;s effort being spent on getting something to run on Z, it is in no way considered a toy by those who&#8217;ll end up using it.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995338&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995338&vote=-1&apivote=1">Vote DOWN</a>";}i:1;a:10:{s:5:"title";s:52:"Log Buffer #471: A Carnival of the Vanities for DBAs";s:4:"guid";s:37:"https://www.pythian.com/blog/?p=87837";s:4:"link";s:67:"https://www.pythian.com/blog/log-buffer-471-carnival-vanities-dbas/";s:11:"description";s:1323:"This Log Buffer Edition covers Oracle, SQL Server and MySQL blog posts of the week.Oracle:Improving PL/SQL performance in APEXA utility to extract and present PeopleSoft Configuration and Performance DataNo, Oracle security vulnerabilities didn’t just get a whole lot worse this quarter.  Instead, Oracle updated the scoring metric used in the Critical Patch Updates (CPU) from CVSS v2 to CVSS v3.0 for the April 2016 CPU.  The Common Vulnerability Score System (CVSS) is a generally accepted method for scoring and rating security vulnerabilities.  CVSS is used by Oracle, Microsoft, Cisco, and other major software vendors.Oracle Cloud – DBaaS instance down for no apparent reasonUsing guaranteed restore points to navigate through timeSQL Server:ANSI SQL with Analytic Functions on Snowflake DBExporting Azure Data Factory (ADF) into TFS Source ControlGetting started with Azure SQL Data WarehousePerformance Surprises and Assumptions : DATEADD()With the new security policy feature in SQL Server 2016 you can restrict write operations at the row level by defining a block predicate.MySQL:How to rename MySQL DB name by moving tablesMySQL 5.7 Introduces a JSON Data TypeUbuntu 16.04 first stable distro with MySQL 5.7MariaDB AWS Key Management Service (KMS) Encryption PluginMySQL Document Store versus Bug hunter";s:7:"content";a:1:{s:7:"encoded";s:3351:"<div><div><p>This Log Buffer Edition covers Oracle, SQL Server and MySQL blog posts of the week.</p><p><span></span><strong>Oracle:</strong></p><p>Improving <a href="http://www.grassroots-oracle.com/2016/04/improving-plsql-performance-in-apex.html">PL/SQL</a> performance in APEX</p><p>A utility to extract and present <a href="http://blog.psftdba.com/2016/04/ps360-utility-to-extract-and-present.html">PeopleSoft</a> Configuration and Performance Data</p><p>No, Oracle security vulnerabilities didn’t just get a whole lot worse this quarter.  Instead, Oracle updated the scoring metric used in the Critical Patch Updates (CPU) from CVSS v2 to CVSS v3.0 for the April 2016 <a href="https://www.integrigy.com/security-resources/oracle-security-vulnerability-scoring-metric-change-cvss">CPU</a>.  The Common Vulnerability Score System (CVSS) is a generally accepted method for scoring and rating security vulnerabilities.  CVSS is used by Oracle, Microsoft, Cisco, and other major software vendors.</p><p>Oracle Cloud – <a href="https://technology.amis.nl/2016/04/20/oracle-cloud-dbaas-instance-down-for-no-apparent-reason-how-archive-log-mode-and-storage-shortage-forced-the-instance-to-its-knees/">DBaaS</a> instance down for no apparent reason</p><p>Using guaranteed restore points to <a href="https://technology.amis.nl/2016/04/20/using-guaranteed-restore-points-navigate-time/">navigate</a> through time</p><p><strong>SQL Server:</strong></p><p>ANSI SQL with Analytic Functions on <a href="https://kentgraziano.com/2016/04/21/ansi-sql-with-analytic-functions-on-snowflake-db/">Snowflake</a> DB</p><p>Exporting Azure Data Factory (<a href="http://www.sqlservercentral.com/articles/Azure+Data+Factory/139478/">ADF</a>) into TFS Source Control</p><p>Getting started with Azure <a href="https://www.simple-talk.com/cloud/cloud-data/getting-started-with-azure-sql-data-warehouse/">SQL</a> Data Warehouse</p><p>Performance Surprises and Assumptions : <a href="http://sqlperformance.com/2016/04/sql-performance/surprises-dateadd">DATEADD</a>()</p><p>With the new security policy feature in <a href="http://www.databasejournal.com/features/mssql/row-level-security-with-sql-server-2016-part-2-blocking-updates-at-the-row-level.html">SQL</a> Server 2016 you can restrict write operations at the row level by defining a block predicate.</p><p><strong>MySQL:</strong></p><p>How to rename <a href="http://www.geeksww.com/tutorials/database_management_systems/mysql/administration/how_to_rename_mysql_db_name_by_moving_tables.php">MySQL</a> DB name by moving tables</p><p>MySQL 5.7 Introduces a <a href="http://www.lornajane.net/posts/2016/mysql-5-7-json-features">JSON</a> Data Type</p><p>Ubuntu 16.04 first stable distro with <a href="http://mysqlserverteam.com/ubuntu-16-04-first-stable-distro-with-mysql-5-7/">MySQL</a> 5.7</p><p>MariaDB AWS Key Management Service (<a href="http://serge.frezefond.com/2016/04/mariadb-aws-key-management-service-kms-encryption-plugin/">KMS</a>) Encryption Plugin</p><p>MySQL Document Store versus <a href="http://mysql.az/2016/04/20/mysql-document-store-versus-bug-hunter/">Bug</a> hunter</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995330&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995330&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 28 Apr 2016 15:14:40 +0000";s:2:"dc";a:1:{s:7:"creator";s:17:"The Pythian Group";}s:8:"category";s:25:"Log BufferTechnical Track";s:7:"summary";s:1323:"This Log Buffer Edition covers Oracle, SQL Server and MySQL blog posts of the week.Oracle:Improving PL/SQL performance in APEXA utility to extract and present PeopleSoft Configuration and Performance DataNo, Oracle security vulnerabilities didn’t just get a whole lot worse this quarter.  Instead, Oracle updated the scoring metric used in the Critical Patch Updates (CPU) from CVSS v2 to CVSS v3.0 for the April 2016 CPU.  The Common Vulnerability Score System (CVSS) is a generally accepted method for scoring and rating security vulnerabilities.  CVSS is used by Oracle, Microsoft, Cisco, and other major software vendors.Oracle Cloud – DBaaS instance down for no apparent reasonUsing guaranteed restore points to navigate through timeSQL Server:ANSI SQL with Analytic Functions on Snowflake DBExporting Azure Data Factory (ADF) into TFS Source ControlGetting started with Azure SQL Data WarehousePerformance Surprises and Assumptions : DATEADD()With the new security policy feature in SQL Server 2016 you can restrict write operations at the row level by defining a block predicate.MySQL:How to rename MySQL DB name by moving tablesMySQL 5.7 Introduces a JSON Data TypeUbuntu 16.04 first stable distro with MySQL 5.7MariaDB AWS Key Management Service (KMS) Encryption PluginMySQL Document Store versus Bug hunter";s:12:"atom_content";s:3351:"<div><div><p>This Log Buffer Edition covers Oracle, SQL Server and MySQL blog posts of the week.</p><p><span></span><strong>Oracle:</strong></p><p>Improving <a href="http://www.grassroots-oracle.com/2016/04/improving-plsql-performance-in-apex.html">PL/SQL</a> performance in APEX</p><p>A utility to extract and present <a href="http://blog.psftdba.com/2016/04/ps360-utility-to-extract-and-present.html">PeopleSoft</a> Configuration and Performance Data</p><p>No, Oracle security vulnerabilities didn’t just get a whole lot worse this quarter.  Instead, Oracle updated the scoring metric used in the Critical Patch Updates (CPU) from CVSS v2 to CVSS v3.0 for the April 2016 <a href="https://www.integrigy.com/security-resources/oracle-security-vulnerability-scoring-metric-change-cvss">CPU</a>.  The Common Vulnerability Score System (CVSS) is a generally accepted method for scoring and rating security vulnerabilities.  CVSS is used by Oracle, Microsoft, Cisco, and other major software vendors.</p><p>Oracle Cloud – <a href="https://technology.amis.nl/2016/04/20/oracle-cloud-dbaas-instance-down-for-no-apparent-reason-how-archive-log-mode-and-storage-shortage-forced-the-instance-to-its-knees/">DBaaS</a> instance down for no apparent reason</p><p>Using guaranteed restore points to <a href="https://technology.amis.nl/2016/04/20/using-guaranteed-restore-points-navigate-time/">navigate</a> through time</p><p><strong>SQL Server:</strong></p><p>ANSI SQL with Analytic Functions on <a href="https://kentgraziano.com/2016/04/21/ansi-sql-with-analytic-functions-on-snowflake-db/">Snowflake</a> DB</p><p>Exporting Azure Data Factory (<a href="http://www.sqlservercentral.com/articles/Azure+Data+Factory/139478/">ADF</a>) into TFS Source Control</p><p>Getting started with Azure <a href="https://www.simple-talk.com/cloud/cloud-data/getting-started-with-azure-sql-data-warehouse/">SQL</a> Data Warehouse</p><p>Performance Surprises and Assumptions : <a href="http://sqlperformance.com/2016/04/sql-performance/surprises-dateadd">DATEADD</a>()</p><p>With the new security policy feature in <a href="http://www.databasejournal.com/features/mssql/row-level-security-with-sql-server-2016-part-2-blocking-updates-at-the-row-level.html">SQL</a> Server 2016 you can restrict write operations at the row level by defining a block predicate.</p><p><strong>MySQL:</strong></p><p>How to rename <a href="http://www.geeksww.com/tutorials/database_management_systems/mysql/administration/how_to_rename_mysql_db_name_by_moving_tables.php">MySQL</a> DB name by moving tables</p><p>MySQL 5.7 Introduces a <a href="http://www.lornajane.net/posts/2016/mysql-5-7-json-features">JSON</a> Data Type</p><p>Ubuntu 16.04 first stable distro with <a href="http://mysqlserverteam.com/ubuntu-16-04-first-stable-distro-with-mysql-5-7/">MySQL</a> 5.7</p><p>MariaDB AWS Key Management Service (<a href="http://serge.frezefond.com/2016/04/mariadb-aws-key-management-service-kms-encryption-plugin/">KMS</a>) Encryption Plugin</p><p>MySQL Document Store versus <a href="http://mysql.az/2016/04/20/mysql-document-store-versus-bug-hunter/">Bug</a> hunter</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995330&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995330&vote=-1&apivote=1">Vote DOWN</a>";}i:2;a:9:{s:5:"title";s:54:"Writing SQL that works on PostgreSQL, MySQL and SQLite";s:4:"guid";s:57:"http://evertpot.com/writing-sql-for-postgres-mysql-sqlite";s:4:"link";s:97:"http://feedproxy.google.com/~r/bijsterespoor/~3/_BRoV7zVClY/writing-sql-for-postgres-mysql-sqlite";s:11:"description";s:9785:"I am one of those crazy people who attempts to write SQL that works on
SQlite, MySQL and PostgreSQL. First I should explain why:

This is all for my project sabre/dav. sabre/dav is a server for CalDAV,
CardDAV and WebDAV. One of the big design goals is that it this project has to
be a library first, and should be easily integratable into existing
applications.

To do this effectively, it’s important that it’s largely agnostic to the host
platform, and one of the best ways (in my opinion) to achieve that is to have
as little dependencies as possible. Adding dependencies such as Doctrine
is a great idea for applications or more opinionated frameworks, but for
sabre/dav lightweight is key, and I need people to be able to understand the
extension points fairly easily, without requiring them to familiarize them
with the details of the dependency graph.

So while you are completely free to choose to add Doctrine or Propel
yourself, the core adapters (wich function both as a default implementation
and as samples for writing your own), all only depend on an instance of
PDO.

The nice thing is that ORMs such as Doctrine and Propel, you can get access
to the underlying PDO connection object and pass that, thus reusing your
existing configuration.

For the longest time we only supported SQLite and MySQL, but I’m now working
on adding PostgreSQL support. So I figured, I might as well write down my
notes.

But how feasable is it to write SQL that works everywhere?

Well, it turns out that this is actually not super easy. There is such as
thing as Standard SQL, but all of these databases have many of their
own extensions and deviations.

The most important thing is that this will likely only work well for you if
you have a very simple schema and simple queries.

Well, this blog post is not intended as a full guide, I’m just listing the
particular things I’ve ran into. If you have your own, you can edit this blog
post on github, or leave a comment.

My approach


  I try to keep my queries as simple as possible.
  If I can rewrite a query to work on every database, that query will have the
preference.
  I avoid stored procedures, triggers, functions, views. I’m really just
dealing with tables and indexes.
  Even if that means that it’s not the most optimal query. So I’m ok with
sarcrificing some performance, if that means my queries can stay generic,
within reason.
  If there’s no possible way to do things in a generic way, I fall back on
something like this:


&lt;?php

if ($pdo-&gt;getAttribute(PDO::ATTR_DRIVER_NAME) === 'pgsql') {

    $query = "...";

} else {

    $query = "...';

}

$stmt = $pdo-&gt;prepare($query);


?&gt;



DDL

First there is the “Data Definition Language” and “Data Manipulation Language”
the former is used for queries starting with CREATE, ALTER, DROP, etc,
and the latter SELECT, UPDATE, DELETE, INSERT.

There really is no sane way to generalize your CREATE TABLE queries, as the
types and syntax are vastly different.

So for those we have a set of .sql files for every server.

Quoting

In MySQL and SQlite you can use either quotes ' or double quotes " to wrap
a string.

In PostgreSQL, you always have to use single quotes '.

In MySQL and SQLite you use backticks for identifiers. PostgreSQL uses single
quotes. SQlite can also use single quotes here if the result is unambigious,
but I would strongly suggest to avoid that.

This means that this MySQL query:

SELECT * FROM `foo` WHERE `a` = "b"



is equivalent to this PostgreSQL query:

SELECT * FROM "foo" WHERE "a" = 'b'



Luckily you can often just write this query, which works for all databases:

SELECT * FROM foo WHERE a = 'b'



But keep in mind that when you create your tables, using double quotes will
cause PostgreSQL to retain the upper/lower case characters. If you do not use
quotes, it will normalize everything to lower case.

For compatibility I would therefore suggest to make sure that all your table
and column names are in lower case.

REPLACE INTO

The REPLACE INTO is a useful extension that is supported by both SQLite and
MySQL. The syntax is identical INSERT INTO, except that if it runs into a
key conflict, it will overwrite the existing record instead of inserting a new
one.

So REPLACE INTO basically either updates or inserts a new record.

This works on both SQLite and MySQL, but not PostgreSQL. Since version 9.5
PostgreSQL gained a new feature that allows you to achieve the same effect.

This statement from MySQL or SQLite:

REPLACE INTO blog (uuid, title) VALUES (:uuid, :title)



then might become something like this in PostgreSQL:

INSERT INTO blog (uuid, title) VALUES (:uuid, :title)
ON CONFLICT (uuid) DO UPDATE SET title = :title



So the major difference here is with PostgreSQL we specifically have to tell
it which key conflict we’re handling (uuid) and what to do in that case
(UPDATE).

In addition to REPLACE INTO, MySQL also has this syntax to do the same thing:

INSERT INTO blog (uuid, title) VALUES (:uuid, :title)
ON DUPLICATE KEY UPDATE title = :title



But as far as I know SQLite does not have a direct equivalent.

BLOB

SQLite and MySQL have a BLOB type. This type is used for storing data as-is.
Whatever (binary) string you store, you will retrieve again and no conversion
is attempted for different character sets.

PostgreSQL has two types that have a similar purpose: Large Objects and
the bytea type.

The best way to describe large objects, is that they are stored ‘separate’ from
the table, and instead of inserting the object itself, you store a reference to
the object (in the form of an id).

bytea is more similar to BLOB, so I opted to use that. But there are some
differences.

First, if you do a select such as this:

&lt;?php

$stmt = $pdo-&gt;prepare('SELECT myblob FROM binaries WHERE id = :id');
$stmt-&gt;execute(['id' =&gt; $id]);

echo $stmt-&gt;fetchColumn();

?&gt;



On MySQL and Sqlite this will just work. The myblob field is represented as
a string.

On PostgreSQL, byta is represented as a PHP stream. So you might have to
rewrite that last statement as:

&lt;?php

echo stream_get_contents($stmt-&gt;fetchColumn());

?&gt;



Or:

&lt;?php

stream_copy_to_stream($stmt-&gt;fetchColumn(), STDOUT);

?&gt;



Luckily in sabre/dav we pretty much support streams where we also support
strings, so we were already agnositic to this, but some unittests had to be
adjusted.

Inserting bytea is also a bit different. I’m not a fan of of using
PDOStatement::bindValue and PDOStatement::bindParam, instead
I prefer to just send all my bound parameters at once using execute:

&lt;?php

$stmt = $pdo-&gt;prepare('INSERT INTO binaries (myblob) (:myblob)');
$stmt-&gt;execute([
    'myblob' =&gt; $blob
]);

?&gt;



While that works for PostgreSQL for some strings, it will throw errors
when you give it data that’s invalid in the current character set. It’s also
dangerous, as PostgreSQL might try to transcode the data into a different
character set.

If you truly need to store binary data (like I do) you must do this:

&lt;?php

$stmt = $pdo-&gt;prepare('INSERT INTO binaries (myblob) (:myblob)');
$stmt-&gt;bindParam('myblob', $blob, PDO::PARAM_LOB);
$stmt-&gt;execute();

?&gt;



Luckily this also just works in SQlite and MySQL.

String concatenation

Standard SQL has a string concatenation operator. It works like this:

SELECT 'foo' || 'bar'
// Output: foobar



This works in PostgreSQL and Sqlite. MySQL has a function for this:

SELECT CONCAT('foo', 'bar')



PostgreSQL also has this function, but SQLite does not. You can enable Standard
SQL concatenation in MySQL by enabling it:

SET SESSION sql_mode = 'PIPES_AS_CONCAT'



I’m not sure why this isn’t the default.

Last insert ID

The PDO object has a lastInsertId() function. For SQLite and MySQL you can
just call it as such:

&lt;?php

$id = $pdo-&gt;lastInsertId();

?&gt;



However, PostgreSQL requires an explicit sequence identifier. By default this
follows the format tablename_idfield_seq, so we might specifiy as this:

&lt;?php

$id = $pdo-&gt;lastInsertId('articles_id_seq');

?&gt;



Luckily the parameter gets ignored by SQLite and MySQL, so we can just specify
it all the time.

Type casting

If you have an INT field (or similar) and you access it in this way:

&lt;?php

$result = $pdo-&gt;query('SELECT id FROM articles');
$id = $result-&gt;fetchColumn();

?&gt;



With PostgreSQL $id will actually have the type php type integer. If you
use MySQL or SQlite, everything gets cast to a php string, which is
unfortunate.

The sane thing to do is to cast everything to int after the fact, so you can
correctly do PHP 7 strict typing with these in the future.

Testing

I unittest my database code. Yep, you read that right! I’m one of those people.
It’s been tremendously useful.

Since adding PostgreSQL I was able to come up with a nice structure. Every
unittest that does something with PDO now generally looks like this:

&lt;?php

abstract PDOTest extends \PHPUnit_Framework_TestCase {

    abstract function getPDO();

    /** all the unittests go here **/

}

?&gt;



Then I create one subclass for PostgreSQL, Sqlite and MySQL that each only
implement the getPDO() function.

This way all my tests are repeated for each driver.

I’ve also rigged up Travis CI to have a MySQL and a PostgreSQL database
server running, so everything automatically gets checked every time.

If a developer is testing locally, we detect if a database server is running,
and automatically just skip the tests if this was not the case. In most cases
this means only the Sqlite tests get hit, which is fine.

Conclusions


  Created a monster.
  PostgreSQL is by far the sanest database, and I would recommend everyone to
  move from MySQL towards it.


";s:7:"content";a:1:{s:7:"encoded";s:16465:"<p>I am one of those crazy people who attempts to write SQL that works on
<a href="https://www.sqlite.org/">SQlite</a>, <a href="https://www.mysql.com/">MySQL</a> and <a href="http://www.postgresql.org/">PostgreSQL</a>. First I should explain why:</p>

<p>This is all for my project <a href="http://sabre.io/dav/" title="sabre/dav">sabre/dav</a>. sabre/dav is a server for CalDAV,
CardDAV and WebDAV. One of the big design goals is that it this project has to
be a library first, and should be easily integratable into existing
applications.</p>

<p>To do this effectively, it’s important that it’s largely agnostic to the host
platform, and one of the best ways (in my opinion) to achieve that is to have
as little dependencies as possible. Adding dependencies such as <a href="http://www.doctrine-project.org/" title="Doctrine project">Doctrine</a>
is a great idea for applications or more opinionated frameworks, but for
sabre/dav lightweight is key, and I need people to be able to understand the
extension points fairly easily, without requiring them to familiarize them
with the details of the dependency graph.</p>

<p>So while you are completely free to choose to add Doctrine or <a href="http://propelorm.org/" title="Propel ORM">Propel</a>
yourself, the core adapters (wich function both as a default implementation
and as samples for writing your own), all only depend on an instance of
<a href="http://ca2.php.net/manual/en/book.pdo.php" title="PHP PDO documentation">PDO</a>.</p>

<p>The nice thing is that ORMs such as Doctrine and Propel, you can get access
to the underlying PDO connection object and pass that, thus reusing your
existing configuration.</p>

<p>For the longest time we only supported SQLite and MySQL, but I’m now working
on adding PostgreSQL support. So I figured, I might as well write down my
notes.</p>

<h2>But how feasable is it to write SQL that works everywhere?</h2>

<p>Well, it turns out that this is actually not super easy. There is such as
thing as <a href="https://en.wikipedia.org/wiki/SQL-92" title="SQL-92">Standard SQL</a>, but all of these databases have many of their
own extensions and deviations.</p>

<p>The most important thing is that this will likely only work well for you if
you have a very simple schema and simple queries.</p>

<p>Well, this blog post is not intended as a full guide, I’m just listing the
particular things I’ve ran into. If you have your own, you can <a href="https://github.com/evert/evert.github.com/blob/master/_posts/2016/2016-04-28-writing-sql-for-postgres-mysql-sqlite.md">edit this blog
post</a> on github, or leave a comment.</p>

<h3>My approach</h3>

<ul>
  <li>I try to keep my queries as simple as possible.</li>
  <li>If I can rewrite a query to work on every database, that query will have the
preference.</li>
  <li>I avoid stored procedures, triggers, functions, views. I’m really just
dealing with tables and indexes.</li>
  <li>Even if that means that it’s not the most optimal query. So I’m ok with
sarcrificing some performance, if that means my queries can stay generic,
within reason.</li>
  <li>If there’s no possible way to do things in a generic way, I fall back on
something like this:</li>
</ul>

<div><pre><code><span>&lt;?php</span>

<span>if</span> <span>(</span><span>$pdo</span><span>-&gt;</span><span>getAttribute</span><span>(</span><span>PDO</span><span>::</span><span>ATTR_DRIVER_NAME</span><span>)</span> <span>===</span> <span>'pgsql'</span><span>)</span> <span>{</span>

    <span>$query</span> <span>=</span> <span>"..."</span><span>;</span>

<span>}</span> <span>else</span> <span>{</span>

    <span>$query</span> <span>=</span> <span>"...';

}

</span><span>$stmt</span><span> = </span><span>$pdo-&gt;prepare</span><span>(</span><span>$query</span><span>);


?&gt;
</span></code></pre>
</div>

<h3>DDL</h3>

<p>First there is the “Data Definition Language” and “Data Manipulation Language”
the former is used for queries starting with <code>CREATE</code>, <code>ALTER</code>, <code>DROP</code>, etc,
and the latter <code>SELECT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>INSERT</code>.</p>

<p>There really is no sane way to generalize your <code>CREATE TABLE</code> queries, as the
types and syntax are vastly different.</p>

<p>So for those we have a <a href="https://github.com/fruux/sabre-dav/tree/master/examples/sql" title="sabre/dav sql samples">set of .sql files</a> for every server.</p>

<h3>Quoting</h3>

<p>In MySQL and SQlite you can use either quotes <code>'</code> or double quotes <code>"</code> to wrap
a string.</p>

<p>In PostgreSQL, you always have to use single quotes <code>'</code>.</p>

<p>In MySQL and SQLite you use backticks for identifiers. PostgreSQL uses single
quotes. SQlite can also use single quotes here if the result is unambigious,
but I would strongly suggest to avoid that.</p>

<p>This means that this MySQL query:</p>

<div><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>`foo`</span> <span>WHERE</span> <span>`a`</span> <span>=</span> <span>"b"</span>
</code></pre>
</div>

<p>is equivalent to this PostgreSQL query:</p>

<div><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>"foo"</span> <span>WHERE</span> <span>"a"</span> <span>=</span> <span>'b'</span>
</code></pre>
</div>

<p>Luckily you can often just write this query, which works for all databases:</p>

<div><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>foo</span> <span>WHERE</span> <span>a</span> <span>=</span> <span>'b'</span>
</code></pre>
</div>

<p>But keep in mind that when you create your tables, using double quotes will
cause PostgreSQL to retain the upper/lower case characters. If you do not use
quotes, it will normalize everything to lower case.</p>

<p>For compatibility I would therefore suggest to make sure that all your table
and column names are in lower case.</p>

<h3>REPLACE INTO</h3>

<p>The <code>REPLACE INTO</code> is a useful extension that is supported by both SQLite and
MySQL. The syntax is identical <code>INSERT INTO</code>, except that if it runs into a
key conflict, it will overwrite the existing record instead of inserting a new
one.</p>

<p>So <code>REPLACE INTO</code> basically either updates or inserts a new record.</p>

<p>This works on both SQLite and MySQL, but not PostgreSQL. Since version 9.5
PostgreSQL gained a new feature that allows you to achieve the same effect.</p>

<p>This statement from MySQL or SQLite:</p>

<div><pre><code><span>REPLACE</span> <span>INTO</span> <span>blog</span> <span>(</span><span>uuid</span><span>,</span> <span>title</span><span>)</span> <span>VALUES</span> <span>(:</span><span>uuid</span><span>,</span> <span>:</span><span>title</span><span>)</span>
</code></pre>
</div>

<p>then might become something like this in PostgreSQL:</p>

<div><pre><code><span>INSERT</span> <span>INTO</span> <span>blog</span> <span>(</span><span>uuid</span><span>,</span> <span>title</span><span>)</span> <span>VALUES</span> <span>(:</span><span>uuid</span><span>,</span> <span>:</span><span>title</span><span>)</span>
<span>ON</span> <span>CONFLICT</span> <span>(</span><span>uuid</span><span>)</span> <span>DO</span> <span>UPDATE</span> <span>SET</span> <span>title</span> <span>=</span> <span>:</span><span>title</span>
</code></pre>
</div>

<p>So the major difference here is with PostgreSQL we specifically have to tell
it which key conflict we’re handling (<code>uuid</code>) and what to do in that case
(<code>UPDATE</code>).</p>

<p>In addition to <code>REPLACE INTO</code>, MySQL also has this syntax to do the same thing:</p>

<div><pre><code><span>INSERT</span> <span>INTO</span> <span>blog</span> <span>(</span><span>uuid</span><span>,</span> <span>title</span><span>)</span> <span>VALUES</span> <span>(:</span><span>uuid</span><span>,</span> <span>:</span><span>title</span><span>)</span>
<span>ON</span> <span>DUPLICATE</span> <span>KEY</span> <span>UPDATE</span> <span>title</span> <span>=</span> <span>:</span><span>title</span>
</code></pre>
</div>

<p>But as far as I know SQLite does not have a direct equivalent.</p>

<h3>BLOB</h3>

<p>SQLite and MySQL have a <code>BLOB</code> type. This type is used for storing data as-is.
Whatever (binary) string you store, you will retrieve again and no conversion
is attempted for different character sets.</p>

<p>PostgreSQL has two types that have a similar purpose: <a href="http://www.postgresql.org/docs/9.5/static/largeobjects.html" title="PostgreSQL large objects">Large Objects</a> and
the <a href="http://www.postgresql.org/docs/9.5/static/datatype-binary.html" title="PostgreSQL byeta type">bytea</a> type.</p>

<p>The best way to describe large objects, is that they are stored ‘separate’ from
the table, and instead of inserting the object itself, you store a reference to
the object (in the form of an id).</p>

<p><code>bytea</code> is more similar to <code>BLOB</code>, so I opted to use that. But there are some
differences.</p>

<p>First, if you do a select such as this:</p>

<div><pre><code><span>&lt;?php</span>

<span>$stmt</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>prepare</span><span>(</span><span>'SELECT myblob FROM binaries WHERE id = :id'</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>execute</span><span>([</span><span>'id'</span> <span>=&gt;</span> <span>$id</span><span>]);</span>

<span>echo</span> <span>$stmt</span><span>-&gt;</span><span>fetchColumn</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>On MySQL and Sqlite this will just work. The <code>myblob</code> field is represented as
a string.</p>

<p>On PostgreSQL, <code>byta</code> is represented as a PHP stream. So you might have to
rewrite that last statement as:</p>

<div><pre><code><span>&lt;?php</span>

<span>echo</span> <span>stream_get_contents</span><span>(</span><span>$stmt</span><span>-&gt;</span><span>fetchColumn</span><span>());</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Or:</p>

<div><pre><code><span>&lt;?php</span>

<span>stream_copy_to_stream</span><span>(</span><span>$stmt</span><span>-&gt;</span><span>fetchColumn</span><span>(),</span> <span>STDOUT</span><span>);</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Luckily in sabre/dav we pretty much support streams where we also support
strings, so we were already agnositic to this, but some unittests had to be
adjusted.</p>

<p>Inserting <code>bytea</code> is also a bit different. I’m not a fan of of using
<a href="http://php.net/manual/en/pdostatement.bindvalue.php" title="PDOStatement::bindValue"><code>PDOStatement::bindValue</code></a> and <a href="http://php.net/manual/en/pdostatement.bindparam.php" title="PDOStatement::bindParam"><code>PDOStatement::bindParam</code></a>, instead
I prefer to just send all my bound parameters at once using <code>execute</code>:</p>

<div><pre><code><span>&lt;?php</span>

<span>$stmt</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>prepare</span><span>(</span><span>'INSERT INTO binaries (myblob) (:myblob)'</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>execute</span><span>([</span>
    <span>'myblob'</span> <span>=&gt;</span> <span>$blob</span>
<span>]);</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>While that works for PostgreSQL for some strings, it will throw errors
when you give it data that’s invalid in the current character set. It’s also
dangerous, as PostgreSQL might try to transcode the data into a different
character set.</p>

<p>If you truly need to store binary data (like I do) you must do this:</p>

<div><pre><code><span>&lt;?php</span>

<span>$stmt</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>prepare</span><span>(</span><span>'INSERT INTO binaries (myblob) (:myblob)'</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>bindParam</span><span>(</span><span>'myblob'</span><span>,</span> <span>$blob</span><span>,</span> <span>PDO</span><span>::</span><span>PARAM_LOB</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>execute</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Luckily this also just works in SQlite and MySQL.</p>

<h3>String concatenation</h3>

<p>Standard SQL has a string concatenation operator. It works like this:</p>

<div><pre><code><span>SELECT</span> <span>'foo'</span> <span>||</span> <span>'bar'</span>
<span>//</span> <span>Output</span><span>:</span> <span>foobar</span>
</code></pre>
</div>

<p>This works in PostgreSQL and Sqlite. MySQL has a function for this:</p>

<div><pre><code><span>SELECT</span> <span>CONCAT</span><span>(</span><span>'foo'</span><span>,</span> <span>'bar'</span><span>)</span>
</code></pre>
</div>

<p>PostgreSQL also has this function, but SQLite does not. You can enable Standard
SQL concatenation in MySQL by enabling it:</p>

<div><pre><code><span>SET</span> <span>SESSION</span> <span>sql_mode</span> <span>=</span> <span>'PIPES_AS_CONCAT'</span>
</code></pre>
</div>

<p>I’m not sure why this isn’t the default.</p>

<h3>Last insert ID</h3>

<p>The <code>PDO</code> object has a <code>lastInsertId()</code> function. For SQLite and MySQL you can
just call it as such:</p>

<div><pre><code><span>&lt;?php</span>

<span>$id</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>lastInsertId</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>However, PostgreSQL requires an explicit sequence identifier. By default this
follows the format <code>tablename_idfield_seq</code>, so we might specifiy as this:</p>

<div><pre><code><span>&lt;?php</span>

<span>$id</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>lastInsertId</span><span>(</span><span>'articles_id_seq'</span><span>);</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Luckily the parameter gets ignored by SQLite and MySQL, so we can just specify
it all the time.</p>

<h3>Type casting</h3>

<p>If you have an <code>INT</code> field (or similar) and you access it in this way:</p>

<div><pre><code><span>&lt;?php</span>

<span>$result</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>query</span><span>(</span><span>'SELECT id FROM articles'</span><span>);</span>
<span>$id</span> <span>=</span> <span>$result</span><span>-&gt;</span><span>fetchColumn</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>With PostgreSQL <code>$id</code> will actually have the type php type <code>integer</code>. If you
use MySQL or SQlite, everything gets cast to a php <code>string</code>, which is
unfortunate.</p>

<p>The sane thing to do is to cast everything to int after the fact, so you can
correctly do PHP 7 strict typing with these in the future.</p>

<h2>Testing</h2>

<p>I unittest my database code. Yep, you read that right! I’m one of those people.
It’s been tremendously useful.</p>

<p>Since adding PostgreSQL I was able to come up with a nice structure. Every
unittest that does something with PDO now generally looks like this:</p>

<div><pre><code><span>&lt;?php</span>

<span>abstract</span> <span>PDOTest</span> <span>extends</span> <span>\PHPUnit_Framework_TestCase</span> <span>{</span>

    <span>abstract</span> <span>function</span> <span>getPDO</span><span>();</span>

    <span>/** all the unittests go here **/</span>

<span>}</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Then I create one subclass for PostgreSQL, Sqlite and MySQL that each only
implement the <code>getPDO()</code> function.</p>

<p>This way all my tests are repeated for each driver.</p>

<p>I’ve also rigged up <a href="https://travis-ci.org/">Travis CI</a> to have a MySQL and a PostgreSQL database
server running, so everything automatically gets checked every time.</p>

<p>If a developer is testing locally, we detect if a database server is running,
and automatically just skip the tests if this was not the case. In most cases
this means only the Sqlite tests get hit, which is fine.</p>

<h2>Conclusions</h2>

<ol>
  <li>Created a monster.</li>
  <li>PostgreSQL is by far the sanest database, and I would recommend everyone to
  move from MySQL towards it.</li>
</ol>

<img src="http://feeds.feedburner.com/~r/bijsterespoor/~4/_BRoV7zVClY" height="1" width="1" alt="" /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995321&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995321&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 28 Apr 2016 08:56:19 +0000";s:2:"dc";a:1:{s:7:"creator";s:9:"Evert Pot";}s:7:"summary";s:9785:"I am one of those crazy people who attempts to write SQL that works on
SQlite, MySQL and PostgreSQL. First I should explain why:

This is all for my project sabre/dav. sabre/dav is a server for CalDAV,
CardDAV and WebDAV. One of the big design goals is that it this project has to
be a library first, and should be easily integratable into existing
applications.

To do this effectively, it’s important that it’s largely agnostic to the host
platform, and one of the best ways (in my opinion) to achieve that is to have
as little dependencies as possible. Adding dependencies such as Doctrine
is a great idea for applications or more opinionated frameworks, but for
sabre/dav lightweight is key, and I need people to be able to understand the
extension points fairly easily, without requiring them to familiarize them
with the details of the dependency graph.

So while you are completely free to choose to add Doctrine or Propel
yourself, the core adapters (wich function both as a default implementation
and as samples for writing your own), all only depend on an instance of
PDO.

The nice thing is that ORMs such as Doctrine and Propel, you can get access
to the underlying PDO connection object and pass that, thus reusing your
existing configuration.

For the longest time we only supported SQLite and MySQL, but I’m now working
on adding PostgreSQL support. So I figured, I might as well write down my
notes.

But how feasable is it to write SQL that works everywhere?

Well, it turns out that this is actually not super easy. There is such as
thing as Standard SQL, but all of these databases have many of their
own extensions and deviations.

The most important thing is that this will likely only work well for you if
you have a very simple schema and simple queries.

Well, this blog post is not intended as a full guide, I’m just listing the
particular things I’ve ran into. If you have your own, you can edit this blog
post on github, or leave a comment.

My approach


  I try to keep my queries as simple as possible.
  If I can rewrite a query to work on every database, that query will have the
preference.
  I avoid stored procedures, triggers, functions, views. I’m really just
dealing with tables and indexes.
  Even if that means that it’s not the most optimal query. So I’m ok with
sarcrificing some performance, if that means my queries can stay generic,
within reason.
  If there’s no possible way to do things in a generic way, I fall back on
something like this:


&lt;?php

if ($pdo-&gt;getAttribute(PDO::ATTR_DRIVER_NAME) === 'pgsql') {

    $query = "...";

} else {

    $query = "...';

}

$stmt = $pdo-&gt;prepare($query);


?&gt;



DDL

First there is the “Data Definition Language” and “Data Manipulation Language”
the former is used for queries starting with CREATE, ALTER, DROP, etc,
and the latter SELECT, UPDATE, DELETE, INSERT.

There really is no sane way to generalize your CREATE TABLE queries, as the
types and syntax are vastly different.

So for those we have a set of .sql files for every server.

Quoting

In MySQL and SQlite you can use either quotes ' or double quotes " to wrap
a string.

In PostgreSQL, you always have to use single quotes '.

In MySQL and SQLite you use backticks for identifiers. PostgreSQL uses single
quotes. SQlite can also use single quotes here if the result is unambigious,
but I would strongly suggest to avoid that.

This means that this MySQL query:

SELECT * FROM `foo` WHERE `a` = "b"



is equivalent to this PostgreSQL query:

SELECT * FROM "foo" WHERE "a" = 'b'



Luckily you can often just write this query, which works for all databases:

SELECT * FROM foo WHERE a = 'b'



But keep in mind that when you create your tables, using double quotes will
cause PostgreSQL to retain the upper/lower case characters. If you do not use
quotes, it will normalize everything to lower case.

For compatibility I would therefore suggest to make sure that all your table
and column names are in lower case.

REPLACE INTO

The REPLACE INTO is a useful extension that is supported by both SQLite and
MySQL. The syntax is identical INSERT INTO, except that if it runs into a
key conflict, it will overwrite the existing record instead of inserting a new
one.

So REPLACE INTO basically either updates or inserts a new record.

This works on both SQLite and MySQL, but not PostgreSQL. Since version 9.5
PostgreSQL gained a new feature that allows you to achieve the same effect.

This statement from MySQL or SQLite:

REPLACE INTO blog (uuid, title) VALUES (:uuid, :title)



then might become something like this in PostgreSQL:

INSERT INTO blog (uuid, title) VALUES (:uuid, :title)
ON CONFLICT (uuid) DO UPDATE SET title = :title



So the major difference here is with PostgreSQL we specifically have to tell
it which key conflict we’re handling (uuid) and what to do in that case
(UPDATE).

In addition to REPLACE INTO, MySQL also has this syntax to do the same thing:

INSERT INTO blog (uuid, title) VALUES (:uuid, :title)
ON DUPLICATE KEY UPDATE title = :title



But as far as I know SQLite does not have a direct equivalent.

BLOB

SQLite and MySQL have a BLOB type. This type is used for storing data as-is.
Whatever (binary) string you store, you will retrieve again and no conversion
is attempted for different character sets.

PostgreSQL has two types that have a similar purpose: Large Objects and
the bytea type.

The best way to describe large objects, is that they are stored ‘separate’ from
the table, and instead of inserting the object itself, you store a reference to
the object (in the form of an id).

bytea is more similar to BLOB, so I opted to use that. But there are some
differences.

First, if you do a select such as this:

&lt;?php

$stmt = $pdo-&gt;prepare('SELECT myblob FROM binaries WHERE id = :id');
$stmt-&gt;execute(['id' =&gt; $id]);

echo $stmt-&gt;fetchColumn();

?&gt;



On MySQL and Sqlite this will just work. The myblob field is represented as
a string.

On PostgreSQL, byta is represented as a PHP stream. So you might have to
rewrite that last statement as:

&lt;?php

echo stream_get_contents($stmt-&gt;fetchColumn());

?&gt;



Or:

&lt;?php

stream_copy_to_stream($stmt-&gt;fetchColumn(), STDOUT);

?&gt;



Luckily in sabre/dav we pretty much support streams where we also support
strings, so we were already agnositic to this, but some unittests had to be
adjusted.

Inserting bytea is also a bit different. I’m not a fan of of using
PDOStatement::bindValue and PDOStatement::bindParam, instead
I prefer to just send all my bound parameters at once using execute:

&lt;?php

$stmt = $pdo-&gt;prepare('INSERT INTO binaries (myblob) (:myblob)');
$stmt-&gt;execute([
    'myblob' =&gt; $blob
]);

?&gt;



While that works for PostgreSQL for some strings, it will throw errors
when you give it data that’s invalid in the current character set. It’s also
dangerous, as PostgreSQL might try to transcode the data into a different
character set.

If you truly need to store binary data (like I do) you must do this:

&lt;?php

$stmt = $pdo-&gt;prepare('INSERT INTO binaries (myblob) (:myblob)');
$stmt-&gt;bindParam('myblob', $blob, PDO::PARAM_LOB);
$stmt-&gt;execute();

?&gt;



Luckily this also just works in SQlite and MySQL.

String concatenation

Standard SQL has a string concatenation operator. It works like this:

SELECT 'foo' || 'bar'
// Output: foobar



This works in PostgreSQL and Sqlite. MySQL has a function for this:

SELECT CONCAT('foo', 'bar')



PostgreSQL also has this function, but SQLite does not. You can enable Standard
SQL concatenation in MySQL by enabling it:

SET SESSION sql_mode = 'PIPES_AS_CONCAT'



I’m not sure why this isn’t the default.

Last insert ID

The PDO object has a lastInsertId() function. For SQLite and MySQL you can
just call it as such:

&lt;?php

$id = $pdo-&gt;lastInsertId();

?&gt;



However, PostgreSQL requires an explicit sequence identifier. By default this
follows the format tablename_idfield_seq, so we might specifiy as this:

&lt;?php

$id = $pdo-&gt;lastInsertId('articles_id_seq');

?&gt;



Luckily the parameter gets ignored by SQLite and MySQL, so we can just specify
it all the time.

Type casting

If you have an INT field (or similar) and you access it in this way:

&lt;?php

$result = $pdo-&gt;query('SELECT id FROM articles');
$id = $result-&gt;fetchColumn();

?&gt;



With PostgreSQL $id will actually have the type php type integer. If you
use MySQL or SQlite, everything gets cast to a php string, which is
unfortunate.

The sane thing to do is to cast everything to int after the fact, so you can
correctly do PHP 7 strict typing with these in the future.

Testing

I unittest my database code. Yep, you read that right! I’m one of those people.
It’s been tremendously useful.

Since adding PostgreSQL I was able to come up with a nice structure. Every
unittest that does something with PDO now generally looks like this:

&lt;?php

abstract PDOTest extends \PHPUnit_Framework_TestCase {

    abstract function getPDO();

    /** all the unittests go here **/

}

?&gt;



Then I create one subclass for PostgreSQL, Sqlite and MySQL that each only
implement the getPDO() function.

This way all my tests are repeated for each driver.

I’ve also rigged up Travis CI to have a MySQL and a PostgreSQL database
server running, so everything automatically gets checked every time.

If a developer is testing locally, we detect if a database server is running,
and automatically just skip the tests if this was not the case. In most cases
this means only the Sqlite tests get hit, which is fine.

Conclusions


  Created a monster.
  PostgreSQL is by far the sanest database, and I would recommend everyone to
  move from MySQL towards it.


";s:12:"atom_content";s:16465:"<p>I am one of those crazy people who attempts to write SQL that works on
<a href="https://www.sqlite.org/">SQlite</a>, <a href="https://www.mysql.com/">MySQL</a> and <a href="http://www.postgresql.org/">PostgreSQL</a>. First I should explain why:</p>

<p>This is all for my project <a href="http://sabre.io/dav/" title="sabre/dav">sabre/dav</a>. sabre/dav is a server for CalDAV,
CardDAV and WebDAV. One of the big design goals is that it this project has to
be a library first, and should be easily integratable into existing
applications.</p>

<p>To do this effectively, it’s important that it’s largely agnostic to the host
platform, and one of the best ways (in my opinion) to achieve that is to have
as little dependencies as possible. Adding dependencies such as <a href="http://www.doctrine-project.org/" title="Doctrine project">Doctrine</a>
is a great idea for applications or more opinionated frameworks, but for
sabre/dav lightweight is key, and I need people to be able to understand the
extension points fairly easily, without requiring them to familiarize them
with the details of the dependency graph.</p>

<p>So while you are completely free to choose to add Doctrine or <a href="http://propelorm.org/" title="Propel ORM">Propel</a>
yourself, the core adapters (wich function both as a default implementation
and as samples for writing your own), all only depend on an instance of
<a href="http://ca2.php.net/manual/en/book.pdo.php" title="PHP PDO documentation">PDO</a>.</p>

<p>The nice thing is that ORMs such as Doctrine and Propel, you can get access
to the underlying PDO connection object and pass that, thus reusing your
existing configuration.</p>

<p>For the longest time we only supported SQLite and MySQL, but I’m now working
on adding PostgreSQL support. So I figured, I might as well write down my
notes.</p>

<h2>But how feasable is it to write SQL that works everywhere?</h2>

<p>Well, it turns out that this is actually not super easy. There is such as
thing as <a href="https://en.wikipedia.org/wiki/SQL-92" title="SQL-92">Standard SQL</a>, but all of these databases have many of their
own extensions and deviations.</p>

<p>The most important thing is that this will likely only work well for you if
you have a very simple schema and simple queries.</p>

<p>Well, this blog post is not intended as a full guide, I’m just listing the
particular things I’ve ran into. If you have your own, you can <a href="https://github.com/evert/evert.github.com/blob/master/_posts/2016/2016-04-28-writing-sql-for-postgres-mysql-sqlite.md">edit this blog
post</a> on github, or leave a comment.</p>

<h3>My approach</h3>

<ul>
  <li>I try to keep my queries as simple as possible.</li>
  <li>If I can rewrite a query to work on every database, that query will have the
preference.</li>
  <li>I avoid stored procedures, triggers, functions, views. I’m really just
dealing with tables and indexes.</li>
  <li>Even if that means that it’s not the most optimal query. So I’m ok with
sarcrificing some performance, if that means my queries can stay generic,
within reason.</li>
  <li>If there’s no possible way to do things in a generic way, I fall back on
something like this:</li>
</ul>

<div><pre><code><span>&lt;?php</span>

<span>if</span> <span>(</span><span>$pdo</span><span>-&gt;</span><span>getAttribute</span><span>(</span><span>PDO</span><span>::</span><span>ATTR_DRIVER_NAME</span><span>)</span> <span>===</span> <span>'pgsql'</span><span>)</span> <span>{</span>

    <span>$query</span> <span>=</span> <span>"..."</span><span>;</span>

<span>}</span> <span>else</span> <span>{</span>

    <span>$query</span> <span>=</span> <span>"...';

}

</span><span>$stmt</span><span> = </span><span>$pdo-&gt;prepare</span><span>(</span><span>$query</span><span>);


?&gt;
</span></code></pre>
</div>

<h3>DDL</h3>

<p>First there is the “Data Definition Language” and “Data Manipulation Language”
the former is used for queries starting with <code>CREATE</code>, <code>ALTER</code>, <code>DROP</code>, etc,
and the latter <code>SELECT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>INSERT</code>.</p>

<p>There really is no sane way to generalize your <code>CREATE TABLE</code> queries, as the
types and syntax are vastly different.</p>

<p>So for those we have a <a href="https://github.com/fruux/sabre-dav/tree/master/examples/sql" title="sabre/dav sql samples">set of .sql files</a> for every server.</p>

<h3>Quoting</h3>

<p>In MySQL and SQlite you can use either quotes <code>'</code> or double quotes <code>"</code> to wrap
a string.</p>

<p>In PostgreSQL, you always have to use single quotes <code>'</code>.</p>

<p>In MySQL and SQLite you use backticks for identifiers. PostgreSQL uses single
quotes. SQlite can also use single quotes here if the result is unambigious,
but I would strongly suggest to avoid that.</p>

<p>This means that this MySQL query:</p>

<div><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>`foo`</span> <span>WHERE</span> <span>`a`</span> <span>=</span> <span>"b"</span>
</code></pre>
</div>

<p>is equivalent to this PostgreSQL query:</p>

<div><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>"foo"</span> <span>WHERE</span> <span>"a"</span> <span>=</span> <span>'b'</span>
</code></pre>
</div>

<p>Luckily you can often just write this query, which works for all databases:</p>

<div><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>foo</span> <span>WHERE</span> <span>a</span> <span>=</span> <span>'b'</span>
</code></pre>
</div>

<p>But keep in mind that when you create your tables, using double quotes will
cause PostgreSQL to retain the upper/lower case characters. If you do not use
quotes, it will normalize everything to lower case.</p>

<p>For compatibility I would therefore suggest to make sure that all your table
and column names are in lower case.</p>

<h3>REPLACE INTO</h3>

<p>The <code>REPLACE INTO</code> is a useful extension that is supported by both SQLite and
MySQL. The syntax is identical <code>INSERT INTO</code>, except that if it runs into a
key conflict, it will overwrite the existing record instead of inserting a new
one.</p>

<p>So <code>REPLACE INTO</code> basically either updates or inserts a new record.</p>

<p>This works on both SQLite and MySQL, but not PostgreSQL. Since version 9.5
PostgreSQL gained a new feature that allows you to achieve the same effect.</p>

<p>This statement from MySQL or SQLite:</p>

<div><pre><code><span>REPLACE</span> <span>INTO</span> <span>blog</span> <span>(</span><span>uuid</span><span>,</span> <span>title</span><span>)</span> <span>VALUES</span> <span>(:</span><span>uuid</span><span>,</span> <span>:</span><span>title</span><span>)</span>
</code></pre>
</div>

<p>then might become something like this in PostgreSQL:</p>

<div><pre><code><span>INSERT</span> <span>INTO</span> <span>blog</span> <span>(</span><span>uuid</span><span>,</span> <span>title</span><span>)</span> <span>VALUES</span> <span>(:</span><span>uuid</span><span>,</span> <span>:</span><span>title</span><span>)</span>
<span>ON</span> <span>CONFLICT</span> <span>(</span><span>uuid</span><span>)</span> <span>DO</span> <span>UPDATE</span> <span>SET</span> <span>title</span> <span>=</span> <span>:</span><span>title</span>
</code></pre>
</div>

<p>So the major difference here is with PostgreSQL we specifically have to tell
it which key conflict we’re handling (<code>uuid</code>) and what to do in that case
(<code>UPDATE</code>).</p>

<p>In addition to <code>REPLACE INTO</code>, MySQL also has this syntax to do the same thing:</p>

<div><pre><code><span>INSERT</span> <span>INTO</span> <span>blog</span> <span>(</span><span>uuid</span><span>,</span> <span>title</span><span>)</span> <span>VALUES</span> <span>(:</span><span>uuid</span><span>,</span> <span>:</span><span>title</span><span>)</span>
<span>ON</span> <span>DUPLICATE</span> <span>KEY</span> <span>UPDATE</span> <span>title</span> <span>=</span> <span>:</span><span>title</span>
</code></pre>
</div>

<p>But as far as I know SQLite does not have a direct equivalent.</p>

<h3>BLOB</h3>

<p>SQLite and MySQL have a <code>BLOB</code> type. This type is used for storing data as-is.
Whatever (binary) string you store, you will retrieve again and no conversion
is attempted for different character sets.</p>

<p>PostgreSQL has two types that have a similar purpose: <a href="http://www.postgresql.org/docs/9.5/static/largeobjects.html" title="PostgreSQL large objects">Large Objects</a> and
the <a href="http://www.postgresql.org/docs/9.5/static/datatype-binary.html" title="PostgreSQL byeta type">bytea</a> type.</p>

<p>The best way to describe large objects, is that they are stored ‘separate’ from
the table, and instead of inserting the object itself, you store a reference to
the object (in the form of an id).</p>

<p><code>bytea</code> is more similar to <code>BLOB</code>, so I opted to use that. But there are some
differences.</p>

<p>First, if you do a select such as this:</p>

<div><pre><code><span>&lt;?php</span>

<span>$stmt</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>prepare</span><span>(</span><span>'SELECT myblob FROM binaries WHERE id = :id'</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>execute</span><span>([</span><span>'id'</span> <span>=&gt;</span> <span>$id</span><span>]);</span>

<span>echo</span> <span>$stmt</span><span>-&gt;</span><span>fetchColumn</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>On MySQL and Sqlite this will just work. The <code>myblob</code> field is represented as
a string.</p>

<p>On PostgreSQL, <code>byta</code> is represented as a PHP stream. So you might have to
rewrite that last statement as:</p>

<div><pre><code><span>&lt;?php</span>

<span>echo</span> <span>stream_get_contents</span><span>(</span><span>$stmt</span><span>-&gt;</span><span>fetchColumn</span><span>());</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Or:</p>

<div><pre><code><span>&lt;?php</span>

<span>stream_copy_to_stream</span><span>(</span><span>$stmt</span><span>-&gt;</span><span>fetchColumn</span><span>(),</span> <span>STDOUT</span><span>);</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Luckily in sabre/dav we pretty much support streams where we also support
strings, so we were already agnositic to this, but some unittests had to be
adjusted.</p>

<p>Inserting <code>bytea</code> is also a bit different. I’m not a fan of of using
<a href="http://php.net/manual/en/pdostatement.bindvalue.php" title="PDOStatement::bindValue"><code>PDOStatement::bindValue</code></a> and <a href="http://php.net/manual/en/pdostatement.bindparam.php" title="PDOStatement::bindParam"><code>PDOStatement::bindParam</code></a>, instead
I prefer to just send all my bound parameters at once using <code>execute</code>:</p>

<div><pre><code><span>&lt;?php</span>

<span>$stmt</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>prepare</span><span>(</span><span>'INSERT INTO binaries (myblob) (:myblob)'</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>execute</span><span>([</span>
    <span>'myblob'</span> <span>=&gt;</span> <span>$blob</span>
<span>]);</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>While that works for PostgreSQL for some strings, it will throw errors
when you give it data that’s invalid in the current character set. It’s also
dangerous, as PostgreSQL might try to transcode the data into a different
character set.</p>

<p>If you truly need to store binary data (like I do) you must do this:</p>

<div><pre><code><span>&lt;?php</span>

<span>$stmt</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>prepare</span><span>(</span><span>'INSERT INTO binaries (myblob) (:myblob)'</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>bindParam</span><span>(</span><span>'myblob'</span><span>,</span> <span>$blob</span><span>,</span> <span>PDO</span><span>::</span><span>PARAM_LOB</span><span>);</span>
<span>$stmt</span><span>-&gt;</span><span>execute</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Luckily this also just works in SQlite and MySQL.</p>

<h3>String concatenation</h3>

<p>Standard SQL has a string concatenation operator. It works like this:</p>

<div><pre><code><span>SELECT</span> <span>'foo'</span> <span>||</span> <span>'bar'</span>
<span>//</span> <span>Output</span><span>:</span> <span>foobar</span>
</code></pre>
</div>

<p>This works in PostgreSQL and Sqlite. MySQL has a function for this:</p>

<div><pre><code><span>SELECT</span> <span>CONCAT</span><span>(</span><span>'foo'</span><span>,</span> <span>'bar'</span><span>)</span>
</code></pre>
</div>

<p>PostgreSQL also has this function, but SQLite does not. You can enable Standard
SQL concatenation in MySQL by enabling it:</p>

<div><pre><code><span>SET</span> <span>SESSION</span> <span>sql_mode</span> <span>=</span> <span>'PIPES_AS_CONCAT'</span>
</code></pre>
</div>

<p>I’m not sure why this isn’t the default.</p>

<h3>Last insert ID</h3>

<p>The <code>PDO</code> object has a <code>lastInsertId()</code> function. For SQLite and MySQL you can
just call it as such:</p>

<div><pre><code><span>&lt;?php</span>

<span>$id</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>lastInsertId</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>However, PostgreSQL requires an explicit sequence identifier. By default this
follows the format <code>tablename_idfield_seq</code>, so we might specifiy as this:</p>

<div><pre><code><span>&lt;?php</span>

<span>$id</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>lastInsertId</span><span>(</span><span>'articles_id_seq'</span><span>);</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Luckily the parameter gets ignored by SQLite and MySQL, so we can just specify
it all the time.</p>

<h3>Type casting</h3>

<p>If you have an <code>INT</code> field (or similar) and you access it in this way:</p>

<div><pre><code><span>&lt;?php</span>

<span>$result</span> <span>=</span> <span>$pdo</span><span>-&gt;</span><span>query</span><span>(</span><span>'SELECT id FROM articles'</span><span>);</span>
<span>$id</span> <span>=</span> <span>$result</span><span>-&gt;</span><span>fetchColumn</span><span>();</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>With PostgreSQL <code>$id</code> will actually have the type php type <code>integer</code>. If you
use MySQL or SQlite, everything gets cast to a php <code>string</code>, which is
unfortunate.</p>

<p>The sane thing to do is to cast everything to int after the fact, so you can
correctly do PHP 7 strict typing with these in the future.</p>

<h2>Testing</h2>

<p>I unittest my database code. Yep, you read that right! I’m one of those people.
It’s been tremendously useful.</p>

<p>Since adding PostgreSQL I was able to come up with a nice structure. Every
unittest that does something with PDO now generally looks like this:</p>

<div><pre><code><span>&lt;?php</span>

<span>abstract</span> <span>PDOTest</span> <span>extends</span> <span>\PHPUnit_Framework_TestCase</span> <span>{</span>

    <span>abstract</span> <span>function</span> <span>getPDO</span><span>();</span>

    <span>/** all the unittests go here **/</span>

<span>}</span>

<span>?&gt;</span>
</code></pre>
</div>

<p>Then I create one subclass for PostgreSQL, Sqlite and MySQL that each only
implement the <code>getPDO()</code> function.</p>

<p>This way all my tests are repeated for each driver.</p>

<p>I’ve also rigged up <a href="https://travis-ci.org/">Travis CI</a> to have a MySQL and a PostgreSQL database
server running, so everything automatically gets checked every time.</p>

<p>If a developer is testing locally, we detect if a database server is running,
and automatically just skip the tests if this was not the case. In most cases
this means only the Sqlite tests get hit, which is fine.</p>

<h2>Conclusions</h2>

<ol>
  <li>Created a monster.</li>
  <li>PostgreSQL is by far the sanest database, and I would recommend everyone to
  move from MySQL towards it.</li>
</ol>

<img src="http://feeds.feedburner.com/~r/bijsterespoor/~4/_BRoV7zVClY" height="1" width="1" alt="" /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995321&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995321&vote=-1&apivote=1">Vote DOWN</a>";}i:3;a:9:{s:5:"title";s:51:"MariaDB MaxScale 1.4.2 GA is available for download";s:4:"guid";s:26:"2551 at http://mariadb.com";s:4:"link";s:66:"http://mariadb.com/blog/mariadb-maxscale-142-ga-available-download";s:11:"description";s:1408:"Thu, 2016-04-28 08:23JohanWe are pleased to announce that MariaDB MaxScale 1.4.2 GA is now available for download!

If MariaDB MaxScale is new to you, we recommend reading this page first.

1.4.2 is a bugfix release, not bringing any new features but fixing certain issues found in 1.4.1.

MariaDB MaxScale 1.4 brings:
The Firewall Filter has been extended and can now be used for either black-listing or white-listing queries. In addition it is capable of logging both queries that match and queries that do not match.
Client-side SSL has been available in MariaDB MaxScale for a while, but it has been somewhat unreliable. We now believe that client side SSL is fully functional and usable.
Additional improvements:
POSIX Extended Regular Expression Syntax can now be used in conjunction with qlafilter, topfilter and namedserverfilter.
Improved user grant detection.
Improved password encryption.
The release notes can be found here and the binaries can be downloaded here.

In case you want to build the binaries yourself, the source can be found at GitHub, tagged with 1.4.2.

We hope you will download and use this release, and we encourage you to create a bug report in Jira for any bugs you might encounter.

On behalf of the MariaDB MaxScale team.
Tags:&nbsp;MariaDB ReleasesMaxScale
About the Author
  
      


 
Johan Wikman is a senior developer working on MaxScale at MariaDB Corporation. 



";s:7:"content";a:1:{s:7:"encoded";s:3625:"<div><div><div>Thu, 2016-04-28 08:23</div></div></div><div><div><div>Johan</div></div></div><div><div><div property="content:encoded"><p><img src="http://mariadb.com/sites/default/files/MariaDB-MaxScale-P1_600px.png" align="right" width="300px" vspace="20px" hspace="20px" />We are pleased to announce that MariaDB MaxScale 1.4.2 GA is now available for download!</p>

<p>If MariaDB MaxScale is new to you, we recommend reading <a href="https://mariadb.com/products/mariadb-maxscale">this page first</a>.</p>

<p>1.4.2 is a bugfix release, not bringing any new features but fixing certain <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/mariadb-maxscale-142-release-notes/#bug-fixes">issues</a> found in 1.4.1.</p>

<p>MariaDB MaxScale 1.4 brings:</p>
<ol><li><a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/maxscale-database-firewall-filter/">The Firewall Filter</a> has been extended and can now be used for either black-listing or white-listing queries. In addition it is capable of logging both queries that match and queries that do not match.</li>
<li>Client-side <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/maxscale-configuration-usage-scenarios/#listener-and-ssl">SSL</a> has been available in MariaDB MaxScale for a while, but it has been somewhat unreliable. We now believe that client side SSL is fully functional and usable.</li>
</ol><p>Additional improvements:</p>
<ul><li>POSIX Extended Regular Expression Syntax can now be used in conjunction with <i>qlafilter</i>, <i>topfilter</i> and <i>namedserverfilter</i>.</li>
<li>Improved user grant detection.</li>
<li>Improved password encryption.</li>
</ul><p>The release notes can be found <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/mariadb-maxscale-142-release-notes/">here</a> and the binaries can be downloaded <a href="http://downloads.mariadb.com/files/MaxScale/1.4.2">here</a>.</p>

<p>In case you want to build the binaries yourself, the source can be found at GitHub, tagged with <a href="https://github.com/mariadb-corporation/MaxScale/tree/1.4.2">1.4.2</a>.</p>

<p>We hope you will download and use this release, and we encourage you to create a bug report in <a href="https://jira.mariadb.org/projects/MXS/issues">Jira</a> for any bugs you might encounter.</p>

<p>On behalf of the MariaDB MaxScale team.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/mariadb-releases" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Releases</a></div><div><a href="http://mariadb.com/blog-tags/maxscale" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MaxScale</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-19783-1458214022.jpg?itok=NUl7zQHc" width="72" height="72" alt="Johan's picture" title="Johan's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Johan Wikman is a senior developer working on MaxScale at MariaDB Corporation. </p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995319&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995319&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 28 Apr 2016 08:23:13 +0000";s:2:"dc";a:1:{s:7:"creator";s:5:"Johan";}s:7:"summary";s:1408:"Thu, 2016-04-28 08:23JohanWe are pleased to announce that MariaDB MaxScale 1.4.2 GA is now available for download!

If MariaDB MaxScale is new to you, we recommend reading this page first.

1.4.2 is a bugfix release, not bringing any new features but fixing certain issues found in 1.4.1.

MariaDB MaxScale 1.4 brings:
The Firewall Filter has been extended and can now be used for either black-listing or white-listing queries. In addition it is capable of logging both queries that match and queries that do not match.
Client-side SSL has been available in MariaDB MaxScale for a while, but it has been somewhat unreliable. We now believe that client side SSL is fully functional and usable.
Additional improvements:
POSIX Extended Regular Expression Syntax can now be used in conjunction with qlafilter, topfilter and namedserverfilter.
Improved user grant detection.
Improved password encryption.
The release notes can be found here and the binaries can be downloaded here.

In case you want to build the binaries yourself, the source can be found at GitHub, tagged with 1.4.2.

We hope you will download and use this release, and we encourage you to create a bug report in Jira for any bugs you might encounter.

On behalf of the MariaDB MaxScale team.
Tags:&nbsp;MariaDB ReleasesMaxScale
About the Author
  
      


 
Johan Wikman is a senior developer working on MaxScale at MariaDB Corporation. 



";s:12:"atom_content";s:3625:"<div><div><div>Thu, 2016-04-28 08:23</div></div></div><div><div><div>Johan</div></div></div><div><div><div property="content:encoded"><p><img src="http://mariadb.com/sites/default/files/MariaDB-MaxScale-P1_600px.png" align="right" width="300px" vspace="20px" hspace="20px" />We are pleased to announce that MariaDB MaxScale 1.4.2 GA is now available for download!</p>

<p>If MariaDB MaxScale is new to you, we recommend reading <a href="https://mariadb.com/products/mariadb-maxscale">this page first</a>.</p>

<p>1.4.2 is a bugfix release, not bringing any new features but fixing certain <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/mariadb-maxscale-142-release-notes/#bug-fixes">issues</a> found in 1.4.1.</p>

<p>MariaDB MaxScale 1.4 brings:</p>
<ol><li><a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/maxscale-database-firewall-filter/">The Firewall Filter</a> has been extended and can now be used for either black-listing or white-listing queries. In addition it is capable of logging both queries that match and queries that do not match.</li>
<li>Client-side <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/maxscale-configuration-usage-scenarios/#listener-and-ssl">SSL</a> has been available in MariaDB MaxScale for a while, but it has been somewhat unreliable. We now believe that client side SSL is fully functional and usable.</li>
</ol><p>Additional improvements:</p>
<ul><li>POSIX Extended Regular Expression Syntax can now be used in conjunction with <i>qlafilter</i>, <i>topfilter</i> and <i>namedserverfilter</i>.</li>
<li>Improved user grant detection.</li>
<li>Improved password encryption.</li>
</ul><p>The release notes can be found <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/mariadb-maxscale-142-release-notes/">here</a> and the binaries can be downloaded <a href="http://downloads.mariadb.com/files/MaxScale/1.4.2">here</a>.</p>

<p>In case you want to build the binaries yourself, the source can be found at GitHub, tagged with <a href="https://github.com/mariadb-corporation/MaxScale/tree/1.4.2">1.4.2</a>.</p>

<p>We hope you will download and use this release, and we encourage you to create a bug report in <a href="https://jira.mariadb.org/projects/MXS/issues">Jira</a> for any bugs you might encounter.</p>

<p>On behalf of the MariaDB MaxScale team.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/mariadb-releases" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Releases</a></div><div><a href="http://mariadb.com/blog-tags/maxscale" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MaxScale</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-19783-1458214022.jpg?itok=NUl7zQHc" width="72" height="72" alt="Johan's picture" title="Johan's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Johan Wikman is a senior developer working on MaxScale at MariaDB Corporation. </p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995319&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995319&vote=-1&apivote=1">Vote DOWN</a>";}i:4;a:9:{s:5:"title";s:36:"MariaDB ColumnStore, a new beginning";s:4:"guid";s:26:"2544 at http://mariadb.com";s:4:"link";s:57:"http://mariadb.com/blog/mariadb-columnstore-new-beginning";s:11:"description";s:3812:"Thu, 2016-04-28 07:33nishantvyasMariaDB’s new analytics engine – MariaDB ColumnStore -  has been in the works for some time. What is it and how did it come about? This post outlines our thinking in choosing the engine and features we implemented.

Databases are expanding from their roots as systems of record into new analytics applications that some people call “systems of intelligence.” That means that instead of just storing and querying transactional data, databases are increasingly being used to yield insights, predict the future and prescribe actions users should take. Led by the open-source Hadoop ecosystem, online analytic processing (OLAP) is making its way out of the corporate data center and into the hands of everyone who needs it.

In the last decade, as data analytics became more important, solving these problems became more challenging. Everyone was led to believe, through hype and skewed opinions, that scale-out, big clusters and data processing without using SQL is probably the only way to do data analytics. We were made to believe that solving analytics would require either scale-out or spending big money on proprietary solutions.

On one end, there are traditional OLAP data warehouses, which are powerful and SQL rich BUT too costly, proprietary and often black box appliances. On the other end, we saw the rise of hadoop ecosystems, which challenged traditional OLAP providers, paved a way to machine learning and data discovery that was not easy with traditional solutions but came with the complexity of scale out and lacked SQL interfaces.

We know our users choose MariaDB because they value performance, scalability, reliability, security and extensibility through open source, with 100% SQL compatibility. We wanted our OLAP choice to reflect the same values.

One commercial product that caught our eye is SAP’s HANA. It’s a database appliance that supports both transactional and analytical applications on the same platform. HANA has gotten rave reviews, not only for functionality but also for simplicity. But HANA appliances are expensive and they’re really intended for organizations that are using SAP (price: don’t ask). We knew there was an open-source, scalable, software-only columnar analytic DBMS alternative: InfiniDB. 

We thought InfiniDB would be a terrific fit with our OLAP and big data strategy. It provides a columnar, massively parallel storage engine with performance that scales with the addition of database servers, Performance Modules. Redundancy and high availability are built in, and InfiniDB supports full SQL syntax including joins, window functions and cross engine joins. It also works with Hadoop’s HDFS file system to provide incredible scalability. 



We saw a unique opportunity to fill the need for high performance analytic data engine in the open-source market by binding InfiniDB’s OLAP engine to MariaDB’s OLTP engine to enable users to run analytic queries on production data in near real-time. Much of our development work has been enabling that tight integration.
Tags:&nbsp;Big DataBusinessCloudClusteringColumnStore
About the Author
  
      


 
Nishant joins MariaDB as Head of Product and Strategy from LinkedIn, where he was one of the early employees. During his almost nine-year tenure at LinkedIn, he contributed to building, scaling and operating production data stores using technologies like Oracle, MySQL, NoSQL and more. Nishant has extensive experience as a database engineer, database architect and DBA, and has held various leadership roles. He holds a bachelor's degree in engineering from Gujarat University and a master's degree in computer science from the University of Bridgeport. Based in the San Francisco bay area, Nishant is co-author of a patent in waterwheel sharding.


";s:7:"content";a:1:{s:7:"encoded";s:5704:"<div><div><div>Thu, 2016-04-28 07:33</div></div></div><div><div><div>nishantvyas</div></div></div><div><div><div property="content:encoded"><p>MariaDB’s new analytics engine – <a href="http://mariadb.com/products/mariadb-columnstore">MariaDB ColumnStore</a> -  has been in the works for some time. What is it and how did it come about? This post outlines our thinking in choosing the engine and features we implemented.</p>

<p>Databases are expanding from their roots as systems of record into new analytics applications that some people call “systems of intelligence.” That means that instead of just storing and querying transactional data, databases are increasingly being used to yield insights, predict the future and prescribe actions users should take. Led by the open-source Hadoop ecosystem, online analytic processing (OLAP) is making its way out of the corporate data center and into the hands of everyone who needs it.</p>

<p>In the last decade, as data analytics became more important, solving these problems became more challenging. Everyone was led to believe, through hype and skewed opinions, that scale-out, big clusters and data processing without using SQL is probably the only way to do data analytics. We were made to believe that solving analytics would require either scale-out or spending big money on proprietary solutions.</p>

<p>On one end, there are traditional OLAP data warehouses, which are powerful and SQL rich BUT too costly, proprietary and often black box appliances. On the other end, we saw the rise of hadoop ecosystems, which challenged traditional OLAP providers, paved a way to machine learning and data discovery that was not easy with traditional solutions but came with the complexity of scale out and lacked SQL interfaces.</p>

<p>We know our users choose MariaDB because they value performance, scalability, reliability, security and extensibility through open source, with 100% SQL compatibility. We wanted our OLAP choice to reflect the same values.</p>

<p>One commercial product that caught our eye is SAP’s <a href="https://hana.sap.com/abouthana.html">HANA</a>. It’s a database appliance that supports both transactional and analytical applications on the same platform. HANA has gotten rave reviews, not only for functionality but also for simplicity. But HANA appliances are expensive and they’re really intended for organizations that are using SAP (price: don’t ask). We knew there was an open-source, scalable, software-only columnar analytic DBMS alternative: InfiniDB. </p>

<p>We thought InfiniDB would be a terrific fit with our OLAP and big data strategy. It provides a columnar, massively parallel storage engine with performance that scales with the addition of database servers, Performance Modules. Redundancy and high availability are built in, and InfiniDB supports full SQL syntax including joins, window functions and cross engine joins. It also works with Hadoop’s HDFS file system to provide incredible scalability. </p>

<center><img alt="MariaDB ColumnStore" src="http://mariadb.com/sites/default/files/columnstore.png" vspace="20px" /></center>

<p>We saw a unique opportunity to fill the need for high performance analytic data engine in the open-source market by binding InfiniDB’s OLAP engine to MariaDB’s OLTP engine to enable users to run analytic queries on production data in near real-time. Much of our development work has been enabling that tight integration.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/big-data" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Big Data</a></div><div><a href="http://mariadb.com/blog-tags/business" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Business</a></div><div><a href="http://mariadb.com/blog-tags/cloud" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Cloud</a></div><div><a href="http://mariadb.com/blog-tags/clustering" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Clustering</a></div><div><a href="http://mariadb.com/blog-tags/columnstore" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">ColumnStore</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-16610-1461828233.jpg?itok=mFztUvbR" width="72" height="72" alt="nishantvyas's picture" title="nishantvyas's picture" />  </div>
</div>
<div>
<div> </div>
<div>Nishant joins MariaDB as Head of Product and Strategy from LinkedIn, where he was one of the early employees. During his almost nine-year tenure at LinkedIn, he contributed to building, scaling and operating production data stores using technologies like Oracle, MySQL, NoSQL and more. Nishant has extensive experience as a database engineer, database architect and DBA, and has held various leadership roles. He holds a bachelor's degree in engineering from Gujarat University and a master's degree in computer science from the University of Bridgeport. Based in the San Francisco bay area, Nishant is co-author of a patent in waterwheel sharding.</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995314&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995314&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 28 Apr 2016 07:33:38 +0000";s:2:"dc";a:1:{s:7:"creator";s:7:"MariaDB";}s:7:"summary";s:3812:"Thu, 2016-04-28 07:33nishantvyasMariaDB’s new analytics engine – MariaDB ColumnStore -  has been in the works for some time. What is it and how did it come about? This post outlines our thinking in choosing the engine and features we implemented.

Databases are expanding from their roots as systems of record into new analytics applications that some people call “systems of intelligence.” That means that instead of just storing and querying transactional data, databases are increasingly being used to yield insights, predict the future and prescribe actions users should take. Led by the open-source Hadoop ecosystem, online analytic processing (OLAP) is making its way out of the corporate data center and into the hands of everyone who needs it.

In the last decade, as data analytics became more important, solving these problems became more challenging. Everyone was led to believe, through hype and skewed opinions, that scale-out, big clusters and data processing without using SQL is probably the only way to do data analytics. We were made to believe that solving analytics would require either scale-out or spending big money on proprietary solutions.

On one end, there are traditional OLAP data warehouses, which are powerful and SQL rich BUT too costly, proprietary and often black box appliances. On the other end, we saw the rise of hadoop ecosystems, which challenged traditional OLAP providers, paved a way to machine learning and data discovery that was not easy with traditional solutions but came with the complexity of scale out and lacked SQL interfaces.

We know our users choose MariaDB because they value performance, scalability, reliability, security and extensibility through open source, with 100% SQL compatibility. We wanted our OLAP choice to reflect the same values.

One commercial product that caught our eye is SAP’s HANA. It’s a database appliance that supports both transactional and analytical applications on the same platform. HANA has gotten rave reviews, not only for functionality but also for simplicity. But HANA appliances are expensive and they’re really intended for organizations that are using SAP (price: don’t ask). We knew there was an open-source, scalable, software-only columnar analytic DBMS alternative: InfiniDB. 

We thought InfiniDB would be a terrific fit with our OLAP and big data strategy. It provides a columnar, massively parallel storage engine with performance that scales with the addition of database servers, Performance Modules. Redundancy and high availability are built in, and InfiniDB supports full SQL syntax including joins, window functions and cross engine joins. It also works with Hadoop’s HDFS file system to provide incredible scalability. 



We saw a unique opportunity to fill the need for high performance analytic data engine in the open-source market by binding InfiniDB’s OLAP engine to MariaDB’s OLTP engine to enable users to run analytic queries on production data in near real-time. Much of our development work has been enabling that tight integration.
Tags:&nbsp;Big DataBusinessCloudClusteringColumnStore
About the Author
  
      


 
Nishant joins MariaDB as Head of Product and Strategy from LinkedIn, where he was one of the early employees. During his almost nine-year tenure at LinkedIn, he contributed to building, scaling and operating production data stores using technologies like Oracle, MySQL, NoSQL and more. Nishant has extensive experience as a database engineer, database architect and DBA, and has held various leadership roles. He holds a bachelor's degree in engineering from Gujarat University and a master's degree in computer science from the University of Bridgeport. Based in the San Francisco bay area, Nishant is co-author of a patent in waterwheel sharding.


";s:12:"atom_content";s:5704:"<div><div><div>Thu, 2016-04-28 07:33</div></div></div><div><div><div>nishantvyas</div></div></div><div><div><div property="content:encoded"><p>MariaDB’s new analytics engine – <a href="http://mariadb.com/products/mariadb-columnstore">MariaDB ColumnStore</a> -  has been in the works for some time. What is it and how did it come about? This post outlines our thinking in choosing the engine and features we implemented.</p>

<p>Databases are expanding from their roots as systems of record into new analytics applications that some people call “systems of intelligence.” That means that instead of just storing and querying transactional data, databases are increasingly being used to yield insights, predict the future and prescribe actions users should take. Led by the open-source Hadoop ecosystem, online analytic processing (OLAP) is making its way out of the corporate data center and into the hands of everyone who needs it.</p>

<p>In the last decade, as data analytics became more important, solving these problems became more challenging. Everyone was led to believe, through hype and skewed opinions, that scale-out, big clusters and data processing without using SQL is probably the only way to do data analytics. We were made to believe that solving analytics would require either scale-out or spending big money on proprietary solutions.</p>

<p>On one end, there are traditional OLAP data warehouses, which are powerful and SQL rich BUT too costly, proprietary and often black box appliances. On the other end, we saw the rise of hadoop ecosystems, which challenged traditional OLAP providers, paved a way to machine learning and data discovery that was not easy with traditional solutions but came with the complexity of scale out and lacked SQL interfaces.</p>

<p>We know our users choose MariaDB because they value performance, scalability, reliability, security and extensibility through open source, with 100% SQL compatibility. We wanted our OLAP choice to reflect the same values.</p>

<p>One commercial product that caught our eye is SAP’s <a href="https://hana.sap.com/abouthana.html">HANA</a>. It’s a database appliance that supports both transactional and analytical applications on the same platform. HANA has gotten rave reviews, not only for functionality but also for simplicity. But HANA appliances are expensive and they’re really intended for organizations that are using SAP (price: don’t ask). We knew there was an open-source, scalable, software-only columnar analytic DBMS alternative: InfiniDB. </p>

<p>We thought InfiniDB would be a terrific fit with our OLAP and big data strategy. It provides a columnar, massively parallel storage engine with performance that scales with the addition of database servers, Performance Modules. Redundancy and high availability are built in, and InfiniDB supports full SQL syntax including joins, window functions and cross engine joins. It also works with Hadoop’s HDFS file system to provide incredible scalability. </p>

<center><img alt="MariaDB ColumnStore" src="http://mariadb.com/sites/default/files/columnstore.png" vspace="20px" /></center>

<p>We saw a unique opportunity to fill the need for high performance analytic data engine in the open-source market by binding InfiniDB’s OLAP engine to MariaDB’s OLTP engine to enable users to run analytic queries on production data in near real-time. Much of our development work has been enabling that tight integration.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/big-data" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Big Data</a></div><div><a href="http://mariadb.com/blog-tags/business" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Business</a></div><div><a href="http://mariadb.com/blog-tags/cloud" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Cloud</a></div><div><a href="http://mariadb.com/blog-tags/clustering" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Clustering</a></div><div><a href="http://mariadb.com/blog-tags/columnstore" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">ColumnStore</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-16610-1461828233.jpg?itok=mFztUvbR" width="72" height="72" alt="nishantvyas's picture" title="nishantvyas's picture" />  </div>
</div>
<div>
<div> </div>
<div>Nishant joins MariaDB as Head of Product and Strategy from LinkedIn, where he was one of the early employees. During his almost nine-year tenure at LinkedIn, he contributed to building, scaling and operating production data stores using technologies like Oracle, MySQL, NoSQL and more. Nishant has extensive experience as a database engineer, database architect and DBA, and has held various leadership roles. He holds a bachelor's degree in engineering from Gujarat University and a master's degree in computer science from the University of Bridgeport. Based in the San Francisco bay area, Nishant is co-author of a patent in waterwheel sharding.</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995314&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995314&vote=-1&apivote=1">Vote DOWN</a>";}i:5;a:9:{s:5:"title";s:87:"Planets9s - Watch the replay: Become a MongoDB DBA (if you’re re really a MySQL user)";s:4:"guid";s:31:"4680 at http://severalnines.com";s:4:"link";s:81:"http://severalnines.com/blog/planets9s-watch-replay-become-mongodb-dba-mysql-user";s:11:"description";s:1978:"Welcome to this week’s Planets9s, covering all the latest resources and technologies we create around automation and management of open source database infrastructures.

Watch the replay: Become a MongoDB DBA (for MySQL users)
Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database. Art also did a live demo on how to carry out the relevant DBA tasks using ClusterControl. The replay is now available to watch in cased you missed it or simply would like to see it again in your own time.
Watch the replay




Severalnines expands the reach of European scientific discovery for CNRS
We’re excited to announce our latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research. As the largest fundamental research organization in Europe, CNRS carries out research in all fields of knowledge. Find out how we help CNRS keep costs down whilst increasing the potential of their open source systems. And how ClusterControl helps them both manage and use their LAMP applications, as well as cloud services.
Read the press release


Infrastructure Automation - Ansible Role for ClusterControl
If you are automating your server infrastructure with Ansible, then this blog is for you. We recently announced the availability of an Ansible Role for ClusterControl. It is available at Ansible Galaxy. And as a reminder, for those of you who are automating with Puppet or Chef, we already published a Puppet Module and Chef Cookbook for ClusterControl.
Read the blog


That’s it for this week! Feel free to share these resources with your colleagues and follow us in our social media channels.
Have a good end of the week,
Jean-Jérôme Schmidt
Planets9s Editor
Severalnines AB
Tags: MongoDBMySQLclustercontrolansibledatabase management";s:7:"content";a:1:{s:7:"encoded";s:4097:"<div><div><div property="content:encoded"><p>Welcome to this week’s <strong>Planets9s,</strong> covering all the latest resources and technologies we create around automation and management of open source database infrastructures.</p>
<table border="0" cellpadding="0" cellspacing="0" width="100%"><tbody><tr><td colspan="2" valign="top">
<h3>Watch the replay: Become a MongoDB DBA (for MySQL users)</h3>
<p>Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database. Art also did a live demo on how to carry out the relevant DBA tasks using ClusterControl. The replay is now available to watch in cased you missed it or simply would like to see it again in your own time.</p>
<p><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user">Watch the replay</a></p>
</td>
<td align="right" valign="top" width="165"><img src="http://severalnines.com/sites/default/files/mail/icons/webinar_replay.png" /></td>
</tr><tr><td align="left" valign="top" width="165"><img src="http://severalnines.com/sites/default/files/mail/icons/blog.png" style="width: 155px; height: 155px;" /></td>
<td colspan="2" valign="top">
<h3>Severalnines expands the reach of European scientific discovery for CNRS</h3>
<p>We’re excited to announce our latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research. As the largest fundamental research organization in Europe, CNRS carries out research in all fields of knowledge. Find out how we help CNRS keep costs down whilst increasing the potential of their open source systems. And how ClusterControl helps them both manage and use their LAMP applications, as well as cloud services.</p>
<p><a data-n="data-n" href="http://severalnines.com/blog/press-release-severalnines-expands-reach-european-scientific-discovery">Read the press release</a></p>
</td>
</tr><tr><td colspan="2" valign="top">
<h3>Infrastructure Automation - Ansible Role for ClusterControl</h3>
<p>If you are automating your server infrastructure with Ansible, then this blog is for you. We recently announced the availability of an Ansible Role for ClusterControl. It is available at Ansible Galaxy. And as a reminder, for those of you who are automating with Puppet or Chef, we already published a Puppet Module and Chef Cookbook for ClusterControl.</p>
<p><a href="http://severalnines.com/blog/infrastructure-automation-ansible-role-clustercontrol">Read the blog</a></p>
</td>
<td align="right" valign="top" width="165"><img src="http://severalnines.com/sites/default/files/mail/icons/blog.png" /></td>
</tr></tbody></table><p>That’s it for this week! Feel free to share these resources with your colleagues and follow us in our social media channels.</p>
<p align="right">Have a good end of the week,</p>
<p align="right"><em>Jean-Jérôme Schmidt<br />
Planets9s Editor<br />
Severalnines AB</em></p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/clustercontrol" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">clustercontrol</a></li><li><a href="http://severalnines.com/blog-tags/ansible" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">ansible</a></li><li><a href="http://severalnines.com/blog-tags/database-management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">database management</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995315&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995315&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 28 Apr 2016 07:32:09 +0000";s:2:"dc";a:1:{s:7:"creator";s:12:"Severalnines";}s:7:"summary";s:1978:"Welcome to this week’s Planets9s, covering all the latest resources and technologies we create around automation and management of open source database infrastructures.

Watch the replay: Become a MongoDB DBA (for MySQL users)
Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database. Art also did a live demo on how to carry out the relevant DBA tasks using ClusterControl. The replay is now available to watch in cased you missed it or simply would like to see it again in your own time.
Watch the replay




Severalnines expands the reach of European scientific discovery for CNRS
We’re excited to announce our latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research. As the largest fundamental research organization in Europe, CNRS carries out research in all fields of knowledge. Find out how we help CNRS keep costs down whilst increasing the potential of their open source systems. And how ClusterControl helps them both manage and use their LAMP applications, as well as cloud services.
Read the press release


Infrastructure Automation - Ansible Role for ClusterControl
If you are automating your server infrastructure with Ansible, then this blog is for you. We recently announced the availability of an Ansible Role for ClusterControl. It is available at Ansible Galaxy. And as a reminder, for those of you who are automating with Puppet or Chef, we already published a Puppet Module and Chef Cookbook for ClusterControl.
Read the blog


That’s it for this week! Feel free to share these resources with your colleagues and follow us in our social media channels.
Have a good end of the week,
Jean-Jérôme Schmidt
Planets9s Editor
Severalnines AB
Tags: MongoDBMySQLclustercontrolansibledatabase management";s:12:"atom_content";s:4097:"<div><div><div property="content:encoded"><p>Welcome to this week’s <strong>Planets9s,</strong> covering all the latest resources and technologies we create around automation and management of open source database infrastructures.</p>
<table border="0" cellpadding="0" cellspacing="0" width="100%"><tbody><tr><td colspan="2" valign="top">
<h3>Watch the replay: Become a MongoDB DBA (for MySQL users)</h3>
<p>Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database. Art also did a live demo on how to carry out the relevant DBA tasks using ClusterControl. The replay is now available to watch in cased you missed it or simply would like to see it again in your own time.</p>
<p><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user">Watch the replay</a></p>
</td>
<td align="right" valign="top" width="165"><img src="http://severalnines.com/sites/default/files/mail/icons/webinar_replay.png" /></td>
</tr><tr><td align="left" valign="top" width="165"><img src="http://severalnines.com/sites/default/files/mail/icons/blog.png" style="width: 155px; height: 155px;" /></td>
<td colspan="2" valign="top">
<h3>Severalnines expands the reach of European scientific discovery for CNRS</h3>
<p>We’re excited to announce our latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research. As the largest fundamental research organization in Europe, CNRS carries out research in all fields of knowledge. Find out how we help CNRS keep costs down whilst increasing the potential of their open source systems. And how ClusterControl helps them both manage and use their LAMP applications, as well as cloud services.</p>
<p><a data-n="data-n" href="http://severalnines.com/blog/press-release-severalnines-expands-reach-european-scientific-discovery">Read the press release</a></p>
</td>
</tr><tr><td colspan="2" valign="top">
<h3>Infrastructure Automation - Ansible Role for ClusterControl</h3>
<p>If you are automating your server infrastructure with Ansible, then this blog is for you. We recently announced the availability of an Ansible Role for ClusterControl. It is available at Ansible Galaxy. And as a reminder, for those of you who are automating with Puppet or Chef, we already published a Puppet Module and Chef Cookbook for ClusterControl.</p>
<p><a href="http://severalnines.com/blog/infrastructure-automation-ansible-role-clustercontrol">Read the blog</a></p>
</td>
<td align="right" valign="top" width="165"><img src="http://severalnines.com/sites/default/files/mail/icons/blog.png" /></td>
</tr></tbody></table><p>That’s it for this week! Feel free to share these resources with your colleagues and follow us in our social media channels.</p>
<p align="right">Have a good end of the week,</p>
<p align="right"><em>Jean-Jérôme Schmidt<br />
Planets9s Editor<br />
Severalnines AB</em></p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/clustercontrol" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">clustercontrol</a></li><li><a href="http://severalnines.com/blog-tags/ansible" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">ansible</a></li><li><a href="http://severalnines.com/blog-tags/database-management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">database management</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995315&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995315&vote=-1&apivote=1">Vote DOWN</a>";}i:6;a:10:{s:5:"title";s:50:"Upgrading to MySQL 5.7, focusing on temporal types";s:4:"guid";s:37:"https://www.percona.com/blog/?p=35277";s:4:"link";s:90:"https://www.percona.com/blog/2016/04/27/upgrading-to-mysql-5-7-focusing-on-temporal-types/";s:11:"description";s:7645:"In this post, we&#8217;ll discuss how MySQL 5.7 handles the old temporal types during an upgrade.
MySQL changed the temporal types in MySQL 5.6.4, and it introduced a new feature: microseconds resolution in the TIME, TIMESTAMP and DATETIME types. Now these parameters can be set down to microsecond granularity. Obviously, this means format changes, but why is this important?
Are they converted automatically to the new format?
If we had tables in MySQL 5.5 that used TIME, TIMESTAMP or DATETIME are these fields are going to be converted to the new format when upgrading to 5.6? The answer is &#8220;NO.&#8221; Even if we run mysql_upgrade, it does not warn us about the old format. If we check the MySQL error log, we cannot find anything regarding this. But the newly created tables are going to use the new format so that we will have two different types of temporal fields.
How can we find these tables?
The following query gives us a summary on the different table formats:SELECT CASE isc.mtype
 WHEN '6' THEN 'OLD'
 WHEN '3' THEN 'NEW'
 END FORMAT,
 count(*) TOTAL
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
 AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
 AND t.table_type = 'base table'
 AND (t.engine = 'innodb')
GROUP BY isc.mtype;+--------+-------+
| FORMAT | TOTAL |
+--------+-------+
| NEW    | 1     |
| OLD    | 9     |
+--------+-------+Or we can use show_old_temporals, which will highlight the old formats during a show create table.CREATE TABLE `mytbl` (
  `ts` timestamp /* 5.5 binary format */ NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `dt` datetime /* 5.5 binary format */ DEFAULT NULL,
  `t` time /* 5.5 binary format */ DEFAULT NULL
) DEFAULT CHARSET=latin1MySQL can handle both types, but with the old format you cannot use microseconds, and the default DATETIME takes more space on disk.
Can I upgrade to MySQL 5.7?
Of course you can! But when mysql_upgrade is running it is going to convert the old fields into the new format by default. This basically means an alter table on every single table, which will contain one of the three types.
Depending on the number of tables, or the size of the tables, this could take hours – so you may need to do some planning.....
test.t1
error : Table rebuild required. Please do "ALTER TABLE `t1` FORCE" or dump/reload to fix it!
test.t2
error : Table rebuild required. Please do "ALTER TABLE `t2` FORCE" or dump/reload to fix it!
test.t3
error : Table rebuild required. Please do "ALTER TABLE `t3` FORCE" or dump/reload to fix it!
Repairing tables
mysql.proxies_priv OK
`test`.`t1`
Running : ALTER TABLE `test`.`t1` FORCE
status : OK
`test`.`t2`
Running : ALTER TABLE `test`.`t2` FORCE
status : OK
`test`.`t3`
Running : ALTER TABLE `test`.`t3` FORCE
status : OK
Upgrade process completed successfully.
Checking if update is needed.
Can we avoid this at upgrade?
We can run alter tables or use pt-online-schema-schange (to avoid locking) before an upgrade, but even without these preparations we can still avoid incompatibility issues.
My colleague Daniel Guzman Burgos pointed out that mysql_upgrade has an option called upgrade-system-tables. This will only upgrade the system tables, and nothing else.
Can we still write these fields?
The following query returns the schema and the table names that still use the old formats.SELECT CASE isc.mtype
           WHEN '6' THEN 'OLD'
           WHEN '3' THEN 'NEW'
       END FORMAT,
       t.schema_name,
       t.table_name
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
    AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
    AND t.table_type = 'base table'
    AND (t.engine = 'innodb');+--------+--------------+------------+
| FORMAT | table_schema | table_name |
+--------+--------------+------------+
| OLD    | test         | t          |
| OLD    | test         | t          |
| OLD    | test         | t          |
| NEW    | sys          | sys_config |
+--------+--------------+------------+
4 rows in set (0.03 sec)
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.11-4  |
+-----------+
1 row in set (0.00 sec)As we can see, we&#8217;re using 5.7 and table &#8220;test.t&#8221; still has the old format.
The schema:CREATE TABLE `t` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`t1` time DEFAULT NULL,
`t2` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
`t3` datetime DEFAULT NULL,
PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=latin1mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |       t2            |        t3           |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
+----+----------+---------------------+---------------------+Let&#8217;s try to insert a new row:mysql&gt; insert into `t` (t1,t3) values ('20:28','2016:04:23 22:22:06');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |         t2          |          t3         |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
| 6  | 20:28:00 | 2016-04-09 01:56:38 | 2016-04-23 22:22:06 |
+----+----------+---------------------+---------------------+
6 rows in set (0.00 sec)It was inserted without a problem, and we can&#8217;t see any related info/warnings in the error log.
Does the Replication work?
In many scenarios, when you are upgrading a replicaset, the slaves are upgraded first. But will the replication work? The short answer is &#8220;yes.&#8221; I configured row-based replication between MySQL 5.6 and 5.7. The 5.6 was the master, and it had all the temporal types in the old format. On 5.7, I had new and old formats.
I replicated from old format to old format, and from old format to new format, and both are working.
Conclusion
Before upgrading to MySQL 5.7, tables should be altered to use the new format. If it isn&#8217;t done, however, the upgrade is still possible without altering all the tables – the drawbacks are you cannot use microseconds, and it takes more space on disk. If you had to upgrade to 5.7, however, you could change the format later using alter table or pt-online-schema-schange.
&nbsp;";s:7:"content";a:1:{s:7:"encoded";s:9776:"<img width="150" height="113" src="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types-150x113.jpg" class="attachment-thumbnail size-thumbnail wp-post-image" alt="temporal types" style="float: left; margin-right: 5px;" /><p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types.jpg" rel="attachment wp-att-35328"><img class="size-medium wp-image-35328 alignright" src="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types-300x225.jpg" alt="temporal types" width="300" height="225" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types-300x225.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types.jpg 500w" sizes="(max-width: 300px) 100vw, 300px" /></a>In this post, we&#8217;ll discuss how <a rel="nofollow" href="https://dev.mysql.com/doc/refman/5.7/en/date-and-time-types.html">MySQL 5.7</a> handles the old temporal types during an upgrade.</p>
<p>MySQL changed the temporal types in <a rel="nofollow" href="https://dev.mysql.com/doc/internals/en/date-and-time-data-type-representation.html">MySQL 5.6.4</a>, and it introduced a new feature: microseconds resolution in the TIME, TIMESTAMP and DATETIME types. Now these parameters can be set down to microsecond granularity. Obviously, this means format changes, but why is this important?</p>
<h1>Are they converted automatically to the new format?</h1>
<p>If we had tables in MySQL 5.5 that used TIME, TIMESTAMP or DATETIME are these fields are going to be converted to the new format when upgrading to 5.6? The answer is &#8220;NO.&#8221; Even if we run mysql_upgrade, it does not warn us about the old format. If we check the MySQL error log, we cannot find anything regarding this. But the newly created tables are going to use the new format so that we will have two different types of temporal fields.</p>
<h1>How can we find these tables?</h1>
<p>The following query gives us a summary on the different table formats:</p><pre>SELECT CASE isc.mtype
 WHEN '6' THEN 'OLD'
 WHEN '3' THEN 'NEW'
 END FORMAT,
 count(*) TOTAL
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
 AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
 AND t.table_type = 'base table'
 AND (t.engine = 'innodb')
GROUP BY isc.mtype;</pre><p></p><pre>+--------+-------+
| FORMAT | TOTAL |
+--------+-------+
| NEW    | 1     |
| OLD    | 9     |
+--------+-------+</pre><p>Or we can use <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_show_old_temporals">show_old_temporals</a>, which will highlight the old formats during a <pre>show create table</pre>.</p><pre>CREATE TABLE `mytbl` (
  `ts` timestamp /* 5.5 binary format */ NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `dt` datetime /* 5.5 binary format */ DEFAULT NULL,
  `t` time /* 5.5 binary format */ DEFAULT NULL
) DEFAULT CHARSET=latin1</pre><p>MySQL can handle both types, but with the old format you cannot use microseconds, and the default DATETIME takes more space on disk.</p>
<h1>Can I upgrade to MySQL 5.7?</h1>
<p>Of course you can! But when <pre>mysql_upgrade</pre> is running <strong>it is going to convert the old fields into the new format by default.</strong> This basically means an <pre>alter table</pre> on every single table, which will contain one of the three types.</p>
<p>Depending on the number of tables, or the size of the tables, this could take hours – so you may need to do some planning.</p><pre>....
test.t1
error : Table rebuild required. Please do "ALTER TABLE `t1` FORCE" or dump/reload to fix it!
test.t2
error : Table rebuild required. Please do "ALTER TABLE `t2` FORCE" or dump/reload to fix it!
test.t3
error : Table rebuild required. Please do "ALTER TABLE `t3` FORCE" or dump/reload to fix it!
Repairing tables
mysql.proxies_priv OK
`test`.`t1`
Running : ALTER TABLE `test`.`t1` FORCE
status : OK
`test`.`t2`
Running : ALTER TABLE `test`.`t2` FORCE
status : OK
`test`.`t3`
Running : ALTER TABLE `test`.`t3` FORCE
status : OK
Upgrade process completed successfully.
Checking if update is needed.</pre><p></p>
<h1>Can we avoid this at upgrade?</h1>
<p>We can run <pre>alter tables</pre> or use <a href="https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html">pt-online-schema-schange</a> (to avoid locking) before an upgrade, but even without these preparations we can still avoid incompatibility issues.</p>
<p>My colleague <a href="https://www.percona.com/blog/author/daniel-guzman-burgos/">Daniel Guzman Burgos</a> pointed out that <pre>mysql_upgrade</pre> has an option called <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.7/en/mysql-upgrade.html#option_mysql_upgrade_upgrade-system-tables">upgrade-system-tables</a>. <strong>This will only upgrade the system tables, and nothing else.</strong></p>
<h1>Can we still write these fields?</h1>
<p>The following query returns the schema and the table names that still use the old formats.</p><pre>SELECT CASE isc.mtype
           WHEN '6' THEN 'OLD'
           WHEN '3' THEN 'NEW'
       END FORMAT,
       t.schema_name,
       t.table_name
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
    AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
    AND t.table_type = 'base table'
    AND (t.engine = 'innodb');</pre><p></p><pre>+--------+--------------+------------+
| FORMAT | table_schema | table_name |
+--------+--------------+------------+
| OLD    | test         | t          |
| OLD    | test         | t          |
| OLD    | test         | t          |
| NEW    | sys          | sys_config |
+--------+--------------+------------+
4 rows in set (0.03 sec)
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.11-4  |
+-----------+
1 row in set (0.00 sec)</pre><p>As we can see, we&#8217;re using 5.7 and table &#8220;test.t&#8221; still has the old format.</p>
<p>The schema:</p><pre>CREATE TABLE `t` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`t1` time DEFAULT NULL,
`t2` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
`t3` datetime DEFAULT NULL,
PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=latin1</pre><p></p><pre>mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |       t2            |        t3           |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
+----+----------+---------------------+---------------------+</pre><p>Let&#8217;s try to insert a new row:</p><pre>mysql&gt; insert into `t` (t1,t3) values ('20:28','2016:04:23 22:22:06');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |         t2          |          t3         |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
| 6  | 20:28:00 | 2016-04-09 01:56:38 | 2016-04-23 22:22:06 |
+----+----------+---------------------+---------------------+
6 rows in set (0.00 sec)</pre><p>It was inserted without a problem, and we can&#8217;t see any related info/warnings in the error log.</p>
<h1>Does the Replication work?</h1>
<p>In many scenarios, when you are upgrading a replicaset, the slaves are upgraded first. But will the replication work? The short answer is &#8220;yes.&#8221; I configured <strong>row-based</strong> replication between MySQL 5.6 and 5.7. The 5.6 was the master, and it had all the temporal types in the old format. On 5.7, I had new and old formats.</p>
<p>I replicated from old format to old format, and from old format to new format, and both are working.</p>
<h1>Conclusion</h1>
<p>Before upgrading to MySQL 5.7, tables should be altered to use the new format. If it isn&#8217;t done, however, the upgrade is still possible without altering all the tables – the drawbacks are you cannot use microseconds, and it takes more space on disk. If you had to upgrade to 5.7, however, you could change the format later using <pre>alter table</pre> or <a href="https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html">pt-online-schema-schange</a>.</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995306&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995306&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 27 Apr 2016 20:30:24 +0000";s:2:"dc";a:1:{s:7:"creator";s:22:"MySQL Performance Blog";}s:8:"category";s:50:"MySQLPercona Serverdatetimefractionaltimetimestamp";s:7:"summary";s:7645:"In this post, we&#8217;ll discuss how MySQL 5.7 handles the old temporal types during an upgrade.
MySQL changed the temporal types in MySQL 5.6.4, and it introduced a new feature: microseconds resolution in the TIME, TIMESTAMP and DATETIME types. Now these parameters can be set down to microsecond granularity. Obviously, this means format changes, but why is this important?
Are they converted automatically to the new format?
If we had tables in MySQL 5.5 that used TIME, TIMESTAMP or DATETIME are these fields are going to be converted to the new format when upgrading to 5.6? The answer is &#8220;NO.&#8221; Even if we run mysql_upgrade, it does not warn us about the old format. If we check the MySQL error log, we cannot find anything regarding this. But the newly created tables are going to use the new format so that we will have two different types of temporal fields.
How can we find these tables?
The following query gives us a summary on the different table formats:SELECT CASE isc.mtype
 WHEN '6' THEN 'OLD'
 WHEN '3' THEN 'NEW'
 END FORMAT,
 count(*) TOTAL
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
 AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
 AND t.table_type = 'base table'
 AND (t.engine = 'innodb')
GROUP BY isc.mtype;+--------+-------+
| FORMAT | TOTAL |
+--------+-------+
| NEW    | 1     |
| OLD    | 9     |
+--------+-------+Or we can use show_old_temporals, which will highlight the old formats during a show create table.CREATE TABLE `mytbl` (
  `ts` timestamp /* 5.5 binary format */ NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `dt` datetime /* 5.5 binary format */ DEFAULT NULL,
  `t` time /* 5.5 binary format */ DEFAULT NULL
) DEFAULT CHARSET=latin1MySQL can handle both types, but with the old format you cannot use microseconds, and the default DATETIME takes more space on disk.
Can I upgrade to MySQL 5.7?
Of course you can! But when mysql_upgrade is running it is going to convert the old fields into the new format by default. This basically means an alter table on every single table, which will contain one of the three types.
Depending on the number of tables, or the size of the tables, this could take hours – so you may need to do some planning.....
test.t1
error : Table rebuild required. Please do "ALTER TABLE `t1` FORCE" or dump/reload to fix it!
test.t2
error : Table rebuild required. Please do "ALTER TABLE `t2` FORCE" or dump/reload to fix it!
test.t3
error : Table rebuild required. Please do "ALTER TABLE `t3` FORCE" or dump/reload to fix it!
Repairing tables
mysql.proxies_priv OK
`test`.`t1`
Running : ALTER TABLE `test`.`t1` FORCE
status : OK
`test`.`t2`
Running : ALTER TABLE `test`.`t2` FORCE
status : OK
`test`.`t3`
Running : ALTER TABLE `test`.`t3` FORCE
status : OK
Upgrade process completed successfully.
Checking if update is needed.
Can we avoid this at upgrade?
We can run alter tables or use pt-online-schema-schange (to avoid locking) before an upgrade, but even without these preparations we can still avoid incompatibility issues.
My colleague Daniel Guzman Burgos pointed out that mysql_upgrade has an option called upgrade-system-tables. This will only upgrade the system tables, and nothing else.
Can we still write these fields?
The following query returns the schema and the table names that still use the old formats.SELECT CASE isc.mtype
           WHEN '6' THEN 'OLD'
           WHEN '3' THEN 'NEW'
       END FORMAT,
       t.schema_name,
       t.table_name
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
    AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
    AND t.table_type = 'base table'
    AND (t.engine = 'innodb');+--------+--------------+------------+
| FORMAT | table_schema | table_name |
+--------+--------------+------------+
| OLD    | test         | t          |
| OLD    | test         | t          |
| OLD    | test         | t          |
| NEW    | sys          | sys_config |
+--------+--------------+------------+
4 rows in set (0.03 sec)
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.11-4  |
+-----------+
1 row in set (0.00 sec)As we can see, we&#8217;re using 5.7 and table &#8220;test.t&#8221; still has the old format.
The schema:CREATE TABLE `t` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`t1` time DEFAULT NULL,
`t2` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
`t3` datetime DEFAULT NULL,
PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=latin1mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |       t2            |        t3           |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
+----+----------+---------------------+---------------------+Let&#8217;s try to insert a new row:mysql&gt; insert into `t` (t1,t3) values ('20:28','2016:04:23 22:22:06');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |         t2          |          t3         |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
| 6  | 20:28:00 | 2016-04-09 01:56:38 | 2016-04-23 22:22:06 |
+----+----------+---------------------+---------------------+
6 rows in set (0.00 sec)It was inserted without a problem, and we can&#8217;t see any related info/warnings in the error log.
Does the Replication work?
In many scenarios, when you are upgrading a replicaset, the slaves are upgraded first. But will the replication work? The short answer is &#8220;yes.&#8221; I configured row-based replication between MySQL 5.6 and 5.7. The 5.6 was the master, and it had all the temporal types in the old format. On 5.7, I had new and old formats.
I replicated from old format to old format, and from old format to new format, and both are working.
Conclusion
Before upgrading to MySQL 5.7, tables should be altered to use the new format. If it isn&#8217;t done, however, the upgrade is still possible without altering all the tables – the drawbacks are you cannot use microseconds, and it takes more space on disk. If you had to upgrade to 5.7, however, you could change the format later using alter table or pt-online-schema-schange.
&nbsp;";s:12:"atom_content";s:9776:"<img width="150" height="113" src="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types-150x113.jpg" class="attachment-thumbnail size-thumbnail wp-post-image" alt="temporal types" style="float: left; margin-right: 5px;" /><p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types.jpg" rel="attachment wp-att-35328"><img class="size-medium wp-image-35328 alignright" src="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types-300x225.jpg" alt="temporal types" width="300" height="225" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types-300x225.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/temporal-types.jpg 500w" sizes="(max-width: 300px) 100vw, 300px" /></a>In this post, we&#8217;ll discuss how <a rel="nofollow" href="https://dev.mysql.com/doc/refman/5.7/en/date-and-time-types.html">MySQL 5.7</a> handles the old temporal types during an upgrade.</p>
<p>MySQL changed the temporal types in <a rel="nofollow" href="https://dev.mysql.com/doc/internals/en/date-and-time-data-type-representation.html">MySQL 5.6.4</a>, and it introduced a new feature: microseconds resolution in the TIME, TIMESTAMP and DATETIME types. Now these parameters can be set down to microsecond granularity. Obviously, this means format changes, but why is this important?</p>
<h1>Are they converted automatically to the new format?</h1>
<p>If we had tables in MySQL 5.5 that used TIME, TIMESTAMP or DATETIME are these fields are going to be converted to the new format when upgrading to 5.6? The answer is &#8220;NO.&#8221; Even if we run mysql_upgrade, it does not warn us about the old format. If we check the MySQL error log, we cannot find anything regarding this. But the newly created tables are going to use the new format so that we will have two different types of temporal fields.</p>
<h1>How can we find these tables?</h1>
<p>The following query gives us a summary on the different table formats:</p><pre>SELECT CASE isc.mtype
 WHEN '6' THEN 'OLD'
 WHEN '3' THEN 'NEW'
 END FORMAT,
 count(*) TOTAL
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
 AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
 AND t.table_type = 'base table'
 AND (t.engine = 'innodb')
GROUP BY isc.mtype;</pre><p></p><pre>+--------+-------+
| FORMAT | TOTAL |
+--------+-------+
| NEW    | 1     |
| OLD    | 9     |
+--------+-------+</pre><p>Or we can use <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_show_old_temporals">show_old_temporals</a>, which will highlight the old formats during a <pre>show create table</pre>.</p><pre>CREATE TABLE `mytbl` (
  `ts` timestamp /* 5.5 binary format */ NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `dt` datetime /* 5.5 binary format */ DEFAULT NULL,
  `t` time /* 5.5 binary format */ DEFAULT NULL
) DEFAULT CHARSET=latin1</pre><p>MySQL can handle both types, but with the old format you cannot use microseconds, and the default DATETIME takes more space on disk.</p>
<h1>Can I upgrade to MySQL 5.7?</h1>
<p>Of course you can! But when <pre>mysql_upgrade</pre> is running <strong>it is going to convert the old fields into the new format by default.</strong> This basically means an <pre>alter table</pre> on every single table, which will contain one of the three types.</p>
<p>Depending on the number of tables, or the size of the tables, this could take hours – so you may need to do some planning.</p><pre>....
test.t1
error : Table rebuild required. Please do "ALTER TABLE `t1` FORCE" or dump/reload to fix it!
test.t2
error : Table rebuild required. Please do "ALTER TABLE `t2` FORCE" or dump/reload to fix it!
test.t3
error : Table rebuild required. Please do "ALTER TABLE `t3` FORCE" or dump/reload to fix it!
Repairing tables
mysql.proxies_priv OK
`test`.`t1`
Running : ALTER TABLE `test`.`t1` FORCE
status : OK
`test`.`t2`
Running : ALTER TABLE `test`.`t2` FORCE
status : OK
`test`.`t3`
Running : ALTER TABLE `test`.`t3` FORCE
status : OK
Upgrade process completed successfully.
Checking if update is needed.</pre><p></p>
<h1>Can we avoid this at upgrade?</h1>
<p>We can run <pre>alter tables</pre> or use <a href="https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html">pt-online-schema-schange</a> (to avoid locking) before an upgrade, but even without these preparations we can still avoid incompatibility issues.</p>
<p>My colleague <a href="https://www.percona.com/blog/author/daniel-guzman-burgos/">Daniel Guzman Burgos</a> pointed out that <pre>mysql_upgrade</pre> has an option called <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.7/en/mysql-upgrade.html#option_mysql_upgrade_upgrade-system-tables">upgrade-system-tables</a>. <strong>This will only upgrade the system tables, and nothing else.</strong></p>
<h1>Can we still write these fields?</h1>
<p>The following query returns the schema and the table names that still use the old formats.</p><pre>SELECT CASE isc.mtype
           WHEN '6' THEN 'OLD'
           WHEN '3' THEN 'NEW'
       END FORMAT,
       t.schema_name,
       t.table_name
FROM information_schema.tables AS t
INNER JOIN information_schema.columns AS c ON c.table_schema = t.table_schema
AND c.table_name = t.table_name
LEFT OUTER JOIN information_schema.innodb_sys_tables AS ist ON ist.name = concat(t.table_schema,'/',t.table_name)
LEFT OUTER JOIN information_schema.innodb_sys_columns AS isc ON isc.table_id = ist.table_id
AND isc.name = c.column_name
WHERE c.column_type IN ('time','timestamp','datetime')
    AND t.table_schema NOT IN ('mysql','information_schema','performance_schema')
    AND t.table_type = 'base table'
    AND (t.engine = 'innodb');</pre><p></p><pre>+--------+--------------+------------+
| FORMAT | table_schema | table_name |
+--------+--------------+------------+
| OLD    | test         | t          |
| OLD    | test         | t          |
| OLD    | test         | t          |
| NEW    | sys          | sys_config |
+--------+--------------+------------+
4 rows in set (0.03 sec)
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.11-4  |
+-----------+
1 row in set (0.00 sec)</pre><p>As we can see, we&#8217;re using 5.7 and table &#8220;test.t&#8221; still has the old format.</p>
<p>The schema:</p><pre>CREATE TABLE `t` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`t1` time DEFAULT NULL,
`t2` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
`t3` datetime DEFAULT NULL,
PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=latin1</pre><p></p><pre>mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |       t2            |        t3           |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
+----+----------+---------------------+---------------------+</pre><p>Let&#8217;s try to insert a new row:</p><pre>mysql&gt; insert into `t` (t1,t3) values ('20:28','2016:04:23 22:22:06');
Query OK, 1 row affected (0.01 sec)
mysql&gt; select * from t;
+----+----------+---------------------+---------------------+
| id |    t1    |         t2          |          t3         |
+----+----------+---------------------+---------------------+
| 1  | 20:28:00 | 2016-04-09 01:41:58 | 2016-04-23 22:22:01 |
| 2  | 20:28:00 | 2016-04-09 01:41:59 | 2016-04-23 22:22:02 |
| 3  | 20:28:00 | 2016-04-09 01:42:01 | 2016-04-23 22:22:03 |
| 4  | 20:28:00 | 2016-04-09 01:42:03 | 2016-04-23 22:22:04 |
| 5  | 20:28:00 | 2016-04-09 01:42:08 | 2016-04-23 22:22:05 |
| 6  | 20:28:00 | 2016-04-09 01:56:38 | 2016-04-23 22:22:06 |
+----+----------+---------------------+---------------------+
6 rows in set (0.00 sec)</pre><p>It was inserted without a problem, and we can&#8217;t see any related info/warnings in the error log.</p>
<h1>Does the Replication work?</h1>
<p>In many scenarios, when you are upgrading a replicaset, the slaves are upgraded first. But will the replication work? The short answer is &#8220;yes.&#8221; I configured <strong>row-based</strong> replication between MySQL 5.6 and 5.7. The 5.6 was the master, and it had all the temporal types in the old format. On 5.7, I had new and old formats.</p>
<p>I replicated from old format to old format, and from old format to new format, and both are working.</p>
<h1>Conclusion</h1>
<p>Before upgrading to MySQL 5.7, tables should be altered to use the new format. If it isn&#8217;t done, however, the upgrade is still possible without altering all the tables – the drawbacks are you cannot use microseconds, and it takes more space on disk. If you had to upgrade to 5.7, however, you could change the format later using <pre>alter table</pre> or <a href="https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html">pt-online-schema-schange</a>.</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995306&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995306&vote=-1&apivote=1">Vote DOWN</a>";}i:7;a:10:{s:5:"title";s:48:"Building MaxScale 1.4.2 form GitHub on Fedora 23";s:4:"guid";s:70:"tag:blogger.com,1999:blog-3080615211468083537.post-3465261761372773564";s:4:"link";s:87:"http://mysqlentomologist.blogspot.com/2016/04/building-maxscale-142-form-github-on.html";s:11:"description";s:18495:"MariaDB MaxScale is mentioned in many blog posts recently. It's Application of the Year 2016 after all! I'd like to test it, follow posts like this etc, all that on my favorite and readily available testing platforms that are now Ubuntu of all kinds and, surely, Fedora 23 (on my wife's workstation, the most powerful hardware at hand).My old habits force me to build open source software I test from source, and I do not want to even discuss the topic of "MaxScale binaries availability" that was quite "popular" some time ago. So, after building MaxScale 1.4.1 on CentOS 6.7 back on March 31, 2016 (mostly just following MariaDB KB article on the topic) using libmysqld.a from MariaDB 10.0.23, this morning I decided to check new branch, 1.4.2, and build it on Fedora 23, following that same KB article (that unfortunately does not even mention Fedora after the fix to MXS-248). Thing is, Fedora is not officially supported as a platform for MaxScale 1.4.x, but why should we, those who can build things from source for testing purposes, care about this?I started with cloning MaxScale:git clone https://github.com/mariadb-corporation/MaxScale.gitcd MaxScaleand then:[openxs@fc23 MaxScale]$ git branch -r...&nbsp; origin/HEAD -&gt; origin/develop...&nbsp; origin/release-1.4.2...I remember spending enough time fighting with develop branch while building on CentOS 6.7, mostly with sqlite-related things it contained, so this time I proceed immediately to the branch I want to build:[openxs@fc23 MaxScale]$ git checkout release-1.4.2Branch release-1.4.2 set up to track remote branch release-1.4.2 from origin.Switched to a new branch 'release-1.4.2'[openxs@fc23 MaxScale]$ git branch&nbsp; develop* release-1.4.2[openxs@fc23 MaxScale]$ mkdir build[openxs@fc23 MaxScale]$ cd buildLast two steps originate from the KB article. We are almost ready for building, but what about the prerequisites? I've collected all the packages required for CentOS in that article and tried to install them all:[openxs@fc23 build]$ sudo yum install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build[sudo] password for openxs:Yum command has been deprecated, redirecting to '/usr/bin/dnf install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build'.See 'man dnf' and 'man yum2dnf' for more information.To transfer transaction metadata from yum to DNF, run:'dnf install python-dnf-plugins-extras-migrate &amp;&amp; dnf-2 migrate'Last metadata expiration check: 0:26:04 ago on Wed Apr 27 10:43:24 2016.Package gcc-5.3.1-6.fc23.x86_64 is already installed, skipping....Package pcre-devel-8.38-7.fc23.x86_64 is already installed, skipping.Dependencies resolved.================================================================================&nbsp;Package&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Arch&nbsp;&nbsp;&nbsp;&nbsp; Version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Repository&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Size================================================================================Installing:&nbsp;autoconf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2.69-21.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 709 k&nbsp;automake&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.15-4.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 695 k&nbsp;dwz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.12-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 106 k&nbsp;flex&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.5.39-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 328 k&nbsp;ghc-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.4.2-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.2 k&nbsp;gnat-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.4 k&nbsp;go-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.0 k&nbsp;libcurl-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 7.43.0-6.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 590 k&nbsp;libedit-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 3.1-13.20150325cvs.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp;&nbsp; 34 k&nbsp;librabbitmq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 43 k&nbsp;librabbitmq-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 52 k&nbsp;libtool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.4.6-8.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 707 k&nbsp;mariadb-common&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 74 k&nbsp;mariadb-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 25 k&nbsp;mariadb-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 869 k&nbsp;mariadb-embedded&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 4.0 M&nbsp;mariadb-embedded-devel&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.3 M&nbsp;mariadb-errmsg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 199 k&nbsp;mariadb-libs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 637 k&nbsp;ocaml-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.1 k&nbsp;patch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.7.5-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 123 k&nbsp;perl-Thread-Queue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3.07-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 22 k&nbsp;perl-generators&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.06-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 15 k&nbsp;perl-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1-17.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 9.7 k&nbsp;python-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3-7.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.1 k&nbsp;redhat-rpm-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 36-1.fc23.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 59 k&nbsp;rpm-build&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 4.13.0-0.rc1.13.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 137 kTransaction Summary================================================================================Install&nbsp; 27 PackagesTotal download size: 18 MInstalled size: 64 MIs this ok [y/N]: Y...Complete!Now, let's try simple approach:[openxs@fc23 build]$ cmake .....-- MySQL version: 10.0.23-- MySQL provider: MariaDB-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUNDCMake Error: The following variables are used in this project, but they are set to NOTFOUND.Please set them or make sure they are set and tested correctly in the CMake files:MYSQL_EMBEDDED_LIBRARIES_STATIC&nbsp;&nbsp;&nbsp; linked by target "cmTC_2494a" in directory /home/openxs/git/MaxScale/build/CMakeFiles/CMakeTmpCMake Error: Internal CMake error, TryCompile configure of cmake failed-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUND - not found-- PCRE libs: /usr/lib64/libpcre.so-- PCRE include directory: /usr/include-- Embedded mysqld does not have pcre_stack_guard, linking with system pcre.CMake Error at cmake/FindMySQL.cmake:115 (message):&nbsp; Library not found: libmysqld.&nbsp; If your install of MySQL is in a non-default&nbsp; location, please provide the location with -DMYSQL_EMBEDDED_LIBRARIES=&lt;path&nbsp; to library&gt;Call Stack (most recent call first):&nbsp; CMakeLists.txt:37 (find_package)-- Configuring incomplete, errors occurred!See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeOutput.log".See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeError.log".Failure, cmake can not find libmysqld.a it seems. Let me try to find it:[openxs@fc23 build]$ sudo find / -name libmysqld.a 2&gt;/dev/null/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a/home/openxs/dbs/5.7/lib/libmysqld.a/home/openxs/dbs/p5.6/lib/libmysqld.a/home/openxs/dbs/fb56/lib/libmysqld.a/home/openxs/10.1.12/lib/libmysqld.aThat's all, even though I installed all packages that looked as required based on the article! I have the library in many places (in my own builds and even in sandbox with MariaDB 10.1.12), but it's not installed where expected. Some more desperate tries (installing MariaDB server with sudo yum install mariadb-server, searches for package that provides libmysqld.a etc), chat with engineers of MariaDB and I've ended up with the fact that my packages are from Fedora (not MariaDB) and they just do not include the static library. Looks like a bug in Fedora packaging, if you ask me.I was not ready to add MariaDB's repository at the moment (to get MariaDB-devel etc, something KB article also suggests for supported platforms), so I decided that it would be fair just to build current MariaDB 10.1.13 from source and use everything needed from there. Last time I built 10.2 branch, so I had to check out 10.1 first:[openxs@fc23 server]$ git checkout 10.1Switched to branch '10.1'Your branch is behind 'origin/10.1' by 2 commits, and can be fast-forwarded.&nbsp; (use "git pull" to update your local branch)[openxs@fc23 server]$ git pullUpdating 1cf852d..071ae30Fast-forward&nbsp;client/mysqlbinlog.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 523 ++++++++++++++++++++++---------&nbsp;mysql-test/r/mysqlbinlog_raw_mode.result | 274 ++++++++++++++++&nbsp;mysql-test/t/mysqlbinlog_raw_mode.test&nbsp;&nbsp; | 387 +++++++++++++++++++++++&nbsp;sql/sql_priv.h&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 3 +-&nbsp;storage/innobase/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-&nbsp;storage/xtradb/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-&nbsp;6 files changed, 1062 insertions(+), 165 deletions(-)&nbsp;create mode 100644 mysql-test/r/mysqlbinlog_raw_mode.result&nbsp;create mode 100644 mysql-test/t/mysqlbinlog_raw_mode.test&nbsp;Then I've executed the following while in server directory:make clean rm CMakeCache.txtcmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON -DWITH_EMBEDDED_SERVER=ON -DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1makemake install &amp;&amp; make cleanNote that I've explicitly asked to build embedded server. I checked that the library is in the location I need:[openxs@fc23 server]$ sudo find / -name libmysqld.a 2&gt;/dev/null/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a/home/openxs/dbs/maria10.1/lib/libmysqld.a/home/openxs/dbs/5.7/lib/libmysqld.a/home/openxs/dbs/p5.6/lib/libmysqld.a/home/openxs/dbs/fb56/lib/libmysqld.a/home/openxs/10.1.12/lib/libmysqld.aThen I moved back to MaxScale/build directory and explicitly pointed out the location of headers, library and messages that I want to use with MaxScale:[openxs@fc23 build]$ cmake .. -DMYSQL_EMBEDDED_INCLUDE_DIR=/home/openxs/dbs/maria10.1/include/mysql -DMYSQL_EMBEDDED_LIBRARIES=/home/openxs/dbs/maria10.1/lib/libmysqld.a -DERRMSG=/home/openxs/dbs/maria10.1/share/english/errmsg.sys -DCMAKE_INSTALL_PREFIX=/home/openxs/maxscale -DWITH_MAXSCALE_CNF=N...-- Build files have been written to: /home/openxs/git/MaxScale/build[openxs@fc23 build]$ make...[ 95%] [BISON][ruleparser] Building parser with bison 3.0.4ruleparser.y:34.1-13: warning: deprecated directive, use Б-?%name-prefixБ-? [-Wdeprecated]&nbsp;%name-prefix="dbfw_yy"&nbsp;^^^^^^^^^^^^^[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/ruleparser.c.o[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/token.c.o[ 97%] Linking C shared library libdbfwfilter.so[ 97%] Built target dbfwfilterScanning dependencies of target maxadmin[ 98%] Building C object client/CMakeFiles/maxadmin.dir/maxadmin.c.o[ 98%] Linking C executable maxadmin[100%] Built target maxadminIt seems build completed without problems this time. We can try to test it (some tests do fail):[openxs@fc23 build]$ make testcore...&nbsp;1/22 Test&nbsp; #1: Internal-TestQueryClassifier .....***Exception: Other&nbsp; 0.35 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 2: Internal-CanonicalQuery&nbsp;2/22 Test&nbsp; #2: Internal-CanonicalQuery ..........***Failed&nbsp;&nbsp;&nbsp; 0.25 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 3: Internal-CanonicalQuerySelect&nbsp;3/22 Test&nbsp; #3: Internal-CanonicalQuerySelect ....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 4: Internal-CanonicalQueryAlter&nbsp;4/22 Test&nbsp; #4: Internal-CanonicalQueryAlter .....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 5: Internal-CanonicalQueryComment&nbsp;5/22 Test&nbsp; #5: Internal-CanonicalQueryComment ...***Failed&nbsp;&nbsp;&nbsp; 0.04 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 6: Internal-TestAdminUsers&nbsp;6/22 Test&nbsp; #6: Internal-TestAdminUsers ..........&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.44 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 7: Internal-TestBuffer&nbsp;7/22 Test&nbsp; #7: Internal-TestBuffer ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 8: Internal-TestDCB&nbsp;8/22 Test&nbsp; #8: Internal-TestDCB .................&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 9: Internal-TestFilter&nbsp;9/22 Test&nbsp; #9: Internal-TestFilter ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.03 sec...(As a side note, make install in my case had NOT installed anything to /home/openxs/maxscale, something to deal with later, as on CentOS 6.7 it worked...)In any case, I now have binaries to work with, of version 1.4.2:[openxs@fc23 build]$ ls bin/maxadmin&nbsp; maxbinlogcheck&nbsp; maxkeys&nbsp; maxpasswd&nbsp; maxscale[openxs@fc23 build]$ bin/maxscale --versionMariaDB Corporation MaxScale 1.4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wed Apr 27 13:24:01 2016------------------------------------------------------MaxScale 1.4.2[openxs@fc23 build]$ bin/maxadmin --versionbin/maxadmin Version 1.4.2To be continued one day... Stay tuned!";s:7:"content";a:1:{s:7:"encoded";s:21581:"MariaDB MaxScale is mentioned in many blog posts recently. It's <a href="https://mariadb.com/news-events/press-releases/mariadb-maxscale-wins-2016-application-year-mysql-community-awards" target="_blank">Application of the Year 2016</a> after all! I'd like to test it, follow posts like <a href="https://www.percona.com/blog/2016/03/29/read-write-split-routing-performance-in-maxscale/" target="_blank">this</a> etc, all that on my favorite and readily available testing platforms that are now Ubuntu of all kinds and, surely, Fedora 23 (on my wife's workstation, the most powerful hardware at hand).<br /><br />My old habits force me to build open source software I test from source, and I do not want to even discuss the topic of "<a href="https://www.percona.com/blog/2016/04/11/downloading-mariadb-maxscale-binaries/" target="_blank">MaxScale binaries availability</a>" that was quite "popular" some time ago. So, after building MaxScale 1.4.1 on CentOS 6.7 back on March 31, 2016 (mostly just following <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/building-maxscale-from-source-code/" target="_blank">MariaDB KB article on the topic</a>) using <b>libmysqld.a</b> from MariaDB 10.0.23, this morning I decided to check new branch, 1.4.2, and build it on Fedora 23, following that same KB article (that unfortunately does not even mention Fedora after the fix to <a href="https://mariadb.atlassian.net/browse/MXS-248" target="_blank">MXS-248</a>). Thing is, Fedora <a href="https://mariadb.com/products/product-faqs?qt-faqs=1#faq_1657" target="_blank">is not officially supported</a> as a platform for MaxScale 1.4.x, but why should we, those who can build things from source for testing purposes, care about this?<br /><br />I started with cloning MaxScale:<br /><br /><blockquote><span>git clone https://github.com/mariadb-corporation/MaxScale.git<br />cd MaxScale</span></blockquote>and then:<br /><br /><blockquote><span>[openxs@fc23 MaxScale]$ <b>git branch -r</b><br />...<br />&nbsp; origin/HEAD -&gt; origin/develop<br />...<br />&nbsp; origin/release-1.4.2<br />...</span></blockquote>I remember spending enough time fighting with <b>develop</b> branch while building on CentOS 6.7, mostly with <b>sqlite</b>-related things it contained, so this time I proceed immediately to the branch I want to build:<br /><br /><blockquote><span>[openxs@fc23 MaxScale]$ <b>git checkout release-1.4.2</b><br />Branch release-1.4.2 set up to track remote branch release-1.4.2 from origin.<br />Switched to a new branch 'release-1.4.2'<br />[openxs@fc23 MaxScale]$ git branch<br />&nbsp; develop<br />* release-1.4.2<br /><br />[openxs@fc23 MaxScale]$ <b>mkdir build</b><br />[openxs@fc23 MaxScale]$ <b>cd build</b></span></blockquote><br />Last two steps originate from the <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/building-maxscale-from-source-code/" target="_blank">KB article</a>. We are almost ready for building, but what about the prerequisites? I've collected all the packages required for CentOS in that article and tried to install them all:<br /><blockquote><span>[openxs@fc23 build]$ <b>sudo yum install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build</b>[sudo] password for openxs:<br />Yum command has been deprecated, redirecting to '/usr/bin/dnf install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build'.<br />See 'man dnf' and 'man yum2dnf' for more information.<br />To transfer transaction metadata from yum to DNF, run:<br />'dnf install python-dnf-plugins-extras-migrate &amp;&amp; dnf-2 migrate'<br /><br />Last metadata expiration check: 0:26:04 ago on Wed Apr 27 10:43:24 2016.<br />Package gcc-5.3.1-6.fc23.x86_64 is already installed, skipping.<br />...<br />Package pcre-devel-8.38-7.fc23.x86_64 is already installed, skipping.<br />Dependencies resolved.<br />================================================================================<br />&nbsp;Package&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Arch&nbsp;&nbsp;&nbsp;&nbsp; Version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Repository<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Size<br />================================================================================<br />Installing:<br />&nbsp;autoconf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2.69-21.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 709 k<br />&nbsp;automake&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.15-4.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 695 k<br />&nbsp;dwz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.12-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 106 k<br />&nbsp;flex&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.5.39-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 328 k<br />&nbsp;ghc-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.4.2-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.2 k<br />&nbsp;gnat-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.4 k<br />&nbsp;go-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.0 k<br />&nbsp;libcurl-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 7.43.0-6.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 590 k<br />&nbsp;libedit-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 3.1-13.20150325cvs.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp;&nbsp; 34 k<br />&nbsp;librabbitmq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 43 k<br />&nbsp;librabbitmq-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 52 k<br />&nbsp;libtool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.4.6-8.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 707 k<br />&nbsp;mariadb-common&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 74 k<br />&nbsp;mariadb-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 25 k<br />&nbsp;mariadb-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 869 k<br />&nbsp;mariadb-embedded&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 4.0 M<br />&nbsp;mariadb-embedded-devel&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.3 M<br />&nbsp;mariadb-errmsg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 199 k<br />&nbsp;mariadb-libs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 637 k<br />&nbsp;ocaml-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.1 k<br />&nbsp;patch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.7.5-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 123 k<br />&nbsp;perl-Thread-Queue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3.07-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 22 k<br />&nbsp;perl-generators&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.06-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 15 k<br />&nbsp;perl-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1-17.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 9.7 k<br />&nbsp;python-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3-7.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.1 k<br />&nbsp;redhat-rpm-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 36-1.fc23.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 59 k<br />&nbsp;rpm-build&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 4.13.0-0.rc1.13.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 137 k<br /><br />Transaction Summary<br />================================================================================<br />Install&nbsp; 27 Packages<br /><br />Total download size: 18 M<br />Installed size: 64 M<br />Is this ok [y/N]: Y<br /><br />...<br /><br />Complete!</span></blockquote>Now, let's try simple approach:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>cmake ..</b><br />...<br />-- MySQL version: 10.0.23<br />-- MySQL provider: MariaDB<br />-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUND<br />CMake Error: The following variables are used in this project, but they are set to NOTFOUND.<br />Please set them or make sure they are set and tested correctly in the CMake files:<br />MYSQL_EMBEDDED_LIBRARIES_STATIC<br />&nbsp;&nbsp;&nbsp; linked by target "cmTC_2494a" in directory /home/openxs/git/MaxScale/build/CMakeFiles/CMakeTmp<br /><br />CMake Error: Internal CMake error, TryCompile configure of cmake failed<br />-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUND - not found<br />-- PCRE libs: /usr/lib64/libpcre.so<br />-- PCRE include directory: /usr/include<br />-- Embedded mysqld does not have pcre_stack_guard, linking with system pcre.<br />CMake Error at cmake/FindMySQL.cmake:115 (message):<br />&nbsp; Library not found: libmysqld.&nbsp; If your install of MySQL is in a non-default<br />&nbsp; location, please provide the location with -DMYSQL_EMBEDDED_LIBRARIES=&lt;path<br />&nbsp; to library&gt;<br />Call Stack (most recent call first):<br />&nbsp; CMakeLists.txt:37 (find_package)<br /><br /><br />-- Configuring incomplete, errors occurred!<br />See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeOutput.log".<br />See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeError.log".</span></blockquote><br />Failure, <b>cmake</b> can not find <b>libmysqld.a</b> it seems. Let me try to find it:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>sudo find / -name libmysqld.a 2&gt;/dev/null</b><br />/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a<br />/home/openxs/dbs/5.7/lib/libmysqld.a<br />/home/openxs/dbs/p5.6/lib/libmysqld.a<br />/home/openxs/dbs/fb56/lib/libmysqld.a<br />/home/openxs/10.1.12/lib/libmysqld.a</span></blockquote>That's all, even though I installed all packages that looked as required based on the article! I have the library in many places (in my own builds and even in sandbox with MariaDB 10.1.12), but it's not installed where expected. Some more desperate tries (installing MariaDB server with <b>sudo yum install mariadb-server</b>, searches for package that provides <b>libmysqld.a</b> etc), chat with engineers of MariaDB and I've ended up with the fact that my packages are from Fedora (not MariaDB) and they just do not include the static library. Looks like a bug in Fedora packaging, if you ask me.<br /><br />I was not ready to add MariaDB's repository at the moment (to get <b>MariaDB-devel</b> etc, something KB article also suggests for supported platforms), so I decided that it would be fair just to build current MariaDB 10.1.13 from source and use everything needed from there. Last time I built 10.2 branch, so I had to check out 10.1 first:<br /><blockquote><span>[openxs@fc23 server]$ <b>git checkout 10.1</b><br />Switched to branch '10.1'<br />Your branch is behind 'origin/10.1' by 2 commits, and can be fast-forwarded.<br />&nbsp; (use "git pull" to update your local branch)<br />[openxs@fc23 server]$ <b>git pull</b><br />Updating 1cf852d..071ae30<br />Fast-forward<br />&nbsp;client/mysqlbinlog.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 523 ++++++++++++++++++++++---------<br />&nbsp;mysql-test/r/mysqlbinlog_raw_mode.result | 274 ++++++++++++++++<br />&nbsp;mysql-test/t/mysqlbinlog_raw_mode.test&nbsp;&nbsp; | 387 +++++++++++++++++++++++<br />&nbsp;sql/sql_priv.h&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 3 +-<br />&nbsp;storage/innobase/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-<br />&nbsp;storage/xtradb/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-<br />&nbsp;6 files changed, 1062 insertions(+), 165 deletions(-)<br />&nbsp;create mode 100644 mysql-test/r/mysqlbinlog_raw_mode.result<br />&nbsp;create mode 100644 mysql-test/t/mysqlbinlog_raw_mode.test</span></blockquote>&nbsp;Then I've executed the following while in <b>server</b> directory:<br /><br /><blockquote><span>make clean </span></blockquote><blockquote><span>rm CMakeCache.txt</span></blockquote><blockquote><span>cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON <b>-DWITH_EMBEDDED_SERVER=ON</b> <b>-DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1</b></span></blockquote><br /><blockquote><span>make<br /><br />make install &amp;&amp; make clean</span></blockquote>Note that I've explicitly asked to build embedded server. I checked that the library is in the location I need:<br /><br /><blockquote><span>[openxs@fc23 server]$ <b>sudo find / -name libmysqld.a 2&gt;/dev/null</b><br />/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a<br /><b>/home/openxs/dbs/maria10.1/lib/libmysqld.a</b><br />/home/openxs/dbs/5.7/lib/libmysqld.a<br />/home/openxs/dbs/p5.6/lib/libmysqld.a<br />/home/openxs/dbs/fb56/lib/libmysqld.a<br />/home/openxs/10.1.12/lib/libmysqld.a</span></blockquote>Then I moved back to <b>MaxScale/build</b> directory and explicitly pointed out the location of headers, library and messages that I want to use with MaxScale:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>cmake .. -DMYSQL_EMBEDDED_INCLUDE_DIR=/home/openxs/dbs/maria10.1/include/mysql -DMYSQL_EMBEDDED_LIBRARIES=/home/openxs/dbs/maria10.1/lib/libmysqld.a -DERRMSG=/home/openxs/dbs/maria10.1/share/english/errmsg.sys -DCMAKE_INSTALL_PREFIX=/home/openxs/maxscale -DWITH_MAXSCALE_CNF=N</b><br />...<br />-- Build files have been written to: /home/openxs/git/MaxScale/build<br /><br />[openxs@fc23 build]$ <b>make</b><br />...<br />[ 95%] [BISON][ruleparser] Building parser with bison 3.0.4<br />ruleparser.y:34.1-13: warning: deprecated directive, use Б-?%name-prefixБ-? [-Wdeprecated]<br />&nbsp;%name-prefix="dbfw_yy"<br />&nbsp;^^^^^^^^^^^^^<br />[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/ruleparser.c.o<br />[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/token.c.o<br />[ 97%] Linking C shared library libdbfwfilter.so<br />[ 97%] Built target dbfwfilter<br />Scanning dependencies of target maxadmin<br />[ 98%] Building C object client/CMakeFiles/maxadmin.dir/maxadmin.c.o<br />[ 98%] Linking C executable maxadmin<br />[100%] Built target maxadmin</span></blockquote><br />It seems build completed without problems this time. We can try to test it (some tests do fail):<br /><br /><br /><br /><br /><br /><br /><br /><blockquote><span>[openxs@fc23 build]$ <b>make testcore</b><br />...<br />&nbsp;1/22 Test&nbsp; #1: Internal-TestQueryClassifier .....***Exception: Other&nbsp; 0.35 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 2: Internal-CanonicalQuery<br />&nbsp;2/22 Test&nbsp; #2: Internal-CanonicalQuery ..........***Failed&nbsp;&nbsp;&nbsp; 0.25 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 3: Internal-CanonicalQuerySelect<br />&nbsp;3/22 Test&nbsp; #3: Internal-CanonicalQuerySelect ....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 4: Internal-CanonicalQueryAlter<br />&nbsp;4/22 Test&nbsp; #4: Internal-CanonicalQueryAlter .....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 5: Internal-CanonicalQueryComment<br />&nbsp;5/22 Test&nbsp; #5: Internal-CanonicalQueryComment ...***Failed&nbsp;&nbsp;&nbsp; 0.04 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 6: Internal-TestAdminUsers<br />&nbsp;6/22 Test&nbsp; #6: Internal-TestAdminUsers ..........&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.44 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 7: Internal-TestBuffer<br />&nbsp;7/22 Test&nbsp; #7: Internal-TestBuffer ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 8: Internal-TestDCB<br />&nbsp;8/22 Test&nbsp; #8: Internal-TestDCB .................&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 9: Internal-TestFilter<br />&nbsp;9/22 Test&nbsp; #9: Internal-TestFilter ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.03 sec<br />...</span></blockquote>(As a side note, <b>make install</b> in my case had NOT installed anything to<span> <b>/home/openxs/maxscale</b>, something to deal with later, as on CentOS 6.7 it worked...)</span><br /><br /><br />In any case, I now have binaries to work with, of version 1.4.2:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>ls bin/</b><br />maxadmin&nbsp; maxbinlogcheck&nbsp; maxkeys&nbsp; maxpasswd&nbsp; maxscale<br />[openxs@fc23 build]$ <b>bin/maxscale --version</b><br /><br /><br />MariaDB Corporation MaxScale 1.4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wed Apr 27 13:24:01 2016<br />------------------------------------------------------<br />MaxScale 1.4.2<br /><br />[openxs@fc23 build]$ <b>bin/maxadmin --version</b><br />bin/maxadmin Version 1.4.2</span></blockquote>To be continued one day... Stay tuned!<br /><br /><br /><br /><br /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995307&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995307&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 27 Apr 2016 19:07:45 +0000";s:2:"dc";a:1:{s:7:"creator";s:16:"Valeriy Kravchuk";}s:8:"category";s:32:"FedoraGitHubMariaDBMaxScalemysql";s:7:"summary";s:18495:"MariaDB MaxScale is mentioned in many blog posts recently. It's Application of the Year 2016 after all! I'd like to test it, follow posts like this etc, all that on my favorite and readily available testing platforms that are now Ubuntu of all kinds and, surely, Fedora 23 (on my wife's workstation, the most powerful hardware at hand).My old habits force me to build open source software I test from source, and I do not want to even discuss the topic of "MaxScale binaries availability" that was quite "popular" some time ago. So, after building MaxScale 1.4.1 on CentOS 6.7 back on March 31, 2016 (mostly just following MariaDB KB article on the topic) using libmysqld.a from MariaDB 10.0.23, this morning I decided to check new branch, 1.4.2, and build it on Fedora 23, following that same KB article (that unfortunately does not even mention Fedora after the fix to MXS-248). Thing is, Fedora is not officially supported as a platform for MaxScale 1.4.x, but why should we, those who can build things from source for testing purposes, care about this?I started with cloning MaxScale:git clone https://github.com/mariadb-corporation/MaxScale.gitcd MaxScaleand then:[openxs@fc23 MaxScale]$ git branch -r...&nbsp; origin/HEAD -&gt; origin/develop...&nbsp; origin/release-1.4.2...I remember spending enough time fighting with develop branch while building on CentOS 6.7, mostly with sqlite-related things it contained, so this time I proceed immediately to the branch I want to build:[openxs@fc23 MaxScale]$ git checkout release-1.4.2Branch release-1.4.2 set up to track remote branch release-1.4.2 from origin.Switched to a new branch 'release-1.4.2'[openxs@fc23 MaxScale]$ git branch&nbsp; develop* release-1.4.2[openxs@fc23 MaxScale]$ mkdir build[openxs@fc23 MaxScale]$ cd buildLast two steps originate from the KB article. We are almost ready for building, but what about the prerequisites? I've collected all the packages required for CentOS in that article and tried to install them all:[openxs@fc23 build]$ sudo yum install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build[sudo] password for openxs:Yum command has been deprecated, redirecting to '/usr/bin/dnf install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build'.See 'man dnf' and 'man yum2dnf' for more information.To transfer transaction metadata from yum to DNF, run:'dnf install python-dnf-plugins-extras-migrate &amp;&amp; dnf-2 migrate'Last metadata expiration check: 0:26:04 ago on Wed Apr 27 10:43:24 2016.Package gcc-5.3.1-6.fc23.x86_64 is already installed, skipping....Package pcre-devel-8.38-7.fc23.x86_64 is already installed, skipping.Dependencies resolved.================================================================================&nbsp;Package&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Arch&nbsp;&nbsp;&nbsp;&nbsp; Version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Repository&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Size================================================================================Installing:&nbsp;autoconf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2.69-21.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 709 k&nbsp;automake&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.15-4.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 695 k&nbsp;dwz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.12-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 106 k&nbsp;flex&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.5.39-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 328 k&nbsp;ghc-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.4.2-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.2 k&nbsp;gnat-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.4 k&nbsp;go-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.0 k&nbsp;libcurl-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 7.43.0-6.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 590 k&nbsp;libedit-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 3.1-13.20150325cvs.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp;&nbsp; 34 k&nbsp;librabbitmq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 43 k&nbsp;librabbitmq-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 52 k&nbsp;libtool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.4.6-8.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 707 k&nbsp;mariadb-common&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 74 k&nbsp;mariadb-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 25 k&nbsp;mariadb-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 869 k&nbsp;mariadb-embedded&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 4.0 M&nbsp;mariadb-embedded-devel&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.3 M&nbsp;mariadb-errmsg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 199 k&nbsp;mariadb-libs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 637 k&nbsp;ocaml-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.1 k&nbsp;patch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.7.5-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 123 k&nbsp;perl-Thread-Queue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3.07-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 22 k&nbsp;perl-generators&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.06-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 15 k&nbsp;perl-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1-17.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 9.7 k&nbsp;python-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3-7.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.1 k&nbsp;redhat-rpm-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 36-1.fc23.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 59 k&nbsp;rpm-build&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 4.13.0-0.rc1.13.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 137 kTransaction Summary================================================================================Install&nbsp; 27 PackagesTotal download size: 18 MInstalled size: 64 MIs this ok [y/N]: Y...Complete!Now, let's try simple approach:[openxs@fc23 build]$ cmake .....-- MySQL version: 10.0.23-- MySQL provider: MariaDB-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUNDCMake Error: The following variables are used in this project, but they are set to NOTFOUND.Please set them or make sure they are set and tested correctly in the CMake files:MYSQL_EMBEDDED_LIBRARIES_STATIC&nbsp;&nbsp;&nbsp; linked by target "cmTC_2494a" in directory /home/openxs/git/MaxScale/build/CMakeFiles/CMakeTmpCMake Error: Internal CMake error, TryCompile configure of cmake failed-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUND - not found-- PCRE libs: /usr/lib64/libpcre.so-- PCRE include directory: /usr/include-- Embedded mysqld does not have pcre_stack_guard, linking with system pcre.CMake Error at cmake/FindMySQL.cmake:115 (message):&nbsp; Library not found: libmysqld.&nbsp; If your install of MySQL is in a non-default&nbsp; location, please provide the location with -DMYSQL_EMBEDDED_LIBRARIES=&lt;path&nbsp; to library&gt;Call Stack (most recent call first):&nbsp; CMakeLists.txt:37 (find_package)-- Configuring incomplete, errors occurred!See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeOutput.log".See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeError.log".Failure, cmake can not find libmysqld.a it seems. Let me try to find it:[openxs@fc23 build]$ sudo find / -name libmysqld.a 2&gt;/dev/null/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a/home/openxs/dbs/5.7/lib/libmysqld.a/home/openxs/dbs/p5.6/lib/libmysqld.a/home/openxs/dbs/fb56/lib/libmysqld.a/home/openxs/10.1.12/lib/libmysqld.aThat's all, even though I installed all packages that looked as required based on the article! I have the library in many places (in my own builds and even in sandbox with MariaDB 10.1.12), but it's not installed where expected. Some more desperate tries (installing MariaDB server with sudo yum install mariadb-server, searches for package that provides libmysqld.a etc), chat with engineers of MariaDB and I've ended up with the fact that my packages are from Fedora (not MariaDB) and they just do not include the static library. Looks like a bug in Fedora packaging, if you ask me.I was not ready to add MariaDB's repository at the moment (to get MariaDB-devel etc, something KB article also suggests for supported platforms), so I decided that it would be fair just to build current MariaDB 10.1.13 from source and use everything needed from there. Last time I built 10.2 branch, so I had to check out 10.1 first:[openxs@fc23 server]$ git checkout 10.1Switched to branch '10.1'Your branch is behind 'origin/10.1' by 2 commits, and can be fast-forwarded.&nbsp; (use "git pull" to update your local branch)[openxs@fc23 server]$ git pullUpdating 1cf852d..071ae30Fast-forward&nbsp;client/mysqlbinlog.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 523 ++++++++++++++++++++++---------&nbsp;mysql-test/r/mysqlbinlog_raw_mode.result | 274 ++++++++++++++++&nbsp;mysql-test/t/mysqlbinlog_raw_mode.test&nbsp;&nbsp; | 387 +++++++++++++++++++++++&nbsp;sql/sql_priv.h&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 3 +-&nbsp;storage/innobase/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-&nbsp;storage/xtradb/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-&nbsp;6 files changed, 1062 insertions(+), 165 deletions(-)&nbsp;create mode 100644 mysql-test/r/mysqlbinlog_raw_mode.result&nbsp;create mode 100644 mysql-test/t/mysqlbinlog_raw_mode.test&nbsp;Then I've executed the following while in server directory:make clean rm CMakeCache.txtcmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON -DWITH_EMBEDDED_SERVER=ON -DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1makemake install &amp;&amp; make cleanNote that I've explicitly asked to build embedded server. I checked that the library is in the location I need:[openxs@fc23 server]$ sudo find / -name libmysqld.a 2&gt;/dev/null/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a/home/openxs/dbs/maria10.1/lib/libmysqld.a/home/openxs/dbs/5.7/lib/libmysqld.a/home/openxs/dbs/p5.6/lib/libmysqld.a/home/openxs/dbs/fb56/lib/libmysqld.a/home/openxs/10.1.12/lib/libmysqld.aThen I moved back to MaxScale/build directory and explicitly pointed out the location of headers, library and messages that I want to use with MaxScale:[openxs@fc23 build]$ cmake .. -DMYSQL_EMBEDDED_INCLUDE_DIR=/home/openxs/dbs/maria10.1/include/mysql -DMYSQL_EMBEDDED_LIBRARIES=/home/openxs/dbs/maria10.1/lib/libmysqld.a -DERRMSG=/home/openxs/dbs/maria10.1/share/english/errmsg.sys -DCMAKE_INSTALL_PREFIX=/home/openxs/maxscale -DWITH_MAXSCALE_CNF=N...-- Build files have been written to: /home/openxs/git/MaxScale/build[openxs@fc23 build]$ make...[ 95%] [BISON][ruleparser] Building parser with bison 3.0.4ruleparser.y:34.1-13: warning: deprecated directive, use Б-?%name-prefixБ-? [-Wdeprecated]&nbsp;%name-prefix="dbfw_yy"&nbsp;^^^^^^^^^^^^^[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/ruleparser.c.o[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/token.c.o[ 97%] Linking C shared library libdbfwfilter.so[ 97%] Built target dbfwfilterScanning dependencies of target maxadmin[ 98%] Building C object client/CMakeFiles/maxadmin.dir/maxadmin.c.o[ 98%] Linking C executable maxadmin[100%] Built target maxadminIt seems build completed without problems this time. We can try to test it (some tests do fail):[openxs@fc23 build]$ make testcore...&nbsp;1/22 Test&nbsp; #1: Internal-TestQueryClassifier .....***Exception: Other&nbsp; 0.35 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 2: Internal-CanonicalQuery&nbsp;2/22 Test&nbsp; #2: Internal-CanonicalQuery ..........***Failed&nbsp;&nbsp;&nbsp; 0.25 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 3: Internal-CanonicalQuerySelect&nbsp;3/22 Test&nbsp; #3: Internal-CanonicalQuerySelect ....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 4: Internal-CanonicalQueryAlter&nbsp;4/22 Test&nbsp; #4: Internal-CanonicalQueryAlter .....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 5: Internal-CanonicalQueryComment&nbsp;5/22 Test&nbsp; #5: Internal-CanonicalQueryComment ...***Failed&nbsp;&nbsp;&nbsp; 0.04 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 6: Internal-TestAdminUsers&nbsp;6/22 Test&nbsp; #6: Internal-TestAdminUsers ..........&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.44 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 7: Internal-TestBuffer&nbsp;7/22 Test&nbsp; #7: Internal-TestBuffer ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 8: Internal-TestDCB&nbsp;8/22 Test&nbsp; #8: Internal-TestDCB .................&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 9: Internal-TestFilter&nbsp;9/22 Test&nbsp; #9: Internal-TestFilter ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.03 sec...(As a side note, make install in my case had NOT installed anything to /home/openxs/maxscale, something to deal with later, as on CentOS 6.7 it worked...)In any case, I now have binaries to work with, of version 1.4.2:[openxs@fc23 build]$ ls bin/maxadmin&nbsp; maxbinlogcheck&nbsp; maxkeys&nbsp; maxpasswd&nbsp; maxscale[openxs@fc23 build]$ bin/maxscale --versionMariaDB Corporation MaxScale 1.4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wed Apr 27 13:24:01 2016------------------------------------------------------MaxScale 1.4.2[openxs@fc23 build]$ bin/maxadmin --versionbin/maxadmin Version 1.4.2To be continued one day... Stay tuned!";s:12:"atom_content";s:21581:"MariaDB MaxScale is mentioned in many blog posts recently. It's <a href="https://mariadb.com/news-events/press-releases/mariadb-maxscale-wins-2016-application-year-mysql-community-awards" target="_blank">Application of the Year 2016</a> after all! I'd like to test it, follow posts like <a href="https://www.percona.com/blog/2016/03/29/read-write-split-routing-performance-in-maxscale/" target="_blank">this</a> etc, all that on my favorite and readily available testing platforms that are now Ubuntu of all kinds and, surely, Fedora 23 (on my wife's workstation, the most powerful hardware at hand).<br /><br />My old habits force me to build open source software I test from source, and I do not want to even discuss the topic of "<a href="https://www.percona.com/blog/2016/04/11/downloading-mariadb-maxscale-binaries/" target="_blank">MaxScale binaries availability</a>" that was quite "popular" some time ago. So, after building MaxScale 1.4.1 on CentOS 6.7 back on March 31, 2016 (mostly just following <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/building-maxscale-from-source-code/" target="_blank">MariaDB KB article on the topic</a>) using <b>libmysqld.a</b> from MariaDB 10.0.23, this morning I decided to check new branch, 1.4.2, and build it on Fedora 23, following that same KB article (that unfortunately does not even mention Fedora after the fix to <a href="https://mariadb.atlassian.net/browse/MXS-248" target="_blank">MXS-248</a>). Thing is, Fedora <a href="https://mariadb.com/products/product-faqs?qt-faqs=1#faq_1657" target="_blank">is not officially supported</a> as a platform for MaxScale 1.4.x, but why should we, those who can build things from source for testing purposes, care about this?<br /><br />I started with cloning MaxScale:<br /><br /><blockquote><span>git clone https://github.com/mariadb-corporation/MaxScale.git<br />cd MaxScale</span></blockquote>and then:<br /><br /><blockquote><span>[openxs@fc23 MaxScale]$ <b>git branch -r</b><br />...<br />&nbsp; origin/HEAD -&gt; origin/develop<br />...<br />&nbsp; origin/release-1.4.2<br />...</span></blockquote>I remember spending enough time fighting with <b>develop</b> branch while building on CentOS 6.7, mostly with <b>sqlite</b>-related things it contained, so this time I proceed immediately to the branch I want to build:<br /><br /><blockquote><span>[openxs@fc23 MaxScale]$ <b>git checkout release-1.4.2</b><br />Branch release-1.4.2 set up to track remote branch release-1.4.2 from origin.<br />Switched to a new branch 'release-1.4.2'<br />[openxs@fc23 MaxScale]$ git branch<br />&nbsp; develop<br />* release-1.4.2<br /><br />[openxs@fc23 MaxScale]$ <b>mkdir build</b><br />[openxs@fc23 MaxScale]$ <b>cd build</b></span></blockquote><br />Last two steps originate from the <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/building-maxscale-from-source-code/" target="_blank">KB article</a>. We are almost ready for building, but what about the prerequisites? I've collected all the packages required for CentOS in that article and tried to install them all:<br /><blockquote><span>[openxs@fc23 build]$ <b>sudo yum install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build</b>[sudo] password for openxs:<br />Yum command has been deprecated, redirecting to '/usr/bin/dnf install mariadb-devel mariadb-embedded-devel libedit-devel gcc gcc-c++ ncurses-devel bison flex glibc-devel cmake libgcc perl make libtool openssl-devel libaio libaio-devel librabbitmq-devel libcurl-devel pcre-devel rpm-build'.<br />See 'man dnf' and 'man yum2dnf' for more information.<br />To transfer transaction metadata from yum to DNF, run:<br />'dnf install python-dnf-plugins-extras-migrate &amp;&amp; dnf-2 migrate'<br /><br />Last metadata expiration check: 0:26:04 ago on Wed Apr 27 10:43:24 2016.<br />Package gcc-5.3.1-6.fc23.x86_64 is already installed, skipping.<br />...<br />Package pcre-devel-8.38-7.fc23.x86_64 is already installed, skipping.<br />Dependencies resolved.<br />================================================================================<br />&nbsp;Package&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Arch&nbsp;&nbsp;&nbsp;&nbsp; Version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Repository<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Size<br />================================================================================<br />Installing:<br />&nbsp;autoconf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2.69-21.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 709 k<br />&nbsp;automake&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.15-4.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 695 k<br />&nbsp;dwz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.12-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 106 k<br />&nbsp;flex&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.5.39-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 328 k<br />&nbsp;ghc-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.4.2-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.2 k<br />&nbsp;gnat-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.4 k<br />&nbsp;go-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.0 k<br />&nbsp;libcurl-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 7.43.0-6.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 590 k<br />&nbsp;libedit-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 3.1-13.20150325cvs.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp;&nbsp; 34 k<br />&nbsp;librabbitmq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 43 k<br />&nbsp;librabbitmq-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 0.8.0-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 52 k<br />&nbsp;libtool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.4.6-8.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 707 k<br />&nbsp;mariadb-common&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 74 k<br />&nbsp;mariadb-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 25 k<br />&nbsp;mariadb-devel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 869 k<br />&nbsp;mariadb-embedded&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 4.0 M<br />&nbsp;mariadb-embedded-devel&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.3 M<br />&nbsp;mariadb-errmsg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 199 k<br />&nbsp;mariadb-libs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 1:10.0.23-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 637 k<br />&nbsp;ocaml-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 2-3.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 8.1 k<br />&nbsp;patch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 2.7.5-2.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 123 k<br />&nbsp;perl-Thread-Queue&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3.07-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 22 k<br />&nbsp;perl-generators&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1.06-1.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 15 k<br />&nbsp;perl-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 1-17.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fedora&nbsp;&nbsp;&nbsp; 9.7 k<br />&nbsp;python-srpm-macros&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 3-7.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 8.1 k<br />&nbsp;redhat-rpm-config&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; noarch&nbsp;&nbsp; 36-1.fc23.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp;&nbsp; 59 k<br />&nbsp;rpm-build&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x86_64&nbsp;&nbsp; 4.13.0-0.rc1.13.fc23&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; updates&nbsp;&nbsp; 137 k<br /><br />Transaction Summary<br />================================================================================<br />Install&nbsp; 27 Packages<br /><br />Total download size: 18 M<br />Installed size: 64 M<br />Is this ok [y/N]: Y<br /><br />...<br /><br />Complete!</span></blockquote>Now, let's try simple approach:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>cmake ..</b><br />...<br />-- MySQL version: 10.0.23<br />-- MySQL provider: MariaDB<br />-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUND<br />CMake Error: The following variables are used in this project, but they are set to NOTFOUND.<br />Please set them or make sure they are set and tested correctly in the CMake files:<br />MYSQL_EMBEDDED_LIBRARIES_STATIC<br />&nbsp;&nbsp;&nbsp; linked by target "cmTC_2494a" in directory /home/openxs/git/MaxScale/build/CMakeFiles/CMakeTmp<br /><br />CMake Error: Internal CMake error, TryCompile configure of cmake failed<br />-- Looking for pcre_stack_guard in MYSQL_EMBEDDED_LIBRARIES_STATIC-NOTFOUND - not found<br />-- PCRE libs: /usr/lib64/libpcre.so<br />-- PCRE include directory: /usr/include<br />-- Embedded mysqld does not have pcre_stack_guard, linking with system pcre.<br />CMake Error at cmake/FindMySQL.cmake:115 (message):<br />&nbsp; Library not found: libmysqld.&nbsp; If your install of MySQL is in a non-default<br />&nbsp; location, please provide the location with -DMYSQL_EMBEDDED_LIBRARIES=&lt;path<br />&nbsp; to library&gt;<br />Call Stack (most recent call first):<br />&nbsp; CMakeLists.txt:37 (find_package)<br /><br /><br />-- Configuring incomplete, errors occurred!<br />See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeOutput.log".<br />See also "/home/openxs/git/MaxScale/build/CMakeFiles/CMakeError.log".</span></blockquote><br />Failure, <b>cmake</b> can not find <b>libmysqld.a</b> it seems. Let me try to find it:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>sudo find / -name libmysqld.a 2&gt;/dev/null</b><br />/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a<br />/home/openxs/dbs/5.7/lib/libmysqld.a<br />/home/openxs/dbs/p5.6/lib/libmysqld.a<br />/home/openxs/dbs/fb56/lib/libmysqld.a<br />/home/openxs/10.1.12/lib/libmysqld.a</span></blockquote>That's all, even though I installed all packages that looked as required based on the article! I have the library in many places (in my own builds and even in sandbox with MariaDB 10.1.12), but it's not installed where expected. Some more desperate tries (installing MariaDB server with <b>sudo yum install mariadb-server</b>, searches for package that provides <b>libmysqld.a</b> etc), chat with engineers of MariaDB and I've ended up with the fact that my packages are from Fedora (not MariaDB) and they just do not include the static library. Looks like a bug in Fedora packaging, if you ask me.<br /><br />I was not ready to add MariaDB's repository at the moment (to get <b>MariaDB-devel</b> etc, something KB article also suggests for supported platforms), so I decided that it would be fair just to build current MariaDB 10.1.13 from source and use everything needed from there. Last time I built 10.2 branch, so I had to check out 10.1 first:<br /><blockquote><span>[openxs@fc23 server]$ <b>git checkout 10.1</b><br />Switched to branch '10.1'<br />Your branch is behind 'origin/10.1' by 2 commits, and can be fast-forwarded.<br />&nbsp; (use "git pull" to update your local branch)<br />[openxs@fc23 server]$ <b>git pull</b><br />Updating 1cf852d..071ae30<br />Fast-forward<br />&nbsp;client/mysqlbinlog.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 523 ++++++++++++++++++++++---------<br />&nbsp;mysql-test/r/mysqlbinlog_raw_mode.result | 274 ++++++++++++++++<br />&nbsp;mysql-test/t/mysqlbinlog_raw_mode.test&nbsp;&nbsp; | 387 +++++++++++++++++++++++<br />&nbsp;sql/sql_priv.h&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 3 +-<br />&nbsp;storage/innobase/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-<br />&nbsp;storage/xtradb/dict/dict0boot.cc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 20 +-<br />&nbsp;6 files changed, 1062 insertions(+), 165 deletions(-)<br />&nbsp;create mode 100644 mysql-test/r/mysqlbinlog_raw_mode.result<br />&nbsp;create mode 100644 mysql-test/t/mysqlbinlog_raw_mode.test</span></blockquote>&nbsp;Then I've executed the following while in <b>server</b> directory:<br /><br /><blockquote><span>make clean </span></blockquote><blockquote><span>rm CMakeCache.txt</span></blockquote><blockquote><span>cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON <b>-DWITH_EMBEDDED_SERVER=ON</b> <b>-DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1</b></span></blockquote><br /><blockquote><span>make<br /><br />make install &amp;&amp; make clean</span></blockquote>Note that I've explicitly asked to build embedded server. I checked that the library is in the location I need:<br /><br /><blockquote><span>[openxs@fc23 server]$ <b>sudo find / -name libmysqld.a 2&gt;/dev/null</b><br />/home/openxs/git/percona-xtrabackup/libmysqld/libmysqld.a<br /><b>/home/openxs/dbs/maria10.1/lib/libmysqld.a</b><br />/home/openxs/dbs/5.7/lib/libmysqld.a<br />/home/openxs/dbs/p5.6/lib/libmysqld.a<br />/home/openxs/dbs/fb56/lib/libmysqld.a<br />/home/openxs/10.1.12/lib/libmysqld.a</span></blockquote>Then I moved back to <b>MaxScale/build</b> directory and explicitly pointed out the location of headers, library and messages that I want to use with MaxScale:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>cmake .. -DMYSQL_EMBEDDED_INCLUDE_DIR=/home/openxs/dbs/maria10.1/include/mysql -DMYSQL_EMBEDDED_LIBRARIES=/home/openxs/dbs/maria10.1/lib/libmysqld.a -DERRMSG=/home/openxs/dbs/maria10.1/share/english/errmsg.sys -DCMAKE_INSTALL_PREFIX=/home/openxs/maxscale -DWITH_MAXSCALE_CNF=N</b><br />...<br />-- Build files have been written to: /home/openxs/git/MaxScale/build<br /><br />[openxs@fc23 build]$ <b>make</b><br />...<br />[ 95%] [BISON][ruleparser] Building parser with bison 3.0.4<br />ruleparser.y:34.1-13: warning: deprecated directive, use Б-?%name-prefixБ-? [-Wdeprecated]<br />&nbsp;%name-prefix="dbfw_yy"<br />&nbsp;^^^^^^^^^^^^^<br />[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/ruleparser.c.o<br />[ 96%] Building C object server/modules/filter/dbfwfilter/CMakeFiles/dbfwfilter.dir/token.c.o<br />[ 97%] Linking C shared library libdbfwfilter.so<br />[ 97%] Built target dbfwfilter<br />Scanning dependencies of target maxadmin<br />[ 98%] Building C object client/CMakeFiles/maxadmin.dir/maxadmin.c.o<br />[ 98%] Linking C executable maxadmin<br />[100%] Built target maxadmin</span></blockquote><br />It seems build completed without problems this time. We can try to test it (some tests do fail):<br /><br /><br /><br /><br /><br /><br /><br /><blockquote><span>[openxs@fc23 build]$ <b>make testcore</b><br />...<br />&nbsp;1/22 Test&nbsp; #1: Internal-TestQueryClassifier .....***Exception: Other&nbsp; 0.35 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 2: Internal-CanonicalQuery<br />&nbsp;2/22 Test&nbsp; #2: Internal-CanonicalQuery ..........***Failed&nbsp;&nbsp;&nbsp; 0.25 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 3: Internal-CanonicalQuerySelect<br />&nbsp;3/22 Test&nbsp; #3: Internal-CanonicalQuerySelect ....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 4: Internal-CanonicalQueryAlter<br />&nbsp;4/22 Test&nbsp; #4: Internal-CanonicalQueryAlter .....***Failed&nbsp;&nbsp;&nbsp; 0.04 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 5: Internal-CanonicalQueryComment<br />&nbsp;5/22 Test&nbsp; #5: Internal-CanonicalQueryComment ...***Failed&nbsp;&nbsp;&nbsp; 0.04 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 6: Internal-TestAdminUsers<br />&nbsp;6/22 Test&nbsp; #6: Internal-TestAdminUsers ..........&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.44 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 7: Internal-TestBuffer<br />&nbsp;7/22 Test&nbsp; #7: Internal-TestBuffer ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 8: Internal-TestDCB<br />&nbsp;8/22 Test&nbsp; #8: Internal-TestDCB .................&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.01 sec<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start&nbsp; 9: Internal-TestFilter<br />&nbsp;9/22 Test&nbsp; #9: Internal-TestFilter ..............&nbsp;&nbsp; Passed&nbsp;&nbsp;&nbsp; 0.03 sec<br />...</span></blockquote>(As a side note, <b>make install</b> in my case had NOT installed anything to<span> <b>/home/openxs/maxscale</b>, something to deal with later, as on CentOS 6.7 it worked...)</span><br /><br /><br />In any case, I now have binaries to work with, of version 1.4.2:<br /><br /><blockquote><span>[openxs@fc23 build]$ <b>ls bin/</b><br />maxadmin&nbsp; maxbinlogcheck&nbsp; maxkeys&nbsp; maxpasswd&nbsp; maxscale<br />[openxs@fc23 build]$ <b>bin/maxscale --version</b><br /><br /><br />MariaDB Corporation MaxScale 1.4.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wed Apr 27 13:24:01 2016<br />------------------------------------------------------<br />MaxScale 1.4.2<br /><br />[openxs@fc23 build]$ <b>bin/maxadmin --version</b><br />bin/maxadmin Version 1.4.2</span></blockquote>To be continued one day... Stay tuned!<br /><br /><br /><br /><br /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995307&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995307&vote=-1&apivote=1">Vote DOWN</a>";}i:8;a:9:{s:5:"title";s:56:"MariaDB Server growth bolstered by OpenStack User Survey";s:4:"guid";s:26:"2543 at http://mariadb.com";s:4:"link";s:77:"http://mariadb.com/blog/mariadb-server-growth-bolstered-openstack-user-survey";s:11:"description";s:1818:"Wed, 2016-04-27 14:20colinWhile many are at the ongoing OpenStack Summit in Austin, it came to my attention that the OpenStack project has recently published their latest user survey. From there, it is clear that there is growth in the usage of MariaDB Server -- up 6 points from the previous survey. MariaDB Galera Cluster dropped by 1 point, but it's worth noting that in MariaDB Server 10.1, you're getting it all as an integrated download so it is quite likely that people are just referring to it as "MariaDB" now.



(Screenshot taken from the user survey)

While MySQL still dominates at 35 percentage points, the largest pie of production OpenStack deployments are either based on MariaDB Server or MariaDB Galera Cluster which are developed by both the MariaDB Corporation &amp; the MariaDB Foundation.

Some may say this is because "defaults matter" (as MariaDB Server gets to be the default in many Linux distributions), but the majority of OpenStack deployments today are using Ubuntu Server, which ships MySQL as a default provider when one requests for MySQL. So this is a conscious choice people are making to go with MariaDB Server or MariaDB Galera Cluster.

Thank you OpenStack deployers! If you ever want to talk about your deployment, feel free to drop me a line or even tweet @bytebot.
Tags:&nbsp;CloudMariaDB Releases
About the Author
  
      


 
Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.



";s:7:"content";a:1:{s:7:"encoded";s:3595:"<div><div><div>Wed, 2016-04-27 14:20</div></div></div><div><div><div>colin</div></div></div><div><div><div property="content:encoded"><p>While many are at the ongoing <a href="https://www.openstack.org/summit/austin-2016/">OpenStack Summit in Austin</a>, it came to my attention that the OpenStack project has recently published their latest <a href="http://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf">user survey</a>. From there, it is clear that there is growth in the usage of MariaDB Server -- up 6 points from the <a href="https://www.openstack.org/assets/survey/Public-User-Survey-Report.pdf">previous survey</a>. MariaDB Galera Cluster dropped by 1 point, but it's worth noting that in MariaDB Server 10.1, you're getting it all as an integrated download so it is quite likely that people are just referring to it as "MariaDB" now.</p>

<p><img src="http://mariadb.com/sites/default/files/openstack-april-2016-1.png" alt="Which databases are used for OpenStack components" title="" /></p>

<p>(Screenshot taken from the <a href="http://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf">user survey</a>)</p>

<p>While MySQL still dominates at 35 percentage points, the largest pie of production OpenStack deployments are either based on MariaDB Server or MariaDB Galera Cluster which are developed by both the MariaDB Corporation &amp; the MariaDB Foundation.</p>

<p>Some may say this is because "defaults matter" (as MariaDB Server gets to be the default in many Linux distributions), but the majority of OpenStack deployments today are using Ubuntu Server, which ships MySQL as a default provider when one requests for MySQL. So this is a conscious choice people are making to go with MariaDB Server or MariaDB Galera Cluster.</p>

<p>Thank you OpenStack deployers! If you ever want to talk about your deployment, feel free to drop me a <a href="mailto:colin@mariadb.com">line</a> or even tweet <a href="https://twitter.com/bytebot">@bytebot</a>.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/cloud" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Cloud</a></div><div><a href="http://mariadb.com/blog-tags/mariadb-releases" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Releases</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-45-1402568369.jpg?itok=nBbL59Ou" width="72" height="72" alt="colin's picture" title="colin's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995300&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995300&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 27 Apr 2016 14:20:27 +0000";s:2:"dc";a:1:{s:7:"creator";s:7:"MariaDB";}s:7:"summary";s:1818:"Wed, 2016-04-27 14:20colinWhile many are at the ongoing OpenStack Summit in Austin, it came to my attention that the OpenStack project has recently published their latest user survey. From there, it is clear that there is growth in the usage of MariaDB Server -- up 6 points from the previous survey. MariaDB Galera Cluster dropped by 1 point, but it's worth noting that in MariaDB Server 10.1, you're getting it all as an integrated download so it is quite likely that people are just referring to it as "MariaDB" now.



(Screenshot taken from the user survey)

While MySQL still dominates at 35 percentage points, the largest pie of production OpenStack deployments are either based on MariaDB Server or MariaDB Galera Cluster which are developed by both the MariaDB Corporation &amp; the MariaDB Foundation.

Some may say this is because "defaults matter" (as MariaDB Server gets to be the default in many Linux distributions), but the majority of OpenStack deployments today are using Ubuntu Server, which ships MySQL as a default provider when one requests for MySQL. So this is a conscious choice people are making to go with MariaDB Server or MariaDB Galera Cluster.

Thank you OpenStack deployers! If you ever want to talk about your deployment, feel free to drop me a line or even tweet @bytebot.
Tags:&nbsp;CloudMariaDB Releases
About the Author
  
      


 
Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.



";s:12:"atom_content";s:3595:"<div><div><div>Wed, 2016-04-27 14:20</div></div></div><div><div><div>colin</div></div></div><div><div><div property="content:encoded"><p>While many are at the ongoing <a href="https://www.openstack.org/summit/austin-2016/">OpenStack Summit in Austin</a>, it came to my attention that the OpenStack project has recently published their latest <a href="http://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf">user survey</a>. From there, it is clear that there is growth in the usage of MariaDB Server -- up 6 points from the <a href="https://www.openstack.org/assets/survey/Public-User-Survey-Report.pdf">previous survey</a>. MariaDB Galera Cluster dropped by 1 point, but it's worth noting that in MariaDB Server 10.1, you're getting it all as an integrated download so it is quite likely that people are just referring to it as "MariaDB" now.</p>

<p><img src="http://mariadb.com/sites/default/files/openstack-april-2016-1.png" alt="Which databases are used for OpenStack components" title="" /></p>

<p>(Screenshot taken from the <a href="http://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf">user survey</a>)</p>

<p>While MySQL still dominates at 35 percentage points, the largest pie of production OpenStack deployments are either based on MariaDB Server or MariaDB Galera Cluster which are developed by both the MariaDB Corporation &amp; the MariaDB Foundation.</p>

<p>Some may say this is because "defaults matter" (as MariaDB Server gets to be the default in many Linux distributions), but the majority of OpenStack deployments today are using Ubuntu Server, which ships MySQL as a default provider when one requests for MySQL. So this is a conscious choice people are making to go with MariaDB Server or MariaDB Galera Cluster.</p>

<p>Thank you OpenStack deployers! If you ever want to talk about your deployment, feel free to drop me a <a href="mailto:colin@mariadb.com">line</a> or even tweet <a href="https://twitter.com/bytebot">@bytebot</a>.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/cloud" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Cloud</a></div><div><a href="http://mariadb.com/blog-tags/mariadb-releases" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Releases</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-45-1402568369.jpg?itok=nBbL59Ou" width="72" height="72" alt="colin's picture" title="colin's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995300&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995300&vote=-1&apivote=1">Vote DOWN</a>";}i:9;a:9:{s:5:"title";s:72:"Watch the replay: Become a MongoDB DBA (if you’re really a MySQL user)";s:4:"guid";s:31:"4679 at http://severalnines.com";s:4:"link";s:88:"http://severalnines.com/blog/watch-replay-become-mongodb-dba-if-you-re-really-mysql-user";s:11:"description";s:1865:"Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database, which included a live demo on how to carry out the relevant DBA tasks using ClusterControl.
The replay and the slides are now available online in case you missed Tuesday’s live session or simply would like to see it again in your own time.
Watch the replay Read the slides
This was the first session of our new webinar series: ‘How to Become a MongoDB DBA’ to answer the question: ‘what does a MongoDB DBA do’?
In this initial webinar, we went beyond the deployment phase and demonstrated how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations. Watch out for invitations for the next session in this series!
This Session's Agenda
Introduction to becoming a MongoDB DBA
Installing &amp; configuring MongoDB
What to monitor and how
How to perform backups
Live Demo
Speaker



Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.
This series is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘Become a ClusterControl DBA’ blog series.
Tags: MySQLMongoDBdatabase managementdbaclustercontrol";s:7:"content";a:1:{s:7:"encoded";s:3470:"<div><div><div property="content:encoded"><p>Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database, which included a live demo on how to carry out the relevant DBA tasks using ClusterControl.</p>
<p>The replay and the slides are now available online in case you missed Tuesday’s live session or simply would like to see it again in your own time.</p>
<p><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user">Watch the replay</a> <a href="http://www.slideshare.net/Severalnines/webinar-slides-become-a-mongodb-dba-if-youre-really-a-mysql-user" target="_blank">Read the slides</a></p>
<p>This was the first session of our new webinar series: ‘How to Become a MongoDB DBA’ to answer the question: ‘what does a MongoDB DBA do’?</p>
<p>In this initial webinar, we went beyond the deployment phase and demonstrated how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations. Watch out for invitations for the next session in this series!</p>
<h2>This Session's Agenda</h2>
<ul><li>Introduction to becoming a MongoDB DBA</li>
<li>Installing &amp; configuring MongoDB</li>
<li>What to monitor and how</li>
<li>How to perform backups</li>
<li>Live Demo</li>
</ul><h2>Speaker</h2>
<div>
<div><img src="http://severalnines.com/sites/default/files/mail/speakers/art_van_scheppingen.png" style="width: 155px; height: 155px;" /></div>
</div>
<p>Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.</p>
<p>This series is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘<a href="http://severalnines.com/blog/become-clustercontrol-dba-deploying-your-databases-and-clusters">Become a ClusterControl DBA</a>’ blog series.</p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/database-management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">database management</a></li><li><a href="http://severalnines.com/blog-tags/dba" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">dba</a></li><li><a href="http://severalnines.com/blog-tags/clustercontrol" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">clustercontrol</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995299&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995299&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 27 Apr 2016 12:49:49 +0000";s:2:"dc";a:1:{s:7:"creator";s:12:"Severalnines";}s:7:"summary";s:1865:"Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database, which included a live demo on how to carry out the relevant DBA tasks using ClusterControl.
The replay and the slides are now available online in case you missed Tuesday’s live session or simply would like to see it again in your own time.
Watch the replay Read the slides
This was the first session of our new webinar series: ‘How to Become a MongoDB DBA’ to answer the question: ‘what does a MongoDB DBA do’?
In this initial webinar, we went beyond the deployment phase and demonstrated how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations. Watch out for invitations for the next session in this series!
This Session's Agenda
Introduction to becoming a MongoDB DBA
Installing &amp; configuring MongoDB
What to monitor and how
How to perform backups
Live Demo
Speaker



Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.
This series is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘Become a ClusterControl DBA’ blog series.
Tags: MySQLMongoDBdatabase managementdbaclustercontrol";s:12:"atom_content";s:3470:"<div><div><div property="content:encoded"><p>Thanks to everyone who participated in this week’s webinar on ‘Become a MongoDB DBA’! Our colleague Art van Scheppingen presented from the perspective of a MySQL DBA who might be called to manage a MongoDB database, which included a live demo on how to carry out the relevant DBA tasks using ClusterControl.</p>
<p>The replay and the slides are now available online in case you missed Tuesday’s live session or simply would like to see it again in your own time.</p>
<p><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user">Watch the replay</a> <a href="http://www.slideshare.net/Severalnines/webinar-slides-become-a-mongodb-dba-if-youre-really-a-mysql-user" target="_blank">Read the slides</a></p>
<p>This was the first session of our new webinar series: ‘How to Become a MongoDB DBA’ to answer the question: ‘what does a MongoDB DBA do’?</p>
<p>In this initial webinar, we went beyond the deployment phase and demonstrated how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations. Watch out for invitations for the next session in this series!</p>
<h2>This Session's Agenda</h2>
<ul><li>Introduction to becoming a MongoDB DBA</li>
<li>Installing &amp; configuring MongoDB</li>
<li>What to monitor and how</li>
<li>How to perform backups</li>
<li>Live Demo</li>
</ul><h2>Speaker</h2>
<div>
<div><img src="http://severalnines.com/sites/default/files/mail/speakers/art_van_scheppingen.png" style="width: 155px; height: 155px;" /></div>
</div>
<p>Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.</p>
<p>This series is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘<a href="http://severalnines.com/blog/become-clustercontrol-dba-deploying-your-databases-and-clusters">Become a ClusterControl DBA</a>’ blog series.</p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/database-management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">database management</a></li><li><a href="http://severalnines.com/blog-tags/dba" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">dba</a></li><li><a href="http://severalnines.com/blog-tags/clustercontrol" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">clustercontrol</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995299&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995299&vote=-1&apivote=1">Vote DOWN</a>";}i:10;a:10:{s:5:"title";s:50:"Take the new MySQL Shell for a Spin … in Docker!";s:4:"guid";s:30:"http://mysqlrelease.com/?p=451";s:4:"link";s:78:"http://mysqlrelease.com/2016/04/take-the-new-mysql-shell-for-a-spin-in-docker/";s:11:"description";s:355:"The most recent release of MySQL Server — 5.7.12 — ships with the X Plugin, which opens up an entirely new area of functionality: In addition to the classical relational approach to data management using SQL, MySQL can now also be used as a schemaless document store, something which is commonly referred to as a NoSQL database. In order [&#8230;]";s:7:"content";a:1:{s:7:"encoded";s:568:"The most recent release of MySQL Server — 5.7.12 — ships with the X Plugin, which opens up an entirely new area of functionality: In addition to the classical relational approach to data management using SQL, MySQL can now also be used as a schemaless document store, something which is commonly referred to as a NoSQL database. In order [&#8230;]<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995296&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995296&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 27 Apr 2016 11:06:28 +0000";s:2:"dc";a:1:{s:7:"creator";s:14:"Yngve Svendsen";}s:8:"category";s:13:"Announcements";s:7:"summary";s:355:"The most recent release of MySQL Server — 5.7.12 — ships with the X Plugin, which opens up an entirely new area of functionality: In addition to the classical relational approach to data management using SQL, MySQL can now also be used as a schemaless document store, something which is commonly referred to as a NoSQL database. In order [&#8230;]";s:12:"atom_content";s:568:"The most recent release of MySQL Server — 5.7.12 — ships with the X Plugin, which opens up an entirely new area of functionality: In addition to the classical relational approach to data management using SQL, MySQL can now also be used as a schemaless document store, something which is commonly referred to as a NoSQL database. In order [&#8230;]<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995296&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995296&vote=-1&apivote=1">Vote DOWN</a>";}i:11;a:9:{s:5:"title";s:49:"Automated server management with opensource tools";s:4:"guid";s:26:"2539 at http://mariadb.com";s:4:"link";s:68:"http://mariadb.com/blog/automated-server-management-opensource-tools";s:11:"description";s:2615:"Wed, 2016-04-27 09:27colinIn a recent presentation by Balazs Pocze from Gawker Media LLC made clear that there are many automation strategies for server management that would be worth packaging up into an extended solution like MariaDB Enterprise.

Automation is good because there are way less "ops people" than "dev people" at many companies. Man-hours are expensive, so you don't want to waste them on routine tasks. Automated tasks also mean that there is a lot less possibility to make mistakes. "Everybody needs automation," says Pocze.

Gawker still uses physical infrastructure, powered by Linux, though there is some migration to the cloud. Cobbler is their Linux installation server of choice, where the hosts are defined, and there is no manual DHCP or DNS management; this tool not only manages but also defines their environment. 

For provisioning the hosts, Gawker's tool of choice is Puppet. Puppet manages all the hosts as well as defines the hosts, and Pocze said, "If it doesn't exist in Puppet, it doesn't exist at all". From a developer enablement standpoint, MariaDB Enterprise provides the MariaDB Enterprise Chef Cookbook, which we could say is as close as can be.

In a bit of a twist, they also make use of Ansible for running commands on hosts; all commands are organised into playbooks, and it is all about agentless installation and management. They do create modules and contribute them, and all nodes are managed over SSH. 

To wrap it all up, they use Jenkins for continuous integration. The demo of how Gawker's Kinja is maintained by Jenkins, with the Ansible playbooks for slave management was also a highlight.

It would be great if more shared what their backend infrastructure and ops ran like. Because its clear that there's plenty of opportunity to have tooling finesse. 

Don't forget to check out banyek on github as well as gawkermedia on github. During the talk, an audience member said it might also be a great idea to checkout Rundeck - a job scheduler and runbook automation, something I have not heard of before but will definitely look at in the near future.Tags:&nbsp;MariaDB Enterprise
About the Author
  
      


 
Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.



";s:7:"content";a:1:{s:7:"encoded";s:4317:"<div><div><div>Wed, 2016-04-27 09:27</div></div></div><div><div><div>colin</div></div></div><div><div><div property="content:encoded"><p>In a recent presentation by Balazs Pocze from Gawker Media LLC made clear that there are many automation strategies for server management that would be worth packaging up into an extended solution like <a href="https://mariadb.com/products/mariadb-enterprise" target="_blank">MariaDB Enterprise</a>.</p>

<p>Automation is good because there are way less "ops people" than "dev people" at many companies. Man-hours are expensive, so you don't want to waste them on routine tasks. Automated tasks also mean that there is a lot less possibility to make mistakes. "Everybody needs automation," says Pocze.</p>

<p>Gawker still uses physical infrastructure, powered by Linux, though there is some migration to the cloud. <a href="http://cobbler.github.io/" target="_blank">Cobbler</a> is their Linux installation server of choice, where the hosts are defined, and there is no manual DHCP or DNS management; this tool not only manages but also defines their environment. </p>

<p>For provisioning the hosts, Gawker's tool of choice is <a href="https://puppet.com/" target="_blank">Puppet</a>. Puppet manages all the hosts as well as defines the hosts, and Pocze said, "If it doesn't exist in Puppet, it doesn't exist at all". From a developer enablement standpoint, MariaDB Enterprise provides the <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-enterprise-chef-cookbook/" target="_blank">MariaDB Enterprise Chef Cookbook</a>, which we could say is as close as can be.</p>

<p>In a bit of a twist, they also make use of <a href="https://www.ansible.com/" target="_blank">Ansible</a> for running commands on hosts; all commands are organised into playbooks, and it is all about agentless installation and management. They do create modules and contribute them, and all nodes are managed over SSH. </p>

<p>To wrap it all up, they use <a href="https://jenkins.io/" target="_blank">Jenkins</a> for continuous integration. The demo of how Gawker's Kinja is maintained by Jenkins, with the Ansible playbooks for slave management was also a highlight.</p>

<p>It would be great if more shared what their backend infrastructure and ops ran like. Because its clear that there's plenty of opportunity to have tooling finesse. </p>

<p>Don't forget to check out <a href="https://github.com/banyek?tab=repositories" target="_blank">banyek on github</a> as well as <a href="https://github.com/gawkermedia/" target="_blank">gawkermedia on github</a>. During the talk, an audience member said it might also be a great idea to checkout <a href="http://rundeck.org/" target="_blank">Rundeck - a job scheduler and runbook automation</a>, something I have not heard of before but will definitely look at in the near future.</p></div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/mariadb-enterprise" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Enterprise</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-45-1402568369.jpg?itok=nBbL59Ou" width="72" height="72" alt="colin's picture" title="colin's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995292&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995292&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 27 Apr 2016 09:27:42 +0000";s:2:"dc";a:1:{s:7:"creator";s:7:"MariaDB";}s:7:"summary";s:2615:"Wed, 2016-04-27 09:27colinIn a recent presentation by Balazs Pocze from Gawker Media LLC made clear that there are many automation strategies for server management that would be worth packaging up into an extended solution like MariaDB Enterprise.

Automation is good because there are way less "ops people" than "dev people" at many companies. Man-hours are expensive, so you don't want to waste them on routine tasks. Automated tasks also mean that there is a lot less possibility to make mistakes. "Everybody needs automation," says Pocze.

Gawker still uses physical infrastructure, powered by Linux, though there is some migration to the cloud. Cobbler is their Linux installation server of choice, where the hosts are defined, and there is no manual DHCP or DNS management; this tool not only manages but also defines their environment. 

For provisioning the hosts, Gawker's tool of choice is Puppet. Puppet manages all the hosts as well as defines the hosts, and Pocze said, "If it doesn't exist in Puppet, it doesn't exist at all". From a developer enablement standpoint, MariaDB Enterprise provides the MariaDB Enterprise Chef Cookbook, which we could say is as close as can be.

In a bit of a twist, they also make use of Ansible for running commands on hosts; all commands are organised into playbooks, and it is all about agentless installation and management. They do create modules and contribute them, and all nodes are managed over SSH. 

To wrap it all up, they use Jenkins for continuous integration. The demo of how Gawker's Kinja is maintained by Jenkins, with the Ansible playbooks for slave management was also a highlight.

It would be great if more shared what their backend infrastructure and ops ran like. Because its clear that there's plenty of opportunity to have tooling finesse. 

Don't forget to check out banyek on github as well as gawkermedia on github. During the talk, an audience member said it might also be a great idea to checkout Rundeck - a job scheduler and runbook automation, something I have not heard of before but will definitely look at in the near future.Tags:&nbsp;MariaDB Enterprise
About the Author
  
      


 
Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.



";s:12:"atom_content";s:4317:"<div><div><div>Wed, 2016-04-27 09:27</div></div></div><div><div><div>colin</div></div></div><div><div><div property="content:encoded"><p>In a recent presentation by Balazs Pocze from Gawker Media LLC made clear that there are many automation strategies for server management that would be worth packaging up into an extended solution like <a href="https://mariadb.com/products/mariadb-enterprise" target="_blank">MariaDB Enterprise</a>.</p>

<p>Automation is good because there are way less "ops people" than "dev people" at many companies. Man-hours are expensive, so you don't want to waste them on routine tasks. Automated tasks also mean that there is a lot less possibility to make mistakes. "Everybody needs automation," says Pocze.</p>

<p>Gawker still uses physical infrastructure, powered by Linux, though there is some migration to the cloud. <a href="http://cobbler.github.io/" target="_blank">Cobbler</a> is their Linux installation server of choice, where the hosts are defined, and there is no manual DHCP or DNS management; this tool not only manages but also defines their environment. </p>

<p>For provisioning the hosts, Gawker's tool of choice is <a href="https://puppet.com/" target="_blank">Puppet</a>. Puppet manages all the hosts as well as defines the hosts, and Pocze said, "If it doesn't exist in Puppet, it doesn't exist at all". From a developer enablement standpoint, MariaDB Enterprise provides the <a href="https://mariadb.com/kb/en/mariadb-enterprise/mariadb-enterprise-chef-cookbook/" target="_blank">MariaDB Enterprise Chef Cookbook</a>, which we could say is as close as can be.</p>

<p>In a bit of a twist, they also make use of <a href="https://www.ansible.com/" target="_blank">Ansible</a> for running commands on hosts; all commands are organised into playbooks, and it is all about agentless installation and management. They do create modules and contribute them, and all nodes are managed over SSH. </p>

<p>To wrap it all up, they use <a href="https://jenkins.io/" target="_blank">Jenkins</a> for continuous integration. The demo of how Gawker's Kinja is maintained by Jenkins, with the Ansible playbooks for slave management was also a highlight.</p>

<p>It would be great if more shared what their backend infrastructure and ops ran like. Because its clear that there's plenty of opportunity to have tooling finesse. </p>

<p>Don't forget to check out <a href="https://github.com/banyek?tab=repositories" target="_blank">banyek on github</a> as well as <a href="https://github.com/gawkermedia/" target="_blank">gawkermedia on github</a>. During the talk, an audience member said it might also be a great idea to checkout <a href="http://rundeck.org/" target="_blank">Rundeck - a job scheduler and runbook automation</a>, something I have not heard of before but will definitely look at in the near future.</p></div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/mariadb-enterprise" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Enterprise</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-45-1402568369.jpg?itok=nBbL59Ou" width="72" height="72" alt="colin's picture" title="colin's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995292&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995292&vote=-1&apivote=1">Vote DOWN</a>";}i:12;a:10:{s:5:"title";s:39:"Video on MySQL master slave Replication";s:4:"guid";s:70:"tag:blogger.com,1999:blog-1764012747348792127.post-3525722045869585726";s:4:"link";s:83:"http://satejkumar.blogspot.com/2016/04/video-on-mysql-master-slave-replication.html";s:11:"description";s:82:"A video session on "Intro to MySQL master slave Replication". Hope you enjoy it :)";s:7:"content";a:1:{s:7:"encoded";s:373:"<div dir="ltr" trbidi="on"><div>A video session on "Intro to MySQL master slave Replication". Hope you enjoy it :)</div><div><br /></div><div></div><br /></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995290&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995290&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 27 Apr 2016 05:39:00 +0000";s:2:"dc";a:1:{s:7:"creator";s:10:"Satej Sahu";}s:8:"category";s:32:"intromastermysqlreplicationslave";s:7:"summary";s:82:"A video session on "Intro to MySQL master slave Replication". Hope you enjoy it :)";s:12:"atom_content";s:373:"<div dir="ltr" trbidi="on"><div>A video session on "Intro to MySQL master slave Replication". Hope you enjoy it :)</div><div><br /></div><div></div><br /></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995290&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995290&vote=-1&apivote=1">Vote DOWN</a>";}i:13;a:9:{s:5:"title";s:53:"dbForge Studio for MySQL v7.1 Comes With New Features";s:4:"guid";s:58:"https://www.devart.com/news/2016/dbforgestudiomysql71.html";s:4:"link";s:58:"https://www.devart.com/news/2016/dbforgestudiomysql71.html";s:11:"description";s:155:"
                        We are glad to announce the new release of dbForge Studio for MySQL, v7.1 that includes several new features.
                    ";s:7:"content";a:1:{s:7:"encoded";s:434:"<p>
                        We are glad to announce the new release of <a href="https://www.devart.com/dbforge/mysql/studio/">dbForge Studio for MySQL, v7.1</a> that includes several new features.
                    </p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995297&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995297&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Tue, 26 Apr 2016 21:00:00 +0000";s:2:"dc";a:1:{s:7:"creator";s:14:"Julia Samarska";}s:7:"summary";s:155:"
                        We are glad to announce the new release of dbForge Studio for MySQL, v7.1 that includes several new features.
                    ";s:12:"atom_content";s:434:"<p>
                        We are glad to announce the new release of <a href="https://www.devart.com/dbforge/mysql/studio/">dbForge Studio for MySQL, v7.1</a> that includes several new features.
                    </p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995297&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995297&vote=-1&apivote=1">Vote DOWN</a>";}i:14;a:10:{s:5:"title";s:67:"How We Made MySQL Great Again, or Upgrading MySQL with Orchestrator";s:4:"guid";s:37:"https://www.percona.com/blog/?p=35271";s:4:"link";s:63:"https://www.percona.com/blog/2016/04/26/make-mysql-great-again/";s:11:"description";s:5483:"In this blog post, we&#8217;ll discuss upgrading MySQL with Orchestrator.
I recently had a client, Life360, that wanted to upgrade from Percona Server 5.5 to Percona Server 5.6, and implement GTID in their high transaction environment. They had co-masters and multiple read slaves.
Orchestrator made this job much easier for us. My colleague, Tibi, recently posted about Orchestrator here and here.
Daniel from Life360 saw Orchestrator and was very interested. So here is how he setup Orchestrator in his own words:
I did some initial testing with the vagrant boxes provided in the Orchestrator repo, to see how to configure the agents and get the Orchestrator server to do what we want.
I then moved to install the Orchestrator server, Orchestrator backend on RDS, and deploy the clients on the slaves and masters in our Amazon VPC MySQL instances.
Once the server setup was done, the clients were auto-detected through CNAME discovery of the masters, and the agents talked to the server (it took a while as CNAMES wasn&#8217;t working as expected, but that&#8217;s fixed in the new server version).
We were pretty amazed at the number of actions you can do through orchestrator itself, such as: moving slaves to a different master through drag and drop, enabling GTID on a node with the push of a button, setting up GTID based failover, taking LVM snapshots using Orchestrator Agent, etc.
We went ahead and tested the master change on drag and drop, and after a few successful attempts, we even brought it back to where it was initially. After those tests, we were pretty confident that we could leverage Orchestrator as one of our main tools to assist in the coming upgrade.
Here is a screenshot of the initial setup:

Manjot: Once Daniel had Orchestrator setup, he wanted to leverage it to help with the MySQL upgrade. We set out to create a plan that worked within his constraints and still kept best practices in mind.
First, we installed Percona Server 5.6 fresh on our dedicated backup slave. That first 5.6 slave was created with MyDumper to achieve forward compatibility and not have any legacy tablespaces. Since MyDumper was already installed with the Percona Backup Service that Life360 has, this was fairly easy to accomplish.
The MyDumper slave rebuild works in the following way:
To take a mydumper backup:

Go to your desired backups directory
Install mydumper (sudo apt-get install mydumper)
mydumper -t 8 -L mydumper.log &#8211;compress

To restore:

Make sure MyDumper is installed: sudo apt-get install mydumper
Copy the MyDumper backups over to a backups dir
Export your BACKUP_DIR as env var
Run this to restore with MyLoader (from https://gist.github.com/Dnile/4658b338d4a101cbe2eeb5080ebddf8e):
#!/usr/bin/env sh
cd $BACKUP_DIR
export DESTHOST=127.0.0.1
export BACKUP_DIR=/vol_mysql/backups
mysqld --skip-grant-tables &amp;
for i in `ls -1 *-schema.dump.gz | cut -d'-' -f1`; do mysql -h $DESTHOST -e "CREATE DATABASE IF NOT EXISTS $i"; zcat $i-schema.dump.gz | mysql -h $DESTHOST $i; zcat $i-schema-post.dump.gz | mysql -h $DESTHOST $i; done
/usr/bin/myloader --host=$DESTHOST --directory=$BACKUP_DIR --enable-binlog --threads=10 --queries-per-transaction=20 -v 3
chown -R mysql:mysql /var/lib/mysql/


Once the first 5.6 slave was caught up, we used Xtrabackup to backup 5.6 and then restored to each slave, cycling them out of the read slave pool one at a time.
Once all the slaves were upgraded, we created a new 5.6 master and had it replicate off our primary 5.5 master.
Then we moved all of the slaves to replicate off the new 5.6 master.
Life360 had long cron jobs that ran on the second 5.5 master. We moved the cron applications to write to the primary 5.5 master, and locked all tables. We then stopped replication on the second co-master. Daniel stopped MySQL and decommissioned it.
We then moved all application writes to the new 5.6 master. While Orchestrator can use external scripts to move IPs, we used a manual process here to change application DSNs and HAProxy configuration.
On the 5.5 master that remained, we used Orchestrator to set it to read only.

Daniel says this didn’t do a whole lot to get rid of connections that were still open on this server.
On the new master, we used the stop slave and reset slave buttons in the Orchestrator panel so it would no longer slave from the old master.
Once some of the thousands of connections had moved to the new master, we stopped MySQL on the 5.5 master, which took care of the rest and the application “gracefully” reconnected to the new 5.6 master.
There was some write downtime, as some connections did not drop off until they were forced to because php-fpm refused to let go. There is also always a high volume of transactions in this environment.
At this point our topology looks like this (ignore the globe icons for now):

But as always Daniel wanted MOAR. It was time for GTID. While we could have done this during the upgrade, Life360 wanted to manage risk and not make too many production changes at one time.
We followed Percona&#8217;s guide, Online GTID Deployment, but used Orchestrator to shuffle the old and new masters and toggle read_only on and off. This made our job a lot easier and faster, and saved us from any downtime.
The globes in the topology screenshot above show that the slaves are now using GTID replication.
Orchestrator makes upgrades and changes much easier than before, just use caution and understand what it is doing in the background.
&nbsp;";s:7:"content";a:1:{s:7:"encoded";s:8496:"<img width="98" height="150" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-98x150.png" class="attachment-thumbnail size-thumbnail wp-post-image" alt="Upgrading MySQL with Orchestrator" style="float: left; margin-right: 5px;" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-98x150.png 98w, https://www.percona.com/blog/wp-content/uploads/2016/04/image01-196x300.png 196w" sizes="(max-width: 98px) 100vw, 98px" /><p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-e1461699651564.png" rel="attachment wp-att-35273"><img class="alignleft size-medium wp-image-35273" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-196x300.png" alt="Upgrading MySQL with Orchestrator" width="196" height="300" /></a>In this blog post, we&#8217;ll discuss upgrading MySQL with Orchestrator.</p>
<p>I recently had a client, <a rel="nofollow" href="http://www.life360.com">Life360</a>, that wanted to upgrade from Percona Server 5.5 to Percona Server 5.6, and implement GTID in their high transaction environment. They had co-masters and multiple read slaves.</p>
<p>Orchestrator made this job much easier for us. My colleague, Tibi, recently posted about Orchestrator <a href="https://www.percona.com/blog/2016/03/08/orchestrator-mysql-replication-topology-manager/">here</a> and <a href="https://www.percona.com/blog/2016/04/13/orchestrator-agent-how-to-recover-a-mysql-database/">here</a>.</p>
<p>Daniel from Life360 saw Orchestrator and was very interested. So here is how he setup Orchestrator in his own words:</p>
<p>I did some initial testing with the vagrant boxes provided in the <a rel="nofollow" href="https://github.com/outbrain/orchestrator">Orchestrator repo</a>, to see how to configure the <a rel="nofollow" href="https://github.com/outbrain/orchestrator-agent">agents</a> and get the Orchestrator server to do what we want.</p>
<p>I then moved to install the Orchestrator server, Orchestrator backend on RDS, and deploy the clients on the slaves and masters in our Amazon VPC MySQL instances.</p>
<p>Once the server setup was done, the clients were auto-detected through CNAME discovery of the masters, and the agents talked to the server (it took a while as CNAMES wasn&#8217;t working as expected, but that&#8217;s fixed in the new server version).</p>
<p>We were pretty amazed at the number of actions you can do through orchestrator itself, such as: moving slaves to a different master through drag and drop, enabling GTID on a node with the push of a button, setting up GTID based failover, taking LVM snapshots using Orchestrator Agent, etc.</p>
<p>We went ahead and tested the master change on drag and drop, and after a few successful attempts, we even brought it back to where it was initially. After those tests, we were pretty confident that we could leverage Orchestrator as one of our main tools to assist in the coming upgrade.</p>
<p>Here is a screenshot of the initial setup:</p>
<p><img class="aligncenter size-large wp-image-35274" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image02-1024x891.png" alt="image02" width="770" height="670" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/image02-300x261.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/image02-1024x891.png 1024w, https://www.percona.com/blog/wp-content/uploads/2016/04/image02.png 1360w" sizes="(max-width: 770px) 100vw, 770px" /></p>
<p>Manjot: Once Daniel had Orchestrator setup, he wanted to leverage it to help with the MySQL upgrade. We set out to create a plan that worked within his constraints and still kept best practices in mind.</p>
<p>First, we installed Percona Server 5.6 fresh on our dedicated backup slave. That first 5.6 slave was created with MyDumper to achieve forward compatibility and not have any legacy tablespaces. Since MyDumper was already installed with the Percona Backup Service that Life360 has, this was fairly easy to accomplish.</p>
<p>The MyDumper slave rebuild works in the following way:</p>
<p>To take a mydumper backup:</p>
<ol>
<li>Go to your desired backups directory</li>
<li>Install mydumper (sudo apt-get install mydumper)</li>
<li>mydumper -t 8 -L mydumper.log &#8211;compress</li>
</ol>
<p>To restore:</p>
<ol>
<li>Make sure MyDumper is installed: sudo apt-get install mydumper</li>
<li>Copy the MyDumper backups over to a backups dir</li>
<li>Export your BACKUP_DIR as env var</li>
<li>Run this to restore with MyLoader (from <a rel="nofollow" href="https://gist.github.com/Dnile/4658b338d4a101cbe2eeb5080ebddf8e">https://gist.github.com/Dnile/4658b338d4a101cbe2eeb5080ebddf8e</a>):<br />
<pre>#!/usr/bin/env sh
cd $BACKUP_DIR
export DESTHOST=127.0.0.1
export BACKUP_DIR=/vol_mysql/backups
mysqld --skip-grant-tables &amp;
for i in `ls -1 *-schema.dump.gz | cut -d'-' -f1`; do mysql -h $DESTHOST -e "CREATE DATABASE IF NOT EXISTS $i"; zcat $i-schema.dump.gz | mysql -h $DESTHOST $i; zcat $i-schema-post.dump.gz | mysql -h $DESTHOST $i; done
/usr/bin/myloader --host=$DESTHOST --directory=$BACKUP_DIR --enable-binlog --threads=10 --queries-per-transaction=20 -v 3
chown -R mysql:mysql /var/lib/mysql/</pre>
</li>
</ol>
<p>Once the first 5.6 slave was caught up, we used Xtrabackup to backup 5.6 and then restored to each slave, cycling them out of the read slave pool one at a time.</p>
<p>Once all the slaves were upgraded, we created a new 5.6 master and had it replicate off our primary 5.5 master.</p>
<p>Then we moved all of the slaves to replicate off the new 5.6 master.</p>
<p>Life360 had long cron jobs that ran on the second 5.5 master. We moved the cron applications to write to the primary 5.5 master, and locked all tables. We then stopped replication on the second co-master. Daniel stopped MySQL and decommissioned it.</p>
<p>We then moved all application writes to the new 5.6 master. While Orchestrator can use external scripts to move IPs, we used a manual process here to change application DSNs and HAProxy configuration.</p>
<p>On the 5.5 master that remained, we used Orchestrator to set it to read only.</p>
<p><img class="aligncenter size-full wp-image-35273" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image01.png" alt="image01" width="579" height="885" /></p>
<p>Daniel says this didn’t do a whole lot to get rid of connections that were still open on this server.</p>
<p>On the new master, we used the stop slave and reset slave buttons in the Orchestrator panel so it would no longer slave from the old master.</p>
<p>Once some of the thousands of connections had moved to the new master, we stopped MySQL on the 5.5 master, which took care of the rest and the application “gracefully” reconnected to the new 5.6 master.</p>
<p>There was some write downtime, as some connections did not drop off until they were forced to because php-fpm refused to let go. There is also always a high volume of transactions in this environment.</p>
<p>At this point our topology looks like this (ignore the globe icons for now):</p>
<p><img class="aligncenter size-full wp-image-35272" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image00.png" alt="image00" width="746" height="701" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/image00-150x141.png 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/image00-300x282.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/image00.png 746w" sizes="(max-width: 746px) 100vw, 746px" /></p>
<p>But as always Daniel wanted MOAR. It was time for GTID. While we could have done this during the upgrade, Life360 wanted to manage risk and not make too many production changes at one time.</p>
<p>We followed Percona&#8217;s guide, <a href="https://www.percona.com/doc/percona-server/5.6/flexibility/online_gtid_deployment.html">Online GTID Deployment</a>, but used Orchestrator to shuffle the old and new masters and toggle read_only on and off. This made our job a lot easier and faster, and saved us from any downtime.</p>
<p>The globes in the topology screenshot above show that the slaves are now using GTID replication.</p>
<p>Orchestrator makes upgrades and changes much easier than before, just use caution and understand what it is doing in the background.</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995282&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995282&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Tue, 26 Apr 2016 19:56:40 +0000";s:2:"dc";a:1:{s:7:"creator";s:22:"MySQL Performance Blog";}s:8:"category";s:63:"GTIDHigh-availabilityMySQLOrchestratorPercona ServerReplication";s:7:"summary";s:5483:"In this blog post, we&#8217;ll discuss upgrading MySQL with Orchestrator.
I recently had a client, Life360, that wanted to upgrade from Percona Server 5.5 to Percona Server 5.6, and implement GTID in their high transaction environment. They had co-masters and multiple read slaves.
Orchestrator made this job much easier for us. My colleague, Tibi, recently posted about Orchestrator here and here.
Daniel from Life360 saw Orchestrator and was very interested. So here is how he setup Orchestrator in his own words:
I did some initial testing with the vagrant boxes provided in the Orchestrator repo, to see how to configure the agents and get the Orchestrator server to do what we want.
I then moved to install the Orchestrator server, Orchestrator backend on RDS, and deploy the clients on the slaves and masters in our Amazon VPC MySQL instances.
Once the server setup was done, the clients were auto-detected through CNAME discovery of the masters, and the agents talked to the server (it took a while as CNAMES wasn&#8217;t working as expected, but that&#8217;s fixed in the new server version).
We were pretty amazed at the number of actions you can do through orchestrator itself, such as: moving slaves to a different master through drag and drop, enabling GTID on a node with the push of a button, setting up GTID based failover, taking LVM snapshots using Orchestrator Agent, etc.
We went ahead and tested the master change on drag and drop, and after a few successful attempts, we even brought it back to where it was initially. After those tests, we were pretty confident that we could leverage Orchestrator as one of our main tools to assist in the coming upgrade.
Here is a screenshot of the initial setup:

Manjot: Once Daniel had Orchestrator setup, he wanted to leverage it to help with the MySQL upgrade. We set out to create a plan that worked within his constraints and still kept best practices in mind.
First, we installed Percona Server 5.6 fresh on our dedicated backup slave. That first 5.6 slave was created with MyDumper to achieve forward compatibility and not have any legacy tablespaces. Since MyDumper was already installed with the Percona Backup Service that Life360 has, this was fairly easy to accomplish.
The MyDumper slave rebuild works in the following way:
To take a mydumper backup:

Go to your desired backups directory
Install mydumper (sudo apt-get install mydumper)
mydumper -t 8 -L mydumper.log &#8211;compress

To restore:

Make sure MyDumper is installed: sudo apt-get install mydumper
Copy the MyDumper backups over to a backups dir
Export your BACKUP_DIR as env var
Run this to restore with MyLoader (from https://gist.github.com/Dnile/4658b338d4a101cbe2eeb5080ebddf8e):
#!/usr/bin/env sh
cd $BACKUP_DIR
export DESTHOST=127.0.0.1
export BACKUP_DIR=/vol_mysql/backups
mysqld --skip-grant-tables &amp;
for i in `ls -1 *-schema.dump.gz | cut -d'-' -f1`; do mysql -h $DESTHOST -e "CREATE DATABASE IF NOT EXISTS $i"; zcat $i-schema.dump.gz | mysql -h $DESTHOST $i; zcat $i-schema-post.dump.gz | mysql -h $DESTHOST $i; done
/usr/bin/myloader --host=$DESTHOST --directory=$BACKUP_DIR --enable-binlog --threads=10 --queries-per-transaction=20 -v 3
chown -R mysql:mysql /var/lib/mysql/


Once the first 5.6 slave was caught up, we used Xtrabackup to backup 5.6 and then restored to each slave, cycling them out of the read slave pool one at a time.
Once all the slaves were upgraded, we created a new 5.6 master and had it replicate off our primary 5.5 master.
Then we moved all of the slaves to replicate off the new 5.6 master.
Life360 had long cron jobs that ran on the second 5.5 master. We moved the cron applications to write to the primary 5.5 master, and locked all tables. We then stopped replication on the second co-master. Daniel stopped MySQL and decommissioned it.
We then moved all application writes to the new 5.6 master. While Orchestrator can use external scripts to move IPs, we used a manual process here to change application DSNs and HAProxy configuration.
On the 5.5 master that remained, we used Orchestrator to set it to read only.

Daniel says this didn’t do a whole lot to get rid of connections that were still open on this server.
On the new master, we used the stop slave and reset slave buttons in the Orchestrator panel so it would no longer slave from the old master.
Once some of the thousands of connections had moved to the new master, we stopped MySQL on the 5.5 master, which took care of the rest and the application “gracefully” reconnected to the new 5.6 master.
There was some write downtime, as some connections did not drop off until they were forced to because php-fpm refused to let go. There is also always a high volume of transactions in this environment.
At this point our topology looks like this (ignore the globe icons for now):

But as always Daniel wanted MOAR. It was time for GTID. While we could have done this during the upgrade, Life360 wanted to manage risk and not make too many production changes at one time.
We followed Percona&#8217;s guide, Online GTID Deployment, but used Orchestrator to shuffle the old and new masters and toggle read_only on and off. This made our job a lot easier and faster, and saved us from any downtime.
The globes in the topology screenshot above show that the slaves are now using GTID replication.
Orchestrator makes upgrades and changes much easier than before, just use caution and understand what it is doing in the background.
&nbsp;";s:12:"atom_content";s:8496:"<img width="98" height="150" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-98x150.png" class="attachment-thumbnail size-thumbnail wp-post-image" alt="Upgrading MySQL with Orchestrator" style="float: left; margin-right: 5px;" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-98x150.png 98w, https://www.percona.com/blog/wp-content/uploads/2016/04/image01-196x300.png 196w" sizes="(max-width: 98px) 100vw, 98px" /><p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-e1461699651564.png" rel="attachment wp-att-35273"><img class="alignleft size-medium wp-image-35273" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image01-196x300.png" alt="Upgrading MySQL with Orchestrator" width="196" height="300" /></a>In this blog post, we&#8217;ll discuss upgrading MySQL with Orchestrator.</p>
<p>I recently had a client, <a rel="nofollow" href="http://www.life360.com">Life360</a>, that wanted to upgrade from Percona Server 5.5 to Percona Server 5.6, and implement GTID in their high transaction environment. They had co-masters and multiple read slaves.</p>
<p>Orchestrator made this job much easier for us. My colleague, Tibi, recently posted about Orchestrator <a href="https://www.percona.com/blog/2016/03/08/orchestrator-mysql-replication-topology-manager/">here</a> and <a href="https://www.percona.com/blog/2016/04/13/orchestrator-agent-how-to-recover-a-mysql-database/">here</a>.</p>
<p>Daniel from Life360 saw Orchestrator and was very interested. So here is how he setup Orchestrator in his own words:</p>
<p>I did some initial testing with the vagrant boxes provided in the <a rel="nofollow" href="https://github.com/outbrain/orchestrator">Orchestrator repo</a>, to see how to configure the <a rel="nofollow" href="https://github.com/outbrain/orchestrator-agent">agents</a> and get the Orchestrator server to do what we want.</p>
<p>I then moved to install the Orchestrator server, Orchestrator backend on RDS, and deploy the clients on the slaves and masters in our Amazon VPC MySQL instances.</p>
<p>Once the server setup was done, the clients were auto-detected through CNAME discovery of the masters, and the agents talked to the server (it took a while as CNAMES wasn&#8217;t working as expected, but that&#8217;s fixed in the new server version).</p>
<p>We were pretty amazed at the number of actions you can do through orchestrator itself, such as: moving slaves to a different master through drag and drop, enabling GTID on a node with the push of a button, setting up GTID based failover, taking LVM snapshots using Orchestrator Agent, etc.</p>
<p>We went ahead and tested the master change on drag and drop, and after a few successful attempts, we even brought it back to where it was initially. After those tests, we were pretty confident that we could leverage Orchestrator as one of our main tools to assist in the coming upgrade.</p>
<p>Here is a screenshot of the initial setup:</p>
<p><img class="aligncenter size-large wp-image-35274" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image02-1024x891.png" alt="image02" width="770" height="670" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/image02-300x261.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/image02-1024x891.png 1024w, https://www.percona.com/blog/wp-content/uploads/2016/04/image02.png 1360w" sizes="(max-width: 770px) 100vw, 770px" /></p>
<p>Manjot: Once Daniel had Orchestrator setup, he wanted to leverage it to help with the MySQL upgrade. We set out to create a plan that worked within his constraints and still kept best practices in mind.</p>
<p>First, we installed Percona Server 5.6 fresh on our dedicated backup slave. That first 5.6 slave was created with MyDumper to achieve forward compatibility and not have any legacy tablespaces. Since MyDumper was already installed with the Percona Backup Service that Life360 has, this was fairly easy to accomplish.</p>
<p>The MyDumper slave rebuild works in the following way:</p>
<p>To take a mydumper backup:</p>
<ol>
<li>Go to your desired backups directory</li>
<li>Install mydumper (sudo apt-get install mydumper)</li>
<li>mydumper -t 8 -L mydumper.log &#8211;compress</li>
</ol>
<p>To restore:</p>
<ol>
<li>Make sure MyDumper is installed: sudo apt-get install mydumper</li>
<li>Copy the MyDumper backups over to a backups dir</li>
<li>Export your BACKUP_DIR as env var</li>
<li>Run this to restore with MyLoader (from <a rel="nofollow" href="https://gist.github.com/Dnile/4658b338d4a101cbe2eeb5080ebddf8e">https://gist.github.com/Dnile/4658b338d4a101cbe2eeb5080ebddf8e</a>):<br />
<pre>#!/usr/bin/env sh
cd $BACKUP_DIR
export DESTHOST=127.0.0.1
export BACKUP_DIR=/vol_mysql/backups
mysqld --skip-grant-tables &amp;
for i in `ls -1 *-schema.dump.gz | cut -d'-' -f1`; do mysql -h $DESTHOST -e "CREATE DATABASE IF NOT EXISTS $i"; zcat $i-schema.dump.gz | mysql -h $DESTHOST $i; zcat $i-schema-post.dump.gz | mysql -h $DESTHOST $i; done
/usr/bin/myloader --host=$DESTHOST --directory=$BACKUP_DIR --enable-binlog --threads=10 --queries-per-transaction=20 -v 3
chown -R mysql:mysql /var/lib/mysql/</pre>
</li>
</ol>
<p>Once the first 5.6 slave was caught up, we used Xtrabackup to backup 5.6 and then restored to each slave, cycling them out of the read slave pool one at a time.</p>
<p>Once all the slaves were upgraded, we created a new 5.6 master and had it replicate off our primary 5.5 master.</p>
<p>Then we moved all of the slaves to replicate off the new 5.6 master.</p>
<p>Life360 had long cron jobs that ran on the second 5.5 master. We moved the cron applications to write to the primary 5.5 master, and locked all tables. We then stopped replication on the second co-master. Daniel stopped MySQL and decommissioned it.</p>
<p>We then moved all application writes to the new 5.6 master. While Orchestrator can use external scripts to move IPs, we used a manual process here to change application DSNs and HAProxy configuration.</p>
<p>On the 5.5 master that remained, we used Orchestrator to set it to read only.</p>
<p><img class="aligncenter size-full wp-image-35273" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image01.png" alt="image01" width="579" height="885" /></p>
<p>Daniel says this didn’t do a whole lot to get rid of connections that were still open on this server.</p>
<p>On the new master, we used the stop slave and reset slave buttons in the Orchestrator panel so it would no longer slave from the old master.</p>
<p>Once some of the thousands of connections had moved to the new master, we stopped MySQL on the 5.5 master, which took care of the rest and the application “gracefully” reconnected to the new 5.6 master.</p>
<p>There was some write downtime, as some connections did not drop off until they were forced to because php-fpm refused to let go. There is also always a high volume of transactions in this environment.</p>
<p>At this point our topology looks like this (ignore the globe icons for now):</p>
<p><img class="aligncenter size-full wp-image-35272" src="https://www.percona.com/blog/wp-content/uploads/2016/04/image00.png" alt="image00" width="746" height="701" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/image00-150x141.png 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/image00-300x282.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/image00.png 746w" sizes="(max-width: 746px) 100vw, 746px" /></p>
<p>But as always Daniel wanted MOAR. It was time for GTID. While we could have done this during the upgrade, Life360 wanted to manage risk and not make too many production changes at one time.</p>
<p>We followed Percona&#8217;s guide, <a href="https://www.percona.com/doc/percona-server/5.6/flexibility/online_gtid_deployment.html">Online GTID Deployment</a>, but used Orchestrator to shuffle the old and new masters and toggle read_only on and off. This made our job a lot easier and faster, and saved us from any downtime.</p>
<p>The globes in the topology screenshot above show that the slaves are now using GTID replication.</p>
<p>Orchestrator makes upgrades and changes much easier than before, just use caution and understand what it is doing in the background.</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995282&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995282&vote=-1&apivote=1">Vote DOWN</a>";}i:15;a:9:{s:5:"title";s:58:"Top Nine Stand Out Facts About Percona Live in Santa Clara";s:4:"guid";s:31:"4678 at http://severalnines.com";s:4:"link";s:84:"http://severalnines.com/blog/top-nine-stand-out-facts-about-percona-live-santa-clara";s:11:"description";s:4826:"We like our 9s at Severalnines, so here is our list of Top 9 Facts (in no particular order) that made the trip to Santa Clara worthwhile once again for this year’s Percona Live Conference. Thanks to the team at Percona and everyone else involved in keeping the Santa Clara tradition going by organising another great conference this year.
1. The conference opened its arms not only to the “traditional” MySQL community, but also put a focus on MongoDB users.
This year in particular, there was a real intent to provide a place to learn and mingle for those database users who deal with not only one datastore, but manage a mixed database environment. More and more users are called to operate in polyglot database landscapes and thus it was good to see this year’s conference provide a platform for that. Percona themselves of course now serve both the MySQL and MongoDB user community with their tools and offerings, and companies like ourselves strive to help organisations with polyglot IT environments with our solutions and resources as well. So it was great to see a good mix of tutorial and talk topics as well as of sponsors at the conference.
2. Galera Cluster continues to be a popular topic in the MySQL universe
With plenty of talks and tutorials discussing Galera Cluster and how to make best use of it, the technology developed by the Codership team confirmed its growing popularity with MySQL users. We ourselves see a lot of interest in it and we blog / talk about Galera Cluster on a regular basis, so it was good to hear about the latest and greatest on it and talk to users directly about their clustering needs.
3. The Oracle MySQL Team is very present in the community
As per last year, it was good to see the Oracle MySQL Team so present at the conference, perhaps even more so than previously. Some great work has been done with MySQL 5.7 and it was good to hear about the MySQL Document Store. The MySQL community has been evolving over the years with its particular history, and it seems that there is a good balance now. We’re looking forward of course to see what the team have in store next for MySQL!
4. Facebook’s RocksDB is gaining traction
Facebook developed RocksDB by forking and improving LevelDB. The RocksDB storage engine is available for both MongoDB and MySQL. Many of the talks at the conference were about the ins- and outs of MongoRocks and MyRocks. We foresee a great future and adoption for this innovation by Facebook.
5. MySQL In Containers
A number of talks addressed the issues users can be faced with when deploying MySQL in containerized environments. Containers allow for rapid creation and deletion, and a lot of care has to be taken when deploying MySQL; especially in production. Going forward it will most likely be common to see MySQL servers running in container environments.
6. Bill Nye’s keynote
After Steve Wozniak last year, this year's invited keynote speaker was Bill Nye. Bill is better known as The Science Guy and with his vision he gives a twist on tech and science. His keynote gave a lot of food for thought, but just in case you have missed it you can watch the recording here: https://www.youtube.com/watch?v=iQDrK3rEtWg
7. Did we mention that MongoDB was a big topic this year?
For everyone who’s been following Percona’s news, you’ll know that Percona is investing in MongoDB following their acquisition of Tokutek last year with the result of the new Percona Server for MongoDB. In addition to that, quite a few MongoDB related talks had been submitted (and accepted) for this year’s conference, which was good to see. One of them being our talk on how to manage MongoDB as a MySQL DBA. So while the prevalence of MongoDB as a topic might still seem unusual to those used to the MySQL user conference, it’s an interesting development that we’re keeping an eye on.
8. On a self-interested note: we hosted a ‘How to become a MySQL DBA’ tutorial this year!
Our colleagues Art van Scheppingen and Ashraf Sharif hosted a tutorial on ‘How to become a MySQL DBA’. It was great to spend the day with database enthusiasts who were keen to learn how to manage MySQL and share experiences with them. The tutorial was based on same-titled blog series, which has proven quite popular with our blog followers. Feel free to check it out.
9. Meet the friends!
We kind of have to part on that note: of course the conference is always a week to look forward to in the year, as it’s really the place where we get to see a lot of friends from past and present in the MySQL (and now MongoDB) community. It really is a grand tradition and it’s good to see that so many members of the community are to keen to continue to keep that going.
So here’s to seeing you all next year again!



Tags: MySQLMongoDBperconapercona livecommunity";s:7:"content";a:1:{s:7:"encoded";s:6379:"<div><div><div property="content:encoded"><p>We like our 9s at Severalnines, so here is our list of Top 9 Facts (in no particular order) that made the trip to Santa Clara worthwhile once again for this year’s Percona Live Conference. Thanks to the team at Percona and everyone else involved in keeping the Santa Clara tradition going by organising another great conference this year.</p>
<h3>1. The conference opened its arms not only to the “traditional” MySQL community, but also put a focus on MongoDB users.</h3>
<p>This year in particular, there was a real intent to provide a place to learn and mingle for those database users who deal with not only one datastore, but manage a mixed database environment. More and more users are called to operate in polyglot database landscapes and thus it was good to see this year’s conference provide a platform for that. Percona themselves of course now serve both the MySQL and MongoDB user community with their tools and offerings, and companies like ourselves strive to help organisations with polyglot IT environments with our <a href="http://severalnines.com/product/clustercontrol">solutions</a> and <a href="http://severalnines.com/resources">resources</a> as well. So it was great to see a good mix of tutorial and talk topics as well as of sponsors at the conference.</p>
<h3>2. Galera Cluster continues to be a popular topic in the MySQL universe</h3>
<p>With plenty of talks and tutorials discussing Galera Cluster and how to make best use of it, the technology developed by the Codership team confirmed its growing popularity with MySQL users. We ourselves see a lot of interest in it and we blog / talk about Galera Cluster on a regular basis, so it was good to hear about the latest and greatest on it and talk to users directly about their clustering needs.</p>
<h3>3. The Oracle MySQL Team is very present in the community</h3>
<p>As per last year, it was good to see the Oracle MySQL Team so present at the conference, perhaps even more so than previously. Some great work has been done with MySQL 5.7 and it was good to hear about the MySQL Document Store. The MySQL community has been evolving over the years with its particular history, and it seems that there is a good balance now. We’re looking forward of course to see what the team have in store next for MySQL!</p>
<h3>4. Facebook’s RocksDB is gaining traction</h3>
<p>Facebook developed RocksDB by forking and improving LevelDB. The RocksDB storage engine is available for both MongoDB and MySQL. Many of the talks at the conference were about the ins- and outs of MongoRocks and MyRocks. We foresee a great future and adoption for this innovation by Facebook.</p>
<h3>5. MySQL In Containers</h3>
<p>A number of talks addressed the issues users can be faced with when deploying MySQL in containerized environments. Containers allow for rapid creation and deletion, and a lot of care has to be taken when deploying MySQL; especially in production. Going forward it will most likely be common to see MySQL servers running in container environments.</p>
<h3>6. Bill Nye’s keynote</h3>
<p>After Steve Wozniak last year, this year's invited keynote speaker was Bill Nye. Bill is better known as The Science Guy and with his vision he gives a twist on tech and science. His keynote gave a lot of food for thought, but just in case you have missed it you can watch the recording here: <a href="https://www.youtube.com/watch?v=iQDrK3rEtWg" rel="nofollow" target="_blank">https://www.youtube.com/watch?v=iQDrK3rEtWg</a></p>
<h3>7. Did we mention that MongoDB was a big topic this year?</h3>
<p>For everyone who’s been following Percona’s news, you’ll know that Percona is investing in MongoDB following their acquisition of Tokutek last year with the result of the new Percona Server for MongoDB. In addition to that, quite a few MongoDB related talks had been submitted (and accepted) for this year’s conference, which was good to see. One of them being our talk on how to manage MongoDB as a MySQL DBA. So while the prevalence of MongoDB as a topic might still seem unusual to those used to the MySQL user conference, it’s an interesting development that we’re keeping an eye on.</p>
<h3>8. On a self-interested note: we hosted a ‘How to become a MySQL DBA’ tutorial this year!</h3>
<p>Our colleagues Art van Scheppingen and Ashraf Sharif hosted a tutorial on ‘How to become a MySQL DBA’. It was great to spend the day with database enthusiasts who were keen to learn how to manage MySQL and share experiences with them. The tutorial was based on same-titled <a href="http://severalnines.com/blog/become-mysql-dba-blog-series-troubleshooting-pt-stalk-part-2">blog series</a>, which has proven quite popular with our blog followers. Feel free to check it out.</p>
<h3>9. Meet the friends!</h3>
<p>We kind of have to part on that note: of course the conference is always a week to look forward to in the year, as it’s really the place where we get to see a lot of friends from past and present in the MySQL (and now MongoDB) community. It really is a grand tradition and it’s good to see that so many members of the community are to keen to continue to keep that going.</p>
<p>So here’s to seeing you all next year again!</p>
<div>
<div><img src="http://severalnines.com/sites/default/files/blog/node_4678/image00.jpg" /></div>
</div>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/percona" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">percona</a></li><li><a href="http://severalnines.com/blog-tags/percona-live" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">percona live</a></li><li><a href="http://severalnines.com/blog-tags/community" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">community</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995283&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995283&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Tue, 26 Apr 2016 19:34:13 +0000";s:2:"dc";a:1:{s:7:"creator";s:12:"Severalnines";}s:7:"summary";s:4826:"We like our 9s at Severalnines, so here is our list of Top 9 Facts (in no particular order) that made the trip to Santa Clara worthwhile once again for this year’s Percona Live Conference. Thanks to the team at Percona and everyone else involved in keeping the Santa Clara tradition going by organising another great conference this year.
1. The conference opened its arms not only to the “traditional” MySQL community, but also put a focus on MongoDB users.
This year in particular, there was a real intent to provide a place to learn and mingle for those database users who deal with not only one datastore, but manage a mixed database environment. More and more users are called to operate in polyglot database landscapes and thus it was good to see this year’s conference provide a platform for that. Percona themselves of course now serve both the MySQL and MongoDB user community with their tools and offerings, and companies like ourselves strive to help organisations with polyglot IT environments with our solutions and resources as well. So it was great to see a good mix of tutorial and talk topics as well as of sponsors at the conference.
2. Galera Cluster continues to be a popular topic in the MySQL universe
With plenty of talks and tutorials discussing Galera Cluster and how to make best use of it, the technology developed by the Codership team confirmed its growing popularity with MySQL users. We ourselves see a lot of interest in it and we blog / talk about Galera Cluster on a regular basis, so it was good to hear about the latest and greatest on it and talk to users directly about their clustering needs.
3. The Oracle MySQL Team is very present in the community
As per last year, it was good to see the Oracle MySQL Team so present at the conference, perhaps even more so than previously. Some great work has been done with MySQL 5.7 and it was good to hear about the MySQL Document Store. The MySQL community has been evolving over the years with its particular history, and it seems that there is a good balance now. We’re looking forward of course to see what the team have in store next for MySQL!
4. Facebook’s RocksDB is gaining traction
Facebook developed RocksDB by forking and improving LevelDB. The RocksDB storage engine is available for both MongoDB and MySQL. Many of the talks at the conference were about the ins- and outs of MongoRocks and MyRocks. We foresee a great future and adoption for this innovation by Facebook.
5. MySQL In Containers
A number of talks addressed the issues users can be faced with when deploying MySQL in containerized environments. Containers allow for rapid creation and deletion, and a lot of care has to be taken when deploying MySQL; especially in production. Going forward it will most likely be common to see MySQL servers running in container environments.
6. Bill Nye’s keynote
After Steve Wozniak last year, this year's invited keynote speaker was Bill Nye. Bill is better known as The Science Guy and with his vision he gives a twist on tech and science. His keynote gave a lot of food for thought, but just in case you have missed it you can watch the recording here: https://www.youtube.com/watch?v=iQDrK3rEtWg
7. Did we mention that MongoDB was a big topic this year?
For everyone who’s been following Percona’s news, you’ll know that Percona is investing in MongoDB following their acquisition of Tokutek last year with the result of the new Percona Server for MongoDB. In addition to that, quite a few MongoDB related talks had been submitted (and accepted) for this year’s conference, which was good to see. One of them being our talk on how to manage MongoDB as a MySQL DBA. So while the prevalence of MongoDB as a topic might still seem unusual to those used to the MySQL user conference, it’s an interesting development that we’re keeping an eye on.
8. On a self-interested note: we hosted a ‘How to become a MySQL DBA’ tutorial this year!
Our colleagues Art van Scheppingen and Ashraf Sharif hosted a tutorial on ‘How to become a MySQL DBA’. It was great to spend the day with database enthusiasts who were keen to learn how to manage MySQL and share experiences with them. The tutorial was based on same-titled blog series, which has proven quite popular with our blog followers. Feel free to check it out.
9. Meet the friends!
We kind of have to part on that note: of course the conference is always a week to look forward to in the year, as it’s really the place where we get to see a lot of friends from past and present in the MySQL (and now MongoDB) community. It really is a grand tradition and it’s good to see that so many members of the community are to keen to continue to keep that going.
So here’s to seeing you all next year again!



Tags: MySQLMongoDBperconapercona livecommunity";s:12:"atom_content";s:6379:"<div><div><div property="content:encoded"><p>We like our 9s at Severalnines, so here is our list of Top 9 Facts (in no particular order) that made the trip to Santa Clara worthwhile once again for this year’s Percona Live Conference. Thanks to the team at Percona and everyone else involved in keeping the Santa Clara tradition going by organising another great conference this year.</p>
<h3>1. The conference opened its arms not only to the “traditional” MySQL community, but also put a focus on MongoDB users.</h3>
<p>This year in particular, there was a real intent to provide a place to learn and mingle for those database users who deal with not only one datastore, but manage a mixed database environment. More and more users are called to operate in polyglot database landscapes and thus it was good to see this year’s conference provide a platform for that. Percona themselves of course now serve both the MySQL and MongoDB user community with their tools and offerings, and companies like ourselves strive to help organisations with polyglot IT environments with our <a href="http://severalnines.com/product/clustercontrol">solutions</a> and <a href="http://severalnines.com/resources">resources</a> as well. So it was great to see a good mix of tutorial and talk topics as well as of sponsors at the conference.</p>
<h3>2. Galera Cluster continues to be a popular topic in the MySQL universe</h3>
<p>With plenty of talks and tutorials discussing Galera Cluster and how to make best use of it, the technology developed by the Codership team confirmed its growing popularity with MySQL users. We ourselves see a lot of interest in it and we blog / talk about Galera Cluster on a regular basis, so it was good to hear about the latest and greatest on it and talk to users directly about their clustering needs.</p>
<h3>3. The Oracle MySQL Team is very present in the community</h3>
<p>As per last year, it was good to see the Oracle MySQL Team so present at the conference, perhaps even more so than previously. Some great work has been done with MySQL 5.7 and it was good to hear about the MySQL Document Store. The MySQL community has been evolving over the years with its particular history, and it seems that there is a good balance now. We’re looking forward of course to see what the team have in store next for MySQL!</p>
<h3>4. Facebook’s RocksDB is gaining traction</h3>
<p>Facebook developed RocksDB by forking and improving LevelDB. The RocksDB storage engine is available for both MongoDB and MySQL. Many of the talks at the conference were about the ins- and outs of MongoRocks and MyRocks. We foresee a great future and adoption for this innovation by Facebook.</p>
<h3>5. MySQL In Containers</h3>
<p>A number of talks addressed the issues users can be faced with when deploying MySQL in containerized environments. Containers allow for rapid creation and deletion, and a lot of care has to be taken when deploying MySQL; especially in production. Going forward it will most likely be common to see MySQL servers running in container environments.</p>
<h3>6. Bill Nye’s keynote</h3>
<p>After Steve Wozniak last year, this year's invited keynote speaker was Bill Nye. Bill is better known as The Science Guy and with his vision he gives a twist on tech and science. His keynote gave a lot of food for thought, but just in case you have missed it you can watch the recording here: <a href="https://www.youtube.com/watch?v=iQDrK3rEtWg" rel="nofollow" target="_blank">https://www.youtube.com/watch?v=iQDrK3rEtWg</a></p>
<h3>7. Did we mention that MongoDB was a big topic this year?</h3>
<p>For everyone who’s been following Percona’s news, you’ll know that Percona is investing in MongoDB following their acquisition of Tokutek last year with the result of the new Percona Server for MongoDB. In addition to that, quite a few MongoDB related talks had been submitted (and accepted) for this year’s conference, which was good to see. One of them being our talk on how to manage MongoDB as a MySQL DBA. So while the prevalence of MongoDB as a topic might still seem unusual to those used to the MySQL user conference, it’s an interesting development that we’re keeping an eye on.</p>
<h3>8. On a self-interested note: we hosted a ‘How to become a MySQL DBA’ tutorial this year!</h3>
<p>Our colleagues Art van Scheppingen and Ashraf Sharif hosted a tutorial on ‘How to become a MySQL DBA’. It was great to spend the day with database enthusiasts who were keen to learn how to manage MySQL and share experiences with them. The tutorial was based on same-titled <a href="http://severalnines.com/blog/become-mysql-dba-blog-series-troubleshooting-pt-stalk-part-2">blog series</a>, which has proven quite popular with our blog followers. Feel free to check it out.</p>
<h3>9. Meet the friends!</h3>
<p>We kind of have to part on that note: of course the conference is always a week to look forward to in the year, as it’s really the place where we get to see a lot of friends from past and present in the MySQL (and now MongoDB) community. It really is a grand tradition and it’s good to see that so many members of the community are to keen to continue to keep that going.</p>
<p>So here’s to seeing you all next year again!</p>
<div>
<div><img src="http://severalnines.com/sites/default/files/blog/node_4678/image00.jpg" /></div>
</div>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/percona" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">percona</a></li><li><a href="http://severalnines.com/blog-tags/percona-live" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">percona live</a></li><li><a href="http://severalnines.com/blog-tags/community" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">community</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995283&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995283&vote=-1&apivote=1">Vote DOWN</a>";}i:16;a:10:{s:5:"title";s:66:"MariaDB Galera Cluster 5.5.49 and updated connectors now available";s:4:"guid";s:27:"https://mariadb.org/?p=3483";s:4:"link";s:83:"https://mariadb.org/mariadb-galera-cluster-5-5-49-updated-connectors-now-available/";s:11:"description";s:513:"The MariaDB project is pleased to announce the immediate availability of MariaDB Galera Cluster 5.5.49, MariaDB Connector/J 1.4.3, and MariaDB Connector/C 2.2.3. See the release notes and changelogs for details on these releases. Download MariaDB Galera Cluster 5.5.49 Release Notes Changelog What is MariaDB Galera Cluster? MariaDB APT and YUM Repository Configuration Generator Download MariaDB [&#8230;]
The post MariaDB Galera Cluster 5.5.49 and updated connectors now available appeared first on MariaDB.org.";s:7:"content";a:1:{s:7:"encoded";s:902:"<p>The MariaDB project is pleased to announce the immediate availability of MariaDB Galera Cluster 5.5.49, MariaDB Connector/J 1.4.3, and MariaDB Connector/C 2.2.3. See the release notes and changelogs for details on these releases. Download MariaDB Galera Cluster 5.5.49 Release Notes Changelog What is MariaDB Galera Cluster? MariaDB APT and YUM Repository Configuration Generator Download MariaDB [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://mariadb.org/mariadb-galera-cluster-5-5-49-updated-connectors-now-available/">MariaDB Galera Cluster 5.5.49 and updated connectors now available</a> appeared first on <a rel="nofollow" href="https://mariadb.org">MariaDB.org</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995281&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995281&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Tue, 26 Apr 2016 17:53:51 +0000";s:2:"dc";a:1:{s:7:"creator";s:18:"Daniel Bartholomew";}s:8:"category";s:107:"AnnouncementsConnector/CConnector/JMariaDB 5.5MariaDB ConnectorsMariaDB Galera ClusterMariaDB Releasesmysql";s:7:"summary";s:513:"The MariaDB project is pleased to announce the immediate availability of MariaDB Galera Cluster 5.5.49, MariaDB Connector/J 1.4.3, and MariaDB Connector/C 2.2.3. See the release notes and changelogs for details on these releases. Download MariaDB Galera Cluster 5.5.49 Release Notes Changelog What is MariaDB Galera Cluster? MariaDB APT and YUM Repository Configuration Generator Download MariaDB [&#8230;]
The post MariaDB Galera Cluster 5.5.49 and updated connectors now available appeared first on MariaDB.org.";s:12:"atom_content";s:902:"<p>The MariaDB project is pleased to announce the immediate availability of MariaDB Galera Cluster 5.5.49, MariaDB Connector/J 1.4.3, and MariaDB Connector/C 2.2.3. See the release notes and changelogs for details on these releases. Download MariaDB Galera Cluster 5.5.49 Release Notes Changelog What is MariaDB Galera Cluster? MariaDB APT and YUM Repository Configuration Generator Download MariaDB [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://mariadb.org/mariadb-galera-cluster-5-5-49-updated-connectors-now-available/">MariaDB Galera Cluster 5.5.49 and updated connectors now available</a> appeared first on <a rel="nofollow" href="https://mariadb.org">MariaDB.org</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995281&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995281&vote=-1&apivote=1">Vote DOWN</a>";}i:17;a:10:{s:5:"title";s:51:"What happens when you create a MySQL Document Store";s:4:"guid";s:70:"tag:blogger.com,1999:blog-8634321333008124397.post-1007208980619189119";s:4:"link";s:83:"http://elephantdolphin.blogspot.com/2016/04/what-happens-when-you-create-mysql.html";s:11:"description";s:3410:"The MySQL Document Store introduced with version 5.7.12 allows developers to create document collections without have to know Structured Query Language. The new feature also comes with a new set of terminology.  So let us create a collection and see what it in it (basically creating a table for us SQL speakin' old timers).   So start the mysqlsh program, connect to the server, change to the world-x schema (database) switch to Python mode, a create a collection (table). What did the server do for us? Switching to SQL mode, we can use describe to see what the server has done for us. We have a two column table.  The first is named doc and is used to store JSON.  And there is also a column named _id and please notice this column is notated as STORED GENERATED.The generated column extracts values from a JSON document and materializes that information into a new column that then can be indexed. But what did the system extract for us to create this new column?Lets use SHOW CREATE TABLE to find out.  mysql-sql> SHOW CREATE TABLE foobar;+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table  | Create Table                                                                      |+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| foobar | CREATE TABLE `foobar` (  `doc` json DEFAULT NULL,  `_id` varchar(32) GENERATED ALWAYS AS (json_unquote(json_extract(`doc`,'$._id'))) STORED NOT NULL,  UNIQUE KEY `_id` (`_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)mysql-sql>So the 5.7.12 document store is creating an index for us on a field named _id in our JSON document. Hmm, what if I do not have an _id field in my data.  So I added two records ("Name" : "Dave" and "Name" : "Jack") into my new collection and then took a peek.  mysql> select * from foobar;+-------------------------------------------------------------+----------------------------------+| doc                                                         | _id                 |+-------------------------------------------------------------+----------------------------------+| {"_id": "819a19383d9fd111901100059a3c7a00", "Name": "Dave"} | 819a19383d9fd111901100059a3c7a00 || {"_id": "d639274c3d9fd111901100059a3c7a00", "Name": "Jack"} | d639274c3d9fd111901100059a3c7a00 |+-------------------------------------------------------------+----------------------------------+2 rows in set (0.00 sec)mysql>But what if i do have a _id of my own?  The system picked up the _id for the Dexter record.  Remember that the index on the _id field is marked UNIQUE which means you can not reuse that number.So we know the document store wants is creating an unique identification number (that we can also use). Update: The client generates the identification number, the server can not due to possible conflicts in future sharding projects.";s:7:"content";a:1:{s:7:"encoded";s:4903:"The MySQL Document Store introduced with version 5.7.12 allows developers to create document collections without have to know Structured Query Language. The new feature also comes with a new set of terminology.  So let us create a collection and see what it in it (basically creating a table for us SQL speakin' old timers). <p><div><a href="https://1.bp.blogspot.com/-VtewtJW42RU/Vx6KOCDRnxI/AAAAAAAAQ0I/hH6Jn6R0Pp4kKr0l8EGROdN1bSU_huGVwCLcB/s1600/pythoncreate.png" imageanchor="1"><img border="0" src="https://1.bp.blogspot.com/-VtewtJW42RU/Vx6KOCDRnxI/AAAAAAAAQ0I/hH6Jn6R0Pp4kKr0l8EGROdN1bSU_huGVwCLcB/s400/pythoncreate.png" /></a></div>  So start the <i>mysqlsh</i> program, connect to the server, change to the world-x schema (database) switch to Python mode, a create a collection (table). <p>What did the server do for us? Switching to SQL mode, we can use describe to see what the server has done for us. <p><div><a href="https://3.bp.blogspot.com/-9Pin7d_UU9c/Vx6PFUVRUtI/AAAAAAAAQ0Y/nYzQPfC3P4MJOBbFUdjTZ5j17yqsJ5t1gCLcB/s1600/sqldescfibar.png" imageanchor="1"><img border="0" src="https://3.bp.blogspot.com/-9Pin7d_UU9c/Vx6PFUVRUtI/AAAAAAAAQ0Y/nYzQPfC3P4MJOBbFUdjTZ5j17yqsJ5t1gCLcB/s640/sqldescfibar.png" /></a></div><p>We have a two column table.  The first is named <i>doc</i> and is used to store JSON.  And there is also a column named <i>_id</i> and please notice this column is notated as STORED GENERATED.<p>The generated column extracts values from a JSON document and materializes that information into a new column that then can be indexed. But what did the system extract for us to create this new column?<p>Lets use SHOW CREATE TABLE to find out.  <pre><br />mysql-sql> SHOW CREATE TABLE foobar;<br />+--------+----------------------------------------------------------------------<br />--------------------------------------------------------------------------------<br />----------------------------------------------------------------------+<br />| Table  | Create Table<br /><br />                                                                      |<br />+--------+----------------------------------------------------------------------<br />--------------------------------------------------------------------------------<br />----------------------------------------------------------------------+<br />| foobar | CREATE TABLE `foobar` (<br />  `doc` json DEFAULT NULL,<br />  `_id` varchar(32) GENERATED ALWAYS AS (json_unquote(json_extract(`doc`,'$._id'<br />))) STORED NOT NULL,<br />  UNIQUE KEY `_id` (`_id`)<br />) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |<br />+--------+----------------------------------------------------------------------<br />--------------------------------------------------------------------------------<br />----------------------------------------------------------------------+<br />1 row in set (0.00 sec)<br /><br />mysql-sql><br /></pre>So the 5.7.12 document store is creating an index for us on a field named <i>_id</i> in our JSON document. Hmm, what if I do not have an <i>_id</i> field in my data.  So I added two records ("Name" : "Dave" and "Name" : "Jack") into my new collection and then took a peek.  <pre><br />mysql> select * from foobar;<br />+-------------------------------------------------------------+-----------------<br />-----------------+<br />| doc                                                         | _id<br />                 |<br />+-------------------------------------------------------------+-----------------<br />-----------------+<br />| {"_id": "819a19383d9fd111901100059a3c7a00", "Name": "Dave"} | 819a19383d9fd111<br />901100059a3c7a00 |<br />| {"_id": "d639274c3d9fd111901100059a3c7a00", "Name": "Jack"} | d639274c3d9fd111<br />901100059a3c7a00 |<br />+-------------------------------------------------------------+-----------------<br />-----------------+<br />2 rows in set (0.00 sec)<br /><br />mysql></pre><p>But what if i do have a <i>_id</i> of my own?  <div><a href="https://4.bp.blogspot.com/-LlLcZbLbZOg/Vx6V2I6gi5I/AAAAAAAAQ0o/pHra78TCI1w7ZwO0HFG_rD9ikti4p96BgCLcB/s1600/dexter.png" imageanchor="1"><img border="0" src="https://4.bp.blogspot.com/-LlLcZbLbZOg/Vx6V2I6gi5I/AAAAAAAAQ0o/pHra78TCI1w7ZwO0HFG_rD9ikti4p96BgCLcB/s640/dexter.png" /></a></div><p>The system picked up the <i>_id</i> for the Dexter record.  Remember that the index on the <i>_id</i> field is marked UNIQUE which means you can not reuse that number.<p>So we know the document store wants is creating an unique identification number (that we can also use). <p><b>Update:</b> The <i>client</i> generates the identification number, the server can not due to possible conflicts in future sharding projects.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995270&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995270&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Mon, 25 Apr 2016 22:15:00 +0000";s:2:"dc";a:1:{s:7:"creator";s:11:"Dave Stokes";}s:8:"category";s:29:"databaseJSONMySQLnosqlPHPRDMS";s:7:"summary";s:3410:"The MySQL Document Store introduced with version 5.7.12 allows developers to create document collections without have to know Structured Query Language. The new feature also comes with a new set of terminology.  So let us create a collection and see what it in it (basically creating a table for us SQL speakin' old timers).   So start the mysqlsh program, connect to the server, change to the world-x schema (database) switch to Python mode, a create a collection (table). What did the server do for us? Switching to SQL mode, we can use describe to see what the server has done for us. We have a two column table.  The first is named doc and is used to store JSON.  And there is also a column named _id and please notice this column is notated as STORED GENERATED.The generated column extracts values from a JSON document and materializes that information into a new column that then can be indexed. But what did the system extract for us to create this new column?Lets use SHOW CREATE TABLE to find out.  mysql-sql> SHOW CREATE TABLE foobar;+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table  | Create Table                                                                      |+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| foobar | CREATE TABLE `foobar` (  `doc` json DEFAULT NULL,  `_id` varchar(32) GENERATED ALWAYS AS (json_unquote(json_extract(`doc`,'$._id'))) STORED NOT NULL,  UNIQUE KEY `_id` (`_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)mysql-sql>So the 5.7.12 document store is creating an index for us on a field named _id in our JSON document. Hmm, what if I do not have an _id field in my data.  So I added two records ("Name" : "Dave" and "Name" : "Jack") into my new collection and then took a peek.  mysql> select * from foobar;+-------------------------------------------------------------+----------------------------------+| doc                                                         | _id                 |+-------------------------------------------------------------+----------------------------------+| {"_id": "819a19383d9fd111901100059a3c7a00", "Name": "Dave"} | 819a19383d9fd111901100059a3c7a00 || {"_id": "d639274c3d9fd111901100059a3c7a00", "Name": "Jack"} | d639274c3d9fd111901100059a3c7a00 |+-------------------------------------------------------------+----------------------------------+2 rows in set (0.00 sec)mysql>But what if i do have a _id of my own?  The system picked up the _id for the Dexter record.  Remember that the index on the _id field is marked UNIQUE which means you can not reuse that number.So we know the document store wants is creating an unique identification number (that we can also use). Update: The client generates the identification number, the server can not due to possible conflicts in future sharding projects.";s:12:"atom_content";s:4903:"The MySQL Document Store introduced with version 5.7.12 allows developers to create document collections without have to know Structured Query Language. The new feature also comes with a new set of terminology.  So let us create a collection and see what it in it (basically creating a table for us SQL speakin' old timers). <p><div><a href="https://1.bp.blogspot.com/-VtewtJW42RU/Vx6KOCDRnxI/AAAAAAAAQ0I/hH6Jn6R0Pp4kKr0l8EGROdN1bSU_huGVwCLcB/s1600/pythoncreate.png" imageanchor="1"><img border="0" src="https://1.bp.blogspot.com/-VtewtJW42RU/Vx6KOCDRnxI/AAAAAAAAQ0I/hH6Jn6R0Pp4kKr0l8EGROdN1bSU_huGVwCLcB/s400/pythoncreate.png" /></a></div>  So start the <i>mysqlsh</i> program, connect to the server, change to the world-x schema (database) switch to Python mode, a create a collection (table). <p>What did the server do for us? Switching to SQL mode, we can use describe to see what the server has done for us. <p><div><a href="https://3.bp.blogspot.com/-9Pin7d_UU9c/Vx6PFUVRUtI/AAAAAAAAQ0Y/nYzQPfC3P4MJOBbFUdjTZ5j17yqsJ5t1gCLcB/s1600/sqldescfibar.png" imageanchor="1"><img border="0" src="https://3.bp.blogspot.com/-9Pin7d_UU9c/Vx6PFUVRUtI/AAAAAAAAQ0Y/nYzQPfC3P4MJOBbFUdjTZ5j17yqsJ5t1gCLcB/s640/sqldescfibar.png" /></a></div><p>We have a two column table.  The first is named <i>doc</i> and is used to store JSON.  And there is also a column named <i>_id</i> and please notice this column is notated as STORED GENERATED.<p>The generated column extracts values from a JSON document and materializes that information into a new column that then can be indexed. But what did the system extract for us to create this new column?<p>Lets use SHOW CREATE TABLE to find out.  <pre><br />mysql-sql> SHOW CREATE TABLE foobar;<br />+--------+----------------------------------------------------------------------<br />--------------------------------------------------------------------------------<br />----------------------------------------------------------------------+<br />| Table  | Create Table<br /><br />                                                                      |<br />+--------+----------------------------------------------------------------------<br />--------------------------------------------------------------------------------<br />----------------------------------------------------------------------+<br />| foobar | CREATE TABLE `foobar` (<br />  `doc` json DEFAULT NULL,<br />  `_id` varchar(32) GENERATED ALWAYS AS (json_unquote(json_extract(`doc`,'$._id'<br />))) STORED NOT NULL,<br />  UNIQUE KEY `_id` (`_id`)<br />) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |<br />+--------+----------------------------------------------------------------------<br />--------------------------------------------------------------------------------<br />----------------------------------------------------------------------+<br />1 row in set (0.00 sec)<br /><br />mysql-sql><br /></pre>So the 5.7.12 document store is creating an index for us on a field named <i>_id</i> in our JSON document. Hmm, what if I do not have an <i>_id</i> field in my data.  So I added two records ("Name" : "Dave" and "Name" : "Jack") into my new collection and then took a peek.  <pre><br />mysql> select * from foobar;<br />+-------------------------------------------------------------+-----------------<br />-----------------+<br />| doc                                                         | _id<br />                 |<br />+-------------------------------------------------------------+-----------------<br />-----------------+<br />| {"_id": "819a19383d9fd111901100059a3c7a00", "Name": "Dave"} | 819a19383d9fd111<br />901100059a3c7a00 |<br />| {"_id": "d639274c3d9fd111901100059a3c7a00", "Name": "Jack"} | d639274c3d9fd111<br />901100059a3c7a00 |<br />+-------------------------------------------------------------+-----------------<br />-----------------+<br />2 rows in set (0.00 sec)<br /><br />mysql></pre><p>But what if i do have a <i>_id</i> of my own?  <div><a href="https://4.bp.blogspot.com/-LlLcZbLbZOg/Vx6V2I6gi5I/AAAAAAAAQ0o/pHra78TCI1w7ZwO0HFG_rD9ikti4p96BgCLcB/s1600/dexter.png" imageanchor="1"><img border="0" src="https://4.bp.blogspot.com/-LlLcZbLbZOg/Vx6V2I6gi5I/AAAAAAAAQ0o/pHra78TCI1w7ZwO0HFG_rD9ikti4p96BgCLcB/s640/dexter.png" /></a></div><p>The system picked up the <i>_id</i> for the Dexter record.  Remember that the index on the <i>_id</i> field is marked UNIQUE which means you can not reuse that number.<p>So we know the document store wants is creating an unique identification number (that we can also use). <p><b>Update:</b> The <i>client</i> generates the identification number, the server can not due to possible conflicts in future sharding projects.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995270&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995270&vote=-1&apivote=1">Vote DOWN</a>";}i:18;a:10:{s:5:"title";s:47:"MySQL for Visual Studio 2.0.2 has been released";s:4:"guid";s:29:"http://insidemysql.com/?p=460";s:4:"link";s:71:"http://insidemysql.com/mysql-for-visual-studio-2-0-2-has-been-released/";s:11:"description";s:1640:"The MySQL Windows Experience Team is proud to announce the release of MySQL for Visual Studio 2.0.2 m1. Note that this is a milestone release and not intended for production usage.
MySQL for Visual Studio 2.0.2 m1 is the first development release of MySQL for Visual Studio to add support for the new X DevAPI. The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.
For more information about how the X DevAPI is implemented in MySQL for Visual Studio, and its usage, please refer to the MySQL for Visual Studio Quick-Start Guide.
Please also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document database, see Using MySQL as a Document Store.
You can download MySQL for Visual Studio 2.0.2 m1 at this link http://dev.mysql.com/downloads/windows/visualstudio/, under the tab &#8220;Development Releases&#8221;.
What’s new in 2.0.2 m1

Added support for the new X DevAPI.
Updated the MySQL parser&#8217;s grammar to include keywords introduced in MySQL 5.7.
Minor optimizations to script editor window.

Known limitations

SSL connections are not yet supported.

Quick links

MySQL for Visual Studio documentation http://dev.mysql.com/doc/visual-studio/en/
Bugs: http://bugs.mysql.com
Forums: http://forums.mysql.com/index.php?174
Blog: http://blogs.oracle.com/MySqlOnWindows

Enjoy and thanks for the support!
MySQL for Visual Studio Team.";s:7:"content";a:1:{s:7:"encoded";s:2713:"<p>The MySQL Windows Experience Team is proud to announce the release of <strong><em>MySQL for Visual Studio 2.0.2 m1</em></strong>. Note that this is a milestone release and not intended for production usage.</p>
<p>MySQL for Visual Studio 2.0.2 m1 is the first development release of MySQL for Visual Studio to add support for the new <a href="https://dev.mysql.com/doc/x-devapi-userguide/en/" target="_blank">X DevAPI</a>. The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.</p>
<p>For more information about how the X DevAPI is implemented in MySQL for Visual Studio, and its usage, please refer to the <a href="http://dev.mysql.com/doc/refman/5.7/en/mysql-shell-visual-studio.html" target="_blank">MySQL for Visual Studio Quick-Start Guide</a>.</p>
<p>Please also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document database, see <a href="http://dev.mysql.com/doc/refman/5.7/en/document-store.html" target="_blank">Using MySQL as a Document Store</a>.</p>
<p>You can download MySQL for Visual Studio 2.0.2 m1 at this link <a href="http://dev.mysql.com/downloads/windows/visualstudio/" target="_blank">http://dev.mysql.com/downloads/windows/visualstudio/</a>, under the tab &#8220;Development Releases&#8221;.</p>
<p><strong>What’s new in 2.0.2 m1</strong></p>
<ul>
<li>Added support for the new X DevAPI.</li>
<li>Updated the MySQL parser&#8217;s grammar to include keywords introduced in MySQL 5.7.</li>
<li>Minor optimizations to script editor window.</li>
</ul>
<p><strong>Known limitations</strong></p>
<ul>
<li>SSL connections are not yet supported.</li>
</ul>
<p><strong>Quick links</strong></p>
<ul>
<li>MySQL for Visual Studio documentation <a href="http://dev.mysql.com/doc/visual-studio/en/" target="_blank">http://dev.mysql.com/doc/visual-studio/en/</a></li>
<li>Bugs: <a href="http://bugs.mysql.com/" target="_blank">http://bugs.mysql.com</a></li>
<li>Forums: <a href="http://forums.mysql.com/index.php?174" target="_blank">http://forums.mysql.com/index.php?174</a></li>
<li>Blog: <a href="http://blogs.oracle.com/MySqlOnWindows" target="_blank">http://blogs.oracle.com/MySqlOnWindows</a></li>
</ul>
<p>Enjoy and thanks for the support!</p>
<p>MySQL for Visual Studio Team.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995269&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995269&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Mon, 25 Apr 2016 21:46:24 +0000";s:2:"dc";a:1:{s:7:"creator";s:15:"InsideMySQL.com";}s:8:"category";s:53:"MySQL DevelopmentMySQL on Windowsmysqlvisual studiovs";s:7:"summary";s:1640:"The MySQL Windows Experience Team is proud to announce the release of MySQL for Visual Studio 2.0.2 m1. Note that this is a milestone release and not intended for production usage.
MySQL for Visual Studio 2.0.2 m1 is the first development release of MySQL for Visual Studio to add support for the new X DevAPI. The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.
For more information about how the X DevAPI is implemented in MySQL for Visual Studio, and its usage, please refer to the MySQL for Visual Studio Quick-Start Guide.
Please also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document database, see Using MySQL as a Document Store.
You can download MySQL for Visual Studio 2.0.2 m1 at this link http://dev.mysql.com/downloads/windows/visualstudio/, under the tab &#8220;Development Releases&#8221;.
What’s new in 2.0.2 m1

Added support for the new X DevAPI.
Updated the MySQL parser&#8217;s grammar to include keywords introduced in MySQL 5.7.
Minor optimizations to script editor window.

Known limitations

SSL connections are not yet supported.

Quick links

MySQL for Visual Studio documentation http://dev.mysql.com/doc/visual-studio/en/
Bugs: http://bugs.mysql.com
Forums: http://forums.mysql.com/index.php?174
Blog: http://blogs.oracle.com/MySqlOnWindows

Enjoy and thanks for the support!
MySQL for Visual Studio Team.";s:12:"atom_content";s:2713:"<p>The MySQL Windows Experience Team is proud to announce the release of <strong><em>MySQL for Visual Studio 2.0.2 m1</em></strong>. Note that this is a milestone release and not intended for production usage.</p>
<p>MySQL for Visual Studio 2.0.2 m1 is the first development release of MySQL for Visual Studio to add support for the new <a href="https://dev.mysql.com/doc/x-devapi-userguide/en/" target="_blank">X DevAPI</a>. The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.</p>
<p>For more information about how the X DevAPI is implemented in MySQL for Visual Studio, and its usage, please refer to the <a href="http://dev.mysql.com/doc/refman/5.7/en/mysql-shell-visual-studio.html" target="_blank">MySQL for Visual Studio Quick-Start Guide</a>.</p>
<p>Please also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document database, see <a href="http://dev.mysql.com/doc/refman/5.7/en/document-store.html" target="_blank">Using MySQL as a Document Store</a>.</p>
<p>You can download MySQL for Visual Studio 2.0.2 m1 at this link <a href="http://dev.mysql.com/downloads/windows/visualstudio/" target="_blank">http://dev.mysql.com/downloads/windows/visualstudio/</a>, under the tab &#8220;Development Releases&#8221;.</p>
<p><strong>What’s new in 2.0.2 m1</strong></p>
<ul>
<li>Added support for the new X DevAPI.</li>
<li>Updated the MySQL parser&#8217;s grammar to include keywords introduced in MySQL 5.7.</li>
<li>Minor optimizations to script editor window.</li>
</ul>
<p><strong>Known limitations</strong></p>
<ul>
<li>SSL connections are not yet supported.</li>
</ul>
<p><strong>Quick links</strong></p>
<ul>
<li>MySQL for Visual Studio documentation <a href="http://dev.mysql.com/doc/visual-studio/en/" target="_blank">http://dev.mysql.com/doc/visual-studio/en/</a></li>
<li>Bugs: <a href="http://bugs.mysql.com/" target="_blank">http://bugs.mysql.com</a></li>
<li>Forums: <a href="http://forums.mysql.com/index.php?174" target="_blank">http://forums.mysql.com/index.php?174</a></li>
<li>Blog: <a href="http://blogs.oracle.com/MySqlOnWindows" target="_blank">http://blogs.oracle.com/MySqlOnWindows</a></li>
</ul>
<p>Enjoy and thanks for the support!</p>
<p>MySQL for Visual Studio Team.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995269&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995269&vote=-1&apivote=1">Vote DOWN</a>";}i:19;a:10:{s:5:"title";s:22:"TRIM, iostat and Linux";s:4:"guid";s:70:"tag:blogger.com,1999:blog-9149523927864751087.post-2876309659922658771";s:4:"link";s:65:"http://smalldatum.blogspot.com/2016/04/trim-iostat-and-linux.html";s:11:"description";s:5040:"I use iostat and vmstat to measure how much CPU and storage is used during my performance tests. Many of the database engines have their own counters to report disk IO but it is good to use the same measurement across engines. I use the "-k" option with iostat so it reports KB written per second per device.The rate of writes to storage can be overstated by a factor of two in one case and I don't think this is widely known. When TRIM is done for an SSD then the Linux kernels that I use report that as bytes written. If I create an 8G file then I will see at least 8G of writes reported by iostat. If I then remove the file I will see an additional 8G of writes reported by iostat assuming TRIM is used. But that second batch of 8G of writes wasn't really writes.One of the database engines that I evaluate, RocksDB, frequently creates and removes files. When TRIM is counted as bytes written then this overstates the amount of storage writes done by RocksDB. The other engines that I evaluate do not create and remove files as frequently -- InnoDB, WiredTiger, TokuDB, mmapv1.The best way to figure out whether TRIM is done for your favorite SSD is to test it yourself. If TRIM is done then iostat reports TRIM as bytes written.&nbsp;If iostat reports TRIM as bytes written and your database engine frequently removes files then iostat wKB/second might be overstated.Testing this:My test case is:output=/path/to/many/GB/file/this/will/create# run this long enough to get a file that is many GB in size        dd if=/dev/zero of=$output bs=1M oflag=direct &amp;dpid=$!sleep 30kill $dpidiostat -kx 1 &gt;&amp; o.io &amp;        ipid=$!sleep 3rm -f $output; syncsleep 10kill $ipid# look at iostat data in o.ioExample iostat output from a 4.0.9 Linux kernel after the rm command:Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%utilmd2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65538.00 &nbsp; &nbsp; 0.00 8387632.00 &nbsp; 255.96 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00Example iostat output from a 3.10.53 Linux kernel after the rm command:Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%utilmd0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 31078.00 &nbsp; &nbsp; 0.00 3977984.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 283935.00 &nbsp; &nbsp; 0.00 36343552.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 288343.00 &nbsp; &nbsp; 0.00 36907908.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 208534.00 &nbsp; &nbsp; 0.00 26692352.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00";s:7:"content";a:1:{s:7:"encoded";s:6321:"I use iostat and vmstat to measure how much CPU and storage is used during my performance tests. Many of the database engines have their own counters to report disk IO but it is good to use the same measurement across engines. I use the "-k" option with iostat so it reports KB written per second per device.<br /><br />The rate of writes to storage can be overstated by a factor of two in one case and I don't think this is widely known. When <a href="https://en.wikipedia.org/wiki/Trim_(computing)">TRIM</a> is done for an SSD then the Linux kernels that I use report that as bytes written. If I create an 8G file then I will see at least 8G of writes reported by iostat. If I then remove the file I will see an additional 8G of writes reported by iostat assuming TRIM is used. But that second batch of 8G of writes wasn't really writes.<br /><br />One of the database engines that I evaluate, <a href="http://rocksdb.org/">RocksDB</a>, frequently creates and removes files. When TRIM is counted as bytes written then this overstates the amount of storage writes done by RocksDB. The other engines that I evaluate do not create and remove files as frequently -- InnoDB, WiredTiger, TokuDB, mmapv1.<br /><br />The best way to figure out whether TRIM is done for your favorite SSD is to test it yourself. <br /><ol><li>If TRIM is done then iostat reports TRIM as bytes written.&nbsp;</li><li>If iostat reports TRIM as bytes written and your database engine frequently removes files then iostat wKB/second might be overstated.</li></ol><br /><h4>Testing this:</h4><br />My test case is:<br /><span><br /></span><span>output=/path/to/many/GB/file/this/will/create</span><br /><span><br /># run this long enough to get a file that is many GB in size        </span><br /><span>dd if=/dev/zero of=$output bs=1M oflag=direct &amp;<br />dpid=$!</span><br /><span>sleep 30<br />kill $dpid</span><br /><div><span><span><br />iostat -kx 1 &gt;&amp; o.io &amp;        </span></span></div><div><span><span>ipid=$!</span></span></div><div><span><span>sleep 3</span></span></div><div><span><span>rm -f $output; sync<br />sleep 10<br />kill $ipid</span></span></div><div><span># look at iostat data in o.io</span></div><div><span><span><br /></span></span></div><div><span><span>Example iostat output from a 4.0.9 Linux kernel after the rm command:</span><span><br /></span></span></div><div><span>Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%util</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65538.00 &nbsp; &nbsp; 0.00 8387632.00 &nbsp; 255.96 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span><span></span></span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span><br /></span></div><div><span>Example iostat output from a 3.10.53 Linux kernel after the rm command:</span></div><div><span>Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%util</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 31078.00 &nbsp; &nbsp; 0.00 3977984.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 283935.00 &nbsp; &nbsp; 0.00 36343552.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 288343.00 &nbsp; &nbsp; 0.00 36907908.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 208534.00 &nbsp; &nbsp; 0.00 26692352.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span></span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995268&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995268&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Mon, 25 Apr 2016 17:06:00 +0000";s:2:"dc";a:1:{s:7:"creator";s:14:"Mark Callaghan";}s:8:"category";s:19:"mongodbmysqlrocksdb";s:7:"summary";s:5040:"I use iostat and vmstat to measure how much CPU and storage is used during my performance tests. Many of the database engines have their own counters to report disk IO but it is good to use the same measurement across engines. I use the "-k" option with iostat so it reports KB written per second per device.The rate of writes to storage can be overstated by a factor of two in one case and I don't think this is widely known. When TRIM is done for an SSD then the Linux kernels that I use report that as bytes written. If I create an 8G file then I will see at least 8G of writes reported by iostat. If I then remove the file I will see an additional 8G of writes reported by iostat assuming TRIM is used. But that second batch of 8G of writes wasn't really writes.One of the database engines that I evaluate, RocksDB, frequently creates and removes files. When TRIM is counted as bytes written then this overstates the amount of storage writes done by RocksDB. The other engines that I evaluate do not create and remove files as frequently -- InnoDB, WiredTiger, TokuDB, mmapv1.The best way to figure out whether TRIM is done for your favorite SSD is to test it yourself. If TRIM is done then iostat reports TRIM as bytes written.&nbsp;If iostat reports TRIM as bytes written and your database engine frequently removes files then iostat wKB/second might be overstated.Testing this:My test case is:output=/path/to/many/GB/file/this/will/create# run this long enough to get a file that is many GB in size        dd if=/dev/zero of=$output bs=1M oflag=direct &amp;dpid=$!sleep 30kill $dpidiostat -kx 1 &gt;&amp; o.io &amp;        ipid=$!sleep 3rm -f $output; syncsleep 10kill $ipid# look at iostat data in o.ioExample iostat output from a 4.0.9 Linux kernel after the rm command:Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%utilmd2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65538.00 &nbsp; &nbsp; 0.00 8387632.00 &nbsp; 255.96 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00Example iostat output from a 3.10.53 Linux kernel after the rm command:Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%utilmd0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 31078.00 &nbsp; &nbsp; 0.00 3977984.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 283935.00 &nbsp; &nbsp; 0.00 36343552.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 288343.00 &nbsp; &nbsp; 0.00 36907908.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 208534.00 &nbsp; &nbsp; 0.00 26692352.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00";s:12:"atom_content";s:6321:"I use iostat and vmstat to measure how much CPU and storage is used during my performance tests. Many of the database engines have their own counters to report disk IO but it is good to use the same measurement across engines. I use the "-k" option with iostat so it reports KB written per second per device.<br /><br />The rate of writes to storage can be overstated by a factor of two in one case and I don't think this is widely known. When <a href="https://en.wikipedia.org/wiki/Trim_(computing)">TRIM</a> is done for an SSD then the Linux kernels that I use report that as bytes written. If I create an 8G file then I will see at least 8G of writes reported by iostat. If I then remove the file I will see an additional 8G of writes reported by iostat assuming TRIM is used. But that second batch of 8G of writes wasn't really writes.<br /><br />One of the database engines that I evaluate, <a href="http://rocksdb.org/">RocksDB</a>, frequently creates and removes files. When TRIM is counted as bytes written then this overstates the amount of storage writes done by RocksDB. The other engines that I evaluate do not create and remove files as frequently -- InnoDB, WiredTiger, TokuDB, mmapv1.<br /><br />The best way to figure out whether TRIM is done for your favorite SSD is to test it yourself. <br /><ol><li>If TRIM is done then iostat reports TRIM as bytes written.&nbsp;</li><li>If iostat reports TRIM as bytes written and your database engine frequently removes files then iostat wKB/second might be overstated.</li></ol><br /><h4>Testing this:</h4><br />My test case is:<br /><span><br /></span><span>output=/path/to/many/GB/file/this/will/create</span><br /><span><br /># run this long enough to get a file that is many GB in size        </span><br /><span>dd if=/dev/zero of=$output bs=1M oflag=direct &amp;<br />dpid=$!</span><br /><span>sleep 30<br />kill $dpid</span><br /><div><span><span><br />iostat -kx 1 &gt;&amp; o.io &amp;        </span></span></div><div><span><span>ipid=$!</span></span></div><div><span><span>sleep 3</span></span></div><div><span><span>rm -f $output; sync<br />sleep 10<br />kill $ipid</span></span></div><div><span># look at iostat data in o.io</span></div><div><span><span><br /></span></span></div><div><span><span>Example iostat output from a 4.0.9 Linux kernel after the rm command:</span><span><br /></span></span></div><div><span>Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%util</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65538.00 &nbsp; &nbsp; 0.00 8387632.00 &nbsp; 255.96 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span><span></span></span></div><div><span>md2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 65528.00 &nbsp; &nbsp; 0.00 8387584.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span><br /></span></div><div><span>Example iostat output from a 3.10.53 Linux kernel after the rm command:</span></div><div><span>Device: &nbsp; &nbsp; &nbsp; &nbsp; rrqm/s &nbsp; wrqm/s &nbsp; &nbsp; r/s &nbsp; &nbsp; w/s &nbsp; &nbsp;rkB/s &nbsp; &nbsp;wkB/s avgrq-sz avgqu-sz &nbsp; await &nbsp;svctm &nbsp;%util</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 31078.00 &nbsp; &nbsp; 0.00 3977984.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 283935.00 &nbsp; &nbsp; 0.00 36343552.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 288343.00 &nbsp; &nbsp; 0.00 36907908.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 208534.00 &nbsp; &nbsp; 0.00 26692352.00 &nbsp; 256.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><div><span></span></div><div><span>md0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp;0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp; 0.00 &nbsp; &nbsp;0.00 &nbsp; 0.00 &nbsp; 0.00</span></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995268&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995268&vote=-1&apivote=1">Vote DOWN</a>";}i:20;a:10:{s:5:"title";s:26:"MySQL Query Best Practices";s:4:"guid";s:37:"https://www.pythian.com/blog/?p=87777";s:4:"link";s:56:"https://www.pythian.com/blog/mysql-query-best-practices/";s:11:"description";s:7074:"You can get many returns from a Google search for &#8220;MySQL Query Best Practices&#8221; or &#8220;MySQL Query Optimization.&#8221; The drawback is that too many rules can provide confusing or even conflicting advice. After doing some research and tests, I outlined the essential and important ones below:1) Use proper data types1.1) Use the smallest data types if possibleMySQL tries to load as much data as possible into memory (innodb-buffer-pool, key-buffer), so a small data type means more rows of data in memory, thus improving performance. Also, small data sizes reduces disk i/o.1.2) Use Fixed-length Data Types if PossibleMySQL can calculate quickly the position of a fixed-length column in a specific row of a table.With the flexible-length data type, the row size is not fixed, so every time it needs to do a seek, MySQL might consult the primary key index. However, the flexible-length data type can save data size, and the disk space required.In practice, if the column data size varies a lot, then use a flexible-length data type (e.g., varchar); if the data length is short or length barely changes, then use a fixed data type.1.3) Use not null unless there is reason not toIt is harder for MySQL to optimize queries that refer to nullable columns, because they make indexes, index statistics, and value comparisons more complicated. A nullable column uses more storage space and requires special processing inside MySQL.When a nullable column is indexed, it requires an extra byte per entry and can even cause a fixed-size index (e.g., an index on a single integer column) to be converted to a variable-sized one in MyISAM.2)Use indexes smartly2.1) Make primary keys short and on meaningful fieldsA shorter primary key will benefit your queries, because the smaller your primary key, the smaller the index, and the less pages in the cache. In addition, a numeric type is prefered because numeric types are stored in a much more compact format than character formats and so it will make primary key shorter.Another reason to make primary key shorter, is because we usually use primary key to join with the other tables.It is a good idea to use a primary key on a meaningful field, because MySQL uses a cluster index on a primary key. We usually just need the info from primary key, and especially when joined with other tables, it will only search in the index without reading from the data file in disk, and benefit the performance. When you use a meaningful field as the primary key, make sure the uniqueness on the fields wouldn’t change, otherwise it might affect all the tables using this as foreign key when you have to change the primary key.2.2) Index on the search fields only when neededUsually we add indexes on the fields that frequently show up in a where clause &#8212; that is the purpose of indexing. But while an index will benefit reads, it can make writes slower (inserting/updating), so index only when you need it and index smartly.2.3) Index and use the same data types for join fieldsMySQL can do joins on different data types, but the performance is poor as it has to convert from one type to the other for each row. Use the same data type for join fields when possible.2.4) Use a composite index if your query has has more than one field in the where clauseWhen the query needs to search on multiple columns of a table, it might be a good idea to create a compound index for those columns. This is because with composite index on multiple columns, the search will be able to narrow down the result set by the first column, then the second, and so on.Please note that the order of the columns in the composite index affects the performance, so put the columns in the order of the efficiency of narrowing down the search.2.5) Covering index for most commonly used fields in resultsIn some cases, we can put all the required fields into an index (i.e., a covering index) with only some of the fields in the index used for searching and the others for data only. This way, MySQL only need to access the index and there is no need to search in another table.2.6) Partial index for long strings or TEXT, BLOB data types by index on prefixThere is a size limitation for indexes (by default, 1000 for MyISAM, 767 for InnoDB). If the prefix part of the string already covers most of the unique values, it is good to just index the prefix part.2.7) Avoid over-indexingDon&#8217;t index on the low cardinality values, MySQL will choose a full table scan instead of use index if it has to scan the index more than 30%.If a field already exists in the first field of a composite index, you may not need an extra index on the single field. If it exists in a composite index but not in the leftmost field, you will usually need a separate index for that field only if required.Bear in mind that indexing will benefit in reading data but there can be a cost for writing (inserting/updating), so index only when you need it and index smartly.3) Others  3.1) Avoid SELECT * There are many reasons to avoid select * from&#8230; queries. First, it can waste time to read all the fields if you don’t need all the columns. Second, even if you do need all columns, it is better to list the all the field names, to make the query more readable. Finally, if you alter the table by adding/removing some fields, and your application uses select * queries, you can get unexpected results.3.2) Prepared Statements Prepared Statements will filter the variables you bind to them by default, which is great for protecting your application against SQL injection attacks.When the same query is being used multiple times in your application, you can assign different values to the same prepared statement, yet MySQL will only have to parse it once.3.3) If you want to check the existence of data, use exists instead SELECT COUNTTo check if the data exists in a table, using select exists (select *&#8230;) from a table will perform better than select count from a table, since the first method will return a result once it gets one row of the required data, while the second one will have to count on the whole table/index.3.4) Use select limit [number]Select&#8230; limit [number] will return the only required lines of rows of data. Including the limit keyword in your SQL queries can have performance improvements.3.5) Be careful with persistent connectionsPersistent connections can reduce the overhead of re-creating connections to MySQL. When a persistent connection is created, it will stay open even after the script finishes running. The drawback is that it might run out of connections if there are too many connections remaining open but in sleep status.3.6) Review your data and queries regularlyMySQL will choose the query plan based on the statistics of the data in the tables. When the data size changes, the query plan might change, and so it is important to check your queries regularly and to make optimizations accordingly. Check regularly by:3.6.1) EXPLAIN your queries3.6.2) Get suggestions with PROCEDURE ANALYSE()3.6.3) Review slow queries";s:7:"content";a:1:{s:7:"encoded";s:7965:"<div><div><p>You can get many returns from a Google search for &#8220;MySQL Query Best Practices&#8221; or &#8220;MySQL Query Optimization.&#8221; The drawback is that too many rules can provide confusing or even conflicting advice. After doing some research and tests, I outlined the essential and important ones below:</p><p><strong>1) Use proper data types</strong></p><p><strong>1.1) Use the smallest data types if possible</strong></p><p>MySQL tries to load as much data as possible into memory (innodb-buffer-pool, key-buffer), so a small data type means more rows of data in memory, thus improving performance. Also, small data sizes reduces disk i/o.</p><p><strong>1.2) Use Fixed-length Data Types if Possible</strong></p><p>MySQL can calculate quickly the position of a fixed-length column in a specific row of a table.</p><p>With the flexible-length data type, the row size is not fixed, so every time it needs to do a seek, MySQL might consult the primary key index. However, the flexible-length data type can save data size, and the disk space required.</p><p>In practice, if the column data size varies a lot, then use a flexible-length data type (e.g., varchar); if the data length is short or length barely changes, then use a fixed data type.</p><p><strong>1.3) Use not null unless there is reason not to</strong></p><p>It is harder for MySQL to optimize queries that refer to nullable columns, because they make indexes, index statistics, and value comparisons more complicated. A nullable column uses more storage space and requires special processing inside MySQL.</p><p>When a nullable column is indexed, it requires an extra byte per entry and can even cause a fixed-size index (e.g., an index on a single integer column) to be converted to a variable-sized one in MyISAM.</p><p><strong>2)Use indexes smartly</strong></p><p><strong>2.1) Make primary keys short and on meaningful fields</strong></p><p>A shorter primary key will benefit your queries, because the smaller your primary key, the smaller the index, and the less pages in the cache. In addition, a numeric type is prefered because numeric types are stored in a much more compact format than character formats and so it will make primary key shorter.</p><p>Another reason to make primary key shorter, is because we usually use primary key to join with the other tables.</p><p>It is a good idea to use a primary key on a meaningful field, because MySQL uses a cluster index on a primary key. We usually just need the info from primary key, and especially when joined with other tables, it will only search in the index without reading from the data file in disk, and benefit the performance. When you use a meaningful field as the primary key, make sure the uniqueness on the fields wouldn’t change, otherwise it might affect all the tables using this as foreign key when you have to change the primary key.</p><p><strong>2.2) Index on the search fields only when needed</strong></p><p>Usually we add indexes on the fields that frequently show up in a where clause &#8212; that is the purpose of indexing. But while an index will benefit reads, it can make writes slower (inserting/updating), so index only when you need it and index smartly.</p><p><strong>2.3) Index and use the same data types for join fields</strong></p><p>MySQL can do joins on different data types, but the performance is poor as it has to convert from one type to the other for each row. Use the same data type for join fields when possible.</p><p><strong>2.4) Use a composite index if your query has has more than one field in the where clause</strong></p><p>When the query needs to search on multiple columns of a table, it might be a good idea to create a compound index for those columns. This is because with composite index on multiple columns, the search will be able to narrow down the result set by the first column, then the second, and so on.</p><p>Please note that the order of the columns in the composite index affects the performance, so put the columns in the order of the efficiency of narrowing down the search.</p><p><strong>2.5) Covering index for most commonly used fields in results</strong></p><p>In some cases, we can put all the required fields into an index (i.e., a covering index) with only some of the fields in the index used for searching and the others for data only. This way, MySQL only need to access the index and there is no need to search in another table.</p><p><strong>2.6) Partial index for long strings or TEXT, BLOB data types by index on prefix</strong></p><p>There is a size limitation for indexes (by default, 1000 for MyISAM, 767 for InnoDB). If the prefix part of the string already covers most of the unique values, it is good to just index the prefix part.</p><p><strong>2.7) Avoid over-indexing</strong></p><p>Don&#8217;t index on the low cardinality values, MySQL will choose a full table scan instead of use index if it has to scan the index more than 30%.</p><p>If a field already exists in the first field of a composite index, you may not need an extra index on the single field. If it exists in a composite index but not in the leftmost field, you will usually need a separate index for that field only if required.</p><p>Bear in mind that indexing will benefit in reading data but there can be a cost for writing (inserting/updating), so index only when you need it and index smartly.</p><p><strong>3) Others</strong><br
/> <strong> 3.1) Avoid SELECT *</strong><br
/> There are many reasons to avoid select * from&#8230; queries. First, it can waste time to read all the fields if you don’t need all the columns. Second, even if you do need all columns, it is better to list the all the field names, to make the query more readable. Finally, if you alter the table by adding/removing some fields, and your application uses select * queries, you can get unexpected results.</p><p><strong>3.2) Prepared Statements</strong><br
/> Prepared Statements will filter the variables you bind to them by default, which is great for protecting your application against SQL injection attacks.</p><p>When the same query is being used multiple times in your application, you can assign different values to the same prepared statement, yet MySQL will only have to parse it once.</p><p><strong>3.3) If you want to check the existence of data, use exists instead SELECT COUNT</strong></p><p>To check if the data exists in a table, using select exists (select *&#8230;) from a table will perform better than select count from a table, since the first method will return a result once it gets one row of the required data, while the second one will have to count on the whole table/index.</p><p><strong>3.4) Use select limit [number]</strong></p><p>Select&#8230; limit [number] will return the only required lines of rows of data. Including the limit keyword in your SQL queries can have performance improvements.</p><p><strong>3.5) Be careful with persistent connections</strong></p><p>Persistent connections can reduce the overhead of re-creating connections to MySQL. When a persistent connection is created, it will stay open even after the script finishes running. The drawback is that it might run out of connections if there are too many connections remaining open but in sleep status.</p><p><strong>3.6) Review your data and queries regularly</strong></p><p>MySQL will choose the query plan based on the statistics of the data in the tables. When the data size changes, the query plan might change, and so it is important to check your queries regularly and to make optimizations accordingly. Check regularly by:</p><p>3.6.1) EXPLAIN your queries</p><p>3.6.2) Get suggestions with PROCEDURE ANALYSE()</p><p>3.6.3) Review slow queries</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995305&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995305&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Mon, 25 Apr 2016 16:30:58 +0000";s:2:"dc";a:1:{s:7:"creator";s:17:"The Pythian Group";}s:8:"category";s:20:"MySQLTechnical Track";s:7:"summary";s:7074:"You can get many returns from a Google search for &#8220;MySQL Query Best Practices&#8221; or &#8220;MySQL Query Optimization.&#8221; The drawback is that too many rules can provide confusing or even conflicting advice. After doing some research and tests, I outlined the essential and important ones below:1) Use proper data types1.1) Use the smallest data types if possibleMySQL tries to load as much data as possible into memory (innodb-buffer-pool, key-buffer), so a small data type means more rows of data in memory, thus improving performance. Also, small data sizes reduces disk i/o.1.2) Use Fixed-length Data Types if PossibleMySQL can calculate quickly the position of a fixed-length column in a specific row of a table.With the flexible-length data type, the row size is not fixed, so every time it needs to do a seek, MySQL might consult the primary key index. However, the flexible-length data type can save data size, and the disk space required.In practice, if the column data size varies a lot, then use a flexible-length data type (e.g., varchar); if the data length is short or length barely changes, then use a fixed data type.1.3) Use not null unless there is reason not toIt is harder for MySQL to optimize queries that refer to nullable columns, because they make indexes, index statistics, and value comparisons more complicated. A nullable column uses more storage space and requires special processing inside MySQL.When a nullable column is indexed, it requires an extra byte per entry and can even cause a fixed-size index (e.g., an index on a single integer column) to be converted to a variable-sized one in MyISAM.2)Use indexes smartly2.1) Make primary keys short and on meaningful fieldsA shorter primary key will benefit your queries, because the smaller your primary key, the smaller the index, and the less pages in the cache. In addition, a numeric type is prefered because numeric types are stored in a much more compact format than character formats and so it will make primary key shorter.Another reason to make primary key shorter, is because we usually use primary key to join with the other tables.It is a good idea to use a primary key on a meaningful field, because MySQL uses a cluster index on a primary key. We usually just need the info from primary key, and especially when joined with other tables, it will only search in the index without reading from the data file in disk, and benefit the performance. When you use a meaningful field as the primary key, make sure the uniqueness on the fields wouldn’t change, otherwise it might affect all the tables using this as foreign key when you have to change the primary key.2.2) Index on the search fields only when neededUsually we add indexes on the fields that frequently show up in a where clause &#8212; that is the purpose of indexing. But while an index will benefit reads, it can make writes slower (inserting/updating), so index only when you need it and index smartly.2.3) Index and use the same data types for join fieldsMySQL can do joins on different data types, but the performance is poor as it has to convert from one type to the other for each row. Use the same data type for join fields when possible.2.4) Use a composite index if your query has has more than one field in the where clauseWhen the query needs to search on multiple columns of a table, it might be a good idea to create a compound index for those columns. This is because with composite index on multiple columns, the search will be able to narrow down the result set by the first column, then the second, and so on.Please note that the order of the columns in the composite index affects the performance, so put the columns in the order of the efficiency of narrowing down the search.2.5) Covering index for most commonly used fields in resultsIn some cases, we can put all the required fields into an index (i.e., a covering index) with only some of the fields in the index used for searching and the others for data only. This way, MySQL only need to access the index and there is no need to search in another table.2.6) Partial index for long strings or TEXT, BLOB data types by index on prefixThere is a size limitation for indexes (by default, 1000 for MyISAM, 767 for InnoDB). If the prefix part of the string already covers most of the unique values, it is good to just index the prefix part.2.7) Avoid over-indexingDon&#8217;t index on the low cardinality values, MySQL will choose a full table scan instead of use index if it has to scan the index more than 30%.If a field already exists in the first field of a composite index, you may not need an extra index on the single field. If it exists in a composite index but not in the leftmost field, you will usually need a separate index for that field only if required.Bear in mind that indexing will benefit in reading data but there can be a cost for writing (inserting/updating), so index only when you need it and index smartly.3) Others  3.1) Avoid SELECT * There are many reasons to avoid select * from&#8230; queries. First, it can waste time to read all the fields if you don’t need all the columns. Second, even if you do need all columns, it is better to list the all the field names, to make the query more readable. Finally, if you alter the table by adding/removing some fields, and your application uses select * queries, you can get unexpected results.3.2) Prepared Statements Prepared Statements will filter the variables you bind to them by default, which is great for protecting your application against SQL injection attacks.When the same query is being used multiple times in your application, you can assign different values to the same prepared statement, yet MySQL will only have to parse it once.3.3) If you want to check the existence of data, use exists instead SELECT COUNTTo check if the data exists in a table, using select exists (select *&#8230;) from a table will perform better than select count from a table, since the first method will return a result once it gets one row of the required data, while the second one will have to count on the whole table/index.3.4) Use select limit [number]Select&#8230; limit [number] will return the only required lines of rows of data. Including the limit keyword in your SQL queries can have performance improvements.3.5) Be careful with persistent connectionsPersistent connections can reduce the overhead of re-creating connections to MySQL. When a persistent connection is created, it will stay open even after the script finishes running. The drawback is that it might run out of connections if there are too many connections remaining open but in sleep status.3.6) Review your data and queries regularlyMySQL will choose the query plan based on the statistics of the data in the tables. When the data size changes, the query plan might change, and so it is important to check your queries regularly and to make optimizations accordingly. Check regularly by:3.6.1) EXPLAIN your queries3.6.2) Get suggestions with PROCEDURE ANALYSE()3.6.3) Review slow queries";s:12:"atom_content";s:7965:"<div><div><p>You can get many returns from a Google search for &#8220;MySQL Query Best Practices&#8221; or &#8220;MySQL Query Optimization.&#8221; The drawback is that too many rules can provide confusing or even conflicting advice. After doing some research and tests, I outlined the essential and important ones below:</p><p><strong>1) Use proper data types</strong></p><p><strong>1.1) Use the smallest data types if possible</strong></p><p>MySQL tries to load as much data as possible into memory (innodb-buffer-pool, key-buffer), so a small data type means more rows of data in memory, thus improving performance. Also, small data sizes reduces disk i/o.</p><p><strong>1.2) Use Fixed-length Data Types if Possible</strong></p><p>MySQL can calculate quickly the position of a fixed-length column in a specific row of a table.</p><p>With the flexible-length data type, the row size is not fixed, so every time it needs to do a seek, MySQL might consult the primary key index. However, the flexible-length data type can save data size, and the disk space required.</p><p>In practice, if the column data size varies a lot, then use a flexible-length data type (e.g., varchar); if the data length is short or length barely changes, then use a fixed data type.</p><p><strong>1.3) Use not null unless there is reason not to</strong></p><p>It is harder for MySQL to optimize queries that refer to nullable columns, because they make indexes, index statistics, and value comparisons more complicated. A nullable column uses more storage space and requires special processing inside MySQL.</p><p>When a nullable column is indexed, it requires an extra byte per entry and can even cause a fixed-size index (e.g., an index on a single integer column) to be converted to a variable-sized one in MyISAM.</p><p><strong>2)Use indexes smartly</strong></p><p><strong>2.1) Make primary keys short and on meaningful fields</strong></p><p>A shorter primary key will benefit your queries, because the smaller your primary key, the smaller the index, and the less pages in the cache. In addition, a numeric type is prefered because numeric types are stored in a much more compact format than character formats and so it will make primary key shorter.</p><p>Another reason to make primary key shorter, is because we usually use primary key to join with the other tables.</p><p>It is a good idea to use a primary key on a meaningful field, because MySQL uses a cluster index on a primary key. We usually just need the info from primary key, and especially when joined with other tables, it will only search in the index without reading from the data file in disk, and benefit the performance. When you use a meaningful field as the primary key, make sure the uniqueness on the fields wouldn’t change, otherwise it might affect all the tables using this as foreign key when you have to change the primary key.</p><p><strong>2.2) Index on the search fields only when needed</strong></p><p>Usually we add indexes on the fields that frequently show up in a where clause &#8212; that is the purpose of indexing. But while an index will benefit reads, it can make writes slower (inserting/updating), so index only when you need it and index smartly.</p><p><strong>2.3) Index and use the same data types for join fields</strong></p><p>MySQL can do joins on different data types, but the performance is poor as it has to convert from one type to the other for each row. Use the same data type for join fields when possible.</p><p><strong>2.4) Use a composite index if your query has has more than one field in the where clause</strong></p><p>When the query needs to search on multiple columns of a table, it might be a good idea to create a compound index for those columns. This is because with composite index on multiple columns, the search will be able to narrow down the result set by the first column, then the second, and so on.</p><p>Please note that the order of the columns in the composite index affects the performance, so put the columns in the order of the efficiency of narrowing down the search.</p><p><strong>2.5) Covering index for most commonly used fields in results</strong></p><p>In some cases, we can put all the required fields into an index (i.e., a covering index) with only some of the fields in the index used for searching and the others for data only. This way, MySQL only need to access the index and there is no need to search in another table.</p><p><strong>2.6) Partial index for long strings or TEXT, BLOB data types by index on prefix</strong></p><p>There is a size limitation for indexes (by default, 1000 for MyISAM, 767 for InnoDB). If the prefix part of the string already covers most of the unique values, it is good to just index the prefix part.</p><p><strong>2.7) Avoid over-indexing</strong></p><p>Don&#8217;t index on the low cardinality values, MySQL will choose a full table scan instead of use index if it has to scan the index more than 30%.</p><p>If a field already exists in the first field of a composite index, you may not need an extra index on the single field. If it exists in a composite index but not in the leftmost field, you will usually need a separate index for that field only if required.</p><p>Bear in mind that indexing will benefit in reading data but there can be a cost for writing (inserting/updating), so index only when you need it and index smartly.</p><p><strong>3) Others</strong><br
/> <strong> 3.1) Avoid SELECT *</strong><br
/> There are many reasons to avoid select * from&#8230; queries. First, it can waste time to read all the fields if you don’t need all the columns. Second, even if you do need all columns, it is better to list the all the field names, to make the query more readable. Finally, if you alter the table by adding/removing some fields, and your application uses select * queries, you can get unexpected results.</p><p><strong>3.2) Prepared Statements</strong><br
/> Prepared Statements will filter the variables you bind to them by default, which is great for protecting your application against SQL injection attacks.</p><p>When the same query is being used multiple times in your application, you can assign different values to the same prepared statement, yet MySQL will only have to parse it once.</p><p><strong>3.3) If you want to check the existence of data, use exists instead SELECT COUNT</strong></p><p>To check if the data exists in a table, using select exists (select *&#8230;) from a table will perform better than select count from a table, since the first method will return a result once it gets one row of the required data, while the second one will have to count on the whole table/index.</p><p><strong>3.4) Use select limit [number]</strong></p><p>Select&#8230; limit [number] will return the only required lines of rows of data. Including the limit keyword in your SQL queries can have performance improvements.</p><p><strong>3.5) Be careful with persistent connections</strong></p><p>Persistent connections can reduce the overhead of re-creating connections to MySQL. When a persistent connection is created, it will stay open even after the script finishes running. The drawback is that it might run out of connections if there are too many connections remaining open but in sleep status.</p><p><strong>3.6) Review your data and queries regularly</strong></p><p>MySQL will choose the query plan based on the statistics of the data in the tables. When the data size changes, the query plan might change, and so it is important to check your queries regularly and to make optimizations accordingly. Check regularly by:</p><p>3.6.1) EXPLAIN your queries</p><p>3.6.2) Get suggestions with PROCEDURE ANALYSE()</p><p>3.6.3) Review slow queries</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995305&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995305&vote=-1&apivote=1">Vote DOWN</a>";}i:21;a:9:{s:5:"title";s:35:"Top database challenges for MariaDB";s:4:"guid";s:26:"2407 at http://mariadb.com";s:4:"link";s:55:"http://mariadb.com/blog/top-database-challenges-mariadb";s:11:"description";s:4413:"Mon, 2016-04-25 11:54janlindstromThe latest top research identifies a number of big trends and challenges for databases. Jan Lindström, developer at MariaDB, summarises the key issues and challenges and how MariaDB approaches them.First of all, the opinions expressed in this article are the author's own and do not necessary reflect the view of the MariaDB Corporation. These views are based on the review article by Abadi, et.all.: The Beckman Report on Database Research, Communications of the ACM, Vol. 59, NO. 02, 02/2016. While this meeting with thirty leaders from the database research community met in October 2013, my view is that issues raised in this meeting are still more than valid.The review article identifies big data as a defining challenge of our time. This is because it has become cheaper to generate data due to inexpensive storage, sensors, smart devices, social software, multiplayer games, and the Internet of Things. Additionally, it has become cheaper to process large amounts of data, due to advances in multicore CPUs, solid state storage, cheap cloud computing, and open source software.By 2020, the International Data Corporation (IDC) predicts that the amount of digital information created and replicated in the world will grow to almost 40 zettabytes (ZB)—more than 50 times what existed in 2010 and amounting to 5,247 gigabytes for every person on the planet (see http://www.datacenterjournal.com/birth-death-big-data/).This means that organizations have more and more unstructured and unused data that could contain valuable information for predicting business trends and making business decisions. Forbes predicted in 2015 that Buying and selling data will become the new business bread and butter.In the recent years the database research and development community has strengthened core research and development in relational DBMSs and branched out into new directions: security, privacy, data pricing, data attribution, social and mobile data, spatiotemporal data, personalization and contextualization, energy constrained processing, and scientific data management.These lofty research challenges must be taken down on a functionality, if not even feature level. Here are some features we're working on, in a chewing-an-elephant-a-bite-at-a-time fashion.Security:Data at rest encryption for InnoDB system tablespace and user tablespaces, InnoDB log files InnoDB temporal files and binlog files. MariaDB implements Password Validation Plugin APIMariaDB implements simple password validation pluginMariaDB implements cracklib password check pluginNew authentication pluginsPersonalizationDefault rolesSpatiotemporal dataSupport for Spatial Reference systems for the GIS data , new REF_SYSTEM_ID column attribute can be used to specify Spatial Reference System ID for columns of spatial data types.More functions from the OGC standard addedST_BoundaryST_ConvexHullST_IsRingST_PointOnSurfaceST_RelateThe review article identifies five big data challenges: scalable big/fast data infrastructures; coping with diversity in data management; end-to-end processing of data; cloud services; and the roles of the people in the data life cycle. Three of the challenges deal with the volume, velocity, and variety aspects of big data. The last two challenges deal with extending big data applications in the cloud and managing the involvement of people in these applications.How can MariaDB address these challenges? By developing new storage engines for Big Data like MariaDB ColumnStore (earlier InfiniDB). MariaDB ColumnStore is a scalable columnar database management system built for big data analytics, business intelligence, data warehousing and other read-intensive application. Column-store architecture enables very quick load and query times. Its massive parallel processing (MPP) technology scales with any type of storage hardware.Furthermore, MariaDB can support new datatypes and SQL-functions like:Window functions: First drop in MariaDB 10.2.0.JSONForbes predicted on 2015 that Security will become the killer app for big data analytics. Now that MariaDB 10.1 server provides tools for data at rest encryption, other storage engines can easily provide security feature for their data.However, as seen from research challenges there is a lot of room for additional vision and development on relational database management systems like MariaDB.Tags:&nbsp;Big Data";s:7:"content";a:1:{s:7:"encoded";s:6956:"<div><div><div>Mon, 2016-04-25 11:54</div></div></div><div><div><div>janlindstrom</div></div></div><div><div><div property="content:encoded"><p>The latest top research identifies a number of big trends and challenges for databases. Jan Lindström, developer at MariaDB, summarises the key issues and challenges and how MariaDB approaches them.</p><p><i>First of all, the opinions expressed in this article are the author's own and do not necessary reflect the view of the MariaDB Corporation. These views are based on the review article by Abadi, et.all.: The Beckman Report on Database Research, Communications of the ACM, Vol. 59, NO. 02, 02/2016. While this meeting with thirty leaders from the database research community met in October 2013, my view is that issues raised in this meeting are still more than valid.</i></p><p>The review article identifies big data as a defining challenge of our time. This is because it has become cheaper to <b>generate</b> data due to inexpensive storage, sensors, smart devices, social software, multiplayer games, and the Internet of Things. Additionally, it has become cheaper to <b>process</b> large amounts of data, due to advances in multicore CPUs, solid state storage, cheap cloud computing, and open source software.</p><p>By 2020, the International Data Corporation (<a href="https://www.idc.com/">IDC</a>) predicts that the amount of digital information created and replicated in the world will grow to almost 40 zettabytes (ZB)—more than 50 times what existed in 2010 and amounting to 5,247 gigabytes for every person on the planet (see <a href="http://www.datacenterjournal.com/birth-death-big-data/">http://www.datacenterjournal.com/birth-death-big-data/</a>).</p><p>This means that organizations have more and more unstructured and unused data that could contain valuable information for predicting business trends and making business decisions. <a href="http://www.forbes.com/sites/gilpress/2014/12/11/6-predictions-for-the-125-billion-big-data-analytics-market-in-2015/#ebfa5252b207">Forbes</a> predicted in 2015 that <em>Buying and selling data will become the new business bread and butter.</em></p><p>In the recent years the database research and development community has strengthened core research and development in relational DBMSs and branched out into new directions: security, privacy, data pricing, data attribution, social and mobile data, spatiotemporal data, personalization and contextualization, energy constrained processing, and scientific data management.</p><p>These lofty research challenges must be taken down on a functionality, if not even feature level. Here are some features we're working on, in a chewing-an-elephant-a-bite-at-a-time fashion.</p><ul><li>Security:<ul><li><a href="https://mariadb.com/kb/en/mariadb/data-at-rest-encryption/">Data at rest encryption for InnoDB system tablespace and user tablespaces, InnoDB log files InnoDB temporal files and binlog files. </a></li><li><a href="https://mariadb.com/kb/en/mariadb/password-validation/">MariaDB implements Password Validation Plugin API</a></li><li><a href="https://mariadb.com/kb/en/mariadb/simple_password_check/">MariaDB implements simple password validation plugin</a></li><li><a href="https://mariadb.com/kb/en/mariadb/simple_password_check/">MariaDB implements cracklib password check plugin</a></li><li><a href="https://mariadb.com/blog/recent-release-mariadb-10111-contains-two-new-authentication-plugins">New authentication plugins</a></li></ul></li><li>Personalization<ul><li><a href="https://mariadb.com/kb/en/mariadb/set-default-role/">Default roles</a></li></ul></li><li>Spatiotemporal data<ul><li>Support for Spatial Reference systems for the GIS data , new <code>REF_SYSTEM_ID</code> column attribute can be used to specify Spatial Reference System ID for columns of spatial data types.</li><li>More functions from the <a href="http://www.opengeospatial.org/">OGC</a> standard added<ul start="1"><li><a href="https://mariadb.com/kb/en/st_boundary/">ST_Boundary</a></li><li><a href="https://mariadb.com/kb/en/st_convexhull/">ST_ConvexHull</a></li><li><a href="https://mariadb.com/kb/en/st_isring/">ST_IsRing</a></li><li><a href="https://mariadb.com/kb/en/st_pointonsurface/">ST_PointOnSurface</a></li><li><a href="https://mariadb.com/kb/en/st_relate/">ST_Relate</a></li></ul></li></ul></li></ul><p>The review article identifies five big data challenges: scalable big/fast data infrastructures; coping with diversity in data management; end-to-end processing of data; cloud services; and the roles of the people in the data life cycle. Three of the challenges deal with the volume, velocity, and variety aspects of big data. The last two challenges deal with extending big data applications in the cloud and managing the involvement of people in these applications.</p><p>How can MariaDB address these challenges? By developing new storage engines for Big Data like <a href="http://www.cio.com/article/3051146/analytics/mariadb-targets-big-data-analytics-market-with-columnstore.html">MariaDB ColumnStore</a> (earlier <a href="https://en.wikipedia.org/wiki/InfiniDB">InfiniDB</a>). MariaDB ColumnStore is a scalable columnar database management system built for big data analytics, business intelligence, data warehousing and other read-intensive application. Column-store architecture enables very quick load and query times. Its massive parallel processing (MPP) technology scales with any type of storage hardware.</p><p>Furthermore, MariaDB can support new datatypes and SQL-functions like:</p><ul><li><a href="https://mariadb.com/kb/en/mariadb/window-functions/">Window functions</a>: First drop in MariaDB 10.2.0.</li><li><a href="https://jira.mariadb.org/browse/MDEV-9056">JSON</a></li></ul><p><a href="http://www.forbes.com/sites/gilpress/2014/12/11/6-predictions-for-the-125-billion-big-data-analytics-market-in-2015/#ebfa5252b207">Forbes</a> predicted on 2015 that <em>Security will become the killer app for big data analytics. </em>Now that MariaDB 10.1 server provides tools for data at rest encryption, other storage engines can easily provide security feature for their data.</p><p>However, as seen from research challenges there is a lot of room for additional vision and development on relational database management systems like MariaDB.</p></div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/big-data" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Big Data</a></div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995266&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995266&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Mon, 25 Apr 2016 11:54:28 +0000";s:2:"dc";a:1:{s:7:"creator";s:7:"MariaDB";}s:7:"summary";s:4413:"Mon, 2016-04-25 11:54janlindstromThe latest top research identifies a number of big trends and challenges for databases. Jan Lindström, developer at MariaDB, summarises the key issues and challenges and how MariaDB approaches them.First of all, the opinions expressed in this article are the author's own and do not necessary reflect the view of the MariaDB Corporation. These views are based on the review article by Abadi, et.all.: The Beckman Report on Database Research, Communications of the ACM, Vol. 59, NO. 02, 02/2016. While this meeting with thirty leaders from the database research community met in October 2013, my view is that issues raised in this meeting are still more than valid.The review article identifies big data as a defining challenge of our time. This is because it has become cheaper to generate data due to inexpensive storage, sensors, smart devices, social software, multiplayer games, and the Internet of Things. Additionally, it has become cheaper to process large amounts of data, due to advances in multicore CPUs, solid state storage, cheap cloud computing, and open source software.By 2020, the International Data Corporation (IDC) predicts that the amount of digital information created and replicated in the world will grow to almost 40 zettabytes (ZB)—more than 50 times what existed in 2010 and amounting to 5,247 gigabytes for every person on the planet (see http://www.datacenterjournal.com/birth-death-big-data/).This means that organizations have more and more unstructured and unused data that could contain valuable information for predicting business trends and making business decisions. Forbes predicted in 2015 that Buying and selling data will become the new business bread and butter.In the recent years the database research and development community has strengthened core research and development in relational DBMSs and branched out into new directions: security, privacy, data pricing, data attribution, social and mobile data, spatiotemporal data, personalization and contextualization, energy constrained processing, and scientific data management.These lofty research challenges must be taken down on a functionality, if not even feature level. Here are some features we're working on, in a chewing-an-elephant-a-bite-at-a-time fashion.Security:Data at rest encryption for InnoDB system tablespace and user tablespaces, InnoDB log files InnoDB temporal files and binlog files. MariaDB implements Password Validation Plugin APIMariaDB implements simple password validation pluginMariaDB implements cracklib password check pluginNew authentication pluginsPersonalizationDefault rolesSpatiotemporal dataSupport for Spatial Reference systems for the GIS data , new REF_SYSTEM_ID column attribute can be used to specify Spatial Reference System ID for columns of spatial data types.More functions from the OGC standard addedST_BoundaryST_ConvexHullST_IsRingST_PointOnSurfaceST_RelateThe review article identifies five big data challenges: scalable big/fast data infrastructures; coping with diversity in data management; end-to-end processing of data; cloud services; and the roles of the people in the data life cycle. Three of the challenges deal with the volume, velocity, and variety aspects of big data. The last two challenges deal with extending big data applications in the cloud and managing the involvement of people in these applications.How can MariaDB address these challenges? By developing new storage engines for Big Data like MariaDB ColumnStore (earlier InfiniDB). MariaDB ColumnStore is a scalable columnar database management system built for big data analytics, business intelligence, data warehousing and other read-intensive application. Column-store architecture enables very quick load and query times. Its massive parallel processing (MPP) technology scales with any type of storage hardware.Furthermore, MariaDB can support new datatypes and SQL-functions like:Window functions: First drop in MariaDB 10.2.0.JSONForbes predicted on 2015 that Security will become the killer app for big data analytics. Now that MariaDB 10.1 server provides tools for data at rest encryption, other storage engines can easily provide security feature for their data.However, as seen from research challenges there is a lot of room for additional vision and development on relational database management systems like MariaDB.Tags:&nbsp;Big Data";s:12:"atom_content";s:6956:"<div><div><div>Mon, 2016-04-25 11:54</div></div></div><div><div><div>janlindstrom</div></div></div><div><div><div property="content:encoded"><p>The latest top research identifies a number of big trends and challenges for databases. Jan Lindström, developer at MariaDB, summarises the key issues and challenges and how MariaDB approaches them.</p><p><i>First of all, the opinions expressed in this article are the author's own and do not necessary reflect the view of the MariaDB Corporation. These views are based on the review article by Abadi, et.all.: The Beckman Report on Database Research, Communications of the ACM, Vol. 59, NO. 02, 02/2016. While this meeting with thirty leaders from the database research community met in October 2013, my view is that issues raised in this meeting are still more than valid.</i></p><p>The review article identifies big data as a defining challenge of our time. This is because it has become cheaper to <b>generate</b> data due to inexpensive storage, sensors, smart devices, social software, multiplayer games, and the Internet of Things. Additionally, it has become cheaper to <b>process</b> large amounts of data, due to advances in multicore CPUs, solid state storage, cheap cloud computing, and open source software.</p><p>By 2020, the International Data Corporation (<a href="https://www.idc.com/">IDC</a>) predicts that the amount of digital information created and replicated in the world will grow to almost 40 zettabytes (ZB)—more than 50 times what existed in 2010 and amounting to 5,247 gigabytes for every person on the planet (see <a href="http://www.datacenterjournal.com/birth-death-big-data/">http://www.datacenterjournal.com/birth-death-big-data/</a>).</p><p>This means that organizations have more and more unstructured and unused data that could contain valuable information for predicting business trends and making business decisions. <a href="http://www.forbes.com/sites/gilpress/2014/12/11/6-predictions-for-the-125-billion-big-data-analytics-market-in-2015/#ebfa5252b207">Forbes</a> predicted in 2015 that <em>Buying and selling data will become the new business bread and butter.</em></p><p>In the recent years the database research and development community has strengthened core research and development in relational DBMSs and branched out into new directions: security, privacy, data pricing, data attribution, social and mobile data, spatiotemporal data, personalization and contextualization, energy constrained processing, and scientific data management.</p><p>These lofty research challenges must be taken down on a functionality, if not even feature level. Here are some features we're working on, in a chewing-an-elephant-a-bite-at-a-time fashion.</p><ul><li>Security:<ul><li><a href="https://mariadb.com/kb/en/mariadb/data-at-rest-encryption/">Data at rest encryption for InnoDB system tablespace and user tablespaces, InnoDB log files InnoDB temporal files and binlog files. </a></li><li><a href="https://mariadb.com/kb/en/mariadb/password-validation/">MariaDB implements Password Validation Plugin API</a></li><li><a href="https://mariadb.com/kb/en/mariadb/simple_password_check/">MariaDB implements simple password validation plugin</a></li><li><a href="https://mariadb.com/kb/en/mariadb/simple_password_check/">MariaDB implements cracklib password check plugin</a></li><li><a href="https://mariadb.com/blog/recent-release-mariadb-10111-contains-two-new-authentication-plugins">New authentication plugins</a></li></ul></li><li>Personalization<ul><li><a href="https://mariadb.com/kb/en/mariadb/set-default-role/">Default roles</a></li></ul></li><li>Spatiotemporal data<ul><li>Support for Spatial Reference systems for the GIS data , new <code>REF_SYSTEM_ID</code> column attribute can be used to specify Spatial Reference System ID for columns of spatial data types.</li><li>More functions from the <a href="http://www.opengeospatial.org/">OGC</a> standard added<ul start="1"><li><a href="https://mariadb.com/kb/en/st_boundary/">ST_Boundary</a></li><li><a href="https://mariadb.com/kb/en/st_convexhull/">ST_ConvexHull</a></li><li><a href="https://mariadb.com/kb/en/st_isring/">ST_IsRing</a></li><li><a href="https://mariadb.com/kb/en/st_pointonsurface/">ST_PointOnSurface</a></li><li><a href="https://mariadb.com/kb/en/st_relate/">ST_Relate</a></li></ul></li></ul></li></ul><p>The review article identifies five big data challenges: scalable big/fast data infrastructures; coping with diversity in data management; end-to-end processing of data; cloud services; and the roles of the people in the data life cycle. Three of the challenges deal with the volume, velocity, and variety aspects of big data. The last two challenges deal with extending big data applications in the cloud and managing the involvement of people in these applications.</p><p>How can MariaDB address these challenges? By developing new storage engines for Big Data like <a href="http://www.cio.com/article/3051146/analytics/mariadb-targets-big-data-analytics-market-with-columnstore.html">MariaDB ColumnStore</a> (earlier <a href="https://en.wikipedia.org/wiki/InfiniDB">InfiniDB</a>). MariaDB ColumnStore is a scalable columnar database management system built for big data analytics, business intelligence, data warehousing and other read-intensive application. Column-store architecture enables very quick load and query times. Its massive parallel processing (MPP) technology scales with any type of storage hardware.</p><p>Furthermore, MariaDB can support new datatypes and SQL-functions like:</p><ul><li><a href="https://mariadb.com/kb/en/mariadb/window-functions/">Window functions</a>: First drop in MariaDB 10.2.0.</li><li><a href="https://jira.mariadb.org/browse/MDEV-9056">JSON</a></li></ul><p><a href="http://www.forbes.com/sites/gilpress/2014/12/11/6-predictions-for-the-125-billion-big-data-analytics-market-in-2015/#ebfa5252b207">Forbes</a> predicted on 2015 that <em>Security will become the killer app for big data analytics. </em>Now that MariaDB 10.1 server provides tools for data at rest encryption, other storage engines can easily provide security feature for their data.</p><p>However, as seen from research challenges there is a lot of room for additional vision and development on relational database management systems like MariaDB.</p></div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/big-data" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Big Data</a></div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995266&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995266&vote=-1&apivote=1">Vote DOWN</a>";}i:22;a:9:{s:5:"title";s:72:"Webinar tomorrow: Become a MongoDB DBA (if you’re really a MySQL user)";s:4:"guid";s:31:"4677 at http://severalnines.com";s:4:"link";s:92:"http://severalnines.com/blog/webinar-tomorrow-become-mongodb-dba-if-you-re-really-mysql-user";s:11:"description";s:2110:"Join us tomorrow, as we introduce a new webinar series: ‘How to Become a MongoDB DBA’ to answer the question: ‘what does a MongoDB DBA do?’.
This is a series of three webinars during which we will walk you through the most important tasks a MongoDB DBA routinely goes through and provide you with options on how to best complete these tasks.
It is not uncommon for MySQL DBAs, developers, network/system administrators or DevOps folks with general background to find themselves in a situation where they’ve been working with MySQL for a while and are now being asked to also properly maintain one or more MongoDB instances. In fact, with more organisations operating polyglot environments, it’s starting to become commonplace.
In this first webinar, we will show you how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations.
Date, Time &amp; Registration
Europe/MEA/APAC
Tuesday, April 26th at 09:00 BST / 10:00 CEST (Germany, France, Sweden)Register Now
North America/LatAm
Tuesday, April 26th at 09:00 Pacific Time (US) / 12:00 Eastern Time (US)Register Now
Agenda
Introduction to becoming a MongoDB DBA
Installing &amp; configuring MongoDB
What to monitor and how
How to perform backups
Live Demo
Speaker



Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.
We look forward to “seeing” you there!
This session is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘Become a ClusterControl DBA’ blog series.
Tags: MongoDBMySQLdbadatabase management";s:7:"content";a:1:{s:7:"encoded";s:3606:"<div><div><div property="content:encoded"><p>Join us tomorrow, as we introduce a new webinar series: ‘<a href="http://severalnines.com/upcoming-webinars">How to Become a MongoDB DBA</a>’ to answer the question: ‘what does a MongoDB DBA do?’.</p>
<p>This is a series of three webinars during which we will walk you through the most important tasks a MongoDB DBA routinely goes through and provide you with options on how to best complete these tasks.</p>
<p>It is not uncommon for MySQL DBAs, developers, network/system administrators or DevOps folks with general background to find themselves in a situation where they’ve been working with MySQL for a while and are now being asked to also properly maintain one or more MongoDB instances. In fact, with more organisations operating polyglot environments, it’s starting to become commonplace.</p>
<p>In this first webinar, we will show you how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations.</p>
<h2>Date, Time &amp; Registration</h2>
<h3>Europe/MEA/APAC</h3>
<p>Tuesday, April 26th at 09:00 BST / 10:00 CEST (Germany, France, Sweden)<br /><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user">Register Now</a></p>
<h3>North America/LatAm</h3>
<p>Tuesday, April 26th at 09:00 Pacific Time (US) / 12:00 Eastern Time (US)<br /><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user-0">Register Now</a></p>
<h2>Agenda</h2>
<ul><li>Introduction to becoming a MongoDB DBA</li>
<li>Installing &amp; configuring MongoDB</li>
<li>What to monitor and how</li>
<li>How to perform backups</li>
<li>Live Demo</li>
</ul><h2>Speaker</h2>
<div>
<div><img src="http://severalnines.com/sites/default/files/mail/speakers/art_van_scheppingen.png" /></div>
</div>
<p>Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.</p>
<p>We look forward to “seeing” you there!</p>
<p>This session is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘<a href="http://severalnines.com/blog/become-clustercontrol-dba-deploying-your-databases-and-clusters">Become a ClusterControl DBA</a>’ blog series.</p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/dba" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">dba</a></li><li><a href="http://severalnines.com/blog-tags/database-management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">database management</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995267&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995267&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Mon, 25 Apr 2016 11:42:26 +0000";s:2:"dc";a:1:{s:7:"creator";s:12:"Severalnines";}s:7:"summary";s:2110:"Join us tomorrow, as we introduce a new webinar series: ‘How to Become a MongoDB DBA’ to answer the question: ‘what does a MongoDB DBA do?’.
This is a series of three webinars during which we will walk you through the most important tasks a MongoDB DBA routinely goes through and provide you with options on how to best complete these tasks.
It is not uncommon for MySQL DBAs, developers, network/system administrators or DevOps folks with general background to find themselves in a situation where they’ve been working with MySQL for a while and are now being asked to also properly maintain one or more MongoDB instances. In fact, with more organisations operating polyglot environments, it’s starting to become commonplace.
In this first webinar, we will show you how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations.
Date, Time &amp; Registration
Europe/MEA/APAC
Tuesday, April 26th at 09:00 BST / 10:00 CEST (Germany, France, Sweden)Register Now
North America/LatAm
Tuesday, April 26th at 09:00 Pacific Time (US) / 12:00 Eastern Time (US)Register Now
Agenda
Introduction to becoming a MongoDB DBA
Installing &amp; configuring MongoDB
What to monitor and how
How to perform backups
Live Demo
Speaker



Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.
We look forward to “seeing” you there!
This session is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘Become a ClusterControl DBA’ blog series.
Tags: MongoDBMySQLdbadatabase management";s:12:"atom_content";s:3606:"<div><div><div property="content:encoded"><p>Join us tomorrow, as we introduce a new webinar series: ‘<a href="http://severalnines.com/upcoming-webinars">How to Become a MongoDB DBA</a>’ to answer the question: ‘what does a MongoDB DBA do?’.</p>
<p>This is a series of three webinars during which we will walk you through the most important tasks a MongoDB DBA routinely goes through and provide you with options on how to best complete these tasks.</p>
<p>It is not uncommon for MySQL DBAs, developers, network/system administrators or DevOps folks with general background to find themselves in a situation where they’ve been working with MySQL for a while and are now being asked to also properly maintain one or more MongoDB instances. In fact, with more organisations operating polyglot environments, it’s starting to become commonplace.</p>
<p>In this first webinar, we will show you how you can automate tasks, monitor a cluster and manage MongoDB; whilst also automating and managing your MySQL and/or PostgreSQL installations.</p>
<h2>Date, Time &amp; Registration</h2>
<h3>Europe/MEA/APAC</h3>
<p>Tuesday, April 26th at 09:00 BST / 10:00 CEST (Germany, France, Sweden)<br /><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user">Register Now</a></p>
<h3>North America/LatAm</h3>
<p>Tuesday, April 26th at 09:00 Pacific Time (US) / 12:00 Eastern Time (US)<br /><a href="http://severalnines.com/webinars/become-mongodb-dba-if-you-re-really-mysql-user-0">Register Now</a></p>
<h2>Agenda</h2>
<ul><li>Introduction to becoming a MongoDB DBA</li>
<li>Installing &amp; configuring MongoDB</li>
<li>What to monitor and how</li>
<li>How to perform backups</li>
<li>Live Demo</li>
</ul><h2>Speaker</h2>
<div>
<div><img src="http://severalnines.com/sites/default/files/mail/speakers/art_van_scheppingen.png" /></div>
</div>
<p>Art van Scheppingen is a Senior Support Engineer at Severalnines. He’s a pragmatic MySQL and Database expert with over 15 years experience in web development. He previously worked at Spil Games as Head of Database Engineering, where he kept a broad vision upon the whole database environment: from MySQL to Couchbase, Vertica to Hadoop and from Sphinx Search to SOLR. He regularly presents his work and projects at various conferences (Percona Live, FOSDEM) and related meetups.</p>
<p>We look forward to “seeing” you there!</p>
<p>This session is based upon the experience we have using MongoDB and implementing it for our database infrastructure management solution, ClusterControl. For more details, read through our ‘<a href="http://severalnines.com/blog/become-clustercontrol-dba-deploying-your-databases-and-clusters">Become a ClusterControl DBA</a>’ blog series.</p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mongodb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MongoDB</a></li><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/dba" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">dba</a></li><li><a href="http://severalnines.com/blog-tags/database-management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">database management</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995267&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995267&vote=-1&apivote=1">Vote DOWN</a>";}i:23;a:9:{s:5:"title";s:62:"AliSQL and some features that have made it into MariaDB Server";s:4:"guid";s:26:"2534 at http://mariadb.com";s:4:"link";s:76:"http://mariadb.com/blog/alisql-and-some-features-have-made-it-mariadb-server";s:11:"description";s:4589:"Mon, 2016-04-25 06:42colinAliSQL is the fork of MySQL (maintained by Alibaba) that powers all of the group from Tmall, Taobao, Alibaba.com, 1688.com, AliExpress, Cainiao and the payment's network Alipay. 

AliSQL isn't new - they've had this fork since 2011: AliSQL 5.1 (Bugfixes for DDL, eliminate race conditions), 2012: AliSQL 5.5 (parallel replication, optimize hot SKUs), 2014: AliSQL 5.6 (Enhanced threadpool, SQL firewall). The team have found/reported/fixed over 40+ bugs, added 41 new features, and optimised 27 bottlenecks. All recent releases of MySQL have had AliSQL contributions (5.6.25 onwards, 5.7.8 onwards, and even 5.8 branch).

AliSQL benefitting MariaDB Server

While not mentioned in the talk, its clear that MariaDB Server 10.0+ has benefited from their tree as well -- our log of contributions lists Lixun Peng/Taobao as having contributed to multi-source replication (something MariaDB Server had in a shipping GA release since March 2014; a similar feature appears in MySQL 5.7 GA released October 2015). But that is not all, there is also per-thread memory counting and usage, fixed in MDEV-4011. Don't forget that they've constantly released many of their features as opensource.

So what is per-thread memory counting? It is exposed either via SHOW STATUS:

show status like 'memory_used'\G
*************************** 1. row ***************************
Variable_name: Memory_used
        Value: 73808
1 row in set (0.00 sec)


But that is obviously not all. Have you seen the INFORMATION_SCHEMA.PROCESSLIST? Compare the difference when it comes to MySQL 5.7.12 and MariaDB Server 10.1.13:

MySQL 5.7.12

select * from INFORMATION_SCHEMA.processlist\G
*************************** 1. row ***************************
     ID: 8
   USER: msandbox
   HOST: localhost
     DB: NULL
COMMAND: Query
   TIME: 0
  STATE: executing
   INFO: select * from INFORMATION_SCHEMA.processlist
1 row in set (0.00 sec)


MariaDB Server 10.1.13

select * from INFORMATION_SCHEMA.processlist\G
*************************** 1. row ***************************
           ID: 4
         USER: msandbox
         HOST: localhost
           DB: NULL
      COMMAND: Query
         TIME: 0
        STATE: Filling schema table
         INFO: select * from INFORMATION_SCHEMA.processlist
      TIME_MS: 0.464
        STAGE: 0
    MAX_STAGE: 0
     PROGRESS: 0.000
  MEMORY_USED: 84552
EXAMINED_ROWS: 0
     QUERY_ID: 24
  INFO_BINARY: select * from INFORMATION_SCHEMA.processlist
          TID: 22005
1 row in set (0.00 sec)


What can we look forward to in the future from AliSQL?

It was brought up in the talk that AliSQL is not currently opensource, but there are some very interesting features around it that would likely benefit many MariaDB Server users:

optimising hot rows, which is a common issue in Alibaba referred to as an "optimisation for hot SKUs", typical during a Single's Day. This has been powering AliSQL in several versions, and they are working on ensuring there is a row cache, a new InnoDB row lock type, as well as group updates of associated transactions.
They have split the redo log buffers into two, one for each reading and writing.
InnoDB column compression using ZLIB, which also supports online DDL.
The idea of flashback and a recycle bin, all related to the "time machine" concept that we can expect to see soon.
Another interesting feature is the idea of thinking in terms of a glass of water. There is low water mark protection in where they have an enhanced threadpool that buffers queries while the thread is running. If for some reason it exceeds the the low water level, send it to the high water mark where they have a kind of "SQL firewall" that denies queries via a blacklist strategy. Their rule syntax makes use of an Abstract Syntax Tree (AST) between the parser and optimiser of their server.
They also do binlog speed throttling, and have enhanced information around deadlocks.
All in, here's hoping that the AliSQL tree is open, so that we can look at features and see what ends up in a future release of MariaDB Server.Tags:&nbsp;CommunityMariaDB ReleasesMySQL
About the Author
  
      


 
Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.



";s:7:"content";a:1:{s:7:"encoded";s:6777:"<div><div><div>Mon, 2016-04-25 06:42</div></div></div><div><div><div>colin</div></div></div><div><div><div property="content:encoded"><p>AliSQL is the fork of MySQL (maintained by Alibaba) that powers all of the group from Tmall, Taobao, Alibaba.com, <a href="http://1688.com" target="_blank">1688.com</a>, AliExpress, Cainiao and the payment's network Alipay. </p>

<p>AliSQL isn't new - they've had this fork since 2011: AliSQL 5.1 (Bugfixes for DDL, eliminate race conditions), 2012: AliSQL 5.5 (parallel replication, optimize hot SKUs), 2014: AliSQL 5.6 (Enhanced threadpool, SQL firewall). The team have found/reported/fixed over 40+ bugs, added 41 new features, and optimised 27 bottlenecks. All recent releases of MySQL have had AliSQL contributions (5.6.25 onwards, 5.7.8 onwards, and even 5.8 branch).</p>

<h2>AliSQL benefitting MariaDB Server</h2>

<p>While not mentioned in the talk, its clear that MariaDB Server 10.0+ has benefited from their tree as well -- our <a href="https://mariadb.com/kb/en/mariadb/log-of-mariadb-contributions/" target="_blank">log of contributions</a> lists Lixun Peng/Taobao as having contributed to <a href="https://mariadb.com/kb/en/mariadb/multi-source-replication/" target="_blank">multi-source replication</a> (something MariaDB Server had in a shipping GA release since March 2014; a similar feature appears in MySQL 5.7 GA released October 2015). But that is not all, there is also per-thread memory counting and usage, fixed in <a href="https://jira.mariadb.org/browse/MDEV-4011" target="_blank">MDEV-4011</a>. Don't forget that they've constantly released many of their <a href="http://mysql.taobao.org/index.php?title=Patch_source_code" target="_blank">features as opensource</a>.</p>

<p>So what is per-thread memory counting? It is exposed either via SHOW STATUS:</p>

<pre><code>show status like 'memory_used'\G
*************************** 1. row ***************************
Variable_name: Memory_used
        Value: 73808
1 row in set (0.00 sec)
</code></pre>

<p>But that is obviously not all. Have you seen the <code>INFORMATION_SCHEMA.PROCESSLIST</code><wbr>? Compare the difference when it comes to MySQL 5.7.12 and MariaDB Server 10.1.13:</wbr></p>

<h3>MySQL 5.7.12</h3>

<pre><code>select * from INFORMATION_SCHEMA.<wbr>processlist\G
*************************** 1. row ***************************
     ID: 8
   USER: msandbox
   HOST: localhost
     DB: NULL
COMMAND: Query
   TIME: 0
  STATE: executing
   INFO: select * from INFORMATION_SCHEMA.processlist
1 row in set (0.00 sec)
</wbr></code></pre>

<h3>MariaDB Server 10.1.13</h3>

<pre><code>select * from INFORMATION_SCHEMA.<wbr>processlist\G
*************************** 1. row ***************************
           ID: 4
         USER: msandbox
         HOST: localhost
           DB: NULL
      COMMAND: Query
         TIME: 0
        STATE: Filling schema table
         INFO: select * from INFORMATION_SCHEMA.processlist
      TIME_MS: 0.464
        STAGE: 0
    MAX_STAGE: 0
     PROGRESS: 0.000
  MEMORY_USED: 84552
EXAMINED_ROWS: 0
     QUERY_ID: 24
  INFO_BINARY: select * from INFORMATION_SCHEMA.processlist
          TID: 22005
1 row in set (0.00 sec)
</wbr></code></pre>

<h2>What can we look forward to in the future from AliSQL?</h2>

<p>It was brought up in the talk that AliSQL is not currently opensource, but there are some very interesting features around it that would likely benefit many MariaDB Server users:</p>

<ul><li>optimising hot rows, which is a common issue in Alibaba referred to as an "optimisation for hot SKUs", typical during a Single's Day. This has been powering AliSQL in several versions, and they are working on ensuring there is a row cache, a new InnoDB row lock type, as well as group updates of associated transactions.</li>
<li>They have split the redo log buffers into two, one for each reading and writing.</li>
<li>InnoDB column compression using ZLIB, which also supports online DDL.</li>
<li>The idea of flashback and a recycle bin, all related to the <a href="https://drive.google.com/open?id=0B7HE7L1OgKJFeGFISElYODh2NEk" target="_blank">"time machine"</a> concept that we can <a href="http://www.bytebot.net/blog/archives/2016/04/14/mariadb-berlin-meetup-notes-slides" target="_blank">expect to see soon</a>.</li>
<li>Another interesting feature is the idea of thinking in terms of a glass of water. There is low water mark protection in where they have an enhanced threadpool that buffers queries while the thread is running. If for some reason it exceeds the the low water level, send it to the high water mark where they have a kind of "SQL firewall" that denies queries via a blacklist strategy. Their rule syntax makes use of an Abstract Syntax Tree (AST) between the parser and optimiser of their server.</li>
<li>They also do binlog speed throttling, and have enhanced information around deadlocks.</li>
</ul><p>All in, here's hoping that the AliSQL tree is open, so that we can look at features and see what ends up in a future release of MariaDB Server.</p></div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/community" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Community</a></div><div><a href="http://mariadb.com/blog-tags/mariadb-releases" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Releases</a></div><div><a href="http://mariadb.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-45-1402568369.jpg?itok=nBbL59Ou" width="72" height="72" alt="colin's picture" title="colin's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995261&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995261&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Mon, 25 Apr 2016 06:42:19 +0000";s:2:"dc";a:1:{s:7:"creator";s:7:"MariaDB";}s:7:"summary";s:4589:"Mon, 2016-04-25 06:42colinAliSQL is the fork of MySQL (maintained by Alibaba) that powers all of the group from Tmall, Taobao, Alibaba.com, 1688.com, AliExpress, Cainiao and the payment's network Alipay. 

AliSQL isn't new - they've had this fork since 2011: AliSQL 5.1 (Bugfixes for DDL, eliminate race conditions), 2012: AliSQL 5.5 (parallel replication, optimize hot SKUs), 2014: AliSQL 5.6 (Enhanced threadpool, SQL firewall). The team have found/reported/fixed over 40+ bugs, added 41 new features, and optimised 27 bottlenecks. All recent releases of MySQL have had AliSQL contributions (5.6.25 onwards, 5.7.8 onwards, and even 5.8 branch).

AliSQL benefitting MariaDB Server

While not mentioned in the talk, its clear that MariaDB Server 10.0+ has benefited from their tree as well -- our log of contributions lists Lixun Peng/Taobao as having contributed to multi-source replication (something MariaDB Server had in a shipping GA release since March 2014; a similar feature appears in MySQL 5.7 GA released October 2015). But that is not all, there is also per-thread memory counting and usage, fixed in MDEV-4011. Don't forget that they've constantly released many of their features as opensource.

So what is per-thread memory counting? It is exposed either via SHOW STATUS:

show status like 'memory_used'\G
*************************** 1. row ***************************
Variable_name: Memory_used
        Value: 73808
1 row in set (0.00 sec)


But that is obviously not all. Have you seen the INFORMATION_SCHEMA.PROCESSLIST? Compare the difference when it comes to MySQL 5.7.12 and MariaDB Server 10.1.13:

MySQL 5.7.12

select * from INFORMATION_SCHEMA.processlist\G
*************************** 1. row ***************************
     ID: 8
   USER: msandbox
   HOST: localhost
     DB: NULL
COMMAND: Query
   TIME: 0
  STATE: executing
   INFO: select * from INFORMATION_SCHEMA.processlist
1 row in set (0.00 sec)


MariaDB Server 10.1.13

select * from INFORMATION_SCHEMA.processlist\G
*************************** 1. row ***************************
           ID: 4
         USER: msandbox
         HOST: localhost
           DB: NULL
      COMMAND: Query
         TIME: 0
        STATE: Filling schema table
         INFO: select * from INFORMATION_SCHEMA.processlist
      TIME_MS: 0.464
        STAGE: 0
    MAX_STAGE: 0
     PROGRESS: 0.000
  MEMORY_USED: 84552
EXAMINED_ROWS: 0
     QUERY_ID: 24
  INFO_BINARY: select * from INFORMATION_SCHEMA.processlist
          TID: 22005
1 row in set (0.00 sec)


What can we look forward to in the future from AliSQL?

It was brought up in the talk that AliSQL is not currently opensource, but there are some very interesting features around it that would likely benefit many MariaDB Server users:

optimising hot rows, which is a common issue in Alibaba referred to as an "optimisation for hot SKUs", typical during a Single's Day. This has been powering AliSQL in several versions, and they are working on ensuring there is a row cache, a new InnoDB row lock type, as well as group updates of associated transactions.
They have split the redo log buffers into two, one for each reading and writing.
InnoDB column compression using ZLIB, which also supports online DDL.
The idea of flashback and a recycle bin, all related to the "time machine" concept that we can expect to see soon.
Another interesting feature is the idea of thinking in terms of a glass of water. There is low water mark protection in where they have an enhanced threadpool that buffers queries while the thread is running. If for some reason it exceeds the the low water level, send it to the high water mark where they have a kind of "SQL firewall" that denies queries via a blacklist strategy. Their rule syntax makes use of an Abstract Syntax Tree (AST) between the parser and optimiser of their server.
They also do binlog speed throttling, and have enhanced information around deadlocks.
All in, here's hoping that the AliSQL tree is open, so that we can look at features and see what ends up in a future release of MariaDB Server.Tags:&nbsp;CommunityMariaDB ReleasesMySQL
About the Author
  
      


 
Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.



";s:12:"atom_content";s:6777:"<div><div><div>Mon, 2016-04-25 06:42</div></div></div><div><div><div>colin</div></div></div><div><div><div property="content:encoded"><p>AliSQL is the fork of MySQL (maintained by Alibaba) that powers all of the group from Tmall, Taobao, Alibaba.com, <a href="http://1688.com" target="_blank">1688.com</a>, AliExpress, Cainiao and the payment's network Alipay. </p>

<p>AliSQL isn't new - they've had this fork since 2011: AliSQL 5.1 (Bugfixes for DDL, eliminate race conditions), 2012: AliSQL 5.5 (parallel replication, optimize hot SKUs), 2014: AliSQL 5.6 (Enhanced threadpool, SQL firewall). The team have found/reported/fixed over 40+ bugs, added 41 new features, and optimised 27 bottlenecks. All recent releases of MySQL have had AliSQL contributions (5.6.25 onwards, 5.7.8 onwards, and even 5.8 branch).</p>

<h2>AliSQL benefitting MariaDB Server</h2>

<p>While not mentioned in the talk, its clear that MariaDB Server 10.0+ has benefited from their tree as well -- our <a href="https://mariadb.com/kb/en/mariadb/log-of-mariadb-contributions/" target="_blank">log of contributions</a> lists Lixun Peng/Taobao as having contributed to <a href="https://mariadb.com/kb/en/mariadb/multi-source-replication/" target="_blank">multi-source replication</a> (something MariaDB Server had in a shipping GA release since March 2014; a similar feature appears in MySQL 5.7 GA released October 2015). But that is not all, there is also per-thread memory counting and usage, fixed in <a href="https://jira.mariadb.org/browse/MDEV-4011" target="_blank">MDEV-4011</a>. Don't forget that they've constantly released many of their <a href="http://mysql.taobao.org/index.php?title=Patch_source_code" target="_blank">features as opensource</a>.</p>

<p>So what is per-thread memory counting? It is exposed either via SHOW STATUS:</p>

<pre><code>show status like 'memory_used'\G
*************************** 1. row ***************************
Variable_name: Memory_used
        Value: 73808
1 row in set (0.00 sec)
</code></pre>

<p>But that is obviously not all. Have you seen the <code>INFORMATION_SCHEMA.PROCESSLIST</code><wbr>? Compare the difference when it comes to MySQL 5.7.12 and MariaDB Server 10.1.13:</wbr></p>

<h3>MySQL 5.7.12</h3>

<pre><code>select * from INFORMATION_SCHEMA.<wbr>processlist\G
*************************** 1. row ***************************
     ID: 8
   USER: msandbox
   HOST: localhost
     DB: NULL
COMMAND: Query
   TIME: 0
  STATE: executing
   INFO: select * from INFORMATION_SCHEMA.processlist
1 row in set (0.00 sec)
</wbr></code></pre>

<h3>MariaDB Server 10.1.13</h3>

<pre><code>select * from INFORMATION_SCHEMA.<wbr>processlist\G
*************************** 1. row ***************************
           ID: 4
         USER: msandbox
         HOST: localhost
           DB: NULL
      COMMAND: Query
         TIME: 0
        STATE: Filling schema table
         INFO: select * from INFORMATION_SCHEMA.processlist
      TIME_MS: 0.464
        STAGE: 0
    MAX_STAGE: 0
     PROGRESS: 0.000
  MEMORY_USED: 84552
EXAMINED_ROWS: 0
     QUERY_ID: 24
  INFO_BINARY: select * from INFORMATION_SCHEMA.processlist
          TID: 22005
1 row in set (0.00 sec)
</wbr></code></pre>

<h2>What can we look forward to in the future from AliSQL?</h2>

<p>It was brought up in the talk that AliSQL is not currently opensource, but there are some very interesting features around it that would likely benefit many MariaDB Server users:</p>

<ul><li>optimising hot rows, which is a common issue in Alibaba referred to as an "optimisation for hot SKUs", typical during a Single's Day. This has been powering AliSQL in several versions, and they are working on ensuring there is a row cache, a new InnoDB row lock type, as well as group updates of associated transactions.</li>
<li>They have split the redo log buffers into two, one for each reading and writing.</li>
<li>InnoDB column compression using ZLIB, which also supports online DDL.</li>
<li>The idea of flashback and a recycle bin, all related to the <a href="https://drive.google.com/open?id=0B7HE7L1OgKJFeGFISElYODh2NEk" target="_blank">"time machine"</a> concept that we can <a href="http://www.bytebot.net/blog/archives/2016/04/14/mariadb-berlin-meetup-notes-slides" target="_blank">expect to see soon</a>.</li>
<li>Another interesting feature is the idea of thinking in terms of a glass of water. There is low water mark protection in where they have an enhanced threadpool that buffers queries while the thread is running. If for some reason it exceeds the the low water level, send it to the high water mark where they have a kind of "SQL firewall" that denies queries via a blacklist strategy. Their rule syntax makes use of an Abstract Syntax Tree (AST) between the parser and optimiser of their server.</li>
<li>They also do binlog speed throttling, and have enhanced information around deadlocks.</li>
</ul><p>All in, here's hoping that the AliSQL tree is open, so that we can look at features and see what ends up in a future release of MariaDB Server.</p></div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/community" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Community</a></div><div><a href="http://mariadb.com/blog-tags/mariadb-releases" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB Releases</a></div><div><a href="http://mariadb.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-45-1402568369.jpg?itok=nBbL59Ou" width="72" height="72" alt="colin's picture" title="colin's picture" />  </div>
</div>
<div>
<div> </div>
<div><p>Colin Charles is the Chief Evangelist for MariaDB since 2009, work ranging from speaking engagements to consultancy and engineering works around MariaDB. He lives in Kuala Lumpur, Malaysia and had worked at MySQL since 2005, and been a MySQL user since 2000. Before joining MySQL, he worked actively on the Fedora and OpenOffice.org projects. He's well known on the conference track having spoken at many of them over the course of his career.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995261&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995261&vote=-1&apivote=1">Vote DOWN</a>";}i:24;a:10:{s:5:"title";s:86:"Building MariaDB 10.1.x and Galera from Source for Multiple Node Cluster Testing Setup";s:4:"guid";s:69:"tag:blogger.com,1999:blog-3080615211468083537.post-107371826280557214";s:4:"link";s:88:"http://mysqlentomologist.blogspot.com/2016/04/building-mariadb-101x-and-galera-from.html";s:11:"description";s:17921:"My Facebook followers probably noted that I quit from Percona some time ago and work for MariaDB since March 1, 2016. I changed the company, but neither the job role (I am still a Support Engineer), nor the approach to do my job. I still prefer to test everything I suggest to customers and I usually use software I build from source myself for these tests.While I try to avoid all kinds of clusters as much as possible for 15 years or so already (it does not matter if it's Oracle RAC, MySQL Cluster or Percona XtraDB Cluster, all of them), it's really hard to avoid Galera clusters while working for MariaDB. One of the reasons for this is that Galera, starting from MariaDB 10.1, can be easily "enabled"/used with any MariaDB 10.1.x instance, any time (at least when we speak about official binaries or those properly built - they are all "Galera ready"). Most of MariaDB customers do use Galera or can try to use it any time, so I have to be ready to test something Galera-specific any moment.For simple cases I decided to use a setup with several (2 to begin with) cluster nodes on one box. This approach is described in the manual for Percona XtraDB Cluster and was also used by my former colleague Fernando Laudares for his blog post and many real life related tests.So, I decided to proceed with the mix of ideas from the sources above and MariaDB's KB article on building Galera from source. As I decided to do this on my wife's Fedora 23 workstation, I checked this KB article for some details also. It lists prerequisites (boost-devel check-devel glibc-devel openssl-devel scons) and some of these packages (like scons in one of my cases) could be missing even on a system previosly used for builds for all kinds of MySQL related software. You can find something missing and fix the problem at later stage, but reading and following the manual or KB articles may help to save some time otherwise spent on trial and error.I've started with making directories in my home directory (/home/openxs) for this Galera related testing setup, like these: [openxs@fc23 ~]$ mkdir galera[openxs@fc23 ~]$ cd galera[openxs@fc23 galera]$ mkdir node1[openxs@fc23 galera]$ mkdir node2[openxs@fc23 galera]$ mkdir node3[openxs@fc23 galera]$ lsnode1&nbsp; node2&nbsp; node3I plan to use 3 nodes one day, but for this blog post I'll set up only 2, to have the smallest possible and simplest cluster as a proof of concept.Then I proceeded with cloning Galera from Codership's GitHub (this is supposed to be the latest and greatest). I changed current directory to my usual git repository and executed git clone https://github.com/codership/galera.git. When this command completed I've got a subdirectory named galera.In that directory, assuming that all prerequisites are installed, to build current Galera library version it's enough to execute simple script while in galera directory, ./scripts/build.sh. I ended up with the following:[openxs@fc23 galera]$ ls -l libgalera_smm.so-rwxrwxr-x. 1 openxs openxs 40204824 Mar 31 12:21 libgalera_smm.so[openxs@fc23 galera]$ file libgalera_smm.solibgalera_smm.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, BuildID[sha1]=11457fa9fd69dabe617708c0dd288b218255a886, not stripped[openxs@fc23 galera]$ pwd/home/openxs/git/galera[openxs@fc23 galera]$ cp libgalera_smm.so ~/galera/and copied the library to the target directory for my testing setup (that should NOT conflict with whatever software I may have installed later from packages).Now, time to build MariaDB properly to let it use Galera if needed. I already had recent (at the moment) 10.1.13 in the server subdirectory of my git repository. I've executed the following commands then:[openxs@fc23 server]$ cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON -DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1-- Running cmake version 3.4.1-- MariaDB 10.1.13...[openxs@fc23 server]$ time make -j 4...real&nbsp;&nbsp;&nbsp; 9m28.164suser&nbsp;&nbsp;&nbsp; 32m43.960ssys&nbsp;&nbsp;&nbsp;&nbsp; 2m45.637sThis was my usual command line to build MariaDB 10.x with only 2 extra options added: -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON.After make completed, I've executed make install &amp;&amp; make clean and was ready to use my shiny new Galera-ready MariaDB 10.1.13.To take into account the directories I am going to use for my cluster nodes and make sure they can start and communicate as separate mysqld instances, I have to create configuration files for them. I've changed working directory to /home/openxs/dbs/mariadb10.1 and started with this configuration file for the first node:[openxs@fc23 maria10.1]$ cat /home/openxs/galera/mynode1.cnf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[mysqld]datadir=/home/openxs/galera/node1port=3306socket=/tmp/mysql-node1.sockpid-file=/tmp/mysql-node1.pidlog-error=/tmp/mysql-node1.errbinlog_format=ROWinnodb_autoinc_lock_mode=2wsrep_on=ON # this is important for 10.1!wsrep_provider=/home/openxs/galera/libgalera_smm.sowsrep_cluster_name = singleboxwsrep_node_name = node1# wsrep_cluster_address=gcomm://wsrep_cluster_address=gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=noIt's one of the shortest possible. I had to specify unique datadir, error log location, pid file, port and socket for the instance, set binlog format and point out Galera library location, set cluster name and node name. With proper planning I was able to specify wsrep_cluster_address referring to all other nodes properly, but for initial setup of the first node I can have it "empty" as commented out in the above, so that we start as a new cluster node. There is one essential setting for MariaDB 10.1.x that is not needed for "cluster-specific" instances like Percona XtraDB Cluster or older 10.0.x Galera packages from MariaDB (where it's ON by default). This is wsrep_on=ON. Without it MariaDB works as normal, non-cluster instance and ignores anything cluster-related. You can save a lot of time in case of upgrade to 10.1.x if you put it in your configuration file explicitly right now, no matter what the version is used.Then I copied and modified configuration file for the second node:[openxs@fc23 maria10.1]$ cp /home/openxs/galera/mynode1.cnf /home/openxs/galera/mynode2.cnf[openxs@fc23 maria10.1]$ vi /home/openxs/galera/mynode2.cnf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[openxs@fc23 maria10.1]$ cat /home/openxs/galera/mynode2.cnf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[mysqld]datadir=/home/openxs/galera/node2port=3307socket=/tmp/mysql-node2.sockpid-file=/tmp/mysql-node2.pidlog-error=/tmp/mysql-node2.errbinlog_format=ROWinnodb_autoinc_lock_mode=2wsrep_on=ON # this is important for 10.1!wsrep_provider=/home/openxs/galera/libgalera_smm.sowsrep_cluster_name = singleboxwsrep_node_name = node2wsrep_cluster_address=gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=nowsrep_provider_options = "base_port=5020;"Note that while Galera node uses 4 ports, I specified only 2 unique ones explicitly, port for MySQL clients and base port for all Galera-related communication like IST and SST, with base_port setting. Note also how I referred to all cluster nodes with wsrep_cluster_address - this same value can be used for the configuration file of the first node actually. We can just start it as the first node of a new cluster (see below).Now we have configuration files for 2 nodes ready (we can always add node3 later in the same way). But before starting new cluster we have to install system databases. For node1 it was performed in the following way:[openxs@fc23 maria10.1]$ scripts/mysql_install_db --defaults-file=/home/openxs/galera/mynode1.cnfInstalling MariaDB/MySQL system tables in '/home/openxs/galera/node1' ...2016-03-31 12:51:34 139766046820480 [Note] ./bin/mysqld (mysqld 10.1.13-MariaDB) starting as process 28297 ......[openxs@fc23 maria10.1]$ ls -l /home/openxs/galera/node1-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp; 16384 Mar 31 12:51 aria_log.00000001-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 52 Mar 31 12:51 aria_log_control-rw-rw----. 1 openxs openxs 12582912 Mar 31 12:51 ibdata1-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile0-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile1drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 mysqldrwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 performance_schemadrwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 testThen I started node1 as a new cluster: [openxs@fc23 maria10.1]$ bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode1.cnf --wsrep-new-cluster &amp;and created a table, t1, with some data in it. After that I repeated installation of system tables etc for node2, just referencing proper configuration file, and started node2 that was supposed to join the cluster:openxs@fc23 maria10.1]$ bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode2.cnf &amp;Let's check if we do have both instances running and communicating in Galera cluster:[openxs@fc23 maria10.1]$ tail /tmp/mysql-node2.err&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2016-03-31 13:40:29 139627414767744 [Note] WSREP: Signalling provider to continue.2016-03-31 13:40:29 139627414767744 [Note] WSREP: SST received: c91d17b6-f72b-11e5-95de-96e95167f593:02016-03-31 13:40:29 139627117668096 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINER -&gt; JOINED (TO: 0)2016-03-31 13:40:29 139627117668096 [Note] WSREP: Member 1.0 (node2) synced with group.2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)2016-03-31 13:40:29 139627414452992 [Note] WSREP: Synchronized with group, ready for connections2016-03-31 13:40:29 139627414452992 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.2016-03-31 13:40:29 139627414767744 [Note] /home/openxs/dbs/maria10.1/bin/mysqld: ready for connections.Version: '10.1.13-MariaDB'&nbsp; socket: '/tmp/mysql-node2.sock'&nbsp; port: 3307&nbsp; Source distribution[openxs@fc23 maria10.1]$ tail /tmp/mysql-node1.err2016-03-31 13:40:27 140071390934784 [Note] WSREP: Provider resumed.2016-03-31 13:40:27 140072133322496 [Note] WSREP: 0.0 (node1): State transfer to 1.0 (node2) complete.2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting DONOR/DESYNCED -&gt; JOINED (TO: 0)2016-03-31 13:40:27 140072133322496 [Note] WSREP: Member 0.0 (node1) synced with group.2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)2016-03-31 13:40:27 140072429247232 [Note] WSREP: Synchronized with group, ready for connections2016-03-31 13:40:27 140072429247232 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.2016-03-31 13:40:27 140072141715200 [Note] WSREP: (c91c99ec, 'tcp://0.0.0.0:4567') turning message relay requesting off2016-03-31 13:40:29 140072133322496 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.2016-03-31 13:40:29 140072133322496 [Note] WSREP: Member 1.0 (node2) synced with group. Familiar messages (unfortunately...) that prove we had a second node joined and performed state transfer from the first one. Now it's time to connect and test how cluster works. This is what I had after node1 started and table with some data created there, but before node2 started:[openxs@fc23 maria10.1]$ bin/mysql -uroot --socket=/tmp/mysql-node1.sockWelcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.Your MariaDB connection id is 5Server version: 10.1.13-MariaDB Source distributionCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; show variables like 'wsrep_cluster%';+-----------------------+-------------------------------------------------------+| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+-----------------------+-------------------------------------------------------+| wsrep_cluster_address | gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=no || wsrep_cluster_name&nbsp;&nbsp;&nbsp; | singlebox&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+-----------------------+-------------------------------------------------------+2 rows in set (0.00 sec)MariaDB [(none)]&gt; show status like 'wsrep_cluster%';+--------------------------+--------------------------------------+| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_state_uuid | c91d17b6-f72b-11e5-95de-96e95167f593 || wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | Primary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+4 rows in set (0.00 sec)MariaDB [(none)]&gt; use testReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [test]&gt; select * from t1;+----+------+| id | c1&nbsp;&nbsp; |+----+------+|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 ||&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |+----+------+2 rows in set (0.00 sec)Then, when node2 joined the cluster, I checked that the data we've added on node1 are there:[openxs@fc23 maria10.1]$ bin/mysql -uroot --socket=/tmp/mysql-node2.sockWelcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.Your MariaDB connection id is 4Server version: 10.1.13-MariaDB Source distributionCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; use testReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [test]&gt; select * from t1;+----+------+| id | c1&nbsp;&nbsp; |+----+------+|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 ||&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |+----+------+2 rows in set (0.00 sec)MariaDB [test]&gt; show status like 'wsrep_cluster%'; +--------------------------+--------------------------------------+| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_state_uuid | c91d17b6-f72b-11e5-95de-96e95167f593 || wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | Primary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+4 rows in set (0.01 sec)So, the first basic test with the Galera cluster of 2 nodes (both running on the same box) built from current source of Galera and MariaDB 10.1.x on Fedora 23 is completed successfully. I plan to play with it more in the future, use current xtrabackup built from source for SST and so on, and create blog posts about these steps and any interesting tests in this setup. Stay tuned.From the dates above you can conclude that it took me 3 weeks to publish this post. That's because I was busy with the company meeting in Berlin and some usual Support work, and was not sure is it really a good idea for me to write any post with "Galera" or "MariaDB" words used in it even once...";s:7:"content";a:1:{s:7:"encoded";s:21502:"<div><a href="https://4.bp.blogspot.com/-7DYRVE2TdUI/Vxzy7ZYTUNI/AAAAAAAAAJs/eTO5As2AXg0DwYOklXUMt_zRfmOB938DwCLcB/s1600/093.jpg" imageanchor="1"><img border="0" height="240" src="https://4.bp.blogspot.com/-7DYRVE2TdUI/Vxzy7ZYTUNI/AAAAAAAAAJs/eTO5As2AXg0DwYOklXUMt_zRfmOB938DwCLcB/s320/093.jpg" width="320" /></a></div>My <a href="https://www.facebook.com/valerii.kravchuk" target="_blank">Facebook</a> followers probably noted that I quit from Percona some time ago and work for MariaDB since March 1, 2016. I changed the company, but neither the job role (I am still a Support Engineer), nor the <a href="http://mysqlentomologist.blogspot.com/2016/01/im-winston-wolf-i-solve-problems.html" target="_blank">approach</a> to do my job. I still prefer to test everything I suggest to customers and I usually use software I build from source myself for these tests.<br /><br />While I try to avoid all kinds of clusters as much as possible for 15 years or so already (it does not matter if it's Oracle RAC, MySQL Cluster or Percona XtraDB Cluster, all of them), it's really hard to avoid Galera clusters while working for MariaDB. One of the reasons for this is that Galera, starting from MariaDB 10.1, can be easily "enabled"/used with any MariaDB 10.1.x instance, any time (at least when we speak about official binaries or those properly built - they are all "Galera ready"). Most of MariaDB customers do use Galera or can try to use it any time, so I have to be ready to test something Galera-specific any moment.<br /><br />For simple cases I decided to use a setup with several (2 to begin with) cluster nodes on one box. This approach is <a href="https://www.percona.com/doc/percona-xtradb-cluster/5.6/howtos/singlebox.html" target="_blank">described in the manual</a> for Percona XtraDB Cluster and was also used by my former colleague Fernando Laudares for his <a href="https://www.percona.com/blog/2014/10/21/percona-xtradb-cluster-how-to-run-a-2-node-cluster-on-a-single-server/" target="_blank">blog post</a> and many real life related tests.<br /><br />So, I decided to proceed with the mix of ideas from the sources above and MariaDB's KB article on <a href="https://mariadb.com/kb/en/mariadb/installating-galera-from-source/" target="_blank">building Galera from source</a>. As I decided to do this on my wife's Fedora 23 workstation, I checked <a href="https://mariadb.com/kb/en/mariadb/building-the-galera-wsrep-package-on-fedora/" target="_blank">this</a> KB article for some details also. It lists prerequisites (<b>boost-devel</b> <b>check-devel</b> <b>glibc-devel</b> <b>openssl-devel</b> <b>scons</b>) and some of these packages (like <b>scons</b> in one of my cases) could be missing even on a system previosly used for builds for all kinds of MySQL related software. You can find something missing and fix the problem at later stage, but reading and following the manual or KB articles may help to save some time otherwise spent on <a href="https://en.wikipedia.org/wiki/Trial_and_error" target="_blank">trial and error</a>.<br /><br />I've started with making directories in my home directory (<b>/home/openxs</b>) for this Galera related testing setup, like these: <br /><blockquote><span>[openxs@fc23 ~]$ <b>mkdir galera</b><br />[openxs@fc23 ~]$ <b>cd galera</b><br />[openxs@fc23 galera]$ <b>mkdir node1</b>[openxs@fc23 galera]$ <b>mkdir node2</b><br />[openxs@fc23 galera]$ <b>mkdir node3</b><br />[openxs@fc23 galera]$ <b>ls</b><br />node1&nbsp; node2&nbsp; node3</span></blockquote>I plan to use 3 nodes one day, but for this blog post I'll set up only 2, to have the smallest possible and simplest cluster as a proof of concept.<br /><br />Then I proceeded with cloning Galera from Codership's GitHub (this is supposed to be the latest and greatest). I changed current directory to my usual <b>git</b> repository and executed <b>git clone https://github.com/codership/galera.git</b>. When this command completed I've got a subdirectory named <b>galera</b>.<br /><br />In that directory, assuming that all prerequisites are installed, to build current Galera library version it's enough to execute simple script while in <b>galera</b> directory, <b>./scripts/build.sh</b>. I ended up with the following:<br /><blockquote><span>[openxs@fc23 galera]$ <b>ls -l libgalera_smm.so</b><br />-rwxrwxr-x. 1 openxs openxs 40204824 Mar 31 12:21 libgalera_smm.so<br />[openxs@fc23 galera]$ <b>file libgalera_smm.so</b><br />libgalera_smm.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, BuildID[sha1]=11457fa9fd69dabe617708c0dd288b218255a886, not stripped<br /><br />[openxs@fc23 galera]$ <b>pwd</b><br />/home/openxs/git/galera<br />[openxs@fc23 galera]$ <b>cp libgalera_smm.so ~/galera/</b></span></blockquote>and copied the library to the target directory for my testing setup (that should NOT conflict with whatever software I may have installed later from packages).<br /><br />Now, time to build MariaDB properly to let it use Galera if needed. I already had recent (at the moment) 10.1.13 in the <b>server</b> subdirectory of my git repository. I've executed the following commands then:<br /><br /><blockquote><span>[openxs@fc23 server]$ <b>cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON -DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1</b><br />-- Running cmake version 3.4.1<br />-- MariaDB 10.1.13<br />...<br /><br />[openxs@fc23 server]$ <b>time make -j 4</b>...<br />real&nbsp;&nbsp;&nbsp; 9m28.164s<br />user&nbsp;&nbsp;&nbsp; 32m43.960s<br />sys&nbsp;&nbsp;&nbsp;&nbsp; 2m45.637s</span></blockquote>This was my usual command line to build MariaDB 10.x with only 2 extra options added: <span><b>-DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON</b>.</span>After make completed, I've executed <b>make install &amp;&amp; make clean</b> and was ready to use my shiny new Galera-ready MariaDB 10.1.13.<br /><br />To take into account the directories I am going to use for my cluster nodes and make sure they can start and communicate as separate mysqld instances, I have to create configuration files for them. I've changed working directory to <b>/home/openxs/dbs/mariadb10.1</b> and started with this configuration file for the first node:<br /><br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>cat /home/openxs/galera/mynode1.cnf</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br /><span>[mysqld]<br /><b>datadir=/home/openxs/galera/node1<br />port=3306<br />socket=/tmp/mysql-node1.sock<br />pid-file=/tmp/mysql-node1.pid<br />log-error=/tmp/mysql-node1.err</b>binlog_format=ROW<br />innodb_autoinc_lock_mode=2<br /><br /><b>wsrep_on=ON # this is important for 10.1!</b><br />wsrep_provider=<b>/home/openxs/galera/libgalera_smm.so</b><br />wsrep_cluster_name = <b>singlebox</b><br />wsrep_node_name = <b>node1</b><br /># wsrep_cluster_address=gcomm://<br />wsrep_cluster_address=<b>gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=no</b></span></blockquote>It's one of the shortest possible. I had to specify unique <b>datadir</b>, error log location, pid file, <b>port</b> and <b>socket</b> for the instance, set <b>binlog format</b> and point out Galera library location, set cluster name and node name. With proper planning I was able to specify <b>wsrep_cluster_address</b> referring to all other nodes properly, but for initial setup of the first node I can have it "empty" as commented out in the above, so that we start as a new cluster node. There is one essential setting for MariaDB 10.1.x that is not needed for "cluster-specific" instances like Percona XtraDB Cluster or older 10.0.x Galera packages from MariaDB (where it's ON by default). This is <span><b>wsrep_on=ON</b></span>. Without it MariaDB works as normal, non-cluster instance and ignores anything cluster-related. You can save a lot of time in case of upgrade to 10.1.x if you put it in your configuration file explicitly right now, no matter what the version is used.<br /><br />Then I copied and modified configuration file for the second node:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>cp /home/openxs/galera/mynode1.cnf /home/openxs/galera/mynode2.cnf</b><br />[openxs@fc23 maria10.1]$ <b>vi /home/openxs/galera/mynode2.cnf</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br /><span>[openxs@fc23 maria10.1]$ <b>cat /home/openxs/galera/mynode2.cnf</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></blockquote><blockquote><span>[mysqld]<br /><b>datadir=/home/openxs/galera/node2<br />port=3307<br />socket=/tmp/mysql-node2.sock<br />pid-file=/tmp/mysql-node2.pid<br />log-error=/tmp/mysql-node2.err</b>binlog_format=ROW<br />innodb_autoinc_lock_mode=2</span><br /><span><span><b>wsrep_on=ON # this is important for 10.1!</b></span><b>wsrep_provider=/home/openxs/galera/libgalera_smm.so<br />wsrep_cluster_name = singlebox<br />wsrep_node_name = node2<br />wsrep_cluster_address=gcomm://127.0.0.1:4567,127.0.0.1:5020</b></span><br /><span><b><span><b>?pc.wait_prim=no</b></span>wsrep_provider_options = "base_port=5020;"</b></span></blockquote>Note that while Galera node uses 4 ports, I specified only 2 unique ones explicitly, port for MySQL clients and base port for all Galera-related communication like IST and SST, with <b>base_port</b> setting. Note also how I referred to all cluster nodes with <b>wsrep_cluster_address</b> - this same value can be used for the configuration file of the first node actually. We can just start it as the first node of a new cluster (see below).<br /><br />Now we have configuration files for 2 nodes ready (we can always add <b>node3</b> later in the same way). But before starting new cluster we have to install system databases. For <b>node1</b> it was performed in the following way:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>scripts/mysql_install_db --defaults-file=/home/openxs/galera/mynode1.cnf</b><br />Installing MariaDB/MySQL system tables in '/home/openxs/galera/node1' ...<br />2016-03-31 12:51:34 139766046820480 [Note] ./bin/mysqld (mysqld 10.1.13-MariaDB) starting as process 28297 ...<br />...<br /><br />[openxs@fc23 maria10.1]$ <b>ls -l /home/openxs/galera/node1</b><br />-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp; 16384 Mar 31 12:51 aria_log.00000001<br />-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 52 Mar 31 12:51 aria_log_control<br />-rw-rw----. 1 openxs openxs 12582912 Mar 31 12:51 ibdata1<br />-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile0<br />-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile1<br />drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 mysql<br />drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 performance_schema<br />drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 test</span></blockquote>Then I started <b>node1</b> as a new cluster: <br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode1.cnf --wsrep-new-cluster &amp;</b></span></blockquote>and created a table, <b>t1</b>, with some data in it. After that I repeated installation of system tables etc for <b>node2</b>, just referencing proper configuration file, and started <b>node2</b> that was supposed to join the cluster:<br /><blockquote><span>openxs@fc23 maria10.1]$ <b>bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode2.cnf &amp;</b></span></blockquote>Let's check if we do have both instances running and communicating in Galera cluster:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>tail /tmp/mysql-node2.err</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br />2016-03-31 13:40:29 139627414767744 [Note] WSREP: Signalling provider to continue.<br />2016-03-31 13:40:29 139627414767744 [Note] WSREP: SST received: c91d17b6-f72b-11e5-95de-96e95167f593:0<br /><b>2016-03-31 13:40:29 139627117668096 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.<br />2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINER -&gt; JOINED (TO: 0)<br />2016-03-31 13:40:29 139627117668096 [Note] WSREP: Member 1.0 (node2) synced with group.<br />2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)</b>2016-03-31 13:40:29 139627414452992 [Note] WSREP: Synchronized with group, ready for connections<br />2016-03-31 13:40:29 139627414452992 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.<br /><b>2016-03-31 13:40:29 139627414767744 [Note] /home/openxs/dbs/maria10.1/bin/mysqld: ready for connections.<br />Version: '10.1.13-MariaDB'&nbsp; socket: '/tmp/mysql-node2.sock'&nbsp; port: 3307&nbsp; Source distribution</b><br />[openxs@fc23 maria10.1]$ <b>tail /tmp/mysql-node1.err</b><br />2016-03-31 13:40:27 140071390934784 [Note] WSREP: Provider resumed.<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: 0.0 (node1): State transfer to 1.0 (node2) complete.<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting DONOR/DESYNCED -&gt; JOINED (TO: 0)<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: Member 0.0 (node1) synced with group.<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)<br />2016-03-31 13:40:27 140072429247232 [Note] WSREP: Synchronized with group, ready for connections<br />2016-03-31 13:40:27 140072429247232 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.<br />2016-03-31 13:40:27 140072141715200 [Note] WSREP: (c91c99ec, 'tcp://0.0.0.0:4567') turning message relay requesting off<br /><b>2016-03-31 13:40:29 140072133322496 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.<br />2016-03-31 13:40:29 140072133322496 [Note] WSREP: Member 1.0 (node2) synced with group.</b></span> </blockquote>Familiar messages (unfortunately...) that prove we had a second node joined and performed state transfer from the first one. Now it's time to connect and test how cluster works. This is what I had after <b>node1</b> started and table with some data created there, but before <b>node2</b> started:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>bin/mysql -uroot --socket=/tmp/mysql-node1.sock</b><br />Welcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.<br />Your MariaDB connection id is 5<br />Server version: 10.1.13-MariaDB Source distribution<br /><br />Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.<br /><br />Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.<br /><br />MariaDB [(none)]&gt; <b>show variables like 'wsrep_cluster%';</b><br />+-----------------------+-------------------------------------------------------+<br />| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+-----------------------+-------------------------------------------------------+<br />| wsrep_cluster_address | gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=no |<br />| wsrep_cluster_name&nbsp;&nbsp;&nbsp; | singlebox&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+-----------------------+-------------------------------------------------------+<br />2 rows in set (0.00 sec)<br /><br />MariaDB [(none)]&gt; <b>show status like 'wsrep_cluster%';</b><br />+--------------------------+--------------------------------------+<br />| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | <b>1</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_state_uuid | <b>c91d17b6-f72b-11e5-95de-96e95167f593</b> |<br />| wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | Primary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />4 rows in set (0.00 sec)<br /><br />MariaDB [(none)]&gt; <b>use test</b><br />Reading table information for completion of table and column names<br />You can turn off this feature to get a quicker startup with -A<br /><br />Database changed<br />MariaDB [test]&gt; <b>select * from t1;</b><br />+----+------+<br />| id | c1&nbsp;&nbsp; |<br />+----+------+<br />|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 |<br />|&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |<br />+----+------+<br />2 rows in set (0.00 sec)</span></blockquote>Then, when <b>node2</b> joined the cluster, I checked that the data we've added on <b>node1</b> are there:<br /><br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>bin/mysql -uroot --socket=/tmp/mysql-node2.sock</b></span><br /><span>Welcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.<br />Your MariaDB connection id is 4<br />Server version: 10.1.13-MariaDB Source distribution<br /><br />Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.<br /><br />Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.<br /><br />MariaDB [(none)]&gt; <b>use test</b><br />Reading table information for completion of table and column names<br />You can turn off this feature to get a quicker startup with -A<br /><br />Database changed<br />MariaDB [test]&gt; <b>select * from t1;</b><br />+----+------+<br />| id | c1&nbsp;&nbsp; |<br />+----+------+<br />|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 |<br />|&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |<br />+----+------+<br />2 rows in set (0.00 sec)<br /><br />MariaDB [test]&gt; <b>show status like 'wsrep_cluster%'; </b>+--------------------------+--------------------------------------+<br />| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | <b>2</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_state_uuid | <b>c91d17b6-f72b-11e5-95de-96e95167f593</b> |<br />| wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | <b>Primary</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />4 rows in set (0.01 sec)</span></blockquote>So, the first basic test with the Galera cluster of 2 nodes (both running on the same box) built from current source of Galera and MariaDB 10.1.x on Fedora 23 is completed successfully. I plan to play with it more in the future, use current <b>xtrabackup</b> built from source for SST and so on, and create blog posts about these steps and any interesting tests in this setup. Stay tuned.<br /><br />From the dates above you can conclude that it took me 3 weeks to publish this post. That's because I was busy with the company meeting in Berlin and some usual Support work, and was not sure is it really a good idea for me to write any post with "Galera" or "MariaDB" words used in it even once...<br /><br /><br /><span></span><br /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995256&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995256&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Sun, 24 Apr 2016 18:33:45 +0000";s:2:"dc";a:1:{s:7:"creator";s:16:"Valeriy Kravchuk";}s:8:"category";s:38:"clusterFedoraGaleraMariaDBmysqlPercona";s:7:"summary";s:17921:"My Facebook followers probably noted that I quit from Percona some time ago and work for MariaDB since March 1, 2016. I changed the company, but neither the job role (I am still a Support Engineer), nor the approach to do my job. I still prefer to test everything I suggest to customers and I usually use software I build from source myself for these tests.While I try to avoid all kinds of clusters as much as possible for 15 years or so already (it does not matter if it's Oracle RAC, MySQL Cluster or Percona XtraDB Cluster, all of them), it's really hard to avoid Galera clusters while working for MariaDB. One of the reasons for this is that Galera, starting from MariaDB 10.1, can be easily "enabled"/used with any MariaDB 10.1.x instance, any time (at least when we speak about official binaries or those properly built - they are all "Galera ready"). Most of MariaDB customers do use Galera or can try to use it any time, so I have to be ready to test something Galera-specific any moment.For simple cases I decided to use a setup with several (2 to begin with) cluster nodes on one box. This approach is described in the manual for Percona XtraDB Cluster and was also used by my former colleague Fernando Laudares for his blog post and many real life related tests.So, I decided to proceed with the mix of ideas from the sources above and MariaDB's KB article on building Galera from source. As I decided to do this on my wife's Fedora 23 workstation, I checked this KB article for some details also. It lists prerequisites (boost-devel check-devel glibc-devel openssl-devel scons) and some of these packages (like scons in one of my cases) could be missing even on a system previosly used for builds for all kinds of MySQL related software. You can find something missing and fix the problem at later stage, but reading and following the manual or KB articles may help to save some time otherwise spent on trial and error.I've started with making directories in my home directory (/home/openxs) for this Galera related testing setup, like these: [openxs@fc23 ~]$ mkdir galera[openxs@fc23 ~]$ cd galera[openxs@fc23 galera]$ mkdir node1[openxs@fc23 galera]$ mkdir node2[openxs@fc23 galera]$ mkdir node3[openxs@fc23 galera]$ lsnode1&nbsp; node2&nbsp; node3I plan to use 3 nodes one day, but for this blog post I'll set up only 2, to have the smallest possible and simplest cluster as a proof of concept.Then I proceeded with cloning Galera from Codership's GitHub (this is supposed to be the latest and greatest). I changed current directory to my usual git repository and executed git clone https://github.com/codership/galera.git. When this command completed I've got a subdirectory named galera.In that directory, assuming that all prerequisites are installed, to build current Galera library version it's enough to execute simple script while in galera directory, ./scripts/build.sh. I ended up with the following:[openxs@fc23 galera]$ ls -l libgalera_smm.so-rwxrwxr-x. 1 openxs openxs 40204824 Mar 31 12:21 libgalera_smm.so[openxs@fc23 galera]$ file libgalera_smm.solibgalera_smm.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, BuildID[sha1]=11457fa9fd69dabe617708c0dd288b218255a886, not stripped[openxs@fc23 galera]$ pwd/home/openxs/git/galera[openxs@fc23 galera]$ cp libgalera_smm.so ~/galera/and copied the library to the target directory for my testing setup (that should NOT conflict with whatever software I may have installed later from packages).Now, time to build MariaDB properly to let it use Galera if needed. I already had recent (at the moment) 10.1.13 in the server subdirectory of my git repository. I've executed the following commands then:[openxs@fc23 server]$ cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON -DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1-- Running cmake version 3.4.1-- MariaDB 10.1.13...[openxs@fc23 server]$ time make -j 4...real&nbsp;&nbsp;&nbsp; 9m28.164suser&nbsp;&nbsp;&nbsp; 32m43.960ssys&nbsp;&nbsp;&nbsp;&nbsp; 2m45.637sThis was my usual command line to build MariaDB 10.x with only 2 extra options added: -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON.After make completed, I've executed make install &amp;&amp; make clean and was ready to use my shiny new Galera-ready MariaDB 10.1.13.To take into account the directories I am going to use for my cluster nodes and make sure they can start and communicate as separate mysqld instances, I have to create configuration files for them. I've changed working directory to /home/openxs/dbs/mariadb10.1 and started with this configuration file for the first node:[openxs@fc23 maria10.1]$ cat /home/openxs/galera/mynode1.cnf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[mysqld]datadir=/home/openxs/galera/node1port=3306socket=/tmp/mysql-node1.sockpid-file=/tmp/mysql-node1.pidlog-error=/tmp/mysql-node1.errbinlog_format=ROWinnodb_autoinc_lock_mode=2wsrep_on=ON # this is important for 10.1!wsrep_provider=/home/openxs/galera/libgalera_smm.sowsrep_cluster_name = singleboxwsrep_node_name = node1# wsrep_cluster_address=gcomm://wsrep_cluster_address=gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=noIt's one of the shortest possible. I had to specify unique datadir, error log location, pid file, port and socket for the instance, set binlog format and point out Galera library location, set cluster name and node name. With proper planning I was able to specify wsrep_cluster_address referring to all other nodes properly, but for initial setup of the first node I can have it "empty" as commented out in the above, so that we start as a new cluster node. There is one essential setting for MariaDB 10.1.x that is not needed for "cluster-specific" instances like Percona XtraDB Cluster or older 10.0.x Galera packages from MariaDB (where it's ON by default). This is wsrep_on=ON. Without it MariaDB works as normal, non-cluster instance and ignores anything cluster-related. You can save a lot of time in case of upgrade to 10.1.x if you put it in your configuration file explicitly right now, no matter what the version is used.Then I copied and modified configuration file for the second node:[openxs@fc23 maria10.1]$ cp /home/openxs/galera/mynode1.cnf /home/openxs/galera/mynode2.cnf[openxs@fc23 maria10.1]$ vi /home/openxs/galera/mynode2.cnf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[openxs@fc23 maria10.1]$ cat /home/openxs/galera/mynode2.cnf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[mysqld]datadir=/home/openxs/galera/node2port=3307socket=/tmp/mysql-node2.sockpid-file=/tmp/mysql-node2.pidlog-error=/tmp/mysql-node2.errbinlog_format=ROWinnodb_autoinc_lock_mode=2wsrep_on=ON # this is important for 10.1!wsrep_provider=/home/openxs/galera/libgalera_smm.sowsrep_cluster_name = singleboxwsrep_node_name = node2wsrep_cluster_address=gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=nowsrep_provider_options = "base_port=5020;"Note that while Galera node uses 4 ports, I specified only 2 unique ones explicitly, port for MySQL clients and base port for all Galera-related communication like IST and SST, with base_port setting. Note also how I referred to all cluster nodes with wsrep_cluster_address - this same value can be used for the configuration file of the first node actually. We can just start it as the first node of a new cluster (see below).Now we have configuration files for 2 nodes ready (we can always add node3 later in the same way). But before starting new cluster we have to install system databases. For node1 it was performed in the following way:[openxs@fc23 maria10.1]$ scripts/mysql_install_db --defaults-file=/home/openxs/galera/mynode1.cnfInstalling MariaDB/MySQL system tables in '/home/openxs/galera/node1' ...2016-03-31 12:51:34 139766046820480 [Note] ./bin/mysqld (mysqld 10.1.13-MariaDB) starting as process 28297 ......[openxs@fc23 maria10.1]$ ls -l /home/openxs/galera/node1-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp; 16384 Mar 31 12:51 aria_log.00000001-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 52 Mar 31 12:51 aria_log_control-rw-rw----. 1 openxs openxs 12582912 Mar 31 12:51 ibdata1-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile0-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile1drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 mysqldrwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 performance_schemadrwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 testThen I started node1 as a new cluster: [openxs@fc23 maria10.1]$ bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode1.cnf --wsrep-new-cluster &amp;and created a table, t1, with some data in it. After that I repeated installation of system tables etc for node2, just referencing proper configuration file, and started node2 that was supposed to join the cluster:openxs@fc23 maria10.1]$ bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode2.cnf &amp;Let's check if we do have both instances running and communicating in Galera cluster:[openxs@fc23 maria10.1]$ tail /tmp/mysql-node2.err&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2016-03-31 13:40:29 139627414767744 [Note] WSREP: Signalling provider to continue.2016-03-31 13:40:29 139627414767744 [Note] WSREP: SST received: c91d17b6-f72b-11e5-95de-96e95167f593:02016-03-31 13:40:29 139627117668096 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINER -&gt; JOINED (TO: 0)2016-03-31 13:40:29 139627117668096 [Note] WSREP: Member 1.0 (node2) synced with group.2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)2016-03-31 13:40:29 139627414452992 [Note] WSREP: Synchronized with group, ready for connections2016-03-31 13:40:29 139627414452992 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.2016-03-31 13:40:29 139627414767744 [Note] /home/openxs/dbs/maria10.1/bin/mysqld: ready for connections.Version: '10.1.13-MariaDB'&nbsp; socket: '/tmp/mysql-node2.sock'&nbsp; port: 3307&nbsp; Source distribution[openxs@fc23 maria10.1]$ tail /tmp/mysql-node1.err2016-03-31 13:40:27 140071390934784 [Note] WSREP: Provider resumed.2016-03-31 13:40:27 140072133322496 [Note] WSREP: 0.0 (node1): State transfer to 1.0 (node2) complete.2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting DONOR/DESYNCED -&gt; JOINED (TO: 0)2016-03-31 13:40:27 140072133322496 [Note] WSREP: Member 0.0 (node1) synced with group.2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)2016-03-31 13:40:27 140072429247232 [Note] WSREP: Synchronized with group, ready for connections2016-03-31 13:40:27 140072429247232 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.2016-03-31 13:40:27 140072141715200 [Note] WSREP: (c91c99ec, 'tcp://0.0.0.0:4567') turning message relay requesting off2016-03-31 13:40:29 140072133322496 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.2016-03-31 13:40:29 140072133322496 [Note] WSREP: Member 1.0 (node2) synced with group. Familiar messages (unfortunately...) that prove we had a second node joined and performed state transfer from the first one. Now it's time to connect and test how cluster works. This is what I had after node1 started and table with some data created there, but before node2 started:[openxs@fc23 maria10.1]$ bin/mysql -uroot --socket=/tmp/mysql-node1.sockWelcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.Your MariaDB connection id is 5Server version: 10.1.13-MariaDB Source distributionCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; show variables like 'wsrep_cluster%';+-----------------------+-------------------------------------------------------+| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+-----------------------+-------------------------------------------------------+| wsrep_cluster_address | gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=no || wsrep_cluster_name&nbsp;&nbsp;&nbsp; | singlebox&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+-----------------------+-------------------------------------------------------+2 rows in set (0.00 sec)MariaDB [(none)]&gt; show status like 'wsrep_cluster%';+--------------------------+--------------------------------------+| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_state_uuid | c91d17b6-f72b-11e5-95de-96e95167f593 || wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | Primary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+4 rows in set (0.00 sec)MariaDB [(none)]&gt; use testReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [test]&gt; select * from t1;+----+------+| id | c1&nbsp;&nbsp; |+----+------+|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 ||&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |+----+------+2 rows in set (0.00 sec)Then, when node2 joined the cluster, I checked that the data we've added on node1 are there:[openxs@fc23 maria10.1]$ bin/mysql -uroot --socket=/tmp/mysql-node2.sockWelcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.Your MariaDB connection id is 4Server version: 10.1.13-MariaDB Source distributionCopyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; use testReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [test]&gt; select * from t1;+----+------+| id | c1&nbsp;&nbsp; |+----+------+|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 ||&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |+----+------+2 rows in set (0.00 sec)MariaDB [test]&gt; show status like 'wsrep_cluster%'; +--------------------------+--------------------------------------+| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || wsrep_cluster_state_uuid | c91d17b6-f72b-11e5-95de-96e95167f593 || wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | Primary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+--------------------------+--------------------------------------+4 rows in set (0.01 sec)So, the first basic test with the Galera cluster of 2 nodes (both running on the same box) built from current source of Galera and MariaDB 10.1.x on Fedora 23 is completed successfully. I plan to play with it more in the future, use current xtrabackup built from source for SST and so on, and create blog posts about these steps and any interesting tests in this setup. Stay tuned.From the dates above you can conclude that it took me 3 weeks to publish this post. That's because I was busy with the company meeting in Berlin and some usual Support work, and was not sure is it really a good idea for me to write any post with "Galera" or "MariaDB" words used in it even once...";s:12:"atom_content";s:21502:"<div><a href="https://4.bp.blogspot.com/-7DYRVE2TdUI/Vxzy7ZYTUNI/AAAAAAAAAJs/eTO5As2AXg0DwYOklXUMt_zRfmOB938DwCLcB/s1600/093.jpg" imageanchor="1"><img border="0" height="240" src="https://4.bp.blogspot.com/-7DYRVE2TdUI/Vxzy7ZYTUNI/AAAAAAAAAJs/eTO5As2AXg0DwYOklXUMt_zRfmOB938DwCLcB/s320/093.jpg" width="320" /></a></div>My <a href="https://www.facebook.com/valerii.kravchuk" target="_blank">Facebook</a> followers probably noted that I quit from Percona some time ago and work for MariaDB since March 1, 2016. I changed the company, but neither the job role (I am still a Support Engineer), nor the <a href="http://mysqlentomologist.blogspot.com/2016/01/im-winston-wolf-i-solve-problems.html" target="_blank">approach</a> to do my job. I still prefer to test everything I suggest to customers and I usually use software I build from source myself for these tests.<br /><br />While I try to avoid all kinds of clusters as much as possible for 15 years or so already (it does not matter if it's Oracle RAC, MySQL Cluster or Percona XtraDB Cluster, all of them), it's really hard to avoid Galera clusters while working for MariaDB. One of the reasons for this is that Galera, starting from MariaDB 10.1, can be easily "enabled"/used with any MariaDB 10.1.x instance, any time (at least when we speak about official binaries or those properly built - they are all "Galera ready"). Most of MariaDB customers do use Galera or can try to use it any time, so I have to be ready to test something Galera-specific any moment.<br /><br />For simple cases I decided to use a setup with several (2 to begin with) cluster nodes on one box. This approach is <a href="https://www.percona.com/doc/percona-xtradb-cluster/5.6/howtos/singlebox.html" target="_blank">described in the manual</a> for Percona XtraDB Cluster and was also used by my former colleague Fernando Laudares for his <a href="https://www.percona.com/blog/2014/10/21/percona-xtradb-cluster-how-to-run-a-2-node-cluster-on-a-single-server/" target="_blank">blog post</a> and many real life related tests.<br /><br />So, I decided to proceed with the mix of ideas from the sources above and MariaDB's KB article on <a href="https://mariadb.com/kb/en/mariadb/installating-galera-from-source/" target="_blank">building Galera from source</a>. As I decided to do this on my wife's Fedora 23 workstation, I checked <a href="https://mariadb.com/kb/en/mariadb/building-the-galera-wsrep-package-on-fedora/" target="_blank">this</a> KB article for some details also. It lists prerequisites (<b>boost-devel</b> <b>check-devel</b> <b>glibc-devel</b> <b>openssl-devel</b> <b>scons</b>) and some of these packages (like <b>scons</b> in one of my cases) could be missing even on a system previosly used for builds for all kinds of MySQL related software. You can find something missing and fix the problem at later stage, but reading and following the manual or KB articles may help to save some time otherwise spent on <a href="https://en.wikipedia.org/wiki/Trial_and_error" target="_blank">trial and error</a>.<br /><br />I've started with making directories in my home directory (<b>/home/openxs</b>) for this Galera related testing setup, like these: <br /><blockquote><span>[openxs@fc23 ~]$ <b>mkdir galera</b><br />[openxs@fc23 ~]$ <b>cd galera</b><br />[openxs@fc23 galera]$ <b>mkdir node1</b>[openxs@fc23 galera]$ <b>mkdir node2</b><br />[openxs@fc23 galera]$ <b>mkdir node3</b><br />[openxs@fc23 galera]$ <b>ls</b><br />node1&nbsp; node2&nbsp; node3</span></blockquote>I plan to use 3 nodes one day, but for this blog post I'll set up only 2, to have the smallest possible and simplest cluster as a proof of concept.<br /><br />Then I proceeded with cloning Galera from Codership's GitHub (this is supposed to be the latest and greatest). I changed current directory to my usual <b>git</b> repository and executed <b>git clone https://github.com/codership/galera.git</b>. When this command completed I've got a subdirectory named <b>galera</b>.<br /><br />In that directory, assuming that all prerequisites are installed, to build current Galera library version it's enough to execute simple script while in <b>galera</b> directory, <b>./scripts/build.sh</b>. I ended up with the following:<br /><blockquote><span>[openxs@fc23 galera]$ <b>ls -l libgalera_smm.so</b><br />-rwxrwxr-x. 1 openxs openxs 40204824 Mar 31 12:21 libgalera_smm.so<br />[openxs@fc23 galera]$ <b>file libgalera_smm.so</b><br />libgalera_smm.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, BuildID[sha1]=11457fa9fd69dabe617708c0dd288b218255a886, not stripped<br /><br />[openxs@fc23 galera]$ <b>pwd</b><br />/home/openxs/git/galera<br />[openxs@fc23 galera]$ <b>cp libgalera_smm.so ~/galera/</b></span></blockquote>and copied the library to the target directory for my testing setup (that should NOT conflict with whatever software I may have installed later from packages).<br /><br />Now, time to build MariaDB properly to let it use Galera if needed. I already had recent (at the moment) 10.1.13 in the <b>server</b> subdirectory of my git repository. I've executed the following commands then:<br /><br /><blockquote><span>[openxs@fc23 server]$ <b>cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_SSL=system -DWITH_ZLIB=bundled -DMYSQL_MAINTAINER_MODE=0 -DENABLED_LOCAL_INFILE=1 -DWITH_JEMALLOC=system -DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON -DCMAKE_INSTALL_PREFIX=/home/openxs/dbs/maria10.1</b><br />-- Running cmake version 3.4.1<br />-- MariaDB 10.1.13<br />...<br /><br />[openxs@fc23 server]$ <b>time make -j 4</b>...<br />real&nbsp;&nbsp;&nbsp; 9m28.164s<br />user&nbsp;&nbsp;&nbsp; 32m43.960s<br />sys&nbsp;&nbsp;&nbsp;&nbsp; 2m45.637s</span></blockquote>This was my usual command line to build MariaDB 10.x with only 2 extra options added: <span><b>-DWITH_WSREP=ON -DWITH_INNODB_DISALLOW_WRITES=ON</b>.</span>After make completed, I've executed <b>make install &amp;&amp; make clean</b> and was ready to use my shiny new Galera-ready MariaDB 10.1.13.<br /><br />To take into account the directories I am going to use for my cluster nodes and make sure they can start and communicate as separate mysqld instances, I have to create configuration files for them. I've changed working directory to <b>/home/openxs/dbs/mariadb10.1</b> and started with this configuration file for the first node:<br /><br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>cat /home/openxs/galera/mynode1.cnf</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br /><span>[mysqld]<br /><b>datadir=/home/openxs/galera/node1<br />port=3306<br />socket=/tmp/mysql-node1.sock<br />pid-file=/tmp/mysql-node1.pid<br />log-error=/tmp/mysql-node1.err</b>binlog_format=ROW<br />innodb_autoinc_lock_mode=2<br /><br /><b>wsrep_on=ON # this is important for 10.1!</b><br />wsrep_provider=<b>/home/openxs/galera/libgalera_smm.so</b><br />wsrep_cluster_name = <b>singlebox</b><br />wsrep_node_name = <b>node1</b><br /># wsrep_cluster_address=gcomm://<br />wsrep_cluster_address=<b>gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=no</b></span></blockquote>It's one of the shortest possible. I had to specify unique <b>datadir</b>, error log location, pid file, <b>port</b> and <b>socket</b> for the instance, set <b>binlog format</b> and point out Galera library location, set cluster name and node name. With proper planning I was able to specify <b>wsrep_cluster_address</b> referring to all other nodes properly, but for initial setup of the first node I can have it "empty" as commented out in the above, so that we start as a new cluster node. There is one essential setting for MariaDB 10.1.x that is not needed for "cluster-specific" instances like Percona XtraDB Cluster or older 10.0.x Galera packages from MariaDB (where it's ON by default). This is <span><b>wsrep_on=ON</b></span>. Without it MariaDB works as normal, non-cluster instance and ignores anything cluster-related. You can save a lot of time in case of upgrade to 10.1.x if you put it in your configuration file explicitly right now, no matter what the version is used.<br /><br />Then I copied and modified configuration file for the second node:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>cp /home/openxs/galera/mynode1.cnf /home/openxs/galera/mynode2.cnf</b><br />[openxs@fc23 maria10.1]$ <b>vi /home/openxs/galera/mynode2.cnf</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br /><span>[openxs@fc23 maria10.1]$ <b>cat /home/openxs/galera/mynode2.cnf</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></blockquote><blockquote><span>[mysqld]<br /><b>datadir=/home/openxs/galera/node2<br />port=3307<br />socket=/tmp/mysql-node2.sock<br />pid-file=/tmp/mysql-node2.pid<br />log-error=/tmp/mysql-node2.err</b>binlog_format=ROW<br />innodb_autoinc_lock_mode=2</span><br /><span><span><b>wsrep_on=ON # this is important for 10.1!</b></span><b>wsrep_provider=/home/openxs/galera/libgalera_smm.so<br />wsrep_cluster_name = singlebox<br />wsrep_node_name = node2<br />wsrep_cluster_address=gcomm://127.0.0.1:4567,127.0.0.1:5020</b></span><br /><span><b><span><b>?pc.wait_prim=no</b></span>wsrep_provider_options = "base_port=5020;"</b></span></blockquote>Note that while Galera node uses 4 ports, I specified only 2 unique ones explicitly, port for MySQL clients and base port for all Galera-related communication like IST and SST, with <b>base_port</b> setting. Note also how I referred to all cluster nodes with <b>wsrep_cluster_address</b> - this same value can be used for the configuration file of the first node actually. We can just start it as the first node of a new cluster (see below).<br /><br />Now we have configuration files for 2 nodes ready (we can always add <b>node3</b> later in the same way). But before starting new cluster we have to install system databases. For <b>node1</b> it was performed in the following way:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>scripts/mysql_install_db --defaults-file=/home/openxs/galera/mynode1.cnf</b><br />Installing MariaDB/MySQL system tables in '/home/openxs/galera/node1' ...<br />2016-03-31 12:51:34 139766046820480 [Note] ./bin/mysqld (mysqld 10.1.13-MariaDB) starting as process 28297 ...<br />...<br /><br />[openxs@fc23 maria10.1]$ <b>ls -l /home/openxs/galera/node1</b><br />-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp; 16384 Mar 31 12:51 aria_log.00000001<br />-rw-rw----. 1 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 52 Mar 31 12:51 aria_log_control<br />-rw-rw----. 1 openxs openxs 12582912 Mar 31 12:51 ibdata1<br />-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile0<br />-rw-rw----. 1 openxs openxs 50331648 Mar 31 12:51 ib_logfile1<br />drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 mysql<br />drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 performance_schema<br />drwx------. 2 openxs openxs&nbsp;&nbsp;&nbsp;&nbsp; 4096 Mar 31 12:51 test</span></blockquote>Then I started <b>node1</b> as a new cluster: <br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode1.cnf --wsrep-new-cluster &amp;</b></span></blockquote>and created a table, <b>t1</b>, with some data in it. After that I repeated installation of system tables etc for <b>node2</b>, just referencing proper configuration file, and started <b>node2</b> that was supposed to join the cluster:<br /><blockquote><span>openxs@fc23 maria10.1]$ <b>bin/mysqld_safe --defaults-file=/home/openxs/galera/mynode2.cnf &amp;</b></span></blockquote>Let's check if we do have both instances running and communicating in Galera cluster:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>tail /tmp/mysql-node2.err</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br />2016-03-31 13:40:29 139627414767744 [Note] WSREP: Signalling provider to continue.<br />2016-03-31 13:40:29 139627414767744 [Note] WSREP: SST received: c91d17b6-f72b-11e5-95de-96e95167f593:0<br /><b>2016-03-31 13:40:29 139627117668096 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.<br />2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINER -&gt; JOINED (TO: 0)<br />2016-03-31 13:40:29 139627117668096 [Note] WSREP: Member 1.0 (node2) synced with group.<br />2016-03-31 13:40:29 139627117668096 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)</b>2016-03-31 13:40:29 139627414452992 [Note] WSREP: Synchronized with group, ready for connections<br />2016-03-31 13:40:29 139627414452992 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.<br /><b>2016-03-31 13:40:29 139627414767744 [Note] /home/openxs/dbs/maria10.1/bin/mysqld: ready for connections.<br />Version: '10.1.13-MariaDB'&nbsp; socket: '/tmp/mysql-node2.sock'&nbsp; port: 3307&nbsp; Source distribution</b><br />[openxs@fc23 maria10.1]$ <b>tail /tmp/mysql-node1.err</b><br />2016-03-31 13:40:27 140071390934784 [Note] WSREP: Provider resumed.<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: 0.0 (node1): State transfer to 1.0 (node2) complete.<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting DONOR/DESYNCED -&gt; JOINED (TO: 0)<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: Member 0.0 (node1) synced with group.<br />2016-03-31 13:40:27 140072133322496 [Note] WSREP: Shifting JOINED -&gt; SYNCED (TO: 0)<br />2016-03-31 13:40:27 140072429247232 [Note] WSREP: Synchronized with group, ready for connections<br />2016-03-31 13:40:27 140072429247232 [Note] WSREP: wsrep_notify_cmd is not defined, skipping notification.<br />2016-03-31 13:40:27 140072141715200 [Note] WSREP: (c91c99ec, 'tcp://0.0.0.0:4567') turning message relay requesting off<br /><b>2016-03-31 13:40:29 140072133322496 [Note] WSREP: 1.0 (node2): State transfer from 0.0 (node1) complete.<br />2016-03-31 13:40:29 140072133322496 [Note] WSREP: Member 1.0 (node2) synced with group.</b></span> </blockquote>Familiar messages (unfortunately...) that prove we had a second node joined and performed state transfer from the first one. Now it's time to connect and test how cluster works. This is what I had after <b>node1</b> started and table with some data created there, but before <b>node2</b> started:<br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>bin/mysql -uroot --socket=/tmp/mysql-node1.sock</b><br />Welcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.<br />Your MariaDB connection id is 5<br />Server version: 10.1.13-MariaDB Source distribution<br /><br />Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.<br /><br />Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.<br /><br />MariaDB [(none)]&gt; <b>show variables like 'wsrep_cluster%';</b><br />+-----------------------+-------------------------------------------------------+<br />| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+-----------------------+-------------------------------------------------------+<br />| wsrep_cluster_address | gcomm://127.0.0.1:4567,127.0.0.1:5020?pc.wait_prim=no |<br />| wsrep_cluster_name&nbsp;&nbsp;&nbsp; | singlebox&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+-----------------------+-------------------------------------------------------+<br />2 rows in set (0.00 sec)<br /><br />MariaDB [(none)]&gt; <b>show status like 'wsrep_cluster%';</b><br />+--------------------------+--------------------------------------+<br />| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | <b>1</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_state_uuid | <b>c91d17b6-f72b-11e5-95de-96e95167f593</b> |<br />| wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | Primary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />4 rows in set (0.00 sec)<br /><br />MariaDB [(none)]&gt; <b>use test</b><br />Reading table information for completion of table and column names<br />You can turn off this feature to get a quicker startup with -A<br /><br />Database changed<br />MariaDB [test]&gt; <b>select * from t1;</b><br />+----+------+<br />| id | c1&nbsp;&nbsp; |<br />+----+------+<br />|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 |<br />|&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |<br />+----+------+<br />2 rows in set (0.00 sec)</span></blockquote>Then, when <b>node2</b> joined the cluster, I checked that the data we've added on <b>node1</b> are there:<br /><br /><blockquote><span>[openxs@fc23 maria10.1]$ <b>bin/mysql -uroot --socket=/tmp/mysql-node2.sock</b></span><br /><span>Welcome to the MariaDB monitor.&nbsp; Commands end with ; or \g.<br />Your MariaDB connection id is 4<br />Server version: 10.1.13-MariaDB Source distribution<br /><br />Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.<br /><br />Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.<br /><br />MariaDB [(none)]&gt; <b>use test</b><br />Reading table information for completion of table and column names<br />You can turn off this feature to get a quicker startup with -A<br /><br />Database changed<br />MariaDB [test]&gt; <b>select * from t1;</b><br />+----+------+<br />| id | c1&nbsp;&nbsp; |<br />+----+------+<br />|&nbsp; 1 |&nbsp;&nbsp;&nbsp; 1 |<br />|&nbsp; 2 |&nbsp;&nbsp;&nbsp; 2 |<br />+----+------+<br />2 rows in set (0.00 sec)<br /><br />MariaDB [test]&gt; <b>show status like 'wsrep_cluster%'; </b>+--------------------------+--------------------------------------+<br />| Variable_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />| wsrep_cluster_conf_id&nbsp;&nbsp;&nbsp; | 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | <b>2</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />| wsrep_cluster_state_uuid | <b>c91d17b6-f72b-11e5-95de-96e95167f593</b> |<br />| wsrep_cluster_status&nbsp;&nbsp;&nbsp;&nbsp; | <b>Primary</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br />+--------------------------+--------------------------------------+<br />4 rows in set (0.01 sec)</span></blockquote>So, the first basic test with the Galera cluster of 2 nodes (both running on the same box) built from current source of Galera and MariaDB 10.1.x on Fedora 23 is completed successfully. I plan to play with it more in the future, use current <b>xtrabackup</b> built from source for SST and so on, and create blog posts about these steps and any interesting tests in this setup. Stay tuned.<br /><br />From the dates above you can conclude that it took me 3 weeks to publish this post. That's because I was busy with the company meeting in Berlin and some usual Support work, and was not sure is it really a good idea for me to write any post with "Galera" or "MariaDB" words used in it even once...<br /><br /><br /><span></span><br /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995256&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995256&vote=-1&apivote=1">Vote DOWN</a>";}i:25;a:9:{s:5:"title";s:40:"Best Monitoring Practices, Greatest Hits";s:4:"guid";s:72:"https://www.vividcortex.com/blog/best-monitoring-practices-greatest-hits";s:4:"link";s:72:"https://www.vividcortex.com/blog/best-monitoring-practices-greatest-hits";s:11:"description";s:1510:" 
Over the last couple years&nbsp;on the VividCortex blog, we've discussed a huge range of&nbsp;database monitoring concepts. We've examined&nbsp;ideas we're excited about, shared monitoring practices we recommend, and explored some of the principles&nbsp;that shape our own projects and goals. Of these many posts, however, there are a few in particular worth emphasizing and discussing&nbsp;again and again. Generally, these posts explore the&nbsp;math and statistics behind key monitoring practices and they champion&nbsp;why smart database monitoring is so important; they're often our most read articles, the most indicative of what we value, the influences that&nbsp;make VividCortex a unique&nbsp;product. &nbsp; 
The&nbsp;list&nbsp;below is a quick review&nbsp;of&nbsp;some of VividCortex's&nbsp;key blog posts, coming together to make a Greatest Hits list of our&nbsp;recommended best practices and fundamental concepts.&nbsp;  
 
  Why Percentiles Don’t Work the Way you Think  
  Is That Query Bad?  
  A Trendline is a Model  
  MySQL Query Performance Statistics in the Performance Schema  
  Analysis of PayPal's Node-vs-Java Benchmarks  
  Why You Should Almost Never Alert on Thresholds  
  When Is It Too Late to Monitor?  
 
Check out these articles if you haven't had the chance to read them before, and, as always, please be in touch with any questions or&nbsp;feedback. Don't hesitate to let us know if you and your team have any interest in experiencing VividCortex firsthand. &nbsp;  
";s:7:"content";a:1:{s:7:"encoded";s:3038:"<p><img alt="GoldeneLP.jpg" src="http://cdn2.hubspot.net/hubfs/498921/GoldeneLP.jpg" title="GoldeneLP.jpg" width="341" /></p> 
<p>Over the last couple years&nbsp;on the VividCortex blog, we've discussed a huge range of&nbsp;database monitoring concepts. We've examined&nbsp;ideas we're excited about, shared monitoring practices we recommend, and explored some of the principles&nbsp;that shape our own projects and goals. Of these many posts, however, there are a few in particular worth emphasizing and discussing&nbsp;again and again. Generally, these posts explore the&nbsp;math and statistics behind key monitoring practices and they champion&nbsp;why smart database monitoring is so important; they're often our most read articles, the most indicative of what we value, the influences that&nbsp;make VividCortex a unique&nbsp;product. &nbsp;</p> 
<p>The&nbsp;list&nbsp;below is a quick review&nbsp;of&nbsp;some of VividCortex's&nbsp;key blog posts, coming together to make a Greatest Hits list of our&nbsp;recommended best practices and fundamental concepts.&nbsp;</p>  
<ul> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/why-percentiles-dont-work-the-way-you-think"><span>Why Percentiles Don’t Work the Way you Think</span></a></span></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/is-that-query-bad">Is That Query Bad?</a></span></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/2015/11/28/a-trendline-is-a-model/">A Trendline is a Model</a></span></p> </li> 
 <li> <p><a href="https://www.vividcortex.com/blog/2014/11/03/mysql-query-performance-statistics-in-the-performance-schema/"><span>MySQL Query Performance Statistics in the Performance Schema</span></a></p> </li> 
 <li> <p><a href="https://www.vividcortex.com/blog/2013/12/09/analysis-of-paypals-node-vs-java-benchmarks/"><span>Analysis of PayPal's Node-vs-Java Benchmarks</span></a></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/2013/04/08/why-you-should-almost-never-alert-on-thresholds/">Why You Should Almost Never Alert on Thresholds</a></span></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/when-is-it-too-late-to-monitor">When Is It Too Late to Monitor?</a></span></p> </li> 
</ul> 
<p>Check out these articles if you haven't had the chance to read them before, and, as always, please be in touch with any questions or&nbsp;feedback. Don't hesitate to let us know if you and your team have any interest in <a href="https://www.vividcortex.com/request-a-demo">experiencing VividCortex firsthand</a>. &nbsp;</p>  
<img src="http://track.hubspot.com/__ptq.gif?a=498921&amp;k=14&amp;r=https://www.vividcortex.com/blog/best-monitoring-practices-greatest-hits&amp;bu=https%253A%252F%252Fwww.vividcortex.com%252Fblog&amp;bvt=rss" alt="" width="1" height="1" /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995242&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995242&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Fri, 22 Apr 2016 17:38:51 +0000";s:2:"dc";a:1:{s:7:"creator";s:11:"VividCortex";}s:7:"summary";s:1510:" 
Over the last couple years&nbsp;on the VividCortex blog, we've discussed a huge range of&nbsp;database monitoring concepts. We've examined&nbsp;ideas we're excited about, shared monitoring practices we recommend, and explored some of the principles&nbsp;that shape our own projects and goals. Of these many posts, however, there are a few in particular worth emphasizing and discussing&nbsp;again and again. Generally, these posts explore the&nbsp;math and statistics behind key monitoring practices and they champion&nbsp;why smart database monitoring is so important; they're often our most read articles, the most indicative of what we value, the influences that&nbsp;make VividCortex a unique&nbsp;product. &nbsp; 
The&nbsp;list&nbsp;below is a quick review&nbsp;of&nbsp;some of VividCortex's&nbsp;key blog posts, coming together to make a Greatest Hits list of our&nbsp;recommended best practices and fundamental concepts.&nbsp;  
 
  Why Percentiles Don’t Work the Way you Think  
  Is That Query Bad?  
  A Trendline is a Model  
  MySQL Query Performance Statistics in the Performance Schema  
  Analysis of PayPal's Node-vs-Java Benchmarks  
  Why You Should Almost Never Alert on Thresholds  
  When Is It Too Late to Monitor?  
 
Check out these articles if you haven't had the chance to read them before, and, as always, please be in touch with any questions or&nbsp;feedback. Don't hesitate to let us know if you and your team have any interest in experiencing VividCortex firsthand. &nbsp;  
";s:12:"atom_content";s:3038:"<p><img alt="GoldeneLP.jpg" src="http://cdn2.hubspot.net/hubfs/498921/GoldeneLP.jpg" title="GoldeneLP.jpg" width="341" /></p> 
<p>Over the last couple years&nbsp;on the VividCortex blog, we've discussed a huge range of&nbsp;database monitoring concepts. We've examined&nbsp;ideas we're excited about, shared monitoring practices we recommend, and explored some of the principles&nbsp;that shape our own projects and goals. Of these many posts, however, there are a few in particular worth emphasizing and discussing&nbsp;again and again. Generally, these posts explore the&nbsp;math and statistics behind key monitoring practices and they champion&nbsp;why smart database monitoring is so important; they're often our most read articles, the most indicative of what we value, the influences that&nbsp;make VividCortex a unique&nbsp;product. &nbsp;</p> 
<p>The&nbsp;list&nbsp;below is a quick review&nbsp;of&nbsp;some of VividCortex's&nbsp;key blog posts, coming together to make a Greatest Hits list of our&nbsp;recommended best practices and fundamental concepts.&nbsp;</p>  
<ul> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/why-percentiles-dont-work-the-way-you-think"><span>Why Percentiles Don’t Work the Way you Think</span></a></span></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/is-that-query-bad">Is That Query Bad?</a></span></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/2015/11/28/a-trendline-is-a-model/">A Trendline is a Model</a></span></p> </li> 
 <li> <p><a href="https://www.vividcortex.com/blog/2014/11/03/mysql-query-performance-statistics-in-the-performance-schema/"><span>MySQL Query Performance Statistics in the Performance Schema</span></a></p> </li> 
 <li> <p><a href="https://www.vividcortex.com/blog/2013/12/09/analysis-of-paypals-node-vs-java-benchmarks/"><span>Analysis of PayPal's Node-vs-Java Benchmarks</span></a></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/2013/04/08/why-you-should-almost-never-alert-on-thresholds/">Why You Should Almost Never Alert on Thresholds</a></span></p> </li> 
 <li> <p><span><a href="https://www.vividcortex.com/blog/when-is-it-too-late-to-monitor">When Is It Too Late to Monitor?</a></span></p> </li> 
</ul> 
<p>Check out these articles if you haven't had the chance to read them before, and, as always, please be in touch with any questions or&nbsp;feedback. Don't hesitate to let us know if you and your team have any interest in <a href="https://www.vividcortex.com/request-a-demo">experiencing VividCortex firsthand</a>. &nbsp;</p>  
<img src="http://track.hubspot.com/__ptq.gif?a=498921&amp;k=14&amp;r=https://www.vividcortex.com/blog/best-monitoring-practices-greatest-hits&amp;bu=https%253A%252F%252Fwww.vividcortex.com%252Fblog&amp;bvt=rss" alt="" width="1" height="1" /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995242&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995242&vote=-1&apivote=1">Vote DOWN</a>";}i:26;a:10:{s:5:"title";s:40:"MySQL Community Awards 2016: the Winners";s:4:"guid";s:36:"http://code.openark.org/blog/?p=7550";s:4:"link";s:74:"http://code.openark.org/blog/mysql/mysql-community-awards-2016-the-winners";s:11:"description";s:3498:"The MySQL Community Awards initiative is an effort to acknowledge and thank individuals and corporates for their contributions to the MySQL ecosystem. It is a from-the-community, by-the-community and for-the-community effort. The committee is composed of an independent group of community members of different orientation and opinion, themselves past winners or known contributors to the community.
The 2016 community awards were presented on April 21st, 2016, during the keynotes at the Percona Live conference. The winners are:
MySQL Community Awards: Community Contributor of the year 2016

Bill Karwin
Bill has been working with the community for years, helping them understand SQL. Bill is the author of the great book "SQL Antipatterns". He has given a large amount of help on sites such as StackOverflow, Quora, and of course many conference talks. Bill has provided a huge amount of help to the community directly.
Domas Mituzas
Domas Mituzas started in the MySQL ecosystem as a MySQL Support Engineer at MySQL AB. Since he had some spare time, he did a lot of work to scale MySQL at Wikipedia. He is now a small data engineer at Facebook, mostly working with user-facing data systems. He continues to write very interesting blog posts and bug reports. Domas is responsible for giving us MyDumper, PoorMansProfiler, and the infamous Query Cache tuner!
Yoshinori Matsunobu
Yoshinori Matsunobu is currently leading the MyRocks effort to get the RocksDB storage engine for MySQL into production at Facebook. Previously (amongst his other accomplishments) he created HandlerSocket, and implemented MHA to support failover automation for MySQL – both of which have been used at many companies. He is a frequent speaker at community events, and his tutorials and slide decks do a lot to increase expertise in the community. He is a frequent bug reporter with a focus on replication (RBR, semi-sync).

MySQL Community Awards: Application of the year 2016


MaxScale
MariaDB MaxScale is an Open Source dynamic routing gateway. It is widely used as a database load balancer for Galera Cluster deployments, for standard replication setups, and as a replication relay. It has a modular architecture which includes plugins for read-write splitting and query logging. It serves a variety of tasks, from load balancing to database firewall filtering to binlog server and is widely used in production in large topologies.

MySQL Community Awards: Corporate Contributor of the year 2016

Booking.com
Booking.com has been a massive contributor to the MySQL ecosystem, sending many of their excellent DBAs to various conferences to talk. They have provided an innovative test bed for testing out, and giving a wealth of invaluable feedback about new releases (across a wide variety of MySQL and related software projects). Booking.com contributes to Open Source foundations, projects and communities by donating, sponsoring, making code contributions and hosting events. The quality of MySQL is undoubtedly much better today because of their help and input.

Congrats to all winners!
Committee members

Baron Schwartz
Colin Charles
Daniël van Eeden
Davi Arnaut
Frederic Descamps
Geoffrey Anderson
Giuseppe Maxia
Justin Swanhart
Mark Leith
Morgan Tocker
Philip Stoev
Ronald Bradford
Santiago Lertora

Co-secretaries:

Jeremy Cole
Shlomi Noach

Special thanks
Thank you to this year's anonymous sponsor for donating the goblets!
Thank you to Colin Charles for acquiring and transporting the goblets!
&nbsp;";s:7:"content";a:1:{s:7:"encoded";s:4180:"<p>The MySQL Community Awards initiative is an effort to acknowledge and thank individuals and corporates for their contributions to the MySQL ecosystem. It is a from-the-community, by-the-community and for-the-community effort. The committee is composed of an independent group of community members of different orientation and opinion, themselves past winners or known contributors to the community.</p>
<p>The 2016 community awards were presented on April 21st, 2016, during the keynotes at the Percona Live conference. The winners are:</p>
<h4>MySQL Community Awards: Community Contributor of the year 2016</h4>
<ul>
<li><strong>Bill Karwin<br />
</strong>Bill has been working with the community for years, helping them understand SQL. Bill is the author of the great book "SQL Antipatterns". He has given a large amount of help on sites such as StackOverflow, Quora, and of course many conference talks. Bill has provided a huge amount of help to the community directly.</li>
<li><strong>Domas Mituzas<br />
</strong>Domas Mituzas started in the MySQL ecosystem as a MySQL Support Engineer at MySQL AB. Since he had some spare time, he did a lot of work to scale MySQL at Wikipedia. He is now a small data engineer at Facebook, mostly working with user-facing data systems. He continues to write very interesting blog posts and bug reports. Domas is responsible for giving us MyDumper, PoorMansProfiler, and the infamous Query Cache tuner!</li>
<li><strong>Yoshinori Matsunobu<br />
</strong>Yoshinori Matsunobu is currently leading the MyRocks effort to get the RocksDB storage engine for MySQL into production at Facebook. Previously (amongst his other accomplishments) he created HandlerSocket, and implemented MHA to support failover automation for MySQL – both of which have been used at many companies. He is a frequent speaker at community events, and his tutorials and slide decks do a lot to increase expertise in the community. He is a frequent bug reporter with a focus on replication (RBR, semi-sync).</li>
</ul>
<h4>MySQL Community Awards: Application of the year 2016</h4>
<p><span></span></p>
<ul>
<li><strong>MaxScale</strong><br />
MariaDB MaxScale is an Open Source dynamic routing gateway. It is widely used as a database load balancer for Galera Cluster deployments, for standard replication setups, and as a replication relay. It has a modular architecture which includes plugins for read-write splitting and query logging. It serves a variety of tasks, from load balancing to database firewall filtering to binlog server and is widely used in production in large topologies.</li>
</ul>
<h4>MySQL Community Awards: Corporate Contributor of the year 2016</h4>
<ul>
<li><strong>Booking.com</strong><br />
Booking.com has been a massive contributor to the MySQL ecosystem, sending many of their excellent DBAs to various conferences to talk. They have provided an innovative test bed for testing out, and giving a wealth of invaluable feedback about new releases (across a wide variety of MySQL and related software projects). Booking.com contributes to Open Source foundations, projects and communities by donating, sponsoring, making code contributions and hosting events. The quality of MySQL is undoubtedly much better today because of their help and input.</li>
</ul>
<p>Congrats to all winners!</p>
<h4>Committee members</h4>
<ul>
<li>Baron Schwartz</li>
<li>Colin Charles</li>
<li>Daniël van Eeden</li>
<li>Davi Arnaut</li>
<li>Frederic Descamps</li>
<li>Geoffrey Anderson</li>
<li>Giuseppe Maxia</li>
<li>Justin Swanhart</li>
<li>Mark Leith</li>
<li>Morgan Tocker</li>
<li>Philip Stoev</li>
<li>Ronald Bradford</li>
<li>Santiago Lertora</li>
</ul>
<p>Co-secretaries:</p>
<ul>
<li>Jeremy Cole</li>
<li>Shlomi Noach</li>
</ul>
<p><strong>Special thanks</strong></p>
<p>Thank you to this year's anonymous sponsor for donating the goblets!</p>
<p>Thank you to Colin Charles for acquiring and transporting the goblets!</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995241&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995241&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Fri, 22 Apr 2016 17:03:42 +0000";s:2:"dc";a:1:{s:7:"creator";s:12:"Shlomi Noach";}s:8:"category";s:5:"MySQL";s:7:"summary";s:3498:"The MySQL Community Awards initiative is an effort to acknowledge and thank individuals and corporates for their contributions to the MySQL ecosystem. It is a from-the-community, by-the-community and for-the-community effort. The committee is composed of an independent group of community members of different orientation and opinion, themselves past winners or known contributors to the community.
The 2016 community awards were presented on April 21st, 2016, during the keynotes at the Percona Live conference. The winners are:
MySQL Community Awards: Community Contributor of the year 2016

Bill Karwin
Bill has been working with the community for years, helping them understand SQL. Bill is the author of the great book "SQL Antipatterns". He has given a large amount of help on sites such as StackOverflow, Quora, and of course many conference talks. Bill has provided a huge amount of help to the community directly.
Domas Mituzas
Domas Mituzas started in the MySQL ecosystem as a MySQL Support Engineer at MySQL AB. Since he had some spare time, he did a lot of work to scale MySQL at Wikipedia. He is now a small data engineer at Facebook, mostly working with user-facing data systems. He continues to write very interesting blog posts and bug reports. Domas is responsible for giving us MyDumper, PoorMansProfiler, and the infamous Query Cache tuner!
Yoshinori Matsunobu
Yoshinori Matsunobu is currently leading the MyRocks effort to get the RocksDB storage engine for MySQL into production at Facebook. Previously (amongst his other accomplishments) he created HandlerSocket, and implemented MHA to support failover automation for MySQL – both of which have been used at many companies. He is a frequent speaker at community events, and his tutorials and slide decks do a lot to increase expertise in the community. He is a frequent bug reporter with a focus on replication (RBR, semi-sync).

MySQL Community Awards: Application of the year 2016


MaxScale
MariaDB MaxScale is an Open Source dynamic routing gateway. It is widely used as a database load balancer for Galera Cluster deployments, for standard replication setups, and as a replication relay. It has a modular architecture which includes plugins for read-write splitting and query logging. It serves a variety of tasks, from load balancing to database firewall filtering to binlog server and is widely used in production in large topologies.

MySQL Community Awards: Corporate Contributor of the year 2016

Booking.com
Booking.com has been a massive contributor to the MySQL ecosystem, sending many of their excellent DBAs to various conferences to talk. They have provided an innovative test bed for testing out, and giving a wealth of invaluable feedback about new releases (across a wide variety of MySQL and related software projects). Booking.com contributes to Open Source foundations, projects and communities by donating, sponsoring, making code contributions and hosting events. The quality of MySQL is undoubtedly much better today because of their help and input.

Congrats to all winners!
Committee members

Baron Schwartz
Colin Charles
Daniël van Eeden
Davi Arnaut
Frederic Descamps
Geoffrey Anderson
Giuseppe Maxia
Justin Swanhart
Mark Leith
Morgan Tocker
Philip Stoev
Ronald Bradford
Santiago Lertora

Co-secretaries:

Jeremy Cole
Shlomi Noach

Special thanks
Thank you to this year's anonymous sponsor for donating the goblets!
Thank you to Colin Charles for acquiring and transporting the goblets!
&nbsp;";s:12:"atom_content";s:4180:"<p>The MySQL Community Awards initiative is an effort to acknowledge and thank individuals and corporates for their contributions to the MySQL ecosystem. It is a from-the-community, by-the-community and for-the-community effort. The committee is composed of an independent group of community members of different orientation and opinion, themselves past winners or known contributors to the community.</p>
<p>The 2016 community awards were presented on April 21st, 2016, during the keynotes at the Percona Live conference. The winners are:</p>
<h4>MySQL Community Awards: Community Contributor of the year 2016</h4>
<ul>
<li><strong>Bill Karwin<br />
</strong>Bill has been working with the community for years, helping them understand SQL. Bill is the author of the great book "SQL Antipatterns". He has given a large amount of help on sites such as StackOverflow, Quora, and of course many conference talks. Bill has provided a huge amount of help to the community directly.</li>
<li><strong>Domas Mituzas<br />
</strong>Domas Mituzas started in the MySQL ecosystem as a MySQL Support Engineer at MySQL AB. Since he had some spare time, he did a lot of work to scale MySQL at Wikipedia. He is now a small data engineer at Facebook, mostly working with user-facing data systems. He continues to write very interesting blog posts and bug reports. Domas is responsible for giving us MyDumper, PoorMansProfiler, and the infamous Query Cache tuner!</li>
<li><strong>Yoshinori Matsunobu<br />
</strong>Yoshinori Matsunobu is currently leading the MyRocks effort to get the RocksDB storage engine for MySQL into production at Facebook. Previously (amongst his other accomplishments) he created HandlerSocket, and implemented MHA to support failover automation for MySQL – both of which have been used at many companies. He is a frequent speaker at community events, and his tutorials and slide decks do a lot to increase expertise in the community. He is a frequent bug reporter with a focus on replication (RBR, semi-sync).</li>
</ul>
<h4>MySQL Community Awards: Application of the year 2016</h4>
<p><span></span></p>
<ul>
<li><strong>MaxScale</strong><br />
MariaDB MaxScale is an Open Source dynamic routing gateway. It is widely used as a database load balancer for Galera Cluster deployments, for standard replication setups, and as a replication relay. It has a modular architecture which includes plugins for read-write splitting and query logging. It serves a variety of tasks, from load balancing to database firewall filtering to binlog server and is widely used in production in large topologies.</li>
</ul>
<h4>MySQL Community Awards: Corporate Contributor of the year 2016</h4>
<ul>
<li><strong>Booking.com</strong><br />
Booking.com has been a massive contributor to the MySQL ecosystem, sending many of their excellent DBAs to various conferences to talk. They have provided an innovative test bed for testing out, and giving a wealth of invaluable feedback about new releases (across a wide variety of MySQL and related software projects). Booking.com contributes to Open Source foundations, projects and communities by donating, sponsoring, making code contributions and hosting events. The quality of MySQL is undoubtedly much better today because of their help and input.</li>
</ul>
<p>Congrats to all winners!</p>
<h4>Committee members</h4>
<ul>
<li>Baron Schwartz</li>
<li>Colin Charles</li>
<li>Daniël van Eeden</li>
<li>Davi Arnaut</li>
<li>Frederic Descamps</li>
<li>Geoffrey Anderson</li>
<li>Giuseppe Maxia</li>
<li>Justin Swanhart</li>
<li>Mark Leith</li>
<li>Morgan Tocker</li>
<li>Philip Stoev</li>
<li>Ronald Bradford</li>
<li>Santiago Lertora</li>
</ul>
<p>Co-secretaries:</p>
<ul>
<li>Jeremy Cole</li>
<li>Shlomi Noach</li>
</ul>
<p><strong>Special thanks</strong></p>
<p>Thank you to this year's anonymous sponsor for donating the goblets!</p>
<p>Thank you to Colin Charles for acquiring and transporting the goblets!</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995241&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995241&vote=-1&apivote=1">Vote DOWN</a>";}i:27;a:9:{s:5:"title";s:57:"The new MySQL X DevAPI Protocol and what it means for PHP";s:4:"guid";s:70:"tag:blogger.com,1999:blog-8634321333008124397.post-2479335815056156179";s:4:"link";s:84:"http://elephantdolphin.blogspot.com/2016/04/the-new-mysql-x-devapi-protocol-and.html";s:11:"description";s:2788:"The relational database world just changed and you didn't notice, did you?  Rather than speaking SQL to your data, what if you could talk to it in PHP directly?  No longer do you have to struggle to remember arcane structured query language syntax and just grab data with PHP. You can still write bad queries full of N+1 errors and a whole host of other problems.  But you are speaking PHP to your data. And this started with MySQL 5.7.12 and its new Document Store Functionality. The Document Store functionality expands on the new JSON data type in MySQL 5.7 with a new server plugin, a new API, and a suite of new components designed to make MySQL accessible for users who are not familiar with the SQL language or prefer to use a schemaless data store. This MySQL Server plugin  enables communication using the X Protocol. And clients that implement X DevAPI and enables using MySQL as a document store easily. So you can talk to your data in the language of your choice (say Python, Javascript, or SQL) via a new shell or use a new connector to use these features. For those in the PHP world I must warn you that the new connector to support the new X DevAPI is on the way -- not here yet but on the way. Trying the XDevAPIYou will need MySQL 5.7.12 with the X plugin enabled and the new mysqlsh. Details on the installation of the shell and plugin can be found under  Using MySQL as a Document Store in the MySQL 5.7 Reference Manual. Fire up the shell and you will see something like this: That mysql-js> prompt lets you know that your shell is currently in Javascript more. Currently the shell has modes for  Javascript, Python, or SQL. To connect to a database simply type  db = session.getSchema('world_x') It is then easy to use Javascript to create, remove, update or delete data. The X Plugin extends MySQL Server to be able to function as a document store and the X Protocol, which is designed to expose the ACID compliant storage abilities of MySQL as a document store. Documents are stored in JSON format and enable schema-less storage. Using the X DevAPI you can use a NoSQL-like syntax to execute Create, Read, Update, Delete (CRUD) operations against these documents. The server will be listening to port 33060 (configurable) for communications in the new protocol.And any data you drop into the document store is avail from the document store in JSON format AND is accessible at the same time from SQL.  And by using the new connectors (Java, .NET, Node.JS for now) that support the X protocol, your code hits the database without have to fidget and grumble about using SQL.  You are talking in your language to a relational database using NoSQL.  And you data you add via the X protocol in Node.JS can be used by someone with Python, or Java, or SQL at the same time.";s:7:"content";a:1:{s:7:"encoded";s:3763:"The relational database world just changed and you didn't notice, did you?  Rather than speaking SQL to your data, what if you could talk to it in PHP directly?  No longer do you have to struggle to remember arcane structured query language syntax and just grab data with PHP. You can still write bad queries full of N+1 errors and a whole host of other problems.  But you are speaking PHP to your data. And this started with MySQL 5.7.12 and its new Document Store Functionality. <p>The Document Store functionality expands on the new JSON data type in MySQL 5.7 with a new server plugin, a new API, and a suite of new components designed to make MySQL accessible for users who are not familiar with the SQL language or prefer to use a schemaless data store. This MySQL Server plugin  enables communication using the X Protocol. And clients that implement X DevAPI and enables using MySQL as a document store easily. So you can talk to your data in the language of your choice (say Python, Javascript, or SQL) via a new shell or use a new connector to use these features. For those in the PHP world I must warn you that the new connector to support the new X DevAPI is on the way -- not here yet but on the way. <h2>Trying the XDevAPI</h2>You will need MySQL 5.7.12 with the X plugin enabled and the new <i>mysqlsh</i>. Details on the installation of the shell and plugin can be found under  <a href="https://dev.mysql.com/doc/refman/5.7/en/document-store.html">Using MySQL as a Document Store</a> in the MySQL 5.7 Reference Manual. Fire up the shell and you will see something like this: <a href="https://4.bp.blogspot.com/-Oz5mIHP60GQ/Vxju97sAQJI/AAAAAAAAQzM/TkLnlB71wV4NRMVCNbd53BuCsxgwOI2RACLcB/s1600/mysqlshellistall03.png" imageanchor="1"><img border="0" src="https://4.bp.blogspot.com/-Oz5mIHP60GQ/Vxju97sAQJI/AAAAAAAAQzM/TkLnlB71wV4NRMVCNbd53BuCsxgwOI2RACLcB/s400/mysqlshellistall03.png" /></a><p>That<i> mysql-js></i> prompt lets you know that your shell is currently in Javascript more. Currently the shell has modes for  Javascript, Python, or SQL. To connect to a database simply type  <pre><br />db = session.getSchema('world_x')<br /></pre> It is then easy to use Javascript to create, remove, update or delete data. <div><a href="https://2.bp.blogspot.com/-uzSuBGi6SaE/Vxj0GpkbxEI/AAAAAAAAQzc/Ltkh8NgWa4MMxIxOCm34Fqe99PzgwBlYgCLcB/s1600/jsfind.png" imageanchor="1"><img border="0" src="https://2.bp.blogspot.com/-uzSuBGi6SaE/Vxj0GpkbxEI/AAAAAAAAQzc/Ltkh8NgWa4MMxIxOCm34Fqe99PzgwBlYgCLcB/s640/jsfind.png" /></a></div><p>The X Plugin extends MySQL Server to be able to function as a document store and the X Protocol, which is designed to expose the ACID compliant storage abilities of MySQL as a document store. Documents are stored in JSON format and enable schema-less storage. Using the X DevAPI you can use a NoSQL-like syntax to execute Create, Read, Update, Delete (CRUD) operations against these documents. The server will be listening to port 33060 (configurable) for communications in the new protocol.<p>And any data you drop into the document store is avail from the document store in JSON format <b>AND<i></i></b> is accessible at the same time from SQL.  And by using the new connectors (Java, .NET, Node.JS for now) that support the X protocol, your code hits the database without have to fidget and grumble about using SQL.  You are talking in your language to a relational database using NoSQL.  <p>And you data you add via the X protocol in Node.JS can be used by someone with Python, or Java, or SQL at the same time.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995240&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995240&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Fri, 22 Apr 2016 16:55:00 +0000";s:2:"dc";a:1:{s:7:"creator";s:11:"Dave Stokes";}s:7:"summary";s:2788:"The relational database world just changed and you didn't notice, did you?  Rather than speaking SQL to your data, what if you could talk to it in PHP directly?  No longer do you have to struggle to remember arcane structured query language syntax and just grab data with PHP. You can still write bad queries full of N+1 errors and a whole host of other problems.  But you are speaking PHP to your data. And this started with MySQL 5.7.12 and its new Document Store Functionality. The Document Store functionality expands on the new JSON data type in MySQL 5.7 with a new server plugin, a new API, and a suite of new components designed to make MySQL accessible for users who are not familiar with the SQL language or prefer to use a schemaless data store. This MySQL Server plugin  enables communication using the X Protocol. And clients that implement X DevAPI and enables using MySQL as a document store easily. So you can talk to your data in the language of your choice (say Python, Javascript, or SQL) via a new shell or use a new connector to use these features. For those in the PHP world I must warn you that the new connector to support the new X DevAPI is on the way -- not here yet but on the way. Trying the XDevAPIYou will need MySQL 5.7.12 with the X plugin enabled and the new mysqlsh. Details on the installation of the shell and plugin can be found under  Using MySQL as a Document Store in the MySQL 5.7 Reference Manual. Fire up the shell and you will see something like this: That mysql-js> prompt lets you know that your shell is currently in Javascript more. Currently the shell has modes for  Javascript, Python, or SQL. To connect to a database simply type  db = session.getSchema('world_x') It is then easy to use Javascript to create, remove, update or delete data. The X Plugin extends MySQL Server to be able to function as a document store and the X Protocol, which is designed to expose the ACID compliant storage abilities of MySQL as a document store. Documents are stored in JSON format and enable schema-less storage. Using the X DevAPI you can use a NoSQL-like syntax to execute Create, Read, Update, Delete (CRUD) operations against these documents. The server will be listening to port 33060 (configurable) for communications in the new protocol.And any data you drop into the document store is avail from the document store in JSON format AND is accessible at the same time from SQL.  And by using the new connectors (Java, .NET, Node.JS for now) that support the X protocol, your code hits the database without have to fidget and grumble about using SQL.  You are talking in your language to a relational database using NoSQL.  And you data you add via the X protocol in Node.JS can be used by someone with Python, or Java, or SQL at the same time.";s:12:"atom_content";s:3763:"The relational database world just changed and you didn't notice, did you?  Rather than speaking SQL to your data, what if you could talk to it in PHP directly?  No longer do you have to struggle to remember arcane structured query language syntax and just grab data with PHP. You can still write bad queries full of N+1 errors and a whole host of other problems.  But you are speaking PHP to your data. And this started with MySQL 5.7.12 and its new Document Store Functionality. <p>The Document Store functionality expands on the new JSON data type in MySQL 5.7 with a new server plugin, a new API, and a suite of new components designed to make MySQL accessible for users who are not familiar with the SQL language or prefer to use a schemaless data store. This MySQL Server plugin  enables communication using the X Protocol. And clients that implement X DevAPI and enables using MySQL as a document store easily. So you can talk to your data in the language of your choice (say Python, Javascript, or SQL) via a new shell or use a new connector to use these features. For those in the PHP world I must warn you that the new connector to support the new X DevAPI is on the way -- not here yet but on the way. <h2>Trying the XDevAPI</h2>You will need MySQL 5.7.12 with the X plugin enabled and the new <i>mysqlsh</i>. Details on the installation of the shell and plugin can be found under  <a href="https://dev.mysql.com/doc/refman/5.7/en/document-store.html">Using MySQL as a Document Store</a> in the MySQL 5.7 Reference Manual. Fire up the shell and you will see something like this: <a href="https://4.bp.blogspot.com/-Oz5mIHP60GQ/Vxju97sAQJI/AAAAAAAAQzM/TkLnlB71wV4NRMVCNbd53BuCsxgwOI2RACLcB/s1600/mysqlshellistall03.png" imageanchor="1"><img border="0" src="https://4.bp.blogspot.com/-Oz5mIHP60GQ/Vxju97sAQJI/AAAAAAAAQzM/TkLnlB71wV4NRMVCNbd53BuCsxgwOI2RACLcB/s400/mysqlshellistall03.png" /></a><p>That<i> mysql-js></i> prompt lets you know that your shell is currently in Javascript more. Currently the shell has modes for  Javascript, Python, or SQL. To connect to a database simply type  <pre><br />db = session.getSchema('world_x')<br /></pre> It is then easy to use Javascript to create, remove, update or delete data. <div><a href="https://2.bp.blogspot.com/-uzSuBGi6SaE/Vxj0GpkbxEI/AAAAAAAAQzc/Ltkh8NgWa4MMxIxOCm34Fqe99PzgwBlYgCLcB/s1600/jsfind.png" imageanchor="1"><img border="0" src="https://2.bp.blogspot.com/-uzSuBGi6SaE/Vxj0GpkbxEI/AAAAAAAAQzc/Ltkh8NgWa4MMxIxOCm34Fqe99PzgwBlYgCLcB/s640/jsfind.png" /></a></div><p>The X Plugin extends MySQL Server to be able to function as a document store and the X Protocol, which is designed to expose the ACID compliant storage abilities of MySQL as a document store. Documents are stored in JSON format and enable schema-less storage. Using the X DevAPI you can use a NoSQL-like syntax to execute Create, Read, Update, Delete (CRUD) operations against these documents. The server will be listening to port 33060 (configurable) for communications in the new protocol.<p>And any data you drop into the document store is avail from the document store in JSON format <b>AND<i></i></b> is accessible at the same time from SQL.  And by using the new connectors (Java, .NET, Node.JS for now) that support the X protocol, your code hits the database without have to fidget and grumble about using SQL.  You are talking in your language to a relational database using NoSQL.  <p>And you data you add via the X protocol in Node.JS can be used by someone with Python, or Java, or SQL at the same time.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995240&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995240&vote=-1&apivote=1">Vote DOWN</a>";}i:28;a:9:{s:5:"title";s:124:"The Perfect Server - Ubuntu 16.04 (Xenial Xerus) with Apache, PHP, MySQL, PureFTPD, BIND, Postfix, Dovecot and ISPConfig 3.1";s:4:"guid";s:123:"https://www.howtoforge.com/tutorial/ubuntu-perfect-server-with-apache-php-myqsl-pureftpd-bind-postfix-doveot-and-ispconfig/";s:4:"link";s:123:"https://www.howtoforge.com/tutorial/ubuntu-perfect-server-with-apache-php-myqsl-pureftpd-bind-postfix-doveot-and-ispconfig/";s:11:"description";s:579:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).";s:7:"content";a:1:{s:7:"encoded";s:792:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5992824&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5992824&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Fri, 22 Apr 2016 14:21:15 +0000";s:8:"category";s:6:"ubuntu";s:7:"summary";s:579:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).";s:12:"atom_content";s:792:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5992824&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5992824&vote=-1&apivote=1">Vote DOWN</a>";}i:29;a:9:{s:5:"title";s:124:"The Perfect Server - Ubuntu 16.04 (Xenial Xerus) with Apache, PHP, MySQL, PureFTPD, BIND, Postfix, Dovecot and ISPConfig 3.1";s:4:"guid";s:129:"https://www.howtoforge.com/tutorial/perfect-server-ubuntu-16.04-with-apache-php-myqsl-pureftpd-bind-postfix-doveot-and-ispconfig/";s:4:"link";s:129:"https://www.howtoforge.com/tutorial/perfect-server-ubuntu-16.04-with-apache-php-myqsl-pureftpd-bind-postfix-doveot-and-ispconfig/";s:11:"description";s:579:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).";s:7:"content";a:1:{s:7:"encoded";s:792:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995238&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995238&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Fri, 22 Apr 2016 14:21:15 +0000";s:8:"category";s:6:"ubuntu";s:7:"summary";s:579:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).";s:12:"atom_content";s:792:"This tutorial shows how to install an Ubuntu 16.04 (Xenial Xerus) server (with Apache2, BIND, Dovecot) for the installation of ISPConfig 3.1, and how to install ISPConfig. ISPConfig 3 is a web hosting control panel that allows you to configure the following services through a web browser: Apache or nginx web server, Postfix mail server, Courier or Dovecot IMAP/POP3 server, MySQL, BIND or MyDNS nameserver, PureFTPd, SpamAssassin, ClamAV, and many more. This setup covers the installation of Apache (instead of nginx), BIND (instead of MyDNS), and Dovecot (instead of Courier).<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995238&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995238&vote=-1&apivote=1">Vote DOWN</a>";}i:30;a:10:{s:5:"title";s:51:"MariaDB 5.5.49 and updated connectors now available";s:4:"guid";s:27:"https://mariadb.org/?p=3478";s:4:"link";s:68:"https://mariadb.org/mariadb-5-5-49-updated-connectors-now-available/";s:11:"description";s:503:"The MariaDB project is pleased to announce the immediate availability of MariaDB 5.5.49, MariaDB Connector/J 1.4.2, and MariaDB Connector/ODBC 2.0.10. See the release notes and changelogs for details on these releases. Download MariaDB 5.5.49 Release Notes Changelog What is MariaDB 5.5? MariaDB APT and YUM Repository Configuration Generator Download MariaDB Connector/J 1.4.2 Release Notes Changelog [&#8230;]
The post MariaDB 5.5.49 and updated connectors now available appeared first on MariaDB.org.";s:7:"content";a:1:{s:7:"encoded";s:877:"<p>The MariaDB project is pleased to announce the immediate availability of MariaDB 5.5.49, MariaDB Connector/J 1.4.2, and MariaDB Connector/ODBC 2.0.10. See the release notes and changelogs for details on these releases. Download MariaDB 5.5.49 Release Notes Changelog What is MariaDB 5.5? MariaDB APT and YUM Repository Configuration Generator Download MariaDB Connector/J 1.4.2 Release Notes Changelog [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://mariadb.org/mariadb-5-5-49-updated-connectors-now-available/">MariaDB 5.5.49 and updated connectors now available</a> appeared first on <a rel="nofollow" href="https://mariadb.org">MariaDB.org</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995229&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995229&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Fri, 22 Apr 2016 14:11:38 +0000";s:2:"dc";a:1:{s:7:"creator";s:18:"Daniel Bartholomew";}s:8:"category";s:88:"AnnouncementsConnector/JConnector/ODBCMariaDB 5.5MariaDB ConnectorsMariaDB Releasesmysql";s:7:"summary";s:503:"The MariaDB project is pleased to announce the immediate availability of MariaDB 5.5.49, MariaDB Connector/J 1.4.2, and MariaDB Connector/ODBC 2.0.10. See the release notes and changelogs for details on these releases. Download MariaDB 5.5.49 Release Notes Changelog What is MariaDB 5.5? MariaDB APT and YUM Repository Configuration Generator Download MariaDB Connector/J 1.4.2 Release Notes Changelog [&#8230;]
The post MariaDB 5.5.49 and updated connectors now available appeared first on MariaDB.org.";s:12:"atom_content";s:877:"<p>The MariaDB project is pleased to announce the immediate availability of MariaDB 5.5.49, MariaDB Connector/J 1.4.2, and MariaDB Connector/ODBC 2.0.10. See the release notes and changelogs for details on these releases. Download MariaDB 5.5.49 Release Notes Changelog What is MariaDB 5.5? MariaDB APT and YUM Repository Configuration Generator Download MariaDB Connector/J 1.4.2 Release Notes Changelog [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://mariadb.org/mariadb-5-5-49-updated-connectors-now-available/">MariaDB 5.5.49 and updated connectors now available</a> appeared first on <a rel="nofollow" href="https://mariadb.org">MariaDB.org</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995229&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995229&vote=-1&apivote=1">Vote DOWN</a>";}i:31;a:10:{s:5:"title";s:43:"Percona Live 2016: Percona Live Game Night!";s:4:"guid";s:37:"https://www.percona.com/blog/?p=35240";s:4:"link";s:77:"https://www.percona.com/blog/2016/04/21/percona-live-2016-percona-game-night/";s:11:"description";s:1282:"Wednesday night at Percona Live 2016 was reserved for fun, fun, fun! Once again, the Percona Live Game Night proved to be a popular and amazing event. There were more games this year than last, as well as food, drinks, and lots of friendly competition!
This year, besides the ever-crowd-pleasing Meltdown Challenge, there were Segway Races, pool, foosball, shuffleboard, Wii Boxing, Pac-Man Attack, a shootout gallery, darts, as well as virtual reality stations and a death-defying trampoline.
You can see Percona&#8217;s CEO Peter Zaitsev demonstrating how you use it, pro-level:

Below are some more photos of this outstanding night:

Coed boxing: guys, you need to improve your skills!


Some very intense foosball action!


This Pac-Man Attack reminds me of 1983!


Keep an eye on your wallets, gentlemen, I detect a hustle.


More trampoline.


For those who like less effort with their trampoline, virtual reality.


A little social lubrication.


Happy attendees (must have stopped at the previous picture).


Hmm, that guy looks a bit confused. Must be too much tech talk for one day!


I&#8217;d stay away from this table. Just saying.


More happy.

Thanks to everybody who came out and participated in an awesome night! We&#8217;ll see you all next year!
&nbsp;
&nbsp;";s:7:"content";a:1:{s:7:"encoded";s:8693:"<img width="150" height="100" src="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-150x100.jpg" class="attachment-thumbnail size-thumbnail wp-post-image" alt="percona live 2016" style="float: left; margin-right: 5px;" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy.jpg 988w" sizes="(max-width: 150px) 100vw, 150px" /><p>Wednesday night at Percona Live 2016 was reserved for fun, fun, fun! Once again, the Percona Live Game Night proved to be a popular and amazing event. There were more games this year than last, as well as food, drinks, and lots of friendly competition!</p>
<p>This year, besides the ever-crowd-pleasing Meltdown Challenge, there were Segway Races, pool, foosball, shuffleboard, Wii Boxing, Pac-Man Attack, a shootout gallery, darts, as well as virtual reality stations and a death-defying trampoline.</p>
<p>You can see Percona&#8217;s CEO Peter Zaitsev demonstrating how you use it, pro-level:</p>
<p></p>
<p>Below are some more photos of this outstanding night:</p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/boxing.jpg" rel="attachment wp-att-35242"><img class="size-medium wp-image-35242 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/boxing-200x300.jpg" alt="percona live 2016" width="200" height="300" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/boxing-100x150.jpg 100w, https://www.percona.com/blog/wp-content/uploads/2016/04/boxing-200x300.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2016/04/boxing.jpg 659w" sizes="(max-width: 200px) 100vw, 200px" /></a></p>
<p>Coed boxing: guys, you need to improve your skills!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/foosball.jpg" rel="attachment wp-att-35243"><img class="size-medium wp-image-35243 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/foosball-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/foosball-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/foosball-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/foosball.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Some very intense foosball action!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/pacman.jpg" rel="attachment wp-att-35244"><img class="size-medium wp-image-35244 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/pacman-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/pacman-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/pacman-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/pacman.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>This Pac-Man Attack reminds me of 1983!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/pool.jpg" rel="attachment wp-att-35245"><img class="size-medium wp-image-35245 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/pool-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/pool-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/pool-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/pool.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Keep an eye on your wallets, gentlemen, I detect a hustle.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline.jpg" rel="attachment wp-att-35247"><img class="size-medium wp-image-35247 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>More trampoline.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2.jpg" rel="attachment wp-att-35246"><img class="size-medium wp-image-35246 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>For those who like less effort with their trampoline, virtual reality.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/drinks.jpg" rel="attachment wp-att-35248"><img class="size-medium wp-image-35248 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/drinks-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/drinks-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/drinks-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/drinks.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>A little social lubrication.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2.jpg" rel="attachment wp-att-35249"><img class="wp-image-35249 size-medium alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Happy attendees (must have stopped at the previous picture).</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/confused.jpg" rel="attachment wp-att-35250"><img class="size-medium wp-image-35250 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/confused-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/confused-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/confused-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/confused.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Hmm, that guy looks a bit confused. Must be too much tech talk for one day!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants.jpg" rel="attachment wp-att-35252"><img class="size-medium wp-image-35252 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>I&#8217;d stay away from this table. Just saying.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/happy.jpg" rel="attachment wp-att-35251"><img class="size-medium wp-image-35251 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>More happy.</p>
<p>
<p>Thanks to everybody who came out and participated in an awesome night! We&#8217;ll see you all next year!</p>
<p>&nbsp;</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995212&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995212&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 21 Apr 2016 20:09:01 +0000";s:2:"dc";a:1:{s:7:"creator";s:22:"MySQL Performance Blog";}s:8:"category";s:52:"MySQLPercona LivePercona Game NightPercona Live 2016";s:7:"summary";s:1282:"Wednesday night at Percona Live 2016 was reserved for fun, fun, fun! Once again, the Percona Live Game Night proved to be a popular and amazing event. There were more games this year than last, as well as food, drinks, and lots of friendly competition!
This year, besides the ever-crowd-pleasing Meltdown Challenge, there were Segway Races, pool, foosball, shuffleboard, Wii Boxing, Pac-Man Attack, a shootout gallery, darts, as well as virtual reality stations and a death-defying trampoline.
You can see Percona&#8217;s CEO Peter Zaitsev demonstrating how you use it, pro-level:

Below are some more photos of this outstanding night:

Coed boxing: guys, you need to improve your skills!


Some very intense foosball action!


This Pac-Man Attack reminds me of 1983!


Keep an eye on your wallets, gentlemen, I detect a hustle.


More trampoline.


For those who like less effort with their trampoline, virtual reality.


A little social lubrication.


Happy attendees (must have stopped at the previous picture).


Hmm, that guy looks a bit confused. Must be too much tech talk for one day!


I&#8217;d stay away from this table. Just saying.


More happy.

Thanks to everybody who came out and participated in an awesome night! We&#8217;ll see you all next year!
&nbsp;
&nbsp;";s:12:"atom_content";s:8693:"<img width="150" height="100" src="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-150x100.jpg" class="attachment-thumbnail size-thumbnail wp-post-image" alt="percona live 2016" style="float: left; margin-right: 5px;" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy.jpg 988w" sizes="(max-width: 150px) 100vw, 150px" /><p>Wednesday night at Percona Live 2016 was reserved for fun, fun, fun! Once again, the Percona Live Game Night proved to be a popular and amazing event. There were more games this year than last, as well as food, drinks, and lots of friendly competition!</p>
<p>This year, besides the ever-crowd-pleasing Meltdown Challenge, there were Segway Races, pool, foosball, shuffleboard, Wii Boxing, Pac-Man Attack, a shootout gallery, darts, as well as virtual reality stations and a death-defying trampoline.</p>
<p>You can see Percona&#8217;s CEO Peter Zaitsev demonstrating how you use it, pro-level:</p>
<p></p>
<p>Below are some more photos of this outstanding night:</p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/boxing.jpg" rel="attachment wp-att-35242"><img class="size-medium wp-image-35242 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/boxing-200x300.jpg" alt="percona live 2016" width="200" height="300" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/boxing-100x150.jpg 100w, https://www.percona.com/blog/wp-content/uploads/2016/04/boxing-200x300.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2016/04/boxing.jpg 659w" sizes="(max-width: 200px) 100vw, 200px" /></a></p>
<p>Coed boxing: guys, you need to improve your skills!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/foosball.jpg" rel="attachment wp-att-35243"><img class="size-medium wp-image-35243 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/foosball-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/foosball-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/foosball-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/foosball.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Some very intense foosball action!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/pacman.jpg" rel="attachment wp-att-35244"><img class="size-medium wp-image-35244 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/pacman-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/pacman-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/pacman-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/pacman.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>This Pac-Man Attack reminds me of 1983!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/pool.jpg" rel="attachment wp-att-35245"><img class="size-medium wp-image-35245 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/pool-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/pool-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/pool-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/pool.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Keep an eye on your wallets, gentlemen, I detect a hustle.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline.jpg" rel="attachment wp-att-35247"><img class="size-medium wp-image-35247 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/trampoline.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>More trampoline.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2.jpg" rel="attachment wp-att-35246"><img class="size-medium wp-image-35246 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/virtual-reality-2.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>For those who like less effort with their trampoline, virtual reality.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/drinks.jpg" rel="attachment wp-att-35248"><img class="size-medium wp-image-35248 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/drinks-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/drinks-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/drinks-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/drinks.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>A little social lubrication.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2.jpg" rel="attachment wp-att-35249"><img class="wp-image-35249 size-medium alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-2.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Happy attendees (must have stopped at the previous picture).</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/confused.jpg" rel="attachment wp-att-35250"><img class="size-medium wp-image-35250 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/confused-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/confused-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/confused-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/confused.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Hmm, that guy looks a bit confused. Must be too much tech talk for one day!</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants.jpg" rel="attachment wp-att-35252"><img class="size-medium wp-image-35252 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/miscreants.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>I&#8217;d stay away from this table. Just saying.</p>
<p>
<p><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/happy.jpg" rel="attachment wp-att-35251"><img class="size-medium wp-image-35251 alignnone" src="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-300x200.jpg" alt="percona live 2016" width="300" height="200" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/happy-150x100.jpg 150w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy-300x200.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/happy.jpg 988w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>More happy.</p>
<p>
<p>Thanks to everybody who came out and participated in an awesome night! We&#8217;ll see you all next year!</p>
<p>&nbsp;</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995212&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995212&vote=-1&apivote=1">Vote DOWN</a>";}i:32;a:10:{s:5:"title";s:42:"Percona Live 2016: Day Three Keynote Talks";s:4:"guid";s:37:"https://www.percona.com/blog/?p=35230";s:4:"link";s:58:"https://www.percona.com/blog/2016/04/21/percona-live-2016/";s:11:"description";s:2580:"We&#8217;re heading into the final day here at Percona Live 2016! People are looking a little tired, but still excited for some excellent talks today. Once again the day started off with two great keynote lectures. Read to the end for an important announcement!
Peter Zaitsev, CEO of Percona
Winning with Open Source Databases
Peter discussed what makes the open source community so vibrant and powerful, and why it is essential to preserve that spirit. Data is critical to the success of your business. You rely on your database and its supporting systems to power the applications that drive your business. These systems must be reliable, scalable, efficient – and increasingly, open source. With the complexity of today’s web applications and the databases, systems and hardware that support them, organizations must use the right open source tools and technology for each job – without getting locked into a proprietary solution. With Percona, customers are assured a choice in technology options that are completely open source (and include enterprise features). We help our customers find the right technology for their specific needs, rather than sell a one-size-fits-all product. Percona is a true open source partner that helps you optimize your database performance to better run your business.
Patrick McFadin, Chief Evangelist at DataStax
Take back the power in your cloud applications with Apache Cassandra
Patrick discussed how cloud applications can help you to develop the applications you need in your business, but also outline why the cloud isn&#8217;t a panacea for every business issue. Database engineers have had to support the crazy dreams of application developers since the beginning of the internet. Patrick says it&#8217;s time to take back the power! He believes that Apache Cassandra is the tool that can help you eliminate downtime or span your data around the world with ease. Deploying to the cloud isn’t always easy, but Cassandra might be able to give your application developers the best chance they can get and sleep easy at night.
&nbsp;
Post MongoDB World New York Conference, June 30th, Hilton Mid-Town Manhattan
Peter also made an important announcement: Percona and ObjectRocket/Rackspace will be sponsoring a free post-MongoDB World Community Event! The event will take place on Thursday, June 30th at the Hilton Mid-Town Manhattan.
All are welcome.
Don&#8217;t miss out on this amazing opportunity to share ideas and get insights after MongoDB World.
Check out the rest of today&#8217;s Percona Live 2016 schedule here.";s:7:"content";a:1:{s:7:"encoded";s:5216:"<p>We&#8217;re heading into the final day here at Percona Live 2016! People are looking a little tired, but still excited for some excellent talks today. Once again the day started off with two great keynote lectures. Read to the end for an important announcement!</p>
<p><strong><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter.png" rel="attachment wp-att-35232"><img class="size-medium wp-image-35232 alignleft" src="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter-300x169.png" alt="Percona Live 2016" width="300" height="169" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter.png 888w" sizes="(max-width: 300px) 100vw, 300px" /></a><a href="https://www.percona.com/live/data-performance-conference-2016/users/peter-zaitsev">Peter Zaitsev</a>, CEO of Percona</strong><br />
<strong><a href="https://www.percona.com/live/data-performance-conference-2016/sessions/winning-open-source-databases">Winning with Open Source Databases</a></strong></p>
<p>Peter discussed what makes the open source community so vibrant and powerful, and why it is essential to preserve that spirit. Data is critical to the success of your business. You rely on your database and its supporting systems to power the applications that drive your business. These systems must be reliable, scalable, efficient – and increasingly, open source. With the complexity of today’s web applications and the databases, systems and hardware that support them, organizations must use the right open source tools and technology for each job – without getting locked into a proprietary solution. With Percona, customers are assured a choice in technology options that are completely open source (and include enterprise features). We help our customers find the right technology for their specific needs, rather than sell a one-size-fits-all product. Percona is a true open source partner that helps you optimize your database performance to better run your business.</p>
<p><strong><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick.png" rel="attachment wp-att-35233"><img class="size-medium wp-image-35233 alignright" src="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick-300x169.png" alt="Percona Live 2016" width="300" height="169" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick.png 888w" sizes="(max-width: 300px) 100vw, 300px" /></a><a href="https://www.percona.com/live/data-performance-conference-2016/users/patrick-mcfadin">Patrick McFadin</a>, Chief Evangelist at DataStax</strong></p>
<p><strong><a href="https://www.percona.com/live/data-performance-conference-2016/sessions/take-back-power-your-cloud-applications-apache-cassandra">Take back the power in your cloud applications with Apache Cassandra</a></strong></p>
<p>Patrick discussed how cloud applications can help you to develop the applications you need in your business, but also outline why the cloud isn&#8217;t a panacea for every business issue. Database engineers have had to support the crazy dreams of application developers since the beginning of the internet. Patrick says it&#8217;s time to take back the power! He believes that Apache Cassandra is the tool that can help you eliminate downtime or span your data around the world with ease. Deploying to the cloud isn’t always easy, but Cassandra might be able to give your application developers the best chance they can get and sleep easy at night.</p>
<p>&nbsp;</p>
<p><strong><img class="size-medium wp-image-35231 alignleft" src="https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show-200x300.png" alt="Post MongoDB World NY Show" width="200" height="300" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show-100x150.png 100w, https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show-200x300.png 200w, https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show.png 531w" sizes="(max-width: 200px) 100vw, 200px" />Post MongoDB World New York Conference, June 30th, Hilton Mid-Town Manhattan</strong></p>
<p>Peter also made an important announcement: Percona and ObjectRocket/Rackspace will be sponsoring a free post-MongoDB World Community Event! The event will take place on Thursday, June 30th at the Hilton Mid-Town Manhattan.</p>
<p>All are welcome.</p>
<p>Don&#8217;t miss out on this amazing opportunity to share ideas and get insights after MongoDB World.</p>
<p><a href="https://www.percona.com/live/data-performance-conference-2016/program/schedule/sessions-day-3">Check out the rest of today&#8217;s Percona Live 2016 schedule here</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995211&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995211&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 21 Apr 2016 18:52:51 +0000";s:2:"dc";a:1:{s:7:"creator";s:22:"MySQL Performance Blog";}s:8:"category";s:55:"MySQLPercona LiveDatastaxPercona Live 2016Peter Zaitsev";s:7:"summary";s:2580:"We&#8217;re heading into the final day here at Percona Live 2016! People are looking a little tired, but still excited for some excellent talks today. Once again the day started off with two great keynote lectures. Read to the end for an important announcement!
Peter Zaitsev, CEO of Percona
Winning with Open Source Databases
Peter discussed what makes the open source community so vibrant and powerful, and why it is essential to preserve that spirit. Data is critical to the success of your business. You rely on your database and its supporting systems to power the applications that drive your business. These systems must be reliable, scalable, efficient – and increasingly, open source. With the complexity of today’s web applications and the databases, systems and hardware that support them, organizations must use the right open source tools and technology for each job – without getting locked into a proprietary solution. With Percona, customers are assured a choice in technology options that are completely open source (and include enterprise features). We help our customers find the right technology for their specific needs, rather than sell a one-size-fits-all product. Percona is a true open source partner that helps you optimize your database performance to better run your business.
Patrick McFadin, Chief Evangelist at DataStax
Take back the power in your cloud applications with Apache Cassandra
Patrick discussed how cloud applications can help you to develop the applications you need in your business, but also outline why the cloud isn&#8217;t a panacea for every business issue. Database engineers have had to support the crazy dreams of application developers since the beginning of the internet. Patrick says it&#8217;s time to take back the power! He believes that Apache Cassandra is the tool that can help you eliminate downtime or span your data around the world with ease. Deploying to the cloud isn’t always easy, but Cassandra might be able to give your application developers the best chance they can get and sleep easy at night.
&nbsp;
Post MongoDB World New York Conference, June 30th, Hilton Mid-Town Manhattan
Peter also made an important announcement: Percona and ObjectRocket/Rackspace will be sponsoring a free post-MongoDB World Community Event! The event will take place on Thursday, June 30th at the Hilton Mid-Town Manhattan.
All are welcome.
Don&#8217;t miss out on this amazing opportunity to share ideas and get insights after MongoDB World.
Check out the rest of today&#8217;s Percona Live 2016 schedule here.";s:12:"atom_content";s:5216:"<p>We&#8217;re heading into the final day here at Percona Live 2016! People are looking a little tired, but still excited for some excellent talks today. Once again the day started off with two great keynote lectures. Read to the end for an important announcement!</p>
<p><strong><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter.png" rel="attachment wp-att-35232"><img class="size-medium wp-image-35232 alignleft" src="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter-300x169.png" alt="Percona Live 2016" width="300" height="169" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Peter.png 888w" sizes="(max-width: 300px) 100vw, 300px" /></a><a href="https://www.percona.com/live/data-performance-conference-2016/users/peter-zaitsev">Peter Zaitsev</a>, CEO of Percona</strong><br />
<strong><a href="https://www.percona.com/live/data-performance-conference-2016/sessions/winning-open-source-databases">Winning with Open Source Databases</a></strong></p>
<p>Peter discussed what makes the open source community so vibrant and powerful, and why it is essential to preserve that spirit. Data is critical to the success of your business. You rely on your database and its supporting systems to power the applications that drive your business. These systems must be reliable, scalable, efficient – and increasingly, open source. With the complexity of today’s web applications and the databases, systems and hardware that support them, organizations must use the right open source tools and technology for each job – without getting locked into a proprietary solution. With Percona, customers are assured a choice in technology options that are completely open source (and include enterprise features). We help our customers find the right technology for their specific needs, rather than sell a one-size-fits-all product. Percona is a true open source partner that helps you optimize your database performance to better run your business.</p>
<p><strong><a href="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick.png" rel="attachment wp-att-35233"><img class="size-medium wp-image-35233 alignright" src="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick-300x169.png" alt="Percona Live 2016" width="300" height="169" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2016/04/Percona-Live-2016-Patrick.png 888w" sizes="(max-width: 300px) 100vw, 300px" /></a><a href="https://www.percona.com/live/data-performance-conference-2016/users/patrick-mcfadin">Patrick McFadin</a>, Chief Evangelist at DataStax</strong></p>
<p><strong><a href="https://www.percona.com/live/data-performance-conference-2016/sessions/take-back-power-your-cloud-applications-apache-cassandra">Take back the power in your cloud applications with Apache Cassandra</a></strong></p>
<p>Patrick discussed how cloud applications can help you to develop the applications you need in your business, but also outline why the cloud isn&#8217;t a panacea for every business issue. Database engineers have had to support the crazy dreams of application developers since the beginning of the internet. Patrick says it&#8217;s time to take back the power! He believes that Apache Cassandra is the tool that can help you eliminate downtime or span your data around the world with ease. Deploying to the cloud isn’t always easy, but Cassandra might be able to give your application developers the best chance they can get and sleep easy at night.</p>
<p>&nbsp;</p>
<p><strong><img class="size-medium wp-image-35231 alignleft" src="https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show-200x300.png" alt="Post MongoDB World NY Show" width="200" height="300" srcset="https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show-100x150.png 100w, https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show-200x300.png 200w, https://www.percona.com/blog/wp-content/uploads/2016/04/Post-MongoDB-World-NY-Show.png 531w" sizes="(max-width: 200px) 100vw, 200px" />Post MongoDB World New York Conference, June 30th, Hilton Mid-Town Manhattan</strong></p>
<p>Peter also made an important announcement: Percona and ObjectRocket/Rackspace will be sponsoring a free post-MongoDB World Community Event! The event will take place on Thursday, June 30th at the Hilton Mid-Town Manhattan.</p>
<p>All are welcome.</p>
<p>Don&#8217;t miss out on this amazing opportunity to share ideas and get insights after MongoDB World.</p>
<p><a href="https://www.percona.com/live/data-performance-conference-2016/program/schedule/sessions-day-3">Check out the rest of today&#8217;s Percona Live 2016 schedule here</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995211&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995211&vote=-1&apivote=1">Vote DOWN</a>";}i:33;a:9:{s:5:"title";s:46:"Unique primary keys(PKs) for MySQL using Redis";s:4:"guid";s:132:"http://www.geeksww.com/tutorials/database_management_systems/mysql/tips_and_tricks/unique_primary_keys_pks_for_mysql_using_redis.php";s:4:"link";s:132:"http://www.geeksww.com/tutorials/database_management_systems/mysql/tips_and_tricks/unique_primary_keys_pks_for_mysql_using_redis.php";s:11:"description";s:304:"MySQL provides an auto increment feature (AUTO_INCREMENT attribute) that can be used to generate unique identities for new rows/records.

However, this may become a problem when you are writing to more than one MySQL databases and merging them, since all of them will be generating their own unique keys.";s:7:"content";a:1:{s:7:"encoded";s:517:"MySQL provides an auto increment feature (AUTO_INCREMENT attribute) that can be used to generate unique identities for new rows/records.

However, this may become a problem when you are writing to more than one MySQL databases and merging them, since all of them will be generating their own unique keys.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995243&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995243&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 21 Apr 2016 17:15:24 +0000";s:2:"dc";a:1:{s:7:"creator";s:14:"Shahryar Ghazi";}s:7:"summary";s:304:"MySQL provides an auto increment feature (AUTO_INCREMENT attribute) that can be used to generate unique identities for new rows/records.

However, this may become a problem when you are writing to more than one MySQL databases and merging them, since all of them will be generating their own unique keys.";s:12:"atom_content";s:517:"MySQL provides an auto increment feature (AUTO_INCREMENT attribute) that can be used to generate unique identities for new rows/records.

However, this may become a problem when you are writing to more than one MySQL databases and merging them, since all of them will be generating their own unique keys.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995243&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995243&vote=-1&apivote=1">Vote DOWN</a>";}i:34;a:9:{s:5:"title";s:39:"How to rename MySQL DB by moving tables";s:4:"guid";s:130:"http://www.geeksww.com/tutorials/database_management_systems/mysql/administration/how_to_rename_mysql_db_name_by_moving_tables.php";s:4:"link";s:130:"http://www.geeksww.com/tutorials/database_management_systems/mysql/administration/how_to_rename_mysql_db_name_by_moving_tables.php";s:11:"description";s:142:"RENAME DATABASE statement was removed from MySQL because it was found to be dangerous.
However, MySQL still supports the RENAME TABLE command.";s:7:"content";a:1:{s:7:"encoded";s:355:"RENAME DATABASE statement was removed from MySQL because it was found to be dangerous.
However, MySQL still supports the RENAME TABLE command.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995209&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995209&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 21 Apr 2016 16:45:37 +0000";s:2:"dc";a:1:{s:7:"creator";s:14:"Shahryar Ghazi";}s:7:"summary";s:142:"RENAME DATABASE statement was removed from MySQL because it was found to be dangerous.
However, MySQL still supports the RENAME TABLE command.";s:12:"atom_content";s:355:"RENAME DATABASE statement was removed from MySQL because it was found to be dangerous.
However, MySQL still supports the RENAME TABLE command.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995209&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995209&vote=-1&apivote=1">Vote DOWN</a>";}i:35;a:10:{s:5:"title";s:37:"MySQL 5.7 Introduces a JSON Data Type";s:4:"guid";s:32:"http://www.lornajane.net/?p=3363";s:4:"link";s:59:"http://www.lornajane.net/posts/2016/mysql-5-7-json-features";s:11:"description";s:15548:"There's a new JSON data type available in MySQL 5.7 that I've been playing with.  I wanted to share some examples of when it's useful to have JSON data in your MySQL database and how to work with the new data types (not least so I can refer back to them later!)
MySQL isn't the first database to offer JSON storage; the document databases (such as MongoDB, CouchDB) work on a JSON or JSON-ish basis by design, and other platforms including PostgreSQL, Oracle and SQL Server also have varying degress of JSON support.  With such wide adoption as MySQL has, the JSON features are now reaching a new tribe of developers.
Why JSON Is Awesome
Traditional database structures have us design table-shaped ways of storing all our data.  As long as all your records (or "rows") are the same shape and have the same sorts of data in approximately the same quantities, this works brilliantly well.  There are some common problems that aren't a good fit, however.  How about freeform website content?  Or sparsely populated sets of attributes for each row?
A classic example is an online shop which sells a variety of products.  A t-shirt is sold by size or colour, but handbags don't have a size and trousers also have length.  We end up with either a very, very wide table which is mostly empty since most attributes don't apply to most products, or we use a solution like the Entity-Attribute-Value (EAV) pattern, which can be cumbersome to work with.
Enter JSON data: a way of storing nested data with all the required information, and no raft of blank fields.  In the web and mobile worlds, JSON is widely used and loved so it makes sense to use it at the database level also.  MySQL's new features allow us to do just that, but also gives us access to query on the values nested inside that JSON data.
An Example Data Set
For these examples, I'm working with a sample data set that I created from JSON Generator to give me something to play with.  If you want to try it out too, you can find both my table definition and the data from it in this gist.
Storing JSON Data
To store JSON, we'll need to use the new JSON data type.  To add the tags field to my table, I used the following SQL:
ALTER TABLE people ADD COLUMN (tags json);

Once the column is there, you can insert a JSON string as the value of the field and it will be stored appropriately.  When we select a JSON field from the table, we'll see the value as the string representation of JSON (as shown in the example below) - so far, it just looks like a standard text field, but it does have superpowers!
SELECT name, tags FROM people LIMIT 5;

This gives me the name and (Lorem Ipsum inspired) tags data for the first five people in the table:
+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis"]                                  |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+

So the JSON data is alive and well, what else can we do with it?
Work Effectively with JSON Arrays
The tags field has a simple JSON array in it - a list of values without keys.  MySQL knows how to work with the data so we can just ask it to add or remove values as we wish.  To add a value, we can use the JSON_ARRAY_APPEND() function.  Here's an example of adding the "Lorem" tag to the first person in the table:
UPDATE people SET tags = JSON_ARRAY_APPEND(tags, "$", "Lorem") WHERE id = 0;

The three arguments to JSON_ARRAY_APPEND() are:

The column to append to
The path within the column, using the JSON Path Syntax
The value to append

That second argument can be quite confusing, the single "$" sign here just means "at the top level".  If we run this query, then the dataset from before now looks like this, with the extra "Lorem" entry for the "Howard Ortega" record.
+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+

We can also use the data to filter our results, for example if we needed to find the users with the tag "Lorem", we could use the JSON_SEARCH() function.  This one also takes three arguments (not the same three as before, that would be too easy!):

The column to search
Either 'one' or 'all', depending on whether you want MySQL to just return the first match it finds, or return all matches
The value we're looking for

The JSON_SEARCH() function actually returns the path of where it found the value - in this case we don't need that information since we only care if there was a value found or not.  We can search for people who are tagged "Lorem" using this query:
SELECT name, tags from people WHERE JSON_SEARCH(tags, 'one', 'Lorem') IS NOT NULL;

There are only 9 people in my whole data set with this tag (including Howard Ortega of course as we modified him ourselves):
+--------------------+--------------------------------------------------------------------------------------------+
| name               | tags                                                                                       |
+--------------------+--------------------------------------------------------------------------------------------+
| Howard Ortega      | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Cox Huff           | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
| Fitzpatrick Hinton | ["esse", "nostrud", "proident", "laborum", "Lorem", "minim", "sit"]                        |
| Austin Yates       | ["id", "ullamco", "Lorem", "aliquip", "aute", "proident", "laboris"]                       |
| Beasley Mccarty    | ["ullamco", "officia", "minim", "Lorem", "laboris", "culpa", "et"]                         |
| Hamilton Zamora    | ["irure", "sit", "reprehenderit", "anim", "deserunt", "Lorem", "consequat"]                |
| Norton Russo       | ["quis", "Lorem", "enim", "sunt", "proident", "labore", "ea"]                              |
| Savannah Hunter    | ["officia", "non", "Lorem", "officia", "mollit", "ad", "enim"]                             |
| Betty Webb         | ["non", "enim", "et", "cupidatat", "Lorem", "ut", "fugiat"]                                |
+--------------------+--------------------------------------------------------------------------------------------+

Having functionality like this available gives us the ability to store document data with our traditional relational database records, and still be able to use the data within those fields for finding and filtering data.  We can also perform updates on the data very easily.  This example, of an undefined number of arbitrary tags being applied to each record, is a pretty common pattern and hopefully this example gives you insight into how the implementation could look in the newer versions of MySQL.
We already mentioned that any kind of JSON can be stored so let's take a look at more complicated data in the next section.
Associative Data in JSON Columns
In addition to the simple array shown above, this table also has a profile column that holds a variety of information about each user.  The same fields aren't present every time and there are various data types, each with a named key.  Let's look at the data in the first few rows:
SELECT name, profile FROM people LIMIT 5;

And the result of the query:
+----------------+-------------------------------------------------------------------------------------------------+
| name           | profile                                                                                         |
+----------------+-------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}     |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                    |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"} |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 50000, "vegetarian": false, "direct_reports": 12}      |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                |
+----------------+-------------------------------------------------------------------------------------------------+

Again, MySQL has some great features built in for us to work easily with the elements inside the JSON data.  The JSON_SET() function will update an existing value, or insert it if it doesn't exist.  For example, if we use JSON_SET to give the user called "Claudia Durham" (with id = 3) a pay rise and also a field called first_aid on her record, we use the same syntax each time.  The existing salary field will update but the first_aid is a new entry in the JSON structure.
Here are the queries to use to achieve this:
UPDATE people SET profile = JSON_SET(profile, "$.salary", 52000) WHERE id = 3;
UPDATE people SET profile = JSON_SET(profile, "$.first_aid", true) WHERE id = 3;

Once we've made these changes, you can inspect the state of the dataset again:
+----------------+---------------------------------------------------------------------------------------------------------------+
| name           | profile                                                                                                       |
+----------------+---------------------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}                   |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                                  |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"}               |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 52000, "first_aid": true, "vegetarian": false, "direct_reports": 12} |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                              |
+----------------+---------------------------------------------------------------------------------------------------------------+

The JSON_SET() function takes care of the inserting and updating operations, but what about removing unwanted entries?  To remove entries from within the JSON data structure, use the JSON_REMOVE() function, it takes the same first two arguments as JSON_SET() does: the column containing the json, and the path.
Once the data is correct, we can use the data inside the JSON field in our queries as we wish.  As an example, let's query the table for each user's salary, and filter by only those people who have 10 or more direct reports.  This example uses the column-&gt;path syntax, which is equivalent to the JSON_EXTRACT() function, simply accessing the data at that path from the JSON document held in that column.
SELECT name, profile-&gt;"$.direct_reports" reports, profile-&gt;"$.salary" salary FROM people WHERE profile-&gt;"$.direct_reports" &gt;= 10;

The query shows both accessing fields from within the dataset as fields in our query (note that I've aliased these to give rather more humane column headings), and the use of a numeric field in the where clause to filter our results.  And the dataset?  Here it is:
+-------------------+---------+--------+
| name              | reports | salary |
+-------------------+---------+--------+
| Claudia Durham    | 12      | 52000  |
| Schwartz Bowers   | 10      | 66000  |
| Carrillo Michael  | 10      | NULL   |
| Miriam Mcgowan    | 10      | 39000  |
| Austin Yates      | 12      | 59000  |
| Beasley Mccarty   | 12      | 45000  |
| Norton Russo      | 12      | 70000  |
| Mccullough Patton | 12      | 46000  |
+-------------------+---------+--------+

Using the MySQL JSON features, we can bring the document-style data into our existing database work very easily.
MySQL, JSON and the Future
This introduction has given you a taste of what can be done in the newest MySQL editions with JSON.  JSON is pretty familiar, and crucially it's very approachable, every programming language I know has native handling for the format - so to bring the two together is a very powerful tool for developers.
Recommended Reading
If you want to try these examples for yourself, you'll need MySQL 5.7 installed.  Either use the MySQL Installer, your usual operating system package manager, or there is also a MySQL 5.7 docker image that would make a good quick start.
The MySQL documentation is good, but doesn't have a lot of realistic examples in it.  However, I still found it very helpful to find my way around which features are available - start here on the JSON Function Reference and see where the links lead you.
I am often asked what the performance of the JSON columns is like in MySQL and the answer is: better than I expected!  There are also some important techniques to use when working with data from inside the JSON columns.  Another feature in the MySQL 5.7 release was the introduction of virtual columns - the ability to have columns made from SQL expressions that are either calculated on the fly or stored (think SQL expression meets materialised view, in a column).  It's possible to index on virtual columns, so by creating a virtual column with a JSON expression and then adding an index, we can improve the performance of queries like these that work with JSON significantly.  There's a good writeup of this approach over on the Percona blog.
MySQL 5.7 Introduces a JSON Data Type was originally published on LornaJane by Lorna.  Lorna is a web development consultant, tech lead, author, trainer, and open source maintainer, and she is occasionally available for freelance work.";s:7:"content";a:1:{s:7:"encoded";s:17354:"<p>There's a new JSON data type available in MySQL 5.7 that I've been playing with.  I wanted to share some examples of when it's useful to have JSON data in your MySQL database and how to work with the new data types (not least so I can refer back to them later!)</p>
<p>MySQL isn't the first database to offer JSON storage; the document databases (such as MongoDB, CouchDB) work on a JSON or JSON-ish basis by design, and other platforms including PostgreSQL, Oracle and SQL Server also have varying degress of JSON support.  With such wide adoption as MySQL has, the JSON features are now reaching a new tribe of developers.<span></span></p>
<h2>Why JSON Is Awesome</h2>
<p>Traditional database structures have us design table-shaped ways of storing all our data.  As long as all your records (or "rows") are the same shape and have the same sorts of data in approximately the same quantities, this works brilliantly well.  There are some common problems that aren't a good fit, however.  How about freeform website content?  Or sparsely populated sets of attributes for each row?</p>
<p>A classic example is an online shop which sells a variety of products.  A t-shirt is sold by size or colour, but handbags don't have a size and trousers also have length.  We end up with either a very, very wide table which is mostly empty since most attributes don't apply to most products, or we use a solution like the Entity-Attribute-Value (EAV) pattern, which can be cumbersome to work with.</p>
<p>Enter JSON data: a way of storing nested data with all the required information, and no raft of blank fields.  In the web and mobile worlds, JSON is widely used and loved so it makes sense to use it at the database level also.  MySQL's new features allow us to do just that, but also gives us access to query on the values nested inside that JSON data.</p>
<h2>An Example Data Set</h2>
<p>For these examples, I'm working with a sample data set that I created from <a href="http://www.json-generator.com/">JSON Generator</a> to give me something to play with.  If you want to try it out too, you can find both my table definition and the data from it <a href="https://gist.github.com/lornajane/c05cf1db435979ec7b2647b686aafd98">in this gist</a>.</p>
<h2>Storing JSON Data</h2>
<p>To store JSON, we'll need to use the new JSON data type.  To add the <code>tags</code> field to my table, I used the following SQL:</p>
<pre><code>ALTER TABLE people ADD COLUMN (tags json);
</code></pre>
<p>Once the column is there, you can insert a JSON string as the value of the field and it will be stored appropriately.  When we select a JSON field from the table, we'll see the value as the string representation of JSON (as shown in the example below) - so far, it just looks like a standard <code>text</code> field, but it does have superpowers!</p>
<pre><code>SELECT name, tags FROM people LIMIT 5;
</code></pre>
<p>This gives me the name and (Lorem Ipsum inspired) tags data for the first five people in the table:</p>
<pre><code>+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis"]                                  |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+
</code></pre>
<p>So the JSON data is alive and well, what else can we do with it?</p>
<h3>Work Effectively with JSON Arrays</h3>
<p>The <code>tags</code> field has a simple JSON array in it - a list of values without keys.  MySQL knows how to work with the data so we can just ask it to add or remove values as we wish.  To add a value, we can use the <code>JSON_ARRAY_APPEND()</code> function.  Here's an example of adding the "Lorem" tag to the first person in the table:</p>
<pre><code>UPDATE people SET tags = JSON_ARRAY_APPEND(tags, "$", "Lorem") WHERE id = 0;
</code></pre>
<p>The three arguments to <code>JSON_ARRAY_APPEND()</code> are:</p>
<ul>
<li>The column to append to</li>
<li>The <em>path</em> within the column, using the <a href="https://dev.mysql.com/doc/refman/5.7/en/json-path-syntax.html">JSON Path Syntax</a></li>
<li>The value to append</li>
</ul>
<p>That second argument can be quite confusing, the single "$" sign here just means "at the top level".  If we run this query, then the dataset from before now looks like this, with the extra "Lorem" entry for the "Howard Ortega" record.</p>
<pre><code>+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+
</code></pre>
<p>We can also use the data to filter our results, for example if we needed to find the users with the tag "Lorem", we could use the <code>JSON_SEARCH()</code> function.  This one also takes three arguments (not the same three as before, that would be too easy!):</p>
<ul>
<li>The column to search</li>
<li>Either <code>'one'</code> or <code>'all'</code>, depending on whether you want MySQL to just return the first match it finds, or return all matches</li>
<li>The value we're looking for</li>
</ul>
<p>The <code>JSON_SEARCH()</code> function actually returns the path of where it found the value - in this case we don't need that information since we only care if there was a value found or not.  We can search for people who are tagged "Lorem" using this query:</p>
<pre><code>SELECT name, tags from people WHERE JSON_SEARCH(tags, 'one', 'Lorem') IS NOT NULL;
</code></pre>
<p>There are only 9 people in my whole data set with this tag (including Howard Ortega of course as we modified him ourselves):</p>
<pre><code>+--------------------+--------------------------------------------------------------------------------------------+
| name               | tags                                                                                       |
+--------------------+--------------------------------------------------------------------------------------------+
| Howard Ortega      | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Cox Huff           | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
| Fitzpatrick Hinton | ["esse", "nostrud", "proident", "laborum", "Lorem", "minim", "sit"]                        |
| Austin Yates       | ["id", "ullamco", "Lorem", "aliquip", "aute", "proident", "laboris"]                       |
| Beasley Mccarty    | ["ullamco", "officia", "minim", "Lorem", "laboris", "culpa", "et"]                         |
| Hamilton Zamora    | ["irure", "sit", "reprehenderit", "anim", "deserunt", "Lorem", "consequat"]                |
| Norton Russo       | ["quis", "Lorem", "enim", "sunt", "proident", "labore", "ea"]                              |
| Savannah Hunter    | ["officia", "non", "Lorem", "officia", "mollit", "ad", "enim"]                             |
| Betty Webb         | ["non", "enim", "et", "cupidatat", "Lorem", "ut", "fugiat"]                                |
+--------------------+--------------------------------------------------------------------------------------------+
</code></pre>
<p>Having functionality like this available gives us the ability to store document data with our traditional relational database records, and still be able to use the data within those fields for finding and filtering data.  We can also perform updates on the data very easily.  This example, of an undefined number of arbitrary tags being applied to each record, is a pretty common pattern and hopefully this example gives you insight into how the implementation could look in the newer versions of MySQL.</p>
<p>We already mentioned that any kind of JSON can be stored so let's take a look at more complicated data in the next section.</p>
<h3>Associative Data in JSON Columns</h3>
<p>In addition to the simple array shown above, this table also has a <code>profile</code> column that holds a variety of information about each user.  The same fields aren't present every time and there are various data types, each with a named key.  Let's look at the data in the first few rows:</p>
<pre><code>SELECT name, profile FROM people LIMIT 5;
</code></pre>
<p>And the result of the query:</p>
<pre><code>+----------------+-------------------------------------------------------------------------------------------------+
| name           | profile                                                                                         |
+----------------+-------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}     |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                    |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"} |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 50000, "vegetarian": false, "direct_reports": 12}      |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                |
+----------------+-------------------------------------------------------------------------------------------------+
</code></pre>
<p>Again, MySQL has some great features built in for us to work easily with the elements inside the JSON data.  The <code>JSON_SET()</code> function will update an existing value, or insert it if it doesn't exist.  For example, if we use <code>JSON_SET</code> to give the user called "Claudia Durham" (with <code>id = 3</code>) a pay rise and also a field called <code>first_aid</code> on her record, we use the same syntax each time.  The existing <code>salary</code> field will update but the <code>first_aid</code> is a new entry in the JSON structure.</p>
<p>Here are the queries to use to achieve this:</p>
<pre><code>UPDATE people SET profile = JSON_SET(profile, "$.salary", 52000) WHERE id = 3;
UPDATE people SET profile = JSON_SET(profile, "$.first_aid", true) WHERE id = 3;
</code></pre>
<p>Once we've made these changes, you can inspect the state of the dataset again:</p>
<pre><code>+----------------+---------------------------------------------------------------------------------------------------------------+
| name           | profile                                                                                                       |
+----------------+---------------------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}                   |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                                  |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"}               |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 52000, "first_aid": true, "vegetarian": false, "direct_reports": 12} |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                              |
+----------------+---------------------------------------------------------------------------------------------------------------+
</code></pre>
<p>The <code>JSON_SET()</code> function takes care of the inserting and updating operations, but what about removing unwanted entries?  To remove entries from within the JSON data structure, use the <code>JSON_REMOVE()</code> function, it takes the same first two arguments as <code>JSON_SET()</code> does: the column containing the json, and the path.</p>
<p>Once the data is correct, we can use the data inside the JSON field in our queries as we wish.  As an example, let's query the table for each user's salary, and filter by only those people who have 10 or more direct reports.  This example uses the <code>column-&gt;path</code> syntax, which is equivalent to the <code>JSON_EXTRACT()</code> function, simply accessing the data at that path from the JSON document held in that column.</p>
<pre><code>SELECT name, profile-&gt;"$.direct_reports" reports, profile-&gt;"$.salary" salary FROM people WHERE profile-&gt;"$.direct_reports" &gt;= 10;
</code></pre>
<p>The query shows both accessing fields from within the dataset as fields in our query (note that I've aliased these to give rather more humane column headings), and the use of a numeric field in the where clause to filter our results.  And the dataset?  Here it is:</p>
<pre><code>+-------------------+---------+--------+
| name              | reports | salary |
+-------------------+---------+--------+
| Claudia Durham    | 12      | 52000  |
| Schwartz Bowers   | 10      | 66000  |
| Carrillo Michael  | 10      | NULL   |
| Miriam Mcgowan    | 10      | 39000  |
| Austin Yates      | 12      | 59000  |
| Beasley Mccarty   | 12      | 45000  |
| Norton Russo      | 12      | 70000  |
| Mccullough Patton | 12      | 46000  |
+-------------------+---------+--------+
</code></pre>
<p>Using the MySQL JSON features, we can bring the document-style data into our existing database work very easily.</p>
<h2>MySQL, JSON and the Future</h2>
<p>This introduction has given you a taste of what can be done in the newest MySQL editions with JSON.  JSON is pretty familiar, and crucially it's very approachable, every programming language I know has native handling for the format - so to bring the two together is a very powerful tool for developers.</p>
<h2>Recommended Reading</h2>
<p>If you want to try these examples for yourself, you'll need MySQL 5.7 installed.  Either use the <a href="https://dev.mysql.com/downloads/installer/">MySQL Installer</a>, your usual operating system package manager, or there is also a <a href="https://hub.docker.com/_/mysql/">MySQL 5.7 docker image</a> that would make a good quick start.</p>
<p>The MySQL documentation is good, but doesn't have a lot of realistic examples in it.  However, I still found it very helpful to find my way around which features are available - start here on the <a href="https://dev.mysql.com/doc/refman/5.7/en/json-function-reference.html">JSON Function Reference</a> and see where the links lead you.</p>
<p>I am often asked what the performance of the JSON columns is like in MySQL and the answer is: better than I expected!  There are also some important techniques to use when working with data from inside the JSON columns.  Another feature in the MySQL 5.7 release was the introduction of virtual columns - the ability to have columns made from SQL expressions that are either calculated on the fly or stored (think SQL expression meets materialised view, in a column).  It's possible to index on virtual columns, so by creating a virtual column with a JSON expression and then adding an index, we can improve the performance of queries like these that work with JSON significantly.  There's a good writeup of this approach over on the <a href="https://www.percona.com/blog/2016/03/07/json-document-fast-lookup-with-mysql-5-7/">Percona blog</a>.</p>
<p><a rel="nofollow" href="http://www.lornajane.net/posts/2016/mysql-5-7-json-features">MySQL 5.7 Introduces a JSON Data Type</a> was originally published on <a rel="nofollow" href="http://www.lornajane.net">LornaJane</a> by Lorna.  Lorna is a web development consultant, tech lead, author, trainer, and open source maintainer, and she is occasionally available for freelance work.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995207&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995207&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 21 Apr 2016 16:41:42 +0000";s:2:"dc";a:1:{s:7:"creator";s:14:"Lorna Mitchell";}s:8:"category";s:22:"techdatabasesjsonmysql";s:7:"summary";s:15548:"There's a new JSON data type available in MySQL 5.7 that I've been playing with.  I wanted to share some examples of when it's useful to have JSON data in your MySQL database and how to work with the new data types (not least so I can refer back to them later!)
MySQL isn't the first database to offer JSON storage; the document databases (such as MongoDB, CouchDB) work on a JSON or JSON-ish basis by design, and other platforms including PostgreSQL, Oracle and SQL Server also have varying degress of JSON support.  With such wide adoption as MySQL has, the JSON features are now reaching a new tribe of developers.
Why JSON Is Awesome
Traditional database structures have us design table-shaped ways of storing all our data.  As long as all your records (or "rows") are the same shape and have the same sorts of data in approximately the same quantities, this works brilliantly well.  There are some common problems that aren't a good fit, however.  How about freeform website content?  Or sparsely populated sets of attributes for each row?
A classic example is an online shop which sells a variety of products.  A t-shirt is sold by size or colour, but handbags don't have a size and trousers also have length.  We end up with either a very, very wide table which is mostly empty since most attributes don't apply to most products, or we use a solution like the Entity-Attribute-Value (EAV) pattern, which can be cumbersome to work with.
Enter JSON data: a way of storing nested data with all the required information, and no raft of blank fields.  In the web and mobile worlds, JSON is widely used and loved so it makes sense to use it at the database level also.  MySQL's new features allow us to do just that, but also gives us access to query on the values nested inside that JSON data.
An Example Data Set
For these examples, I'm working with a sample data set that I created from JSON Generator to give me something to play with.  If you want to try it out too, you can find both my table definition and the data from it in this gist.
Storing JSON Data
To store JSON, we'll need to use the new JSON data type.  To add the tags field to my table, I used the following SQL:
ALTER TABLE people ADD COLUMN (tags json);

Once the column is there, you can insert a JSON string as the value of the field and it will be stored appropriately.  When we select a JSON field from the table, we'll see the value as the string representation of JSON (as shown in the example below) - so far, it just looks like a standard text field, but it does have superpowers!
SELECT name, tags FROM people LIMIT 5;

This gives me the name and (Lorem Ipsum inspired) tags data for the first five people in the table:
+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis"]                                  |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+

So the JSON data is alive and well, what else can we do with it?
Work Effectively with JSON Arrays
The tags field has a simple JSON array in it - a list of values without keys.  MySQL knows how to work with the data so we can just ask it to add or remove values as we wish.  To add a value, we can use the JSON_ARRAY_APPEND() function.  Here's an example of adding the "Lorem" tag to the first person in the table:
UPDATE people SET tags = JSON_ARRAY_APPEND(tags, "$", "Lorem") WHERE id = 0;

The three arguments to JSON_ARRAY_APPEND() are:

The column to append to
The path within the column, using the JSON Path Syntax
The value to append

That second argument can be quite confusing, the single "$" sign here just means "at the top level".  If we run this query, then the dataset from before now looks like this, with the extra "Lorem" entry for the "Howard Ortega" record.
+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+

We can also use the data to filter our results, for example if we needed to find the users with the tag "Lorem", we could use the JSON_SEARCH() function.  This one also takes three arguments (not the same three as before, that would be too easy!):

The column to search
Either 'one' or 'all', depending on whether you want MySQL to just return the first match it finds, or return all matches
The value we're looking for

The JSON_SEARCH() function actually returns the path of where it found the value - in this case we don't need that information since we only care if there was a value found or not.  We can search for people who are tagged "Lorem" using this query:
SELECT name, tags from people WHERE JSON_SEARCH(tags, 'one', 'Lorem') IS NOT NULL;

There are only 9 people in my whole data set with this tag (including Howard Ortega of course as we modified him ourselves):
+--------------------+--------------------------------------------------------------------------------------------+
| name               | tags                                                                                       |
+--------------------+--------------------------------------------------------------------------------------------+
| Howard Ortega      | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Cox Huff           | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
| Fitzpatrick Hinton | ["esse", "nostrud", "proident", "laborum", "Lorem", "minim", "sit"]                        |
| Austin Yates       | ["id", "ullamco", "Lorem", "aliquip", "aute", "proident", "laboris"]                       |
| Beasley Mccarty    | ["ullamco", "officia", "minim", "Lorem", "laboris", "culpa", "et"]                         |
| Hamilton Zamora    | ["irure", "sit", "reprehenderit", "anim", "deserunt", "Lorem", "consequat"]                |
| Norton Russo       | ["quis", "Lorem", "enim", "sunt", "proident", "labore", "ea"]                              |
| Savannah Hunter    | ["officia", "non", "Lorem", "officia", "mollit", "ad", "enim"]                             |
| Betty Webb         | ["non", "enim", "et", "cupidatat", "Lorem", "ut", "fugiat"]                                |
+--------------------+--------------------------------------------------------------------------------------------+

Having functionality like this available gives us the ability to store document data with our traditional relational database records, and still be able to use the data within those fields for finding and filtering data.  We can also perform updates on the data very easily.  This example, of an undefined number of arbitrary tags being applied to each record, is a pretty common pattern and hopefully this example gives you insight into how the implementation could look in the newer versions of MySQL.
We already mentioned that any kind of JSON can be stored so let's take a look at more complicated data in the next section.
Associative Data in JSON Columns
In addition to the simple array shown above, this table also has a profile column that holds a variety of information about each user.  The same fields aren't present every time and there are various data types, each with a named key.  Let's look at the data in the first few rows:
SELECT name, profile FROM people LIMIT 5;

And the result of the query:
+----------------+-------------------------------------------------------------------------------------------------+
| name           | profile                                                                                         |
+----------------+-------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}     |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                    |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"} |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 50000, "vegetarian": false, "direct_reports": 12}      |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                |
+----------------+-------------------------------------------------------------------------------------------------+

Again, MySQL has some great features built in for us to work easily with the elements inside the JSON data.  The JSON_SET() function will update an existing value, or insert it if it doesn't exist.  For example, if we use JSON_SET to give the user called "Claudia Durham" (with id = 3) a pay rise and also a field called first_aid on her record, we use the same syntax each time.  The existing salary field will update but the first_aid is a new entry in the JSON structure.
Here are the queries to use to achieve this:
UPDATE people SET profile = JSON_SET(profile, "$.salary", 52000) WHERE id = 3;
UPDATE people SET profile = JSON_SET(profile, "$.first_aid", true) WHERE id = 3;

Once we've made these changes, you can inspect the state of the dataset again:
+----------------+---------------------------------------------------------------------------------------------------------------+
| name           | profile                                                                                                       |
+----------------+---------------------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}                   |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                                  |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"}               |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 52000, "first_aid": true, "vegetarian": false, "direct_reports": 12} |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                              |
+----------------+---------------------------------------------------------------------------------------------------------------+

The JSON_SET() function takes care of the inserting and updating operations, but what about removing unwanted entries?  To remove entries from within the JSON data structure, use the JSON_REMOVE() function, it takes the same first two arguments as JSON_SET() does: the column containing the json, and the path.
Once the data is correct, we can use the data inside the JSON field in our queries as we wish.  As an example, let's query the table for each user's salary, and filter by only those people who have 10 or more direct reports.  This example uses the column-&gt;path syntax, which is equivalent to the JSON_EXTRACT() function, simply accessing the data at that path from the JSON document held in that column.
SELECT name, profile-&gt;"$.direct_reports" reports, profile-&gt;"$.salary" salary FROM people WHERE profile-&gt;"$.direct_reports" &gt;= 10;

The query shows both accessing fields from within the dataset as fields in our query (note that I've aliased these to give rather more humane column headings), and the use of a numeric field in the where clause to filter our results.  And the dataset?  Here it is:
+-------------------+---------+--------+
| name              | reports | salary |
+-------------------+---------+--------+
| Claudia Durham    | 12      | 52000  |
| Schwartz Bowers   | 10      | 66000  |
| Carrillo Michael  | 10      | NULL   |
| Miriam Mcgowan    | 10      | 39000  |
| Austin Yates      | 12      | 59000  |
| Beasley Mccarty   | 12      | 45000  |
| Norton Russo      | 12      | 70000  |
| Mccullough Patton | 12      | 46000  |
+-------------------+---------+--------+

Using the MySQL JSON features, we can bring the document-style data into our existing database work very easily.
MySQL, JSON and the Future
This introduction has given you a taste of what can be done in the newest MySQL editions with JSON.  JSON is pretty familiar, and crucially it's very approachable, every programming language I know has native handling for the format - so to bring the two together is a very powerful tool for developers.
Recommended Reading
If you want to try these examples for yourself, you'll need MySQL 5.7 installed.  Either use the MySQL Installer, your usual operating system package manager, or there is also a MySQL 5.7 docker image that would make a good quick start.
The MySQL documentation is good, but doesn't have a lot of realistic examples in it.  However, I still found it very helpful to find my way around which features are available - start here on the JSON Function Reference and see where the links lead you.
I am often asked what the performance of the JSON columns is like in MySQL and the answer is: better than I expected!  There are also some important techniques to use when working with data from inside the JSON columns.  Another feature in the MySQL 5.7 release was the introduction of virtual columns - the ability to have columns made from SQL expressions that are either calculated on the fly or stored (think SQL expression meets materialised view, in a column).  It's possible to index on virtual columns, so by creating a virtual column with a JSON expression and then adding an index, we can improve the performance of queries like these that work with JSON significantly.  There's a good writeup of this approach over on the Percona blog.
MySQL 5.7 Introduces a JSON Data Type was originally published on LornaJane by Lorna.  Lorna is a web development consultant, tech lead, author, trainer, and open source maintainer, and she is occasionally available for freelance work.";s:12:"atom_content";s:17354:"<p>There's a new JSON data type available in MySQL 5.7 that I've been playing with.  I wanted to share some examples of when it's useful to have JSON data in your MySQL database and how to work with the new data types (not least so I can refer back to them later!)</p>
<p>MySQL isn't the first database to offer JSON storage; the document databases (such as MongoDB, CouchDB) work on a JSON or JSON-ish basis by design, and other platforms including PostgreSQL, Oracle and SQL Server also have varying degress of JSON support.  With such wide adoption as MySQL has, the JSON features are now reaching a new tribe of developers.<span></span></p>
<h2>Why JSON Is Awesome</h2>
<p>Traditional database structures have us design table-shaped ways of storing all our data.  As long as all your records (or "rows") are the same shape and have the same sorts of data in approximately the same quantities, this works brilliantly well.  There are some common problems that aren't a good fit, however.  How about freeform website content?  Or sparsely populated sets of attributes for each row?</p>
<p>A classic example is an online shop which sells a variety of products.  A t-shirt is sold by size or colour, but handbags don't have a size and trousers also have length.  We end up with either a very, very wide table which is mostly empty since most attributes don't apply to most products, or we use a solution like the Entity-Attribute-Value (EAV) pattern, which can be cumbersome to work with.</p>
<p>Enter JSON data: a way of storing nested data with all the required information, and no raft of blank fields.  In the web and mobile worlds, JSON is widely used and loved so it makes sense to use it at the database level also.  MySQL's new features allow us to do just that, but also gives us access to query on the values nested inside that JSON data.</p>
<h2>An Example Data Set</h2>
<p>For these examples, I'm working with a sample data set that I created from <a href="http://www.json-generator.com/">JSON Generator</a> to give me something to play with.  If you want to try it out too, you can find both my table definition and the data from it <a href="https://gist.github.com/lornajane/c05cf1db435979ec7b2647b686aafd98">in this gist</a>.</p>
<h2>Storing JSON Data</h2>
<p>To store JSON, we'll need to use the new JSON data type.  To add the <code>tags</code> field to my table, I used the following SQL:</p>
<pre><code>ALTER TABLE people ADD COLUMN (tags json);
</code></pre>
<p>Once the column is there, you can insert a JSON string as the value of the field and it will be stored appropriately.  When we select a JSON field from the table, we'll see the value as the string representation of JSON (as shown in the example below) - so far, it just looks like a standard <code>text</code> field, but it does have superpowers!</p>
<pre><code>SELECT name, tags FROM people LIMIT 5;
</code></pre>
<p>This gives me the name and (Lorem Ipsum inspired) tags data for the first five people in the table:</p>
<pre><code>+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis"]                                  |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+
</code></pre>
<p>So the JSON data is alive and well, what else can we do with it?</p>
<h3>Work Effectively with JSON Arrays</h3>
<p>The <code>tags</code> field has a simple JSON array in it - a list of values without keys.  MySQL knows how to work with the data so we can just ask it to add or remove values as we wish.  To add a value, we can use the <code>JSON_ARRAY_APPEND()</code> function.  Here's an example of adding the "Lorem" tag to the first person in the table:</p>
<pre><code>UPDATE people SET tags = JSON_ARRAY_APPEND(tags, "$", "Lorem") WHERE id = 0;
</code></pre>
<p>The three arguments to <code>JSON_ARRAY_APPEND()</code> are:</p>
<ul>
<li>The column to append to</li>
<li>The <em>path</em> within the column, using the <a href="https://dev.mysql.com/doc/refman/5.7/en/json-path-syntax.html">JSON Path Syntax</a></li>
<li>The value to append</li>
</ul>
<p>That second argument can be quite confusing, the single "$" sign here just means "at the top level".  If we run this query, then the dataset from before now looks like this, with the extra "Lorem" entry for the "Howard Ortega" record.</p>
<pre><code>+----------------+--------------------------------------------------------------------------------------------+
| name           | tags                                                                                       |
+----------------+--------------------------------------------------------------------------------------------+
| Howard Ortega  | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Miller Gamble  | ["et", "officia", "culpa", "excepteur", "ullamco", "exercitation", "in"]                   |
| Harriett Leon  | ["laboris", "consectetur", "mollit", "dolore", "aute", "consectetur", "adipisicing"]       |
| Claudia Durham | ["ad", "veniam", "sunt", "eiusmod", "pariatur", "veniam", "reprehenderit"]                 |
| Cox Huff       | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
+----------------+--------------------------------------------------------------------------------------------+
</code></pre>
<p>We can also use the data to filter our results, for example if we needed to find the users with the tag "Lorem", we could use the <code>JSON_SEARCH()</code> function.  This one also takes three arguments (not the same three as before, that would be too easy!):</p>
<ul>
<li>The column to search</li>
<li>Either <code>'one'</code> or <code>'all'</code>, depending on whether you want MySQL to just return the first match it finds, or return all matches</li>
<li>The value we're looking for</li>
</ul>
<p>The <code>JSON_SEARCH()</code> function actually returns the path of where it found the value - in this case we don't need that information since we only care if there was a value found or not.  We can search for people who are tagged "Lorem" using this query:</p>
<pre><code>SELECT name, tags from people WHERE JSON_SEARCH(tags, 'one', 'Lorem') IS NOT NULL;
</code></pre>
<p>There are only 9 people in my whole data set with this tag (including Howard Ortega of course as we modified him ourselves):</p>
<pre><code>+--------------------+--------------------------------------------------------------------------------------------+
| name               | tags                                                                                       |
+--------------------+--------------------------------------------------------------------------------------------+
| Howard Ortega      | ["officia", "et", "anim", "dolore", "ut", "duis", "quis", "Lorem"]                         |
| Cox Huff           | ["reprehenderit", "Lorem", "adipisicing", "ipsum", "cupidatat", "deserunt", "consectetur"] |
| Fitzpatrick Hinton | ["esse", "nostrud", "proident", "laborum", "Lorem", "minim", "sit"]                        |
| Austin Yates       | ["id", "ullamco", "Lorem", "aliquip", "aute", "proident", "laboris"]                       |
| Beasley Mccarty    | ["ullamco", "officia", "minim", "Lorem", "laboris", "culpa", "et"]                         |
| Hamilton Zamora    | ["irure", "sit", "reprehenderit", "anim", "deserunt", "Lorem", "consequat"]                |
| Norton Russo       | ["quis", "Lorem", "enim", "sunt", "proident", "labore", "ea"]                              |
| Savannah Hunter    | ["officia", "non", "Lorem", "officia", "mollit", "ad", "enim"]                             |
| Betty Webb         | ["non", "enim", "et", "cupidatat", "Lorem", "ut", "fugiat"]                                |
+--------------------+--------------------------------------------------------------------------------------------+
</code></pre>
<p>Having functionality like this available gives us the ability to store document data with our traditional relational database records, and still be able to use the data within those fields for finding and filtering data.  We can also perform updates on the data very easily.  This example, of an undefined number of arbitrary tags being applied to each record, is a pretty common pattern and hopefully this example gives you insight into how the implementation could look in the newer versions of MySQL.</p>
<p>We already mentioned that any kind of JSON can be stored so let's take a look at more complicated data in the next section.</p>
<h3>Associative Data in JSON Columns</h3>
<p>In addition to the simple array shown above, this table also has a <code>profile</code> column that holds a variety of information about each user.  The same fields aren't present every time and there are various data types, each with a named key.  Let's look at the data in the first few rows:</p>
<pre><code>SELECT name, profile FROM people LIMIT 5;
</code></pre>
<p>And the result of the query:</p>
<pre><code>+----------------+-------------------------------------------------------------------------------------------------+
| name           | profile                                                                                         |
+----------------+-------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}     |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                    |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"} |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 50000, "vegetarian": false, "direct_reports": 12}      |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                |
+----------------+-------------------------------------------------------------------------------------------------+
</code></pre>
<p>Again, MySQL has some great features built in for us to work easily with the elements inside the JSON data.  The <code>JSON_SET()</code> function will update an existing value, or insert it if it doesn't exist.  For example, if we use <code>JSON_SET</code> to give the user called "Claudia Durham" (with <code>id = 3</code>) a pay rise and also a field called <code>first_aid</code> on her record, we use the same syntax each time.  The existing <code>salary</code> field will update but the <code>first_aid</code> is a new entry in the JSON structure.</p>
<p>Here are the queries to use to achieve this:</p>
<pre><code>UPDATE people SET profile = JSON_SET(profile, "$.salary", 52000) WHERE id = 3;
UPDATE people SET profile = JSON_SET(profile, "$.first_aid", true) WHERE id = 3;
</code></pre>
<p>Once we've made these changes, you can inspect the state of the dataset again:</p>
<pre><code>+----------------+---------------------------------------------------------------------------------------------------------------+
| name           | profile                                                                                                       |
+----------------+---------------------------------------------------------------------------------------------------------------+
| Howard Ortega  | {"email": "Shaw@example.com", "salary": 52000, "twitter": "@estvelit", "direct_reports": 7}                   |
| Miller Gamble  | {"salary": 52000, "join_date": "2013-05-13T10:02:22 -01:00"}                                                  |
| Harriett Leon  | {"email": "Melton@example.com", "driver": true, "salary": 63000, "twitter": "@commodoproident"}               |
| Claudia Durham | {"email": "Sykes@example.com", "salary": 52000, "first_aid": true, "vegetarian": false, "direct_reports": 12} |
| Cox Huff       | {"salary": 61000, "twitter": "@mollitconsequat"}                                                              |
+----------------+---------------------------------------------------------------------------------------------------------------+
</code></pre>
<p>The <code>JSON_SET()</code> function takes care of the inserting and updating operations, but what about removing unwanted entries?  To remove entries from within the JSON data structure, use the <code>JSON_REMOVE()</code> function, it takes the same first two arguments as <code>JSON_SET()</code> does: the column containing the json, and the path.</p>
<p>Once the data is correct, we can use the data inside the JSON field in our queries as we wish.  As an example, let's query the table for each user's salary, and filter by only those people who have 10 or more direct reports.  This example uses the <code>column-&gt;path</code> syntax, which is equivalent to the <code>JSON_EXTRACT()</code> function, simply accessing the data at that path from the JSON document held in that column.</p>
<pre><code>SELECT name, profile-&gt;"$.direct_reports" reports, profile-&gt;"$.salary" salary FROM people WHERE profile-&gt;"$.direct_reports" &gt;= 10;
</code></pre>
<p>The query shows both accessing fields from within the dataset as fields in our query (note that I've aliased these to give rather more humane column headings), and the use of a numeric field in the where clause to filter our results.  And the dataset?  Here it is:</p>
<pre><code>+-------------------+---------+--------+
| name              | reports | salary |
+-------------------+---------+--------+
| Claudia Durham    | 12      | 52000  |
| Schwartz Bowers   | 10      | 66000  |
| Carrillo Michael  | 10      | NULL   |
| Miriam Mcgowan    | 10      | 39000  |
| Austin Yates      | 12      | 59000  |
| Beasley Mccarty   | 12      | 45000  |
| Norton Russo      | 12      | 70000  |
| Mccullough Patton | 12      | 46000  |
+-------------------+---------+--------+
</code></pre>
<p>Using the MySQL JSON features, we can bring the document-style data into our existing database work very easily.</p>
<h2>MySQL, JSON and the Future</h2>
<p>This introduction has given you a taste of what can be done in the newest MySQL editions with JSON.  JSON is pretty familiar, and crucially it's very approachable, every programming language I know has native handling for the format - so to bring the two together is a very powerful tool for developers.</p>
<h2>Recommended Reading</h2>
<p>If you want to try these examples for yourself, you'll need MySQL 5.7 installed.  Either use the <a href="https://dev.mysql.com/downloads/installer/">MySQL Installer</a>, your usual operating system package manager, or there is also a <a href="https://hub.docker.com/_/mysql/">MySQL 5.7 docker image</a> that would make a good quick start.</p>
<p>The MySQL documentation is good, but doesn't have a lot of realistic examples in it.  However, I still found it very helpful to find my way around which features are available - start here on the <a href="https://dev.mysql.com/doc/refman/5.7/en/json-function-reference.html">JSON Function Reference</a> and see where the links lead you.</p>
<p>I am often asked what the performance of the JSON columns is like in MySQL and the answer is: better than I expected!  There are also some important techniques to use when working with data from inside the JSON columns.  Another feature in the MySQL 5.7 release was the introduction of virtual columns - the ability to have columns made from SQL expressions that are either calculated on the fly or stored (think SQL expression meets materialised view, in a column).  It's possible to index on virtual columns, so by creating a virtual column with a JSON expression and then adding an index, we can improve the performance of queries like these that work with JSON significantly.  There's a good writeup of this approach over on the <a href="https://www.percona.com/blog/2016/03/07/json-document-fast-lookup-with-mysql-5-7/">Percona blog</a>.</p>
<p><a rel="nofollow" href="http://www.lornajane.net/posts/2016/mysql-5-7-json-features">MySQL 5.7 Introduces a JSON Data Type</a> was originally published on <a rel="nofollow" href="http://www.lornajane.net">LornaJane</a> by Lorna.  Lorna is a web development consultant, tech lead, author, trainer, and open source maintainer, and she is occasionally available for freelance work.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995207&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995207&vote=-1&apivote=1">Vote DOWN</a>";}i:36;a:10:{s:5:"title";s:47:"Ubuntu 16.04 first stable distro with MySQL 5.7";s:4:"guid";s:34:"http://mysqlserverteam.com/?p=6248";s:4:"link";s:75:"http://mysqlserverteam.com/ubuntu-16-04-first-stable-distro-with-mysql-5-7/";s:11:"description";s:293:"Ubuntu 16.04 artwork by Canonical Ltd (CC-BY-SA).
Congratulations to Ubuntu on releasing 16.04 LTS with MySQL 5.7! As far as I know, it&#8217;s the first stable release of a Linux distro that contains MySQL 5.7. Fedora and openSUSE also have MySQL 5.7, but not yet in a stable release.&hellip;";s:7:"content";a:1:{s:7:"encoded";s:1153:"<figure><a href="http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04.png" rel="attachment wp-att-6254"><img class="size-full wp-image-6254" src="http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04.png" alt="Ubuntu 16.04 artwork by Canonical Ltd (CC-BY-SA)" width="752" height="374" srcset="http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04-300x149.png 300w, http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04.png 752w" sizes="(max-width: 752px) 100vw, 752px" /></a><figcaption>Ubuntu 16.04 artwork by Canonical Ltd (<a href="http://creativecommons.org/licenses/by-sa/3.0/">CC-BY-SA</a>).</figcaption></figure>
<p><strong>Congratulations to Ubuntu on releasing 16.04 LTS with MySQL 5.7!</strong> As far as I know, it&#8217;s the first stable release of a Linux distro that contains MySQL 5.7. Fedora and openSUSE also have MySQL 5.7, but not yet in a stable release.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995206&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995206&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 21 Apr 2016 13:20:26 +0000";s:2:"dc";a:1:{s:7:"creator";s:21:"MySQL Server Dev Team";}s:8:"category";s:37:"LinuxMySQLUpgradesLinux distrosUbuntu";s:7:"summary";s:293:"Ubuntu 16.04 artwork by Canonical Ltd (CC-BY-SA).
Congratulations to Ubuntu on releasing 16.04 LTS with MySQL 5.7! As far as I know, it&#8217;s the first stable release of a Linux distro that contains MySQL 5.7. Fedora and openSUSE also have MySQL 5.7, but not yet in a stable release.&hellip;";s:12:"atom_content";s:1153:"<figure><a href="http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04.png" rel="attachment wp-att-6254"><img class="size-full wp-image-6254" src="http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04.png" alt="Ubuntu 16.04 artwork by Canonical Ltd (CC-BY-SA)" width="752" height="374" srcset="http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04-300x149.png 300w, http://mysqlserverteam.com/wp-content/uploads/2016/04/ubuntu-16.04.png 752w" sizes="(max-width: 752px) 100vw, 752px" /></a><figcaption>Ubuntu 16.04 artwork by Canonical Ltd (<a href="http://creativecommons.org/licenses/by-sa/3.0/">CC-BY-SA</a>).</figcaption></figure>
<p><strong>Congratulations to Ubuntu on releasing 16.04 LTS with MySQL 5.7!</strong> As far as I know, it&#8217;s the first stable release of a Linux distro that contains MySQL 5.7. Fedora and openSUSE also have MySQL 5.7, but not yet in a stable release.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995206&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995206&vote=-1&apivote=1">Vote DOWN</a>";}i:37;a:10:{s:5:"title";s:23:"Thanks for stopping by!";s:4:"guid";s:34:"http://mysqlserverteam.com/?p=6229";s:4:"link";s:50:"http://mysqlserverteam.com/thanks-for-stopping-by/";s:11:"description";s:269:"The MySQL Engineering Team at their BOF session on Tuesday night. Sunny Bains in focus answering InnoDB questions.
On behalf of the entire team, I would like to thank you for stopping by our &#8220;meet the MySQL engineering team&#8221; BOF held Tuesday night.&hellip;";s:7:"content";a:1:{s:7:"encoded";s:1300:"<figure><img class="size-full wp-image-6230" src="http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n.jpg" alt="The MySQL Engineering Team answering questions at their BOF session. " width="960" height="720" srcset="http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n-300x225.jpg 300w, http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n-768x576.jpg 768w, http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n.jpg 960w" sizes="(max-width: 960px) 100vw, 960px" /><figcaption>The MySQL Engineering Team at their BOF session on Tuesday night. Sunny Bains in focus answering InnoDB questions.</figcaption></figure>
<p>On behalf of the entire team, I would like to thank you for stopping by our &#8220;<a href="https://www.percona.com/live/data-performance-conference-2016/sessions/meet-mysql-engineering-team">meet the MySQL engineering team</a>&#8221; BOF held Tuesday night.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995203&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995203&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Thu, 21 Apr 2016 11:02:18 +0000";s:2:"dc";a:1:{s:7:"creator";s:13:"Morgan Tocker";}s:8:"category";s:5:"MySQL";s:7:"summary";s:269:"The MySQL Engineering Team at their BOF session on Tuesday night. Sunny Bains in focus answering InnoDB questions.
On behalf of the entire team, I would like to thank you for stopping by our &#8220;meet the MySQL engineering team&#8221; BOF held Tuesday night.&hellip;";s:12:"atom_content";s:1300:"<figure><img class="size-full wp-image-6230" src="http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n.jpg" alt="The MySQL Engineering Team answering questions at their BOF session. " width="960" height="720" srcset="http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n-300x225.jpg 300w, http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n-768x576.jpg 768w, http://mysqlserverteam.com/wp-content/uploads/2016/04/13015332_10209651419061707_7273078857965881680_n.jpg 960w" sizes="(max-width: 960px) 100vw, 960px" /><figcaption>The MySQL Engineering Team at their BOF session on Tuesday night. Sunny Bains in focus answering InnoDB questions.</figcaption></figure>
<p>On behalf of the entire team, I would like to thank you for stopping by our &#8220;<a href="https://www.percona.com/live/data-performance-conference-2016/sessions/meet-mysql-engineering-team">meet the MySQL engineering team</a>&#8221; BOF held Tuesday night.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995203&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995203&vote=-1&apivote=1">Vote DOWN</a>";}i:38;a:10:{s:5:"title";s:58:"MariaDB AWS Key Management Service (KMS) Encryption Plugin";s:4:"guid";s:34:"http://serge.frezefond.com/?p=1919";s:4:"link";s:92:"http://serge.frezefond.com/2016/04/mariadb-aws-key-management-service-kms-encryption-plugin/";s:11:"description";s:327:"MariaDB 10.1 introduced Data at Rest Encryption. By default we provide a file_key_management plugin. This is a basic plugin storing keys in a file that can be itself encrypted. This file can come from a usb stick removed once keys have been brought into memory. But this remains a basic solution not suitable for security [...]";s:7:"content";a:1:{s:7:"encoded";s:543:"<p>MariaDB 10.1 introduced Data at Rest Encryption. By default we provide a file_key_management plugin. This is a basic plugin storing keys in a file that can be itself encrypted. This file can come from a usb stick removed once keys have been brought into memory. But this remains a basic solution not suitable for security [...]<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995183&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995183&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 19:08:43 +0000";s:2:"dc";a:1:{s:7:"creator";s:15:"Serge Frezefond";}s:8:"category";s:40:"communityDevelopmentMariaDBmysqlsecurity";s:7:"summary";s:327:"MariaDB 10.1 introduced Data at Rest Encryption. By default we provide a file_key_management plugin. This is a basic plugin storing keys in a file that can be itself encrypted. This file can come from a usb stick removed once keys have been brought into memory. But this remains a basic solution not suitable for security [...]";s:12:"atom_content";s:543:"<p>MariaDB 10.1 introduced Data at Rest Encryption. By default we provide a file_key_management plugin. This is a basic plugin storing keys in a file that can be itself encrypted. This file can come from a usb stick removed once keys have been brought into memory. But this remains a basic solution not suitable for security [...]<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995183&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995183&vote=-1&apivote=1">Vote DOWN</a>";}i:39;a:9:{s:5:"title";s:79:"Comment on Eating our own dog food – Running JIRA on MariaDB by Rick Barzilli";s:4:"guid";s:40:"https://mariadb.org/?p=3422#comment-7560";s:4:"link";s:70:"https://mariadb.org/eating-dog-food-running-jira-mariadb/#comment-7560";s:11:"description";s:542:"Thank you for the write up.  I&#8217;m considering doing exactly what this article describes.  Our developers are interested in setting up a mission critical JIRA instance for QA and bug tracking.  This translates to 1000 active users and several million bugs (eventually).
You mention having 2000 active JIRA users.  I was wondering if you could share additional infrastructure details about your JIRA instance.  How large is the production DB in terms of disk space?  What are you using to backup the DB (xtradump, mysqldump, SAN snapshot)?";s:7:"content";a:1:{s:7:"encoded";s:769:"<p>Thank you for the write up.  I&#8217;m considering doing exactly what this article describes.  Our developers are interested in setting up a mission critical JIRA instance for QA and bug tracking.  This translates to 1000 active users and several million bugs (eventually).</p>
<p>You mention having 2000 active JIRA users.  I was wondering if you could share additional infrastructure details about your JIRA instance.  How large is the production DB in terms of disk space?  What are you using to backup the DB (xtradump, mysqldump, SAN snapshot)?</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995213&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995213&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 18:55:02 +0000";s:2:"dc";a:1:{s:7:"creator";s:7:"MariaDB";}s:7:"summary";s:542:"Thank you for the write up.  I&#8217;m considering doing exactly what this article describes.  Our developers are interested in setting up a mission critical JIRA instance for QA and bug tracking.  This translates to 1000 active users and several million bugs (eventually).
You mention having 2000 active JIRA users.  I was wondering if you could share additional infrastructure details about your JIRA instance.  How large is the production DB in terms of disk space?  What are you using to backup the DB (xtradump, mysqldump, SAN snapshot)?";s:12:"atom_content";s:769:"<p>Thank you for the write up.  I&#8217;m considering doing exactly what this article describes.  Our developers are interested in setting up a mission critical JIRA instance for QA and bug tracking.  This translates to 1000 active users and several million bugs (eventually).</p>
<p>You mention having 2000 active JIRA users.  I was wondering if you could share additional infrastructure details about your JIRA instance.  How large is the production DB in terms of disk space?  What are you using to backup the DB (xtradump, mysqldump, SAN snapshot)?</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995213&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995213&vote=-1&apivote=1">Vote DOWN</a>";}i:40;a:10:{s:5:"title";s:43:"Data Encryption at Rest in Oracle MySQL 5.7";s:4:"guid";s:37:"https://www.pythian.com/blog/?p=87495";s:4:"link";s:73:"https://www.pythian.com/blog/data-encryption-at-rest-in-oracle-mysql-5-7/";s:11:"description";s:8406:"&nbsp;I&#8217;ve previously evaluated MariaDB&#8217;s 10.1 implementation of data encryption at rest (https://www.pythian.com/blog/data-encryption-rest), and recently did the same for Oracle&#8217;s implementation (https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html) in their MySQL 5.7.&nbsp;First, here&#8217;s a walkthrough of enabling encryption for MySQL 5.7:1. Install keyring plugin.1a. Add the following to the [mysqld] section of /etc/my.cnf:View the code on Gist.1b. Restart the server:...
service mysqld restart1c. Verify:...
mysql&gt; SELECT PLUGIN_NAME, PLUGIN_STATUS FROM INFORMATION_SCHEMA.PLUGINS WHERE PLUGIN_NAME LIKE 'keyring%';
+--------------+---------------+
| PLUGIN_NAME  | PLUGIN_STATUS |
+--------------+---------------+
| keyring_file | ACTIVE        |
+--------------+---------------+2. Ensure innodb_file_per_table is on.2a. Check....
mysql&gt; show global variables like 'innodb_file_per_table';
+-----------------------+-------+
| Variable_name         | Value |
+-----------------------+-------+
| innodb_file_per_table | ON    |
+-----------------------+-------+2b. If OFF, add the following to the [mysqld] section of /etc/my.cnf, restart, and alter each existing table to move it to its own tablespace:
innodb_file_per_table=ONGet list of available InnoDB tables:
mysql&gt;select table_schema, table_name, engine from information_schema.tables where engine='innodb' and table_schema not in ('information_schema');Run ALTER &#8230; ENGINE=INNODB on each above InnoDB tables:
mysql&gt;&lt;strong&gt;ALTER&lt;/strong&gt; TABLE [TABLE_SCHEMA].[TABLE_NAME] ENGINE=INNODB;&nbsp;Next, I walked through some testing.1. Create some data....
[root@localhost ~]# mysqlslap --concurrency=50 --number-int-cols=2 --number-char-cols=3 --auto-generate-sql --auto-generate-sql-write-number=10000 --no-drop2. Observe the mysqlslap.t1 table is not automatically encrypted. Unlike MariaDB&#8217;s implementation, there is not an option to encrypt tables by default.2a. Via the mysql client:...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
Empty set (0.05 sec)2b. Via the command line:(Install xxd if required.)...
[root@localhost ~]# yum install vim-common...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0010dc0: 5967 4b30 7530 7942 4266 664e 6666 3143  YgK0u0yBBffNff1C
0010dd0: 5175 6470 3332 536e 7647 5761 3654 6365  Qudp32SnvGWa6Tce
0010de0: 3977 6576 7053 3730 3765 4665 4838 7162  9wevpS707eFeH8qb
0010df0: 3253 5078 4d6c 6439 3137 6a7a 634a 5465  2SPxMld917jzcJTe
...3. Insert some identifiable data into the table:...
mysql&gt; &lt;strong&gt;insert&lt;/strong&gt; into mysqlslap.t1 values (1,2,&quot;private&quot;,&quot;sensitive&quot;,&quot;data&quot;);
Query OK, 1 row affected (0.01 sec)

mysql&gt; select * from mysqlslap.t1 where charcol2=&quot;sensitive&quot;;
+---------+---------+----------+-----------+----------+
| intcol1 | intcol2 | charcol1 | charcol2  | charcol3 |
+---------+---------+----------+-----------+----------+
|       1 |       2 | private  | sensitive | data     |
+---------+---------+----------+-----------+----------+
1 row in set (0.02 sec)4. Observe this data via the command line:...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
04fa290: 0002 7072 6976 6174 6573 656e 7369 7469  ..privatesensiti
...5. Encrypt the mysqlslap.t1 table:...
mysql&gt; &lt;strong&gt;alter&lt;/strong&gt; table mysqlslap.t1 encryption='Y';
Query OK, 10300 rows affected (0.31 sec)
Records: 10300  Duplicates: 0  Warnings: 06. Observe the mysqlslap.t1 table is now encrypted:6a. Via the mysql client:...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
+--------------+------------+----------------+
| TABLE_SCHEMA | TABLE_NAME | CREATE_OPTIONS |
+--------------+------------+----------------+
| mysqlslap    | t1         | ENCRYPTION=&quot;Y&quot; |
+--------------+------------+----------------+6b. Via the command line:...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep &quot;private&quot;
[root@localhost ~]#6c. Observe snippet of the file:...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0004160: 56e4 2930 bbea 167f 7c82 93b4 2fcf 8cc1  V.)0....|.../...
0004170: f443 9d6f 2e1e 9ac2 170a 3b7c 8f38 60bf  .C.o......;|.8`.
0004180: 3c75 2a42 0cc9 a79b 4309 cd83 da74 1b06  &amp;amp;lt;u*B....C....t..
0004190: 3a32 e104 43c5 8dfd f913 0f69 bda6 5e76  :2..C......i..^v
...7. Observe redo log is not encrypted:...
[root@localhost ~]# xxd /var/lib/mysql/ib_logfile0 | less
...
23c6930: 0000 0144 0110 8000 0001 8000 0002 7072  ...D..........pr
23c6940: 6976 6174 6573 656e 7369 7469 7665 6461  ivatesensitiveda
23c6950: 7461 3723 0000 132e 2f6d 7973 716c 736c  ta7#..../mysqlsl
...This is expected because the documentation (https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html) reports encryption of files outside the tablespace is not supported: &#8220;Tablespace encryption only applies to data in the tablespace. Data is not encrypted in the redo log, undo log, or binary log.&#8221;ConclusionsI found in my testing of MariaDB&#8217;s implementation of data encryption at rest that there were still places on the file system that a bad actor could view sensitive data. I&#8217;ve found the same in this test of Oracle&#8217;s implementation. Both leave data exposed in log files surrounding the tablespace files.BonusAs a bonus to this walkthrough, during this testing, the table definition caught my eye:...
mysql&gt; show create table mysqlslap.t1\G
*************************** 1. row ***************************
       Table: t1
Create Table: CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y'
1 row in set (0.00 sec)As discussed in https://jira.mariadb.org/browse/MDEV-9571, the MariaDB implementation does not include the &#8220;encrypted=yes&#8221; information in the table definition when tables are implicitly encrypted.I was curious what would happen if I did a mysqldump of this encrypted table and attempted to restore it to a nonencrypted server. DBAs expect mysqldump to create a portable file to recreate the table definition and data on a different version of mysql. During upgrades, for example, you might expect to use this for rollback.Here is my test. I first did the dump and looked inside the file....
[root@localhost ~]# mysqldump mysqlslap t1 &gt; mysqlslap_t1_dump
[root@localhost ~]# less mysqlslap_t1_dump
...
CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y';

&lt;strong&gt;INSERT&lt;/strong&gt; INTO `t1` VALUES (
...
,(1,2,'private','sensitive','data');As expected, that definition makes the dump less portable. The restore from dump is not completed and throws an error (this is not remedied by using &#8211;force):On a slightly older 5.7 version:...
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.8-rc  |
+-----------+

[root@centosmysql57 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1064 (42000) at line 25: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'ENCRYPTION='Y'' at line 7On a different fork:...
MariaDB [(none)]&gt; select version();
+-----------------+
| version()       |
+-----------------+
| 10.1.12-MariaDB |
+-----------------+
1 row in set (0.00 sec)

[root@maria101 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1911 (HY000) at line 25: Unknown option 'ENCRYPTION'This doesn&#8217;t have anything to do with the encrypted state of the data in the table, just the table definition. I do like the encryption showing up in the table definition, for better visibility of encryption. Maybe the fix is to have mysqldump strip this when writing to the dump file.";s:7:"content";a:1:{s:7:"encoded";s:9609:"<div><div><p>&nbsp;</p><p>I&#8217;ve previously evaluated MariaDB&#8217;s 10.1 implementation of data encryption at rest (<a href="https://www.pythian.com/blog/data-encryption-rest">https://www.pythian.com/blog/data-encryption-rest</a>), and recently did the same for Oracle&#8217;s implementation (<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html</a>) in their MySQL 5.7.</p><p>&nbsp;</p><p><strong>First, here&#8217;s a walkthrough of enabling encryption for MySQL 5.7:</strong></p><p>1. Install keyring plugin.</p><p>1a. Add the following to the [mysqld] section of /etc/my.cnf:</p><p>View the code on <a href="https://gist.github.com/parham-pythian/a625bf472456da4774dec424dbbb4932">Gist</a>.</p><p>1b. Restart the server:</p><pre>...
service mysqld restart</pre><p>1c. Verify:</p><pre>...
mysql&gt; SELECT PLUGIN_NAME, PLUGIN_STATUS FROM INFORMATION_SCHEMA.PLUGINS WHERE PLUGIN_NAME LIKE 'keyring%';
+--------------+---------------+
| PLUGIN_NAME  | PLUGIN_STATUS |
+--------------+---------------+
| keyring_file | ACTIVE        |
+--------------+---------------+</pre><p>2. Ensure innodb_file_per_table is on.</p><p>2a. Check.</p><pre>...
mysql&gt; show global variables like 'innodb_file_per_table';
+-----------------------+-------+
| Variable_name         | Value |
+-----------------------+-------+
| innodb_file_per_table | ON    |
+-----------------------+-------+</pre><p>2b. If OFF, add the following to the [mysqld] section of /etc/my.cnf, restart, and alter each existing table to move it to its own tablespace:</p><pre>
innodb_file_per_table=ON</pre><p>Get list of available InnoDB tables:</p><pre>
mysql&gt;select table_schema, table_name, engine from information_schema.tables where engine='innodb' and table_schema not in ('information_schema');</pre><p>Run ALTER &#8230; ENGINE=INNODB on each above InnoDB tables:</p><pre>
mysql&gt;&lt;strong&gt;ALTER&lt;/strong&gt; TABLE [TABLE_SCHEMA].[TABLE_NAME] ENGINE=INNODB;</pre><p>&nbsp;</p><p><strong>Next, I walked through some testing.</strong></p><p>1. Create some data.</p><pre>...
[root@localhost ~]# mysqlslap --concurrency=50 --number-int-cols=2 --number-char-cols=3 --auto-generate-sql --auto-generate-sql-write-number=10000 --no-drop</pre><p>2. Observe the mysqlslap.t1 table is not automatically encrypted. Unlike MariaDB&#8217;s implementation, there is not an option to encrypt tables by default.</p><p>2a. Via the mysql client:</p><pre>...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
Empty set (0.05 sec)</pre><p>2b. Via the command line:</p><p>(Install xxd if required.)</p><pre>...
[root@localhost ~]# yum install vim-common</pre><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0010dc0: 5967 4b30 7530 7942 4266 664e 6666 3143  YgK0u0yBBffNff1C
0010dd0: 5175 6470 3332 536e 7647 5761 3654 6365  Qudp32SnvGWa6Tce
0010de0: 3977 6576 7053 3730 3765 4665 4838 7162  9wevpS707eFeH8qb
0010df0: 3253 5078 4d6c 6439 3137 6a7a 634a 5465  2SPxMld917jzcJTe
...</pre><p>3. Insert some identifiable data into the table:</p><pre>...
mysql&gt; &lt;strong&gt;insert&lt;/strong&gt; into mysqlslap.t1 values (1,2,&quot;private&quot;,&quot;sensitive&quot;,&quot;data&quot;);
Query OK, 1 row affected (0.01 sec)

mysql&gt; select * from mysqlslap.t1 where charcol2=&quot;sensitive&quot;;
+---------+---------+----------+-----------+----------+
| intcol1 | intcol2 | charcol1 | charcol2  | charcol3 |
+---------+---------+----------+-----------+----------+
|       1 |       2 | private  | sensitive | data     |
+---------+---------+----------+-----------+----------+
1 row in set (0.02 sec)</pre><p>4. Observe this data via the command line:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
04fa290: 0002 7072 6976 6174 6573 656e 7369 7469  ..privatesensiti
...</pre><p>5. Encrypt the mysqlslap.t1 table:</p><pre>...
mysql&gt; &lt;strong&gt;alter&lt;/strong&gt; table mysqlslap.t1 encryption='Y';
Query OK, 10300 rows affected (0.31 sec)
Records: 10300  Duplicates: 0  Warnings: 0</pre><p>6. Observe the mysqlslap.t1 table is now encrypted:</p><p>6a. Via the mysql client:</p><pre>...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
+--------------+------------+----------------+
| TABLE_SCHEMA | TABLE_NAME | CREATE_OPTIONS |
+--------------+------------+----------------+
| mysqlslap    | t1         | ENCRYPTION=&quot;Y&quot; |
+--------------+------------+----------------+</pre><p>6b. Via the command line:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep &quot;private&quot;
[root@localhost ~]#</pre><p>6c. Observe snippet of the file:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0004160: 56e4 2930 bbea 167f 7c82 93b4 2fcf 8cc1  V.)0....|.../...
0004170: f443 9d6f 2e1e 9ac2 170a 3b7c 8f38 60bf  .C.o......;|.8`.
0004180: 3c75 2a42 0cc9 a79b 4309 cd83 da74 1b06  &amp;amp;lt;u*B....C....t..
0004190: 3a32 e104 43c5 8dfd f913 0f69 bda6 5e76  :2..C......i..^v
...</pre><p>7. Observe redo log is not encrypted:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/ib_logfile0 | less
...
23c6930: 0000 0144 0110 8000 0001 8000 0002 7072  ...D..........pr
23c6940: 6976 6174 6573 656e 7369 7469 7665 6461  ivatesensitiveda
23c6950: 7461 3723 0000 132e 2f6d 7973 716c 736c  ta7#..../mysqlsl
...</pre><p>This is expected because the documentation (<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html</a>) reports encryption of files outside the tablespace is not supported: &#8220;Tablespace encryption only applies to data in the tablespace. Data is not encrypted in the redo log, undo log, or binary log.&#8221;</p><p><strong>Conclusions</strong></p><p>I found in my testing of MariaDB&#8217;s implementation of data encryption at rest that there were still places on the file system that a bad actor could view sensitive data. I&#8217;ve found the same in this test of Oracle&#8217;s implementation. Both leave data exposed in log files surrounding the tablespace files.</p><p><strong>Bonus</strong></p><p>As a bonus to this walkthrough, during this testing, the table definition caught my eye:</p><pre>...
mysql&gt; show create table mysqlslap.t1\G
*************************** 1. row ***************************
       Table: t1
Create Table: CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y'
1 row in set (0.00 sec)</pre><p>As discussed in <a href="https://jira.mariadb.org/browse/MDEV-9571">https://jira.mariadb.org/browse/MDEV-9571</a>, the MariaDB implementation does not include the &#8220;encrypted=yes&#8221; information in the table definition when tables are implicitly encrypted.</p><p>I was curious what would happen if I did a mysqldump of this encrypted table and attempted to restore it to a nonencrypted server. DBAs expect mysqldump to create a portable file to recreate the table definition and data on a different version of mysql. During upgrades, for example, you might expect to use this for rollback.</p><p>Here is my test. I first did the dump and looked inside the file.</p><pre>...
[root@localhost ~]# mysqldump mysqlslap t1 &gt; mysqlslap_t1_dump
[root@localhost ~]# less mysqlslap_t1_dump
...
CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y';

&lt;strong&gt;INSERT&lt;/strong&gt; INTO `t1` VALUES (
...
,(1,2,'private','sensitive','data');</pre><p>As expected, that definition makes the dump less portable. The restore from dump is not completed and throws an error (this is not remedied by using &#8211;force):</p><p>On a slightly older 5.7 version:</p><pre>...
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.8-rc  |
+-----------+

[root@centosmysql57 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1064 (42000) at line 25: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'ENCRYPTION='Y'' at line 7</pre><p>On a different fork:</p><pre>...
MariaDB [(none)]&gt; select version();
+-----------------+
| version()       |
+-----------------+
| 10.1.12-MariaDB |
+-----------------+
1 row in set (0.00 sec)

[root@maria101 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1911 (HY000) at line 25: Unknown option 'ENCRYPTION'</pre><p>This doesn&#8217;t have anything to do with the encrypted state of the data in the table, just the table definition. I do like the encryption showing up in the table definition, for better visibility of encryption. Maybe the fix is to have mysqldump strip this when writing to the dump file.</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995182&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995182&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 18:28:52 +0000";s:2:"dc";a:1:{s:7:"creator";s:17:"The Pythian Group";}s:8:"category";s:54:"MySQLOpen SourceTechnical Trackdata encryptionsecurity";s:7:"summary";s:8406:"&nbsp;I&#8217;ve previously evaluated MariaDB&#8217;s 10.1 implementation of data encryption at rest (https://www.pythian.com/blog/data-encryption-rest), and recently did the same for Oracle&#8217;s implementation (https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html) in their MySQL 5.7.&nbsp;First, here&#8217;s a walkthrough of enabling encryption for MySQL 5.7:1. Install keyring plugin.1a. Add the following to the [mysqld] section of /etc/my.cnf:View the code on Gist.1b. Restart the server:...
service mysqld restart1c. Verify:...
mysql&gt; SELECT PLUGIN_NAME, PLUGIN_STATUS FROM INFORMATION_SCHEMA.PLUGINS WHERE PLUGIN_NAME LIKE 'keyring%';
+--------------+---------------+
| PLUGIN_NAME  | PLUGIN_STATUS |
+--------------+---------------+
| keyring_file | ACTIVE        |
+--------------+---------------+2. Ensure innodb_file_per_table is on.2a. Check....
mysql&gt; show global variables like 'innodb_file_per_table';
+-----------------------+-------+
| Variable_name         | Value |
+-----------------------+-------+
| innodb_file_per_table | ON    |
+-----------------------+-------+2b. If OFF, add the following to the [mysqld] section of /etc/my.cnf, restart, and alter each existing table to move it to its own tablespace:
innodb_file_per_table=ONGet list of available InnoDB tables:
mysql&gt;select table_schema, table_name, engine from information_schema.tables where engine='innodb' and table_schema not in ('information_schema');Run ALTER &#8230; ENGINE=INNODB on each above InnoDB tables:
mysql&gt;&lt;strong&gt;ALTER&lt;/strong&gt; TABLE [TABLE_SCHEMA].[TABLE_NAME] ENGINE=INNODB;&nbsp;Next, I walked through some testing.1. Create some data....
[root@localhost ~]# mysqlslap --concurrency=50 --number-int-cols=2 --number-char-cols=3 --auto-generate-sql --auto-generate-sql-write-number=10000 --no-drop2. Observe the mysqlslap.t1 table is not automatically encrypted. Unlike MariaDB&#8217;s implementation, there is not an option to encrypt tables by default.2a. Via the mysql client:...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
Empty set (0.05 sec)2b. Via the command line:(Install xxd if required.)...
[root@localhost ~]# yum install vim-common...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0010dc0: 5967 4b30 7530 7942 4266 664e 6666 3143  YgK0u0yBBffNff1C
0010dd0: 5175 6470 3332 536e 7647 5761 3654 6365  Qudp32SnvGWa6Tce
0010de0: 3977 6576 7053 3730 3765 4665 4838 7162  9wevpS707eFeH8qb
0010df0: 3253 5078 4d6c 6439 3137 6a7a 634a 5465  2SPxMld917jzcJTe
...3. Insert some identifiable data into the table:...
mysql&gt; &lt;strong&gt;insert&lt;/strong&gt; into mysqlslap.t1 values (1,2,&quot;private&quot;,&quot;sensitive&quot;,&quot;data&quot;);
Query OK, 1 row affected (0.01 sec)

mysql&gt; select * from mysqlslap.t1 where charcol2=&quot;sensitive&quot;;
+---------+---------+----------+-----------+----------+
| intcol1 | intcol2 | charcol1 | charcol2  | charcol3 |
+---------+---------+----------+-----------+----------+
|       1 |       2 | private  | sensitive | data     |
+---------+---------+----------+-----------+----------+
1 row in set (0.02 sec)4. Observe this data via the command line:...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
04fa290: 0002 7072 6976 6174 6573 656e 7369 7469  ..privatesensiti
...5. Encrypt the mysqlslap.t1 table:...
mysql&gt; &lt;strong&gt;alter&lt;/strong&gt; table mysqlslap.t1 encryption='Y';
Query OK, 10300 rows affected (0.31 sec)
Records: 10300  Duplicates: 0  Warnings: 06. Observe the mysqlslap.t1 table is now encrypted:6a. Via the mysql client:...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
+--------------+------------+----------------+
| TABLE_SCHEMA | TABLE_NAME | CREATE_OPTIONS |
+--------------+------------+----------------+
| mysqlslap    | t1         | ENCRYPTION=&quot;Y&quot; |
+--------------+------------+----------------+6b. Via the command line:...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep &quot;private&quot;
[root@localhost ~]#6c. Observe snippet of the file:...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0004160: 56e4 2930 bbea 167f 7c82 93b4 2fcf 8cc1  V.)0....|.../...
0004170: f443 9d6f 2e1e 9ac2 170a 3b7c 8f38 60bf  .C.o......;|.8`.
0004180: 3c75 2a42 0cc9 a79b 4309 cd83 da74 1b06  &amp;amp;lt;u*B....C....t..
0004190: 3a32 e104 43c5 8dfd f913 0f69 bda6 5e76  :2..C......i..^v
...7. Observe redo log is not encrypted:...
[root@localhost ~]# xxd /var/lib/mysql/ib_logfile0 | less
...
23c6930: 0000 0144 0110 8000 0001 8000 0002 7072  ...D..........pr
23c6940: 6976 6174 6573 656e 7369 7469 7665 6461  ivatesensitiveda
23c6950: 7461 3723 0000 132e 2f6d 7973 716c 736c  ta7#..../mysqlsl
...This is expected because the documentation (https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html) reports encryption of files outside the tablespace is not supported: &#8220;Tablespace encryption only applies to data in the tablespace. Data is not encrypted in the redo log, undo log, or binary log.&#8221;ConclusionsI found in my testing of MariaDB&#8217;s implementation of data encryption at rest that there were still places on the file system that a bad actor could view sensitive data. I&#8217;ve found the same in this test of Oracle&#8217;s implementation. Both leave data exposed in log files surrounding the tablespace files.BonusAs a bonus to this walkthrough, during this testing, the table definition caught my eye:...
mysql&gt; show create table mysqlslap.t1\G
*************************** 1. row ***************************
       Table: t1
Create Table: CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y'
1 row in set (0.00 sec)As discussed in https://jira.mariadb.org/browse/MDEV-9571, the MariaDB implementation does not include the &#8220;encrypted=yes&#8221; information in the table definition when tables are implicitly encrypted.I was curious what would happen if I did a mysqldump of this encrypted table and attempted to restore it to a nonencrypted server. DBAs expect mysqldump to create a portable file to recreate the table definition and data on a different version of mysql. During upgrades, for example, you might expect to use this for rollback.Here is my test. I first did the dump and looked inside the file....
[root@localhost ~]# mysqldump mysqlslap t1 &gt; mysqlslap_t1_dump
[root@localhost ~]# less mysqlslap_t1_dump
...
CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y';

&lt;strong&gt;INSERT&lt;/strong&gt; INTO `t1` VALUES (
...
,(1,2,'private','sensitive','data');As expected, that definition makes the dump less portable. The restore from dump is not completed and throws an error (this is not remedied by using &#8211;force):On a slightly older 5.7 version:...
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.8-rc  |
+-----------+

[root@centosmysql57 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1064 (42000) at line 25: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'ENCRYPTION='Y'' at line 7On a different fork:...
MariaDB [(none)]&gt; select version();
+-----------------+
| version()       |
+-----------------+
| 10.1.12-MariaDB |
+-----------------+
1 row in set (0.00 sec)

[root@maria101 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1911 (HY000) at line 25: Unknown option 'ENCRYPTION'This doesn&#8217;t have anything to do with the encrypted state of the data in the table, just the table definition. I do like the encryption showing up in the table definition, for better visibility of encryption. Maybe the fix is to have mysqldump strip this when writing to the dump file.";s:12:"atom_content";s:9609:"<div><div><p>&nbsp;</p><p>I&#8217;ve previously evaluated MariaDB&#8217;s 10.1 implementation of data encryption at rest (<a href="https://www.pythian.com/blog/data-encryption-rest">https://www.pythian.com/blog/data-encryption-rest</a>), and recently did the same for Oracle&#8217;s implementation (<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html</a>) in their MySQL 5.7.</p><p>&nbsp;</p><p><strong>First, here&#8217;s a walkthrough of enabling encryption for MySQL 5.7:</strong></p><p>1. Install keyring plugin.</p><p>1a. Add the following to the [mysqld] section of /etc/my.cnf:</p><p>View the code on <a href="https://gist.github.com/parham-pythian/a625bf472456da4774dec424dbbb4932">Gist</a>.</p><p>1b. Restart the server:</p><pre>...
service mysqld restart</pre><p>1c. Verify:</p><pre>...
mysql&gt; SELECT PLUGIN_NAME, PLUGIN_STATUS FROM INFORMATION_SCHEMA.PLUGINS WHERE PLUGIN_NAME LIKE 'keyring%';
+--------------+---------------+
| PLUGIN_NAME  | PLUGIN_STATUS |
+--------------+---------------+
| keyring_file | ACTIVE        |
+--------------+---------------+</pre><p>2. Ensure innodb_file_per_table is on.</p><p>2a. Check.</p><pre>...
mysql&gt; show global variables like 'innodb_file_per_table';
+-----------------------+-------+
| Variable_name         | Value |
+-----------------------+-------+
| innodb_file_per_table | ON    |
+-----------------------+-------+</pre><p>2b. If OFF, add the following to the [mysqld] section of /etc/my.cnf, restart, and alter each existing table to move it to its own tablespace:</p><pre>
innodb_file_per_table=ON</pre><p>Get list of available InnoDB tables:</p><pre>
mysql&gt;select table_schema, table_name, engine from information_schema.tables where engine='innodb' and table_schema not in ('information_schema');</pre><p>Run ALTER &#8230; ENGINE=INNODB on each above InnoDB tables:</p><pre>
mysql&gt;&lt;strong&gt;ALTER&lt;/strong&gt; TABLE [TABLE_SCHEMA].[TABLE_NAME] ENGINE=INNODB;</pre><p>&nbsp;</p><p><strong>Next, I walked through some testing.</strong></p><p>1. Create some data.</p><pre>...
[root@localhost ~]# mysqlslap --concurrency=50 --number-int-cols=2 --number-char-cols=3 --auto-generate-sql --auto-generate-sql-write-number=10000 --no-drop</pre><p>2. Observe the mysqlslap.t1 table is not automatically encrypted. Unlike MariaDB&#8217;s implementation, there is not an option to encrypt tables by default.</p><p>2a. Via the mysql client:</p><pre>...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
Empty set (0.05 sec)</pre><p>2b. Via the command line:</p><p>(Install xxd if required.)</p><pre>...
[root@localhost ~]# yum install vim-common</pre><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0010dc0: 5967 4b30 7530 7942 4266 664e 6666 3143  YgK0u0yBBffNff1C
0010dd0: 5175 6470 3332 536e 7647 5761 3654 6365  Qudp32SnvGWa6Tce
0010de0: 3977 6576 7053 3730 3765 4665 4838 7162  9wevpS707eFeH8qb
0010df0: 3253 5078 4d6c 6439 3137 6a7a 634a 5465  2SPxMld917jzcJTe
...</pre><p>3. Insert some identifiable data into the table:</p><pre>...
mysql&gt; &lt;strong&gt;insert&lt;/strong&gt; into mysqlslap.t1 values (1,2,&quot;private&quot;,&quot;sensitive&quot;,&quot;data&quot;);
Query OK, 1 row affected (0.01 sec)

mysql&gt; select * from mysqlslap.t1 where charcol2=&quot;sensitive&quot;;
+---------+---------+----------+-----------+----------+
| intcol1 | intcol2 | charcol1 | charcol2  | charcol3 |
+---------+---------+----------+-----------+----------+
|       1 |       2 | private  | sensitive | data     |
+---------+---------+----------+-----------+----------+
1 row in set (0.02 sec)</pre><p>4. Observe this data via the command line:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
04fa290: 0002 7072 6976 6174 6573 656e 7369 7469  ..privatesensiti
...</pre><p>5. Encrypt the mysqlslap.t1 table:</p><pre>...
mysql&gt; &lt;strong&gt;alter&lt;/strong&gt; table mysqlslap.t1 encryption='Y';
Query OK, 10300 rows affected (0.31 sec)
Records: 10300  Duplicates: 0  Warnings: 0</pre><p>6. Observe the mysqlslap.t1 table is now encrypted:</p><p>6a. Via the mysql client:</p><pre>...
mysql&gt; SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM INFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION=&quot;Y&quot;%';
+--------------+------------+----------------+
| TABLE_SCHEMA | TABLE_NAME | CREATE_OPTIONS |
+--------------+------------+----------------+
| mysqlslap    | t1         | ENCRYPTION=&quot;Y&quot; |
+--------------+------------+----------------+</pre><p>6b. Via the command line:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep &quot;private&quot;
[root@localhost ~]#</pre><p>6c. Observe snippet of the file:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/mysqlslap/t1.ibd | grep -v &quot;0000 0000&quot; | less
...
0004160: 56e4 2930 bbea 167f 7c82 93b4 2fcf 8cc1  V.)0....|.../...
0004170: f443 9d6f 2e1e 9ac2 170a 3b7c 8f38 60bf  .C.o......;|.8`.
0004180: 3c75 2a42 0cc9 a79b 4309 cd83 da74 1b06  &amp;amp;lt;u*B....C....t..
0004190: 3a32 e104 43c5 8dfd f913 0f69 bda6 5e76  :2..C......i..^v
...</pre><p>7. Observe redo log is not encrypted:</p><pre>...
[root@localhost ~]# xxd /var/lib/mysql/ib_logfile0 | less
...
23c6930: 0000 0144 0110 8000 0001 8000 0002 7072  ...D..........pr
23c6940: 6976 6174 6573 656e 7369 7469 7665 6461  ivatesensitiveda
23c6950: 7461 3723 0000 132e 2f6d 7973 716c 736c  ta7#..../mysqlsl
...</pre><p>This is expected because the documentation (<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-tablespace-encryption.html</a>) reports encryption of files outside the tablespace is not supported: &#8220;Tablespace encryption only applies to data in the tablespace. Data is not encrypted in the redo log, undo log, or binary log.&#8221;</p><p><strong>Conclusions</strong></p><p>I found in my testing of MariaDB&#8217;s implementation of data encryption at rest that there were still places on the file system that a bad actor could view sensitive data. I&#8217;ve found the same in this test of Oracle&#8217;s implementation. Both leave data exposed in log files surrounding the tablespace files.</p><p><strong>Bonus</strong></p><p>As a bonus to this walkthrough, during this testing, the table definition caught my eye:</p><pre>...
mysql&gt; show create table mysqlslap.t1\G
*************************** 1. row ***************************
       Table: t1
Create Table: CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y'
1 row in set (0.00 sec)</pre><p>As discussed in <a href="https://jira.mariadb.org/browse/MDEV-9571">https://jira.mariadb.org/browse/MDEV-9571</a>, the MariaDB implementation does not include the &#8220;encrypted=yes&#8221; information in the table definition when tables are implicitly encrypted.</p><p>I was curious what would happen if I did a mysqldump of this encrypted table and attempted to restore it to a nonencrypted server. DBAs expect mysqldump to create a portable file to recreate the table definition and data on a different version of mysql. During upgrades, for example, you might expect to use this for rollback.</p><p>Here is my test. I first did the dump and looked inside the file.</p><pre>...
[root@localhost ~]# mysqldump mysqlslap t1 &gt; mysqlslap_t1_dump
[root@localhost ~]# less mysqlslap_t1_dump
...
CREATE TABLE `t1` (
  `intcol1` int(32) DEFAULT NULL,
  `intcol2` int(32) DEFAULT NULL,
  `charcol1` varchar(128) DEFAULT NULL,
  `charcol2` varchar(128) DEFAULT NULL,
  `charcol3` varchar(128) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 ENCRYPTION='Y';

&lt;strong&gt;INSERT&lt;/strong&gt; INTO `t1` VALUES (
...
,(1,2,'private','sensitive','data');</pre><p>As expected, that definition makes the dump less portable. The restore from dump is not completed and throws an error (this is not remedied by using &#8211;force):</p><p>On a slightly older 5.7 version:</p><pre>...
mysql&gt; select version();
+-----------+
| version() |
+-----------+
| 5.7.8-rc  |
+-----------+

[root@centosmysql57 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1064 (42000) at line 25: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'ENCRYPTION='Y'' at line 7</pre><p>On a different fork:</p><pre>...
MariaDB [(none)]&gt; select version();
+-----------------+
| version()       |
+-----------------+
| 10.1.12-MariaDB |
+-----------------+
1 row in set (0.00 sec)

[root@maria101 ~]# mysql mysqlslap &lt; mysqlslap_t1_dump
ERROR 1911 (HY000) at line 25: Unknown option 'ENCRYPTION'</pre><p>This doesn&#8217;t have anything to do with the encrypted state of the data in the table, just the table definition. I do like the encryption showing up in the table definition, for better visibility of encryption. Maybe the fix is to have mysqldump strip this when writing to the dump file.</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995182&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995182&vote=-1&apivote=1">Vote DOWN</a>";}i:41;a:10:{s:5:"title";s:38:"MySQL Document Store versus Bug hunter";s:4:"guid";s:22:"http://mysql.az/?p=491";s:4:"link";s:66:"http://mysql.az/2016/04/20/mysql-document-store-versus-bug-hunter/";s:11:"description";s:1765:"Oh, there is a cool new feature added? And you are going to implement and use it?
Well, the brain of Bug lover is not working in that way, for me new feature is cool in terms of playing. It is a new place where hunt season is opened suddenly 
Currently I am in Percona Live 2016 and you can catch me every time -> my name is some sort of pain to pronounce but you can just say Shako instead of Shahriyar.
Yesterday there was an interesting discussion about bug reports and in general what is a &#8220;Bug&#8221; and how i can identify that if it is a Bug or not?
Here are general rules:
1. Trust in yourself. If somebody says that, hey it is an expected behaviour, just do not believe. Go ahead and fill a report. Insist in your opinion at last it will be converted to DOC issue, pretty valid DOC Bug -> http://bugs.mysql.com/bug.php?id=80350
2. Use Debug or Debug+Valgrind builds for your tests -> http://bugs.mysql.com/bug.php?id=80745
3. Be prepared for additional required infos. It is crucial that you give as much as possible info what you exactly did.
4. Time to time search and whatch for new bug reports. In fact reading bug reports and comments from experts is equal to reading a heavy book.
5. Be social. All open source guys are very friendly. You can contact them via Twitter or some other tool and just say hello.
6. Wait for new features  (Go and test MySQL Document Store just like me)
http://bugs.mysql.com/bug.php?id=81036
http://bugs.mysql.com/bug.php?id=81037
http://bugs.mysql.com/bug.php?id=81040
http://bugs.mysql.com/bug.php?id=81044
http://bugs.mysql.com/bug.php?id=81060
http://bugs.mysql.com/bug.php?id=81061
http://bugs.mysql.com/bug.php?id=81162
If you are in Percona Live 2016 come and find me to have a discussion about bugs and life.";s:7:"content";a:1:{s:7:"encoded";s:3291:"<p>Oh, there is a cool new feature added? And you are going to implement and use it?<br />
Well, the brain of Bug lover is not working in that way, for me new feature is cool in terms of playing. It is a new place where hunt season is opened suddenly <img src="https://s.w.org/images/core/emoji/72x72/1f600.png" alt="?" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>Currently I am in <strong>Percona Live 2016</strong> and you can catch me every time -> my name is some sort of pain to pronounce but you can just say Shako instead of Shahriyar.</p>
<p>Yesterday there was an interesting discussion about bug reports and in general what is a &#8220;Bug&#8221; and how i can identify that if it is a Bug or not?<br />
Here are general rules:</p>
<p>1. Trust in yourself. If somebody says that, hey it is an expected behaviour, just do not believe. Go ahead and fill a report. Insist in your opinion at last it will be converted to DOC issue, pretty valid DOC Bug -> <a href="http://bugs.mysql.com/bug.php?id=80350" title="http://bugs.mysql.com/bug.php?id=80350">http://bugs.mysql.com/bug.php?id=80350</a></p>
<p>2. Use Debug or Debug+Valgrind builds for your tests -> <a href="http://bugs.mysql.com/bug.php?id=80745" title="http://bugs.mysql.com/bug.php?id=80745">http://bugs.mysql.com/bug.php?id=80745</a></p>
<p>3. Be prepared for additional required infos. It is crucial that you give as much as possible info what you exactly did.</p>
<p>4. Time to time search and whatch for new bug reports. In fact reading bug reports and comments from experts is equal to reading a heavy book.</p>
<p>5. Be social. All open source guys are very friendly. You can contact them via Twitter or some other tool and just say hello.</p>
<p>6. Wait for new features <img src="https://s.w.org/images/core/emoji/72x72/1f600.png" alt="?" class="wp-smiley" style="height: 1em; max-height: 1em;" /> (Go and test MySQL Document Store just like me)</p>
<p><a href="http://bugs.mysql.com/bug.php?id=81036" title="http://bugs.mysql.com/bug.php?id=81036">http://bugs.mysql.com/bug.php?id=81036</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81037" title="http://bugs.mysql.com/bug.php?id=81037">http://bugs.mysql.com/bug.php?id=81037</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81040" title="http://bugs.mysql.com/bug.php?id=81040">http://bugs.mysql.com/bug.php?id=81040</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81044" title="http://bugs.mysql.com/bug.php?id=81044">http://bugs.mysql.com/bug.php?id=81044</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81060" title="http://bugs.mysql.com/bug.php?id=81060">http://bugs.mysql.com/bug.php?id=81060</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81061" title="http://bugs.mysql.com/bug.php?id=81061">http://bugs.mysql.com/bug.php?id=81061</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81162" title="http://bugs.mysql.com/bug.php?id=81162">http://bugs.mysql.com/bug.php?id=81162</a></p>
<p>If you are in <strong>Percona Live 2016</strong> come and find me to have a discussion about bugs and life.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995180&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995180&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 17:52:18 +0000";s:2:"dc";a:1:{s:7:"creator";s:16:"Shahriyar Rzayev";}s:8:"category";s:67:"BUG ReportsCommunity Lovemysql document storewhat is the mysql bug?";s:7:"summary";s:1765:"Oh, there is a cool new feature added? And you are going to implement and use it?
Well, the brain of Bug lover is not working in that way, for me new feature is cool in terms of playing. It is a new place where hunt season is opened suddenly 
Currently I am in Percona Live 2016 and you can catch me every time -> my name is some sort of pain to pronounce but you can just say Shako instead of Shahriyar.
Yesterday there was an interesting discussion about bug reports and in general what is a &#8220;Bug&#8221; and how i can identify that if it is a Bug or not?
Here are general rules:
1. Trust in yourself. If somebody says that, hey it is an expected behaviour, just do not believe. Go ahead and fill a report. Insist in your opinion at last it will be converted to DOC issue, pretty valid DOC Bug -> http://bugs.mysql.com/bug.php?id=80350
2. Use Debug or Debug+Valgrind builds for your tests -> http://bugs.mysql.com/bug.php?id=80745
3. Be prepared for additional required infos. It is crucial that you give as much as possible info what you exactly did.
4. Time to time search and whatch for new bug reports. In fact reading bug reports and comments from experts is equal to reading a heavy book.
5. Be social. All open source guys are very friendly. You can contact them via Twitter or some other tool and just say hello.
6. Wait for new features  (Go and test MySQL Document Store just like me)
http://bugs.mysql.com/bug.php?id=81036
http://bugs.mysql.com/bug.php?id=81037
http://bugs.mysql.com/bug.php?id=81040
http://bugs.mysql.com/bug.php?id=81044
http://bugs.mysql.com/bug.php?id=81060
http://bugs.mysql.com/bug.php?id=81061
http://bugs.mysql.com/bug.php?id=81162
If you are in Percona Live 2016 come and find me to have a discussion about bugs and life.";s:12:"atom_content";s:3291:"<p>Oh, there is a cool new feature added? And you are going to implement and use it?<br />
Well, the brain of Bug lover is not working in that way, for me new feature is cool in terms of playing. It is a new place where hunt season is opened suddenly <img src="https://s.w.org/images/core/emoji/72x72/1f600.png" alt="?" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>Currently I am in <strong>Percona Live 2016</strong> and you can catch me every time -> my name is some sort of pain to pronounce but you can just say Shako instead of Shahriyar.</p>
<p>Yesterday there was an interesting discussion about bug reports and in general what is a &#8220;Bug&#8221; and how i can identify that if it is a Bug or not?<br />
Here are general rules:</p>
<p>1. Trust in yourself. If somebody says that, hey it is an expected behaviour, just do not believe. Go ahead and fill a report. Insist in your opinion at last it will be converted to DOC issue, pretty valid DOC Bug -> <a href="http://bugs.mysql.com/bug.php?id=80350" title="http://bugs.mysql.com/bug.php?id=80350">http://bugs.mysql.com/bug.php?id=80350</a></p>
<p>2. Use Debug or Debug+Valgrind builds for your tests -> <a href="http://bugs.mysql.com/bug.php?id=80745" title="http://bugs.mysql.com/bug.php?id=80745">http://bugs.mysql.com/bug.php?id=80745</a></p>
<p>3. Be prepared for additional required infos. It is crucial that you give as much as possible info what you exactly did.</p>
<p>4. Time to time search and whatch for new bug reports. In fact reading bug reports and comments from experts is equal to reading a heavy book.</p>
<p>5. Be social. All open source guys are very friendly. You can contact them via Twitter or some other tool and just say hello.</p>
<p>6. Wait for new features <img src="https://s.w.org/images/core/emoji/72x72/1f600.png" alt="?" class="wp-smiley" style="height: 1em; max-height: 1em;" /> (Go and test MySQL Document Store just like me)</p>
<p><a href="http://bugs.mysql.com/bug.php?id=81036" title="http://bugs.mysql.com/bug.php?id=81036">http://bugs.mysql.com/bug.php?id=81036</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81037" title="http://bugs.mysql.com/bug.php?id=81037">http://bugs.mysql.com/bug.php?id=81037</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81040" title="http://bugs.mysql.com/bug.php?id=81040">http://bugs.mysql.com/bug.php?id=81040</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81044" title="http://bugs.mysql.com/bug.php?id=81044">http://bugs.mysql.com/bug.php?id=81044</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81060" title="http://bugs.mysql.com/bug.php?id=81060">http://bugs.mysql.com/bug.php?id=81060</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81061" title="http://bugs.mysql.com/bug.php?id=81061">http://bugs.mysql.com/bug.php?id=81061</a><br />
<a href="http://bugs.mysql.com/bug.php?id=81162" title="http://bugs.mysql.com/bug.php?id=81162">http://bugs.mysql.com/bug.php?id=81162</a></p>
<p>If you are in <strong>Percona Live 2016</strong> come and find me to have a discussion about bugs and life.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995180&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995180&vote=-1&apivote=1">Vote DOWN</a>";}i:42;a:10:{s:5:"title";s:52:"Log Buffer #470: A Carnival of the Vanities for DBAs";s:4:"guid";s:37:"https://www.pythian.com/blog/?p=87429";s:4:"link";s:67:"https://www.pythian.com/blog/log-buffer-470-carnival-vanities-dbas/";s:11:"description";s:1072:"This Log Buffer Edition rounds up blog posts from Oracle, SQL Server and MySQL.Oracle:jq is a unix utility that can parse json files and pull out individual elements &#8211; think of it as sed/awk for json files.Some thoughts about analyzing performance problems.Microsites: Add a Map Component and Publish your MicrositeNew Installation Cookbook: Oracle Linux 6.7 with Oracle 11.2.0.4 RACAre you worried about what to do now that Discoverer is almost out of supportAutomatic Big Table Caching in RAC&nbsp;SQL Server:sp_msforeachdb: Improving on an Undocumented Stored ProcedureThe Practical Problems of Determining Equality and Equivalence in SQLTEMPDB Enhancements in SQL Server 2016Performance Surprises and Assumptions : DATEADD()Knee-Jerk Performance Tuning : Incorrect Use of Temporary Tables&nbsp;MySQL:Virtual Hosting with vsftpd + TLS encryption and MySQL on Ubuntu 15.10MySQL 5.7.12 – Part 4: A new MySQL Command Line ShellDatabase Firewall Filter in MaxScale 1.4.1Orchestrator-agent: How to recover a MySQL databaseRosetta Stone: MySQL, Pig and Spark (Basics)";s:7:"content";a:1:{s:7:"encoded";s:3003:"<div><div><p>This Log Buffer Edition rounds up blog posts from Oracle, SQL Server and MySQL.</p><p><span></span><strong>Oracle:</strong></p><p>jq is a <a href="http://dbaharrison.blogspot.com/2016/04/jq-really-useful-unix-utility.html">unix</a> utility that can parse json files and pull out individual elements &#8211; think of it as sed/awk for json files.</p><p>Some <a href="https://amitzil.wordpress.com/2016/04/13/analyzing-performance/">thoughts</a> about analyzing performance problems.</p><p>Microsites: Add a Map Component and Publish your <a href="http://www.olrichs.nl/2016/04/microsites-add-map-component-and.html">Microsite</a></p><p>New Installation Cookbook: Oracle Linux 6.7 with Oracle 11.2.0.4 <a href="https://flashdba.com/2016/04/13/new-installation-cookbook-oracle-linux-6-7-with-oracle-11-2-0-4-rac/">RAC</a></p><p>Are you worried about what to do now that <a href="http://learndiscoverer.blogspot.com/2016/04/are-you-worried-about-what-to-do-now.html">Discoverer</a> is almost out of support</p><p>Automatic Big Table Caching in <a href="http://allthingsoracle.com/automatic-big-table-caching-in-rac/">RAC</a></p><p>&nbsp;</p><p><strong>SQL Server:</strong></p><p>sp_msforeachdb: Improving on an Undocumented <a href="http://www.sqlservercentral.com/articles/sp_msforeachdb/117654/">Stored</a> Procedure</p><p>The Practical Problems of Determining Equality and Equivalence in <a href="https://www.simple-talk.com/opinion/opinion-pieces/the-practical-problems-of-determining-equality-and-equivalence-in-sql/">SQL</a></p><p>TEMPDB Enhancements in <a href="http://www.sqlservercentral.com/articles/tempdb/139206/">SQL</a> Server 2016</p><p>Performance Surprises and Assumptions : <a href="http://sqlperformance.com/2016/04/sql-performance/surprises-dateadd">DATEADD</a>()</p><p><a href="http://sqlperformance.com/2016/04/t-sql-queries/knee-jerk-temporary-tables">Knee</a>-Jerk Performance Tuning : Incorrect Use of Temporary Tables</p><p>&nbsp;</p><p><strong>MySQL:</strong></p><p>Virtual Hosting with vsftpd + <a href="https://www.howtoforge.com/tutorial/virtual-hosting-with-vsftpd-and-mysql-on-ubuntu-15.10/">TLS</a> encryption and MySQL on Ubuntu 15.10</p><p>MySQL 5.7.12 – <a href="http://mysqlserverteam.com/mysql-5-7-12-part-4-a-new-mysql-command-line-shell/">Part</a> 4: A new MySQL Command Line Shell</p><p>Database Firewall Filter in <a href="http://mariadb.com/blog/database-firewall-filter-maxscale-141-0">MaxScale</a> 1.4.1</p><p>Orchestrator-agent: How to recover a <a href="https://www.percona.com/blog/2016/04/13/orchestrator-agent-how-to-recover-a-mysql-database/">MySQL</a> database</p><p>Rosetta Stone: <a href="http://mysqlblog.fivefarmers.com/2016/04/13/rosetta-stone-mysql-pig-and-spark-basics/">MySQL</a>, Pig and Spark (Basics)</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995181&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995181&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 17:39:36 +0000";s:2:"dc";a:1:{s:7:"creator";s:17:"The Pythian Group";}s:8:"category";s:25:"Log BufferTechnical Track";s:7:"summary";s:1072:"This Log Buffer Edition rounds up blog posts from Oracle, SQL Server and MySQL.Oracle:jq is a unix utility that can parse json files and pull out individual elements &#8211; think of it as sed/awk for json files.Some thoughts about analyzing performance problems.Microsites: Add a Map Component and Publish your MicrositeNew Installation Cookbook: Oracle Linux 6.7 with Oracle 11.2.0.4 RACAre you worried about what to do now that Discoverer is almost out of supportAutomatic Big Table Caching in RAC&nbsp;SQL Server:sp_msforeachdb: Improving on an Undocumented Stored ProcedureThe Practical Problems of Determining Equality and Equivalence in SQLTEMPDB Enhancements in SQL Server 2016Performance Surprises and Assumptions : DATEADD()Knee-Jerk Performance Tuning : Incorrect Use of Temporary Tables&nbsp;MySQL:Virtual Hosting with vsftpd + TLS encryption and MySQL on Ubuntu 15.10MySQL 5.7.12 – Part 4: A new MySQL Command Line ShellDatabase Firewall Filter in MaxScale 1.4.1Orchestrator-agent: How to recover a MySQL databaseRosetta Stone: MySQL, Pig and Spark (Basics)";s:12:"atom_content";s:3003:"<div><div><p>This Log Buffer Edition rounds up blog posts from Oracle, SQL Server and MySQL.</p><p><span></span><strong>Oracle:</strong></p><p>jq is a <a href="http://dbaharrison.blogspot.com/2016/04/jq-really-useful-unix-utility.html">unix</a> utility that can parse json files and pull out individual elements &#8211; think of it as sed/awk for json files.</p><p>Some <a href="https://amitzil.wordpress.com/2016/04/13/analyzing-performance/">thoughts</a> about analyzing performance problems.</p><p>Microsites: Add a Map Component and Publish your <a href="http://www.olrichs.nl/2016/04/microsites-add-map-component-and.html">Microsite</a></p><p>New Installation Cookbook: Oracle Linux 6.7 with Oracle 11.2.0.4 <a href="https://flashdba.com/2016/04/13/new-installation-cookbook-oracle-linux-6-7-with-oracle-11-2-0-4-rac/">RAC</a></p><p>Are you worried about what to do now that <a href="http://learndiscoverer.blogspot.com/2016/04/are-you-worried-about-what-to-do-now.html">Discoverer</a> is almost out of support</p><p>Automatic Big Table Caching in <a href="http://allthingsoracle.com/automatic-big-table-caching-in-rac/">RAC</a></p><p>&nbsp;</p><p><strong>SQL Server:</strong></p><p>sp_msforeachdb: Improving on an Undocumented <a href="http://www.sqlservercentral.com/articles/sp_msforeachdb/117654/">Stored</a> Procedure</p><p>The Practical Problems of Determining Equality and Equivalence in <a href="https://www.simple-talk.com/opinion/opinion-pieces/the-practical-problems-of-determining-equality-and-equivalence-in-sql/">SQL</a></p><p>TEMPDB Enhancements in <a href="http://www.sqlservercentral.com/articles/tempdb/139206/">SQL</a> Server 2016</p><p>Performance Surprises and Assumptions : <a href="http://sqlperformance.com/2016/04/sql-performance/surprises-dateadd">DATEADD</a>()</p><p><a href="http://sqlperformance.com/2016/04/t-sql-queries/knee-jerk-temporary-tables">Knee</a>-Jerk Performance Tuning : Incorrect Use of Temporary Tables</p><p>&nbsp;</p><p><strong>MySQL:</strong></p><p>Virtual Hosting with vsftpd + <a href="https://www.howtoforge.com/tutorial/virtual-hosting-with-vsftpd-and-mysql-on-ubuntu-15.10/">TLS</a> encryption and MySQL on Ubuntu 15.10</p><p>MySQL 5.7.12 – <a href="http://mysqlserverteam.com/mysql-5-7-12-part-4-a-new-mysql-command-line-shell/">Part</a> 4: A new MySQL Command Line Shell</p><p>Database Firewall Filter in <a href="http://mariadb.com/blog/database-firewall-filter-maxscale-141-0">MaxScale</a> 1.4.1</p><p>Orchestrator-agent: How to recover a <a href="https://www.percona.com/blog/2016/04/13/orchestrator-agent-how-to-recover-a-mysql-database/">MySQL</a> database</p><p>Rosetta Stone: <a href="http://mysqlblog.fivefarmers.com/2016/04/13/rosetta-stone-mysql-pig-and-spark-basics/">MySQL</a>, Pig and Spark (Basics)</p></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995181&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995181&vote=-1&apivote=1">Vote DOWN</a>";}i:43;a:9:{s:5:"title";s:52:"MySQL Replication failover: MaxScale vs MHA (part 4)";s:4:"guid";s:31:"4676 at http://severalnines.com";s:4:"link";s:78:"http://severalnines.com/blog/mysql-replication-failover-maxscale-vs-mha-part-4";s:11:"description";s:4015:"In the earlier blogs in this series, we concluded that MaxScale with MariaDB Replication Manager still has some way to go as a failover solution.
The failover mechanism relied on MariaDB GTID, needed a wrapper script around the decoupled replication manager and had no protection against flapping. Since then, MaxScale and MariaDB Replication Manager (MRM) have received a couple of updates to improve them. For MaxScale, the greatest improvement must be the availability of the community edition repository.
Integration
Previously, to configure MaxScale to execute the MariaDB Replication Manager upon master failure, one would add a wrapper shell script to translate MaxScale’s parameters to the command line options of MRM. This has now been improved, there is no need for the wrapper script anymore. That also means that there is now less chance of parameter mismatch.
The new configuration syntax for MaxScale is now:

[MySQL Monitor]
type=monitor
module=mysqlmon
servers=svr_10101811,svr_10101812,svr_10101813
user=admin
passwd=B4F3CB4FD8132F78DC2994A3C2AC7EC0
monitor_interval=1000
script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --failover=force --interactive=false --logfile=/var/log/failover.log
events=master_downAlso, to know what happened during the failover, you can read this from the failover log as defined above.
Preferred master
In the second blog post, we mentioned you can’t set candidate masters like you are used to with MHA. Actually, as the author indicated in the comments, this is possible with MRM: by defining the non-candidate masters as servers to be ignored by MRM during slave promotion.
The syntax in the MaxScale configuration would be:

script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --ignore-servers=’172.16.2.123:3306,172.16.2.126’ --failover=force --interactive=false --logfile=/var/log/failover.logFlapping
We also concluded the combination of MRM and MaxScale lacks the protection against flapping back and forth between nodes. This is mostly due to the fact that MRM and MaxScale are decoupled, they implement their own topology discovery. After MRM has performed its tasks, it exits. This could lead to an infinite loop where a slave gets promoted, fails due to the increase in load while the old-master becomes  healthy again and is re-promoted.
MRM actually has protection against flapping when used in the so called monitoring mode, where MRM runs as an application. The monitoring mode is an interactive mode where a DBA can either invoke a failover or have this done automatically. With the failover-limit parameter, you can limit the number of failovers before MRM will back off and stop promoting. Naturally this only works because MRM is keeping state in the interactive mode.
It would actually make sense to also add this functionality to the non-interactive mode and somewhere keep the state after the last failover(s). Then MRM would be able to stop performing the failover multiple times within a short timeframe.
Monitoring mode
MRM also features a so called “monitoring” mode where it constantly monitors the topology and could failover automatically if there is a master failure. With MaxScale we always set the mode to “force” to have MRM perform the failover without the need of a confirmation.  The monitoring mode actually invokes interactive mode, so unless you run it in screen, you can’t have MRM run in the background and perform the failover automatically for you.
Conclusion
MariaDB Replication Manager has improved over the past few weeks. With a few improvements, it has become more useful. Seeing the number of issues added (and resolved) indicate people are starting to test/use it. If MariaDB would provide binaries for MRM, the tool could receive wider adoption among the MariaDB users.
Tags: MySQLreplicationfailoverhigh availabilityMariaDBgtidMaxScaleMySQL Master-HA";s:7:"content";a:1:{s:7:"encoded";s:5880:"<div><div><div property="content:encoded"><p>In the <a href="http://severalnines.com/blog/mysql-replication-failover-maxscale-vs-mha-part-1">earlier blogs</a> in this series, we <a href="http://severalnines.com/blog/mysql-replication-failover-maxscale-vs-mha-part-3">concluded</a> that MaxScale with MariaDB Replication Manager still has some way to go as a failover solution.</p>
<p>The failover mechanism relied on MariaDB GTID, needed a wrapper script around the decoupled replication manager and had no protection against flapping. Since then, MaxScale and MariaDB Replication Manager (MRM) have received a couple of updates to improve them. For MaxScale, the greatest improvement must be the availability of the community edition repository.</p>
<h2>Integration</h2>
<p>Previously, to configure MaxScale to execute the MariaDB Replication Manager upon master failure, one would add a wrapper shell script to translate MaxScale’s parameters to the command line options of MRM. This has now been improved, there is no need for the wrapper script anymore. That also means that there is now less chance of parameter mismatch.</p>
<p>The new configuration syntax for MaxScale is now:</p>
<pre>
<code>[MySQL Monitor]
type=monitor
module=mysqlmon
servers=svr_10101811,svr_10101812,svr_10101813
user=admin
passwd=B4F3CB4FD8132F78DC2994A3C2AC7EC0
monitor_interval=1000
script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --failover=force --interactive=false --logfile=/var/log/failover.log
events=master_down</code></pre><p>Also, to know what happened during the failover, you can read this from the failover log as defined above.</p>
<h2>Preferred master</h2>
<p>In<a href="http://severalnines.com/blog/mysql-replication-failover-maxscale-vs-mha-part-2"> the second blog post</a>, we mentioned you can’t set candidate masters like you are used to with MHA. Actually, as the author indicated in the comments, this is possible with MRM: by defining the non-candidate masters as servers to be ignored by MRM during slave promotion.</p>
<p>The syntax in the MaxScale configuration would be:</p>
<pre>
<code>script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --ignore-servers=’172.16.2.123:3306,172.16.2.126’ --failover=force --interactive=false --logfile=/var/log/failover.log</code></pre><h2>Flapping</h2>
<p>We also concluded the combination of MRM and MaxScale lacks the protection against flapping back and forth between nodes. This is mostly due to the fact that MRM and MaxScale are decoupled, they implement their own topology discovery. After MRM has performed its tasks, it exits. This could lead to an infinite loop where a slave gets promoted, fails due to the increase in load while the old-master becomes  healthy again and is re-promoted.</p>
<p>MRM actually has protection against flapping when used in the so called monitoring mode, where MRM runs as an application. The monitoring mode is an interactive mode where a DBA can either invoke a failover or have this done automatically. With the failover-limit parameter, you can limit the number of failovers before MRM will back off and stop promoting. Naturally this only works because MRM is keeping state in the interactive mode.</p>
<p>It would actually make sense to also add this functionality to the non-interactive mode and somewhere keep the state after the last failover(s). Then MRM would be able to stop performing the failover multiple times within a short timeframe.</p>
<h2>Monitoring mode</h2>
<p>MRM also features a so called “monitoring” mode where it constantly monitors the topology and could failover automatically if there is a master failure. With MaxScale we always set the mode to “force” to have MRM perform the failover without the need of a confirmation.  The monitoring mode actually invokes interactive mode, so unless you run it in screen, you can’t have MRM run in the background and perform the failover automatically for you.</p>
<h2>Conclusion</h2>
<p>MariaDB Replication Manager has improved over the past few weeks. With a few improvements, it has become more useful. Seeing the number of issues added (and resolved) indicate people are starting to test/use it. If MariaDB would provide binaries for MRM, the tool could receive wider adoption among the MariaDB users.</p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/replication" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">replication</a></li><li><a href="http://severalnines.com/blog-tags/failover" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">failover</a></li><li><a href="http://severalnines.com/blog-tags/high-availability" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">high availability</a></li><li><a href="http://severalnines.com/blog-tags/mariadb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB</a></li><li><a href="http://severalnines.com/blog-tags/gtid" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">gtid</a></li><li><a href="http://severalnines.com/blog-tags/maxscale" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MaxScale</a></li><li><a href="http://severalnines.com/blog-tags/mysql-master-ha" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL Master-HA</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995199&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995199&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 17:20:56 +0000";s:2:"dc";a:1:{s:7:"creator";s:12:"Severalnines";}s:7:"summary";s:4015:"In the earlier blogs in this series, we concluded that MaxScale with MariaDB Replication Manager still has some way to go as a failover solution.
The failover mechanism relied on MariaDB GTID, needed a wrapper script around the decoupled replication manager and had no protection against flapping. Since then, MaxScale and MariaDB Replication Manager (MRM) have received a couple of updates to improve them. For MaxScale, the greatest improvement must be the availability of the community edition repository.
Integration
Previously, to configure MaxScale to execute the MariaDB Replication Manager upon master failure, one would add a wrapper shell script to translate MaxScale’s parameters to the command line options of MRM. This has now been improved, there is no need for the wrapper script anymore. That also means that there is now less chance of parameter mismatch.
The new configuration syntax for MaxScale is now:

[MySQL Monitor]
type=monitor
module=mysqlmon
servers=svr_10101811,svr_10101812,svr_10101813
user=admin
passwd=B4F3CB4FD8132F78DC2994A3C2AC7EC0
monitor_interval=1000
script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --failover=force --interactive=false --logfile=/var/log/failover.log
events=master_downAlso, to know what happened during the failover, you can read this from the failover log as defined above.
Preferred master
In the second blog post, we mentioned you can’t set candidate masters like you are used to with MHA. Actually, as the author indicated in the comments, this is possible with MRM: by defining the non-candidate masters as servers to be ignored by MRM during slave promotion.
The syntax in the MaxScale configuration would be:

script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --ignore-servers=’172.16.2.123:3306,172.16.2.126’ --failover=force --interactive=false --logfile=/var/log/failover.logFlapping
We also concluded the combination of MRM and MaxScale lacks the protection against flapping back and forth between nodes. This is mostly due to the fact that MRM and MaxScale are decoupled, they implement their own topology discovery. After MRM has performed its tasks, it exits. This could lead to an infinite loop where a slave gets promoted, fails due to the increase in load while the old-master becomes  healthy again and is re-promoted.
MRM actually has protection against flapping when used in the so called monitoring mode, where MRM runs as an application. The monitoring mode is an interactive mode where a DBA can either invoke a failover or have this done automatically. With the failover-limit parameter, you can limit the number of failovers before MRM will back off and stop promoting. Naturally this only works because MRM is keeping state in the interactive mode.
It would actually make sense to also add this functionality to the non-interactive mode and somewhere keep the state after the last failover(s). Then MRM would be able to stop performing the failover multiple times within a short timeframe.
Monitoring mode
MRM also features a so called “monitoring” mode where it constantly monitors the topology and could failover automatically if there is a master failure. With MaxScale we always set the mode to “force” to have MRM perform the failover without the need of a confirmation.  The monitoring mode actually invokes interactive mode, so unless you run it in screen, you can’t have MRM run in the background and perform the failover automatically for you.
Conclusion
MariaDB Replication Manager has improved over the past few weeks. With a few improvements, it has become more useful. Seeing the number of issues added (and resolved) indicate people are starting to test/use it. If MariaDB would provide binaries for MRM, the tool could receive wider adoption among the MariaDB users.
Tags: MySQLreplicationfailoverhigh availabilityMariaDBgtidMaxScaleMySQL Master-HA";s:12:"atom_content";s:5880:"<div><div><div property="content:encoded"><p>In the <a href="http://severalnines.com/blog/mysql-replication-failover-maxscale-vs-mha-part-1">earlier blogs</a> in this series, we <a href="http://severalnines.com/blog/mysql-replication-failover-maxscale-vs-mha-part-3">concluded</a> that MaxScale with MariaDB Replication Manager still has some way to go as a failover solution.</p>
<p>The failover mechanism relied on MariaDB GTID, needed a wrapper script around the decoupled replication manager and had no protection against flapping. Since then, MaxScale and MariaDB Replication Manager (MRM) have received a couple of updates to improve them. For MaxScale, the greatest improvement must be the availability of the community edition repository.</p>
<h2>Integration</h2>
<p>Previously, to configure MaxScale to execute the MariaDB Replication Manager upon master failure, one would add a wrapper shell script to translate MaxScale’s parameters to the command line options of MRM. This has now been improved, there is no need for the wrapper script anymore. That also means that there is now less chance of parameter mismatch.</p>
<p>The new configuration syntax for MaxScale is now:</p>
<pre>
<code>[MySQL Monitor]
type=monitor
module=mysqlmon
servers=svr_10101811,svr_10101812,svr_10101813
user=admin
passwd=B4F3CB4FD8132F78DC2994A3C2AC7EC0
monitor_interval=1000
script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --failover=force --interactive=false --logfile=/var/log/failover.log
events=master_down</code></pre><p>Also, to know what happened during the failover, you can read this from the failover log as defined above.</p>
<h2>Preferred master</h2>
<p>In<a href="http://severalnines.com/blog/mysql-replication-failover-maxscale-vs-mha-part-2"> the second blog post</a>, we mentioned you can’t set candidate masters like you are used to with MHA. Actually, as the author indicated in the comments, this is possible with MRM: by defining the non-candidate masters as servers to be ignored by MRM during slave promotion.</p>
<p>The syntax in the MaxScale configuration would be:</p>
<pre>
<code>script=script=/usr/local/bin/replication-manager --user root:admin --rpluser repluser:replpass --hosts $INITIATOR,$NODELIST --ignore-servers=’172.16.2.123:3306,172.16.2.126’ --failover=force --interactive=false --logfile=/var/log/failover.log</code></pre><h2>Flapping</h2>
<p>We also concluded the combination of MRM and MaxScale lacks the protection against flapping back and forth between nodes. This is mostly due to the fact that MRM and MaxScale are decoupled, they implement their own topology discovery. After MRM has performed its tasks, it exits. This could lead to an infinite loop where a slave gets promoted, fails due to the increase in load while the old-master becomes  healthy again and is re-promoted.</p>
<p>MRM actually has protection against flapping when used in the so called monitoring mode, where MRM runs as an application. The monitoring mode is an interactive mode where a DBA can either invoke a failover or have this done automatically. With the failover-limit parameter, you can limit the number of failovers before MRM will back off and stop promoting. Naturally this only works because MRM is keeping state in the interactive mode.</p>
<p>It would actually make sense to also add this functionality to the non-interactive mode and somewhere keep the state after the last failover(s). Then MRM would be able to stop performing the failover multiple times within a short timeframe.</p>
<h2>Monitoring mode</h2>
<p>MRM also features a so called “monitoring” mode where it constantly monitors the topology and could failover automatically if there is a master failure. With MaxScale we always set the mode to “force” to have MRM perform the failover without the need of a confirmation.  The monitoring mode actually invokes interactive mode, so unless you run it in screen, you can’t have MRM run in the background and perform the failover automatically for you.</p>
<h2>Conclusion</h2>
<p>MariaDB Replication Manager has improved over the past few weeks. With a few improvements, it has become more useful. Seeing the number of issues added (and resolved) indicate people are starting to test/use it. If MariaDB would provide binaries for MRM, the tool could receive wider adoption among the MariaDB users.</p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/replication" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">replication</a></li><li><a href="http://severalnines.com/blog-tags/failover" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">failover</a></li><li><a href="http://severalnines.com/blog-tags/high-availability" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">high availability</a></li><li><a href="http://severalnines.com/blog-tags/mariadb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB</a></li><li><a href="http://severalnines.com/blog-tags/gtid" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">gtid</a></li><li><a href="http://severalnines.com/blog-tags/maxscale" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MaxScale</a></li><li><a href="http://severalnines.com/blog-tags/mysql-master-ha" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL Master-HA</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995199&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995199&vote=-1&apivote=1">Vote DOWN</a>";}i:44;a:10:{s:5:"title";s:55:"MySQL 5.7.12 – MySQL Document Store – YouTube Video";s:4:"guid";s:34:"http://mysqlserverteam.com/?p=6225";s:4:"link";s:75:"http://mysqlserverteam.com/mysql-5-7-12-mysql-document-store-youtube-video/";s:11:"description";s:619:"Today we are introducing a new MySQL Server feature called MySQL Document Store. Make sure to watch the announcement video on the MySQL YouTube Channel!

Find out more by looking into the following documents.

Using MySQL as a Document Store &#8211; Manual
X DevAPI &#8211; Tutorial for JavaScript
X DevAPI &#8211; Tutorial for Python
X DevAPI User Guide &#8211; A detailed look at the  X DevAPI
MySQL Connector/Node.js X DevAPI Reference
MySQL Connector/Net &#8211; X DevAPI Reference
MySQL Connector/J &#8211; X DevAPI Reference

If you got questions or feedback please feel free to post on the MySQL Forums.&hellip;";s:7:"content";a:1:{s:7:"encoded";s:2364:"<p>Today we are introducing a new MySQL Server feature called <strong>MySQL Document Store</strong>. Make sure to watch the <a href="https://youtu.be/1Dk517M-_7o" target="_blank">announcement video</a> on the <a href="https://www.youtube.com/user/MySQLChannel" target="_blank">MySQL YouTube Channel</a>!</p>
<p><a href="https://youtu.be/1Dk517M-_7o" rel="attachment wp-att-6226"><img class="alignnone wp-image-6226 size-large" src="http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-1024x576.jpg" alt="MySQL Document Store Video" width="604" height="340" srcset="http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-300x169.jpg 300w, http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-768x432.jpg 768w, http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-1024x576.jpg 1024w" sizes="(max-width: 604px) 100vw, 604px" /></a></p>
<p>Find out more by looking into the following documents.</p>
<ul>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/document-store.html" target="_blank">Using MySQL as a Document Store &#8211; Manual</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/mysql-shell-tutorial-javascript.html" target="_blank">X DevAPI &#8211; Tutorial for JavaScript</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/mysql-shell-tutorial-python.html" target="_blank">X DevAPI &#8211; Tutorial for Python</a></li>
<li><a href="http://dev.mysql.com/doc/x-devapi-userguide/en/" target="_blank">X DevAPI User Guide &#8211; A detailed look at the  X DevAPI</a></li>
<li><a href="http://dev.mysql.com/doc/dev/connector-nodejs/" target="_blank">MySQL Connector/Node.js X DevAPI Reference</a></li>
<li><a href="http://dev.mysql.com/doc/dev/connector-net/" target="_blank">MySQL Connector/Net &#8211; X DevAPI Reference</a></li>
<li><a href="http://dev.mysql.com/doc/dev/connector-j/" target="_blank">MySQL Connector/J &#8211; X DevAPI Reference</a></li>
</ul>
<p>If you got questions or feedback please feel free to post on the <a href="http://forums.mysql.com/list.php?176" target="_blank">MySQL Forums</a>.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995176&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995176&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 17:16:10 +0000";s:2:"dc";a:1:{s:7:"creator";s:21:"MySQL Server Dev Team";}s:8:"category";s:19:"Document StoreMySQL";s:7:"summary";s:619:"Today we are introducing a new MySQL Server feature called MySQL Document Store. Make sure to watch the announcement video on the MySQL YouTube Channel!

Find out more by looking into the following documents.

Using MySQL as a Document Store &#8211; Manual
X DevAPI &#8211; Tutorial for JavaScript
X DevAPI &#8211; Tutorial for Python
X DevAPI User Guide &#8211; A detailed look at the  X DevAPI
MySQL Connector/Node.js X DevAPI Reference
MySQL Connector/Net &#8211; X DevAPI Reference
MySQL Connector/J &#8211; X DevAPI Reference

If you got questions or feedback please feel free to post on the MySQL Forums.&hellip;";s:12:"atom_content";s:2364:"<p>Today we are introducing a new MySQL Server feature called <strong>MySQL Document Store</strong>. Make sure to watch the <a href="https://youtu.be/1Dk517M-_7o" target="_blank">announcement video</a> on the <a href="https://www.youtube.com/user/MySQLChannel" target="_blank">MySQL YouTube Channel</a>!</p>
<p><a href="https://youtu.be/1Dk517M-_7o" rel="attachment wp-att-6226"><img class="alignnone wp-image-6226 size-large" src="http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-1024x576.jpg" alt="MySQL Document Store Video" width="604" height="340" srcset="http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-300x169.jpg 300w, http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-768x432.jpg 768w, http://mysqlserverteam.com/wp-content/uploads/2016/04/MySQLDocumentStore_1.1.1-1024x576.jpg 1024w" sizes="(max-width: 604px) 100vw, 604px" /></a></p>
<p>Find out more by looking into the following documents.</p>
<ul>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/document-store.html" target="_blank">Using MySQL as a Document Store &#8211; Manual</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/mysql-shell-tutorial-javascript.html" target="_blank">X DevAPI &#8211; Tutorial for JavaScript</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/mysql-shell-tutorial-python.html" target="_blank">X DevAPI &#8211; Tutorial for Python</a></li>
<li><a href="http://dev.mysql.com/doc/x-devapi-userguide/en/" target="_blank">X DevAPI User Guide &#8211; A detailed look at the  X DevAPI</a></li>
<li><a href="http://dev.mysql.com/doc/dev/connector-nodejs/" target="_blank">MySQL Connector/Node.js X DevAPI Reference</a></li>
<li><a href="http://dev.mysql.com/doc/dev/connector-net/" target="_blank">MySQL Connector/Net &#8211; X DevAPI Reference</a></li>
<li><a href="http://dev.mysql.com/doc/dev/connector-j/" target="_blank">MySQL Connector/J &#8211; X DevAPI Reference</a></li>
</ul>
<p>If you got questions or feedback please feel free to post on the <a href="http://forums.mysql.com/list.php?176" target="_blank">MySQL Forums</a>.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995176&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995176&vote=-1&apivote=1">Vote DOWN</a>";}i:45;a:10:{s:5:"title";s:32:"Announcing MySQL Connector/J 6.0";s:4:"guid";s:29:"http://insidemysql.com/?p=444";s:4:"link";s:55:"http://insidemysql.com/announcing-mysql-connectorj-6-0/";s:11:"description";s:1686:"We are pleased to announce the first release of MySQL Connector/J 6.0! This is a new branch of development, which breaks from some of the traditions of the very stable and very mature Connector/J 5.1 branch. We have combed through lots of code and refactored it in order to support development of future features including our X DevAPI implementation and support for X Protocol. This work manifests in a number of visible changes, including revising the growing set of connection and build properties.
Beginning with Connector/J 6.0, we are moving away from having one jar that supports all versions of Java. Instead we are building one jar/package for every supported version of Java. This simplifies the build process as well as lots of code that was required to support many versions of Java. The current package is built for Java 8.
Note that Connector/J 6.0.2 is a milestone release and not intended for production usage.
All changes are documented in the MySQL Connector/J 6.0 Developer Guide.
Updates to connection properties, including removal of several options that are no longer needed with the latest versions of MySQL and Java. Additionally, we removed some legacy behavioral options. You can see the changes in the documentation:
&#8211; Connector/J 6.0 Changes in Connection Properties
If you are building from source or running the test suite, check the following:
&#8211; Connector/J 6.0 Changes for Build Properties
&#8211; Connector/J 6.0 Changes for Test Properties
Finally, API changes and package reorganizations are documented at:
&#8211; Changes in the Connector/J API
Release notes are available at Changes in MySQL Connector/J 6.0.2 (2016-04-11, Milestone 1).";s:7:"content";a:1:{s:7:"encoded";s:2645:"<p>We are pleased to announce the first release of MySQL Connector/J 6.0! This is a new branch of development, which breaks from some of the traditions of the very stable and very mature Connector/J 5.1 branch. We have combed through lots of code and refactored it in order to support development of future features including our <a href="https://dev.mysql.com/doc/x-devapi-userguide/en/">X DevAPI implementation</a> and support for <a href="http://mysqlserverteam.com/mysql-5-7-12-part-2-improving-the-mysql-protocol/">X Protocol</a>. This work manifests in a number of visible changes, including revising the growing set of connection and build properties.</p>
<p>Beginning with Connector/J 6.0, we are moving away from having one jar that supports all versions of Java. Instead we are building one jar/package for every supported version of Java. This simplifies the build process as well as lots of code that was required to support many versions of Java. The current package is built for Java 8.</p>
<p>Note that Connector/J 6.0.2 is a milestone release and not intended for production usage.</p>
<p>All changes are documented in the <a href="http://dev.mysql.com/doc/connector-j/6.0/en/">MySQL Connector/J 6.0 Developer Guide</a>.</p>
<p>Updates to connection properties, including removal of several options that are no longer needed with the latest versions of MySQL and Java. Additionally, we removed some legacy behavioral options. You can see the changes in the documentation:<br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-properties-changed.html">Connector/J 6.0 Changes in Connection Properties</a></p>
<p>If you are building from source or running the test suite, check the following:<br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-build-props-changes.html">Connector/J 6.0 Changes for Build Properties</a><br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-test-props-changes.html">Connector/J 6.0 Changes for Test Properties</a></p>
<p>Finally, API changes and package reorganizations are documented at:<br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-api-changes.html">Changes in the Connector/J API</a></p>
<p>Release notes are available at <a href="http://dev.mysql.com/doc/relnotes/connector-j/6.0/en/news-6-0-2.html">Changes in MySQL Connector/J 6.0.2 (2016-04-11, Milestone 1)</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995178&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995178&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 17:13:46 +0000";s:2:"dc";a:1:{s:7:"creator";s:11:"Jess Balint";}s:8:"category";s:61:"MySQL Developmentconnector-jconnectorsdocument-storejavamysql";s:7:"summary";s:1686:"We are pleased to announce the first release of MySQL Connector/J 6.0! This is a new branch of development, which breaks from some of the traditions of the very stable and very mature Connector/J 5.1 branch. We have combed through lots of code and refactored it in order to support development of future features including our X DevAPI implementation and support for X Protocol. This work manifests in a number of visible changes, including revising the growing set of connection and build properties.
Beginning with Connector/J 6.0, we are moving away from having one jar that supports all versions of Java. Instead we are building one jar/package for every supported version of Java. This simplifies the build process as well as lots of code that was required to support many versions of Java. The current package is built for Java 8.
Note that Connector/J 6.0.2 is a milestone release and not intended for production usage.
All changes are documented in the MySQL Connector/J 6.0 Developer Guide.
Updates to connection properties, including removal of several options that are no longer needed with the latest versions of MySQL and Java. Additionally, we removed some legacy behavioral options. You can see the changes in the documentation:
&#8211; Connector/J 6.0 Changes in Connection Properties
If you are building from source or running the test suite, check the following:
&#8211; Connector/J 6.0 Changes for Build Properties
&#8211; Connector/J 6.0 Changes for Test Properties
Finally, API changes and package reorganizations are documented at:
&#8211; Changes in the Connector/J API
Release notes are available at Changes in MySQL Connector/J 6.0.2 (2016-04-11, Milestone 1).";s:12:"atom_content";s:2645:"<p>We are pleased to announce the first release of MySQL Connector/J 6.0! This is a new branch of development, which breaks from some of the traditions of the very stable and very mature Connector/J 5.1 branch. We have combed through lots of code and refactored it in order to support development of future features including our <a href="https://dev.mysql.com/doc/x-devapi-userguide/en/">X DevAPI implementation</a> and support for <a href="http://mysqlserverteam.com/mysql-5-7-12-part-2-improving-the-mysql-protocol/">X Protocol</a>. This work manifests in a number of visible changes, including revising the growing set of connection and build properties.</p>
<p>Beginning with Connector/J 6.0, we are moving away from having one jar that supports all versions of Java. Instead we are building one jar/package for every supported version of Java. This simplifies the build process as well as lots of code that was required to support many versions of Java. The current package is built for Java 8.</p>
<p>Note that Connector/J 6.0.2 is a milestone release and not intended for production usage.</p>
<p>All changes are documented in the <a href="http://dev.mysql.com/doc/connector-j/6.0/en/">MySQL Connector/J 6.0 Developer Guide</a>.</p>
<p>Updates to connection properties, including removal of several options that are no longer needed with the latest versions of MySQL and Java. Additionally, we removed some legacy behavioral options. You can see the changes in the documentation:<br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-properties-changed.html">Connector/J 6.0 Changes in Connection Properties</a></p>
<p>If you are building from source or running the test suite, check the following:<br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-build-props-changes.html">Connector/J 6.0 Changes for Build Properties</a><br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-test-props-changes.html">Connector/J 6.0 Changes for Test Properties</a></p>
<p>Finally, API changes and package reorganizations are documented at:<br />
&#8211; <a href="http://dev.mysql.com/doc/connector-j/6.0/en/connector-j-api-changes.html">Changes in the Connector/J API</a></p>
<p>Release notes are available at <a href="http://dev.mysql.com/doc/relnotes/connector-j/6.0/en/news-6-0-2.html">Changes in MySQL Connector/J 6.0.2 (2016-04-11, Milestone 1)</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995178&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995178&vote=-1&apivote=1">Vote DOWN</a>";}i:46;a:10:{s:5:"title";s:43:"MySQL Connector/Net 7.0.2 has been released";s:4:"guid";s:29:"http://insidemysql.com/?p=434";s:4:"link";s:66:"http://insidemysql.com/mysql-connectornet-7-0-2-has-been-released/";s:11:"description";s:1543:"Dear MySQL users,
MySQL Connector/Net 7.0.2 M1 is the first development release that adds support for the new X DevAPI.  The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.
To learn more about how to write applications using the X DevAPI, see this User Guide. For more information about how the X DevAPI is implemented in Connector/Net, please check the official product documentation.
Also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document store, see this chapter at the reference manual.
Changes in MySQL Connector/Net 7.0.2 (2016-04-11, Milestone 1)
Functionality Added

Added support for the new X DevAPI

Nuget packages are available at:
https://www.nuget.org/packages/MySql.Data/7.0.2-DMR
https://www.nuget.org/packages/MySql.Data.Entity/7.0.2-DMR
https://www.nuget.org/packages/MySql.Fabric/7.0.2-DMR
https://www.nuget.org/packages/MySql.Web/7.0.2-DMR
We love to hear your thoughts or any comments you have about our product. Please send us your feedback at our forums, fill a bug at our community site, or leave us any comment at the social media channels.

MySQL Connector/net Forums
MySQL Connector/net Documentation
MySQL on Facebook

Enjoy and thanks for the support!
On behalf of the MySQL Connector/Net Team
&nbsp;";s:7:"content";a:1:{s:7:"encoded";s:2661:"<p>Dear MySQL users,</p>
<p>MySQL Connector/Net 7.0.2 M1 is the first development release that adds support for the new X DevAPI.  The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.</p>
<p>To learn more about how to write applications using the X DevAPI, see this <a href="http://dev.mysql.com/doc/x-devapi-userguide/en/index.html">User Guide</a>. For more information about how the X DevAPI is implemented in Connector/Net, please check the <a href="http://dev.mysql.com/doc/dev/connector-net">official product documentation</a>.</p>
<p>Also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document store, see this <a href="http://dev.mysql.com/doc/refman/5.7/en/document-store.html" target="_blank">chapter at the reference manual</a>.</p>
<p><strong>Changes in MySQL Connector/Net 7.0.2 (2016-04-11, Milestone 1)</strong></p>
<p>Functionality Added</p>
<ul>
<li>Added support for the new X DevAPI</li>
</ul>
<p>Nuget packages are available at:</p>
<p><a href="https://www.nuget.org/packages/MySql.Data/7.0.2-DMR">https://www.nuget.org/packages/MySql.Data/7.0.2-DMR</a></p>
<p><a href="https://www.nuget.org/packages/MySql.Data.Entity/7.0.2-DMR">https://www.nuget.org/packages/MySql.Data.Entity/7.0.2-DMR</a></p>
<p><a href="https://www.nuget.org/packages/MySql.Fabric/7.0.2-DMR">https://www.nuget.org/packages/MySql.Fabric/7.0.2-DMR</a></p>
<p><a href="https://www.nuget.org/packages/MySql.Web/7.0.2-DMR">https://www.nuget.org/packages/MySql.Web/7.0.2-DMR</a></p>
<p>We love to hear your thoughts or any comments you have about our product. Please send us your feedback at our forums, fill a bug at <a href="http://bugs.mysql.com/" target="_blank">our community site</a>, or leave us any comment at the social media channels.</p>
<ul>
<li><a href="http://forums.mysql.com/list.php?38">MySQL Connector/net Forums</a></li>
<li><a href="https://dev.mysql.com/doc/connector-net/en/" target="_blank">MySQL Connector/net Documentation</a></li>
<li><a href="https://www.facebook.com/mysql/" target="_blank">MySQL on Facebook</a></li>
</ul>
<p>Enjoy and thanks for the support!</p>
<p>On behalf of the MySQL Connector/Net Team</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995179&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995179&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 17:13:32 +0000";s:2:"dc";a:1:{s:7:"creator";s:17:"Gabriela Martinez";}s:8:"category";s:62:"MySQL Developmentdocument-storemysql-5.7.12mysql-connector-net";s:7:"summary";s:1543:"Dear MySQL users,
MySQL Connector/Net 7.0.2 M1 is the first development release that adds support for the new X DevAPI.  The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.
To learn more about how to write applications using the X DevAPI, see this User Guide. For more information about how the X DevAPI is implemented in Connector/Net, please check the official product documentation.
Also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document store, see this chapter at the reference manual.
Changes in MySQL Connector/Net 7.0.2 (2016-04-11, Milestone 1)
Functionality Added

Added support for the new X DevAPI

Nuget packages are available at:
https://www.nuget.org/packages/MySql.Data/7.0.2-DMR
https://www.nuget.org/packages/MySql.Data.Entity/7.0.2-DMR
https://www.nuget.org/packages/MySql.Fabric/7.0.2-DMR
https://www.nuget.org/packages/MySql.Web/7.0.2-DMR
We love to hear your thoughts or any comments you have about our product. Please send us your feedback at our forums, fill a bug at our community site, or leave us any comment at the social media channels.

MySQL Connector/net Forums
MySQL Connector/net Documentation
MySQL on Facebook

Enjoy and thanks for the support!
On behalf of the MySQL Connector/Net Team
&nbsp;";s:12:"atom_content";s:2661:"<p>Dear MySQL users,</p>
<p>MySQL Connector/Net 7.0.2 M1 is the first development release that adds support for the new X DevAPI.  The X DevAPI enables application developers to write code that combines the strengths of the relational and document models using a modern, NoSQL-like syntax that does not assume previous experience writing traditional SQL.</p>
<p>To learn more about how to write applications using the X DevAPI, see this <a href="http://dev.mysql.com/doc/x-devapi-userguide/en/index.html">User Guide</a>. For more information about how the X DevAPI is implemented in Connector/Net, please check the <a href="http://dev.mysql.com/doc/dev/connector-net">official product documentation</a>.</p>
<p>Also note that the X DevAPI requires at least MySQL Server version 5.7.12 or higher with the X Plugin enabled. For general documentation about how to get started using MySQL as a document store, see this <a href="http://dev.mysql.com/doc/refman/5.7/en/document-store.html" target="_blank">chapter at the reference manual</a>.</p>
<p><strong>Changes in MySQL Connector/Net 7.0.2 (2016-04-11, Milestone 1)</strong></p>
<p>Functionality Added</p>
<ul>
<li>Added support for the new X DevAPI</li>
</ul>
<p>Nuget packages are available at:</p>
<p><a href="https://www.nuget.org/packages/MySql.Data/7.0.2-DMR">https://www.nuget.org/packages/MySql.Data/7.0.2-DMR</a></p>
<p><a href="https://www.nuget.org/packages/MySql.Data.Entity/7.0.2-DMR">https://www.nuget.org/packages/MySql.Data.Entity/7.0.2-DMR</a></p>
<p><a href="https://www.nuget.org/packages/MySql.Fabric/7.0.2-DMR">https://www.nuget.org/packages/MySql.Fabric/7.0.2-DMR</a></p>
<p><a href="https://www.nuget.org/packages/MySql.Web/7.0.2-DMR">https://www.nuget.org/packages/MySql.Web/7.0.2-DMR</a></p>
<p>We love to hear your thoughts or any comments you have about our product. Please send us your feedback at our forums, fill a bug at <a href="http://bugs.mysql.com/" target="_blank">our community site</a>, or leave us any comment at the social media channels.</p>
<ul>
<li><a href="http://forums.mysql.com/list.php?38">MySQL Connector/net Forums</a></li>
<li><a href="https://dev.mysql.com/doc/connector-net/en/" target="_blank">MySQL Connector/net Documentation</a></li>
<li><a href="https://www.facebook.com/mysql/" target="_blank">MySQL on Facebook</a></li>
</ul>
<p>Enjoy and thanks for the support!</p>
<p>On behalf of the MySQL Connector/Net Team</p>
<p>&nbsp;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995179&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995179&vote=-1&apivote=1">Vote DOWN</a>";}i:47;a:10:{s:5:"title";s:82:"MySQL 5.7.12 – Part 6: MySQL Document Store – A New Chapter in the MySQL Story";s:4:"guid";s:34:"http://mysqlserverteam.com/?p=6027";s:4:"link";s:101:"http://mysqlserverteam.com/mysql-5-7-12-part-6-mysql-document-store-a-new-chapter-in-the-mysql-story/";s:11:"description";s:270:"So hopefully you’ve read the first 5 blogs in this 6 part series you should have a good introduction to
•    Part 1 &#8211; What we heard from the MySQL Community (keep it solid, and innovate faster) and our change to new release model those requirements.&hellip;";s:7:"content";a:1:{s:7:"encoded";s:496:"<p>So hopefully you’ve read the first 5 blogs in this 6 part series you should have a good introduction to<br />
•    Part 1 &#8211; What we heard from the MySQL Community (keep it solid, and innovate faster) and our change to new release model those requirements.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995177&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995177&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 16:00:42 +0000";s:2:"dc";a:1:{s:7:"creator";s:10:"Mike Frank";}s:8:"category";s:19:"Document StoreMySQL";s:7:"summary";s:270:"So hopefully you’ve read the first 5 blogs in this 6 part series you should have a good introduction to
•    Part 1 &#8211; What we heard from the MySQL Community (keep it solid, and innovate faster) and our change to new release model those requirements.&hellip;";s:12:"atom_content";s:496:"<p>So hopefully you’ve read the first 5 blogs in this 6 part series you should have a good introduction to<br />
•    Part 1 &#8211; What we heard from the MySQL Community (keep it solid, and innovate faster) and our change to new release model those requirements.&hellip;</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995177&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995177&vote=-1&apivote=1">Vote DOWN</a>";}i:48;a:9:{s:5:"title";s:60:"How to install Lighttpd with PHP-FPM and MariaDB on CentOS 7";s:4:"guid";s:93:"https://www.howtoforge.com/tutorial/installing-lighttpd-with-php-5-fpm-and-mysql-on-centos-7/";s:4:"link";s:93:"https://www.howtoforge.com/tutorial/installing-lighttpd-with-php-5-fpm-and-mysql-on-centos-7/";s:11:"description";s:456:"Lighttpd is a secure, fast, standards-compliant web server designed for speed-critical environments. This tutorial shows how you can install
 Lighttpd on a Centos 7 server with PHP support (through PHP-FPM) and MySQL support. PHP-FPM (FastCGI Process Manager) is an alternative PHP 
FastCGI implementation with some additional features useful for sites of any size, especially busier sites. I use PHP-FPM in this tutorial 
instead of Lighttpd's spawn-fcgi.";s:7:"content";a:1:{s:7:"encoded";s:672:"Lighttpd is a secure, fast, standards-compliant web server designed for speed-critical environments. This tutorial shows how you can install
 Lighttpd on a Centos 7 server with PHP support (through PHP-FPM) and MySQL support. PHP-FPM (FastCGI Process Manager) is an alternative PHP 
FastCGI implementation with some additional features useful for sites of any size, especially busier sites. I use PHP-FPM in this tutorial 
instead of Lighttpd's spawn-fcgi.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995166&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995166&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 14:00:26 +0000";s:8:"category";s:6:"centos";s:7:"summary";s:456:"Lighttpd is a secure, fast, standards-compliant web server designed for speed-critical environments. This tutorial shows how you can install
 Lighttpd on a Centos 7 server with PHP support (through PHP-FPM) and MySQL support. PHP-FPM (FastCGI Process Manager) is an alternative PHP 
FastCGI implementation with some additional features useful for sites of any size, especially busier sites. I use PHP-FPM in this tutorial 
instead of Lighttpd's spawn-fcgi.";s:12:"atom_content";s:672:"Lighttpd is a secure, fast, standards-compliant web server designed for speed-critical environments. This tutorial shows how you can install
 Lighttpd on a Centos 7 server with PHP support (through PHP-FPM) and MySQL support. PHP-FPM (FastCGI Process Manager) is an alternative PHP 
FastCGI implementation with some additional features useful for sites of any size, especially busier sites. I use PHP-FPM in this tutorial 
instead of Lighttpd's spawn-fcgi.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995166&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995166&vote=-1&apivote=1">Vote DOWN</a>";}i:49;a:9:{s:5:"title";s:78:"Press Release: Severalnines expands the reach of European scientific discovery";s:4:"guid";s:31:"4675 at http://severalnines.com";s:4:"link";s:99:"http://severalnines.com/blog/press-release-severalnines-expands-reach-european-scientific-discovery";s:11:"description";s:4624:"Stockholm, Sweden and anywhere else in the world - 20 April 2016 - Severalnines, the provider of database infrastructure management software, today announced its latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research.



The CNRS has over 1,100 research units and is home to some of the largest scientific research facilities in the world. It partners with other global institutions and employs over 33,000 people. Working in partnership with universities, laboratories and dedicated scientists, CNRS has delivered advanced research in areas such as obesity, malaria and organic matter in space.
Having an international outreach means they have a dedicated department to handle the information infrastructure of the organisation called the Directorate of Information Systems (CNRS-DSI). Thousands of gigabytes (GB) of administrative data are processed by CNRS-DSI internal systems every week, but with a tight budget CNRS needed software, which was both cost effective whilst delivering a high quality, robust service.
To manage the high volume of data, CNRS deployed over 100 open source LAMP applications. The growth of the institution led to unprecedented usage of CNRS data from tens of thousands of users across the world accessing or transporting information. There was a need to increase the scalability, availability and robustness of the systems.
After launching a study to find a suitable database solution and realising traditional MySQL clusters were too complicated without a database administrator (DBA), they found Severalnines’ ClusterControl in conjunction with MariaDB Galera Cluster, MySQL’s “little sister fork”. ClusterControl offered a comprehensive solution, which is easy to access for all CNRS-DSI technical staff. The solution integrated well across the technological environment and was able to detect anomalies in the system.
Since Severalnines was deployed, the CNRS-DSI team runs a development and a production MariaDB Galera cluster thanks to ClusterControl with future plans to have all of its LAMP applications running in this environment. In fact, CNRS-DSI just recently extended all of its ClusterControl subscriptions.
Furthermore, beside these classical LAMP applications, CNRS-DSI is deploying a cloud storage solution for thousands of its users. For obvious performance and availability reasons, MariaDB Galera has also been chosen as the database component in place of the classical standalone MySQL; and Severalnines ClusterControl has been naturally chosen as the management solution for this critical service as well.
Olivier Lenormand, Technical Manager of CNRS-DSI, stated: “Technology is the backbone of scientific discovery which ultimately leads to human advancement. Data management is very important at CNRS because we want to continue our groundbreaking research and protect our data. Severalnines has helped us keep costs down whilst increasing the potential of our open source systems. We’ve found a database platform, which can both manage and use our LAMP applications, as well as cloud services. Severalnines is helping us enhance the capabilities at CNRS-DSI for the benefit of the global scientific community.”
Vinay Joosery, Severalnines CEO, said: “Data management in a large organisation like CNRS can present technical as well as economical challenges, but it should not get into the way of scientific research. We are really excited we can help CNRS use the best of open source software to increase collaboration in new, potentially life-saving research projects.
About Severalnines
Severalnines provides automation and management software for database clusters. We help companies deploy their databases in any environment, and manage all operational aspects to achieve high-scale availability.
Severalnines' products are used by developers and administrators of all skills levels to provide the full 'deploy, manage, monitor, scale' database cycle, thus freeing them from the complexity and learning curves that are typically associated with highly available database clusters. The company has enabled over 8,000 deployments to date via its ClusterControl solution. Currently counting BT, Orange, Cisco, CNRS, Technicolour, AVG, Ping Identity and Paytrail as customers. Severalnines is a private company headquartered in Stockholm, Sweden with offices in Singapore and Tokyo, Japan. To see who is using Severalnines today visit, http://www.severalnines.com/customers
Tags: MySQLcnrsopen sourcesciencepress releaseDatabasemanagementclustercontrolMariaDB";s:7:"content";a:1:{s:7:"encoded";s:6592:"<div><div><div property="content:encoded"><p>Stockholm, Sweden and anywhere else in the world - 20 April 2016 - <a href="http://severalnines.com" rel="nofollow" target="_blank">Severalnines</a>, the provider of database infrastructure management software, today announced its latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research.</p>
<div>
<div><img src="http://severalnines.com/sites/default/files/blog/node_4675/1024px-CNRS.svg_small.png" /></div>
</div>
<p>The CNRS has over 1,100 research units and is home to some of the largest scientific research facilities in the world. It partners with other global institutions and employs over 33,000 people. Working in partnership with universities, laboratories and dedicated scientists, CNRS has delivered advanced research in areas such as obesity, malaria and organic matter in space.</p>
<p>Having an international outreach means they have a dedicated department to handle the information infrastructure of the organisation called the Directorate of Information Systems (CNRS-DSI). Thousands of gigabytes (GB) of administrative data are processed by CNRS-DSI internal systems every week, but with a tight budget CNRS needed software, which was both cost effective whilst delivering a high quality, robust service.</p>
<p>To manage the high volume of data, CNRS deployed over 100 open source LAMP applications. The growth of the institution led to unprecedented usage of CNRS data from tens of thousands of users across the world accessing or transporting information. There was a need to increase the scalability, availability and robustness of the systems.</p>
<p>After launching a study to find a suitable database solution and realising traditional MySQL clusters were too complicated without a database administrator (DBA), they found Severalnines’ ClusterControl in conjunction with MariaDB Galera Cluster, MySQL’s “little sister fork”. ClusterControl offered a comprehensive solution, which is easy to access for all CNRS-DSI technical staff. The solution integrated well across the technological environment and was able to detect anomalies in the system.</p>
<p>Since Severalnines was deployed, the CNRS-DSI team runs a development and a production MariaDB Galera cluster thanks to ClusterControl with future plans to have all of its LAMP applications running in this environment. In fact, CNRS-DSI just recently extended all of its ClusterControl subscriptions.</p>
<p>Furthermore, beside these classical LAMP applications, CNRS-DSI is deploying a cloud storage solution for thousands of its users. For obvious performance and availability reasons, MariaDB Galera has also been chosen as the database component in place of the classical standalone MySQL; and Severalnines ClusterControl has been naturally chosen as the management solution for this critical service as well.</p>
<p>Olivier Lenormand, Technical Manager of CNRS-DSI, stated: “Technology is the backbone of scientific discovery which ultimately leads to human advancement. Data management is very important at CNRS because we want to continue our groundbreaking research and protect our data. Severalnines has helped us keep costs down whilst increasing the potential of our open source systems. We’ve found a database platform, which can both manage and use our LAMP applications, as well as cloud services. Severalnines is helping us enhance the capabilities at CNRS-DSI for the benefit of the global scientific community.”</p>
<p>Vinay Joosery, Severalnines CEO, said: “Data management in a large organisation like CNRS can present technical as well as economical challenges, but it should not get into the way of scientific research. We are really excited we can help CNRS use the best of open source software to increase collaboration in new, potentially life-saving research projects.</p>
<h2>About Severalnines</h2>
<p>Severalnines provides automation and management software for database clusters. We help companies deploy their databases in any environment, and manage all operational aspects to achieve high-scale availability.</p>
<p>Severalnines' products are used by developers and administrators of all skills levels to provide the full 'deploy, manage, monitor, scale' database cycle, thus freeing them from the complexity and learning curves that are typically associated with highly available database clusters. The company has enabled over 8,000 deployments to date via its ClusterControl solution. Currently counting BT, Orange, Cisco, CNRS, Technicolour, AVG, Ping Identity and Paytrail as customers. Severalnines is a private company headquartered in Stockholm, Sweden with offices in Singapore and Tokyo, Japan. To see who is using Severalnines today visit, <a href="http://www.severalnines.com/customers" rel="nofollow" target="_blank" title="Goes to website of: http://www.severalnines.com/company">http://www.severalnines.com/customers</a></p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/cnrs" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">cnrs</a></li><li><a href="http://severalnines.com/blog-tags/open-source" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">open source</a></li><li><a href="http://severalnines.com/blog-tags/science" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">science</a></li><li><a href="http://severalnines.com/blog-tags/press-release" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">press release</a></li><li><a href="http://severalnines.com/blog-tags/database" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Database</a></li><li><a href="http://severalnines.com/blog-tags/management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">management</a></li><li><a href="http://severalnines.com/blog-tags/clustercontrol" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">clustercontrol</a></li><li><a href="http://severalnines.com/blog-tags/mariadb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995163&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995163&vote=-1&apivote=1">Vote DOWN</a>";}s:7:"pubdate";s:31:"Wed, 20 Apr 2016 11:25:31 +0000";s:2:"dc";a:1:{s:7:"creator";s:12:"Severalnines";}s:7:"summary";s:4624:"Stockholm, Sweden and anywhere else in the world - 20 April 2016 - Severalnines, the provider of database infrastructure management software, today announced its latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research.



The CNRS has over 1,100 research units and is home to some of the largest scientific research facilities in the world. It partners with other global institutions and employs over 33,000 people. Working in partnership with universities, laboratories and dedicated scientists, CNRS has delivered advanced research in areas such as obesity, malaria and organic matter in space.
Having an international outreach means they have a dedicated department to handle the information infrastructure of the organisation called the Directorate of Information Systems (CNRS-DSI). Thousands of gigabytes (GB) of administrative data are processed by CNRS-DSI internal systems every week, but with a tight budget CNRS needed software, which was both cost effective whilst delivering a high quality, robust service.
To manage the high volume of data, CNRS deployed over 100 open source LAMP applications. The growth of the institution led to unprecedented usage of CNRS data from tens of thousands of users across the world accessing or transporting information. There was a need to increase the scalability, availability and robustness of the systems.
After launching a study to find a suitable database solution and realising traditional MySQL clusters were too complicated without a database administrator (DBA), they found Severalnines’ ClusterControl in conjunction with MariaDB Galera Cluster, MySQL’s “little sister fork”. ClusterControl offered a comprehensive solution, which is easy to access for all CNRS-DSI technical staff. The solution integrated well across the technological environment and was able to detect anomalies in the system.
Since Severalnines was deployed, the CNRS-DSI team runs a development and a production MariaDB Galera cluster thanks to ClusterControl with future plans to have all of its LAMP applications running in this environment. In fact, CNRS-DSI just recently extended all of its ClusterControl subscriptions.
Furthermore, beside these classical LAMP applications, CNRS-DSI is deploying a cloud storage solution for thousands of its users. For obvious performance and availability reasons, MariaDB Galera has also been chosen as the database component in place of the classical standalone MySQL; and Severalnines ClusterControl has been naturally chosen as the management solution for this critical service as well.
Olivier Lenormand, Technical Manager of CNRS-DSI, stated: “Technology is the backbone of scientific discovery which ultimately leads to human advancement. Data management is very important at CNRS because we want to continue our groundbreaking research and protect our data. Severalnines has helped us keep costs down whilst increasing the potential of our open source systems. We’ve found a database platform, which can both manage and use our LAMP applications, as well as cloud services. Severalnines is helping us enhance the capabilities at CNRS-DSI for the benefit of the global scientific community.”
Vinay Joosery, Severalnines CEO, said: “Data management in a large organisation like CNRS can present technical as well as economical challenges, but it should not get into the way of scientific research. We are really excited we can help CNRS use the best of open source software to increase collaboration in new, potentially life-saving research projects.
About Severalnines
Severalnines provides automation and management software for database clusters. We help companies deploy their databases in any environment, and manage all operational aspects to achieve high-scale availability.
Severalnines' products are used by developers and administrators of all skills levels to provide the full 'deploy, manage, monitor, scale' database cycle, thus freeing them from the complexity and learning curves that are typically associated with highly available database clusters. The company has enabled over 8,000 deployments to date via its ClusterControl solution. Currently counting BT, Orange, Cisco, CNRS, Technicolour, AVG, Ping Identity and Paytrail as customers. Severalnines is a private company headquartered in Stockholm, Sweden with offices in Singapore and Tokyo, Japan. To see who is using Severalnines today visit, http://www.severalnines.com/customers
Tags: MySQLcnrsopen sourcesciencepress releaseDatabasemanagementclustercontrolMariaDB";s:12:"atom_content";s:6592:"<div><div><div property="content:encoded"><p>Stockholm, Sweden and anywhere else in the world - 20 April 2016 - <a href="http://severalnines.com" rel="nofollow" target="_blank">Severalnines</a>, the provider of database infrastructure management software, today announced its latest customer, the National Center for Scientific Research (CNRS), which is a subsidiary of the French Ministry of Higher Education and Research.</p>
<div>
<div><img src="http://severalnines.com/sites/default/files/blog/node_4675/1024px-CNRS.svg_small.png" /></div>
</div>
<p>The CNRS has over 1,100 research units and is home to some of the largest scientific research facilities in the world. It partners with other global institutions and employs over 33,000 people. Working in partnership with universities, laboratories and dedicated scientists, CNRS has delivered advanced research in areas such as obesity, malaria and organic matter in space.</p>
<p>Having an international outreach means they have a dedicated department to handle the information infrastructure of the organisation called the Directorate of Information Systems (CNRS-DSI). Thousands of gigabytes (GB) of administrative data are processed by CNRS-DSI internal systems every week, but with a tight budget CNRS needed software, which was both cost effective whilst delivering a high quality, robust service.</p>
<p>To manage the high volume of data, CNRS deployed over 100 open source LAMP applications. The growth of the institution led to unprecedented usage of CNRS data from tens of thousands of users across the world accessing or transporting information. There was a need to increase the scalability, availability and robustness of the systems.</p>
<p>After launching a study to find a suitable database solution and realising traditional MySQL clusters were too complicated without a database administrator (DBA), they found Severalnines’ ClusterControl in conjunction with MariaDB Galera Cluster, MySQL’s “little sister fork”. ClusterControl offered a comprehensive solution, which is easy to access for all CNRS-DSI technical staff. The solution integrated well across the technological environment and was able to detect anomalies in the system.</p>
<p>Since Severalnines was deployed, the CNRS-DSI team runs a development and a production MariaDB Galera cluster thanks to ClusterControl with future plans to have all of its LAMP applications running in this environment. In fact, CNRS-DSI just recently extended all of its ClusterControl subscriptions.</p>
<p>Furthermore, beside these classical LAMP applications, CNRS-DSI is deploying a cloud storage solution for thousands of its users. For obvious performance and availability reasons, MariaDB Galera has also been chosen as the database component in place of the classical standalone MySQL; and Severalnines ClusterControl has been naturally chosen as the management solution for this critical service as well.</p>
<p>Olivier Lenormand, Technical Manager of CNRS-DSI, stated: “Technology is the backbone of scientific discovery which ultimately leads to human advancement. Data management is very important at CNRS because we want to continue our groundbreaking research and protect our data. Severalnines has helped us keep costs down whilst increasing the potential of our open source systems. We’ve found a database platform, which can both manage and use our LAMP applications, as well as cloud services. Severalnines is helping us enhance the capabilities at CNRS-DSI for the benefit of the global scientific community.”</p>
<p>Vinay Joosery, Severalnines CEO, said: “Data management in a large organisation like CNRS can present technical as well as economical challenges, but it should not get into the way of scientific research. We are really excited we can help CNRS use the best of open source software to increase collaboration in new, potentially life-saving research projects.</p>
<h2>About Severalnines</h2>
<p>Severalnines provides automation and management software for database clusters. We help companies deploy their databases in any environment, and manage all operational aspects to achieve high-scale availability.</p>
<p>Severalnines' products are used by developers and administrators of all skills levels to provide the full 'deploy, manage, monitor, scale' database cycle, thus freeing them from the complexity and learning curves that are typically associated with highly available database clusters. The company has enabled over 8,000 deployments to date via its ClusterControl solution. Currently counting BT, Orange, Cisco, CNRS, Technicolour, AVG, Ping Identity and Paytrail as customers. Severalnines is a private company headquartered in Stockholm, Sweden with offices in Singapore and Tokyo, Japan. To see who is using Severalnines today visit, <a href="http://www.severalnines.com/customers" rel="nofollow" target="_blank" title="Goes to website of: http://www.severalnines.com/company">http://www.severalnines.com/customers</a></p>
</div></div></div><div><h3>Tags: </h3><ul><li><a href="http://severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://severalnines.com/blog-tags/cnrs" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">cnrs</a></li><li><a href="http://severalnines.com/blog-tags/open-source" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">open source</a></li><li><a href="http://severalnines.com/blog-tags/science" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">science</a></li><li><a href="http://severalnines.com/blog-tags/press-release" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">press release</a></li><li><a href="http://severalnines.com/blog-tags/database" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Database</a></li><li><a href="http://severalnines.com/blog-tags/management" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">management</a></li><li><a href="http://severalnines.com/blog-tags/clustercontrol" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">clustercontrol</a></li><li><a href="http://severalnines.com/blog-tags/mariadb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5995163&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5995163&vote=-1&apivote=1">Vote DOWN</a>";}}s:7:"channel";a:6:{s:5:"title";s:12:"Planet MySQL";s:4:"link";s:27:"http://www.planetmysql.org/";s:7:"pubdate";s:31:"Fri, 29 Apr 2016 14:00:01 +0000";s:8:"language";s:2:"en";s:11:"description";s:42:"Planet MySQL - http://www.planetmysql.org/";s:7:"tagline";s:42:"Planet MySQL - http://www.planetmysql.org/";}s:9:"textinput";a:0:{}s:5:"image";a:0:{}s:9:"feed_type";s:3:"RSS";s:12:"feed_version";s:3:"2.0";s:5:"stack";a:0:{}s:9:"inchannel";b:0;s:6:"initem";b:0;s:9:"incontent";b:0;s:11:"intextinput";b:0;s:7:"inimage";b:0;s:13:"current_field";s:0:"";s:17:"current_namespace";b:0;s:5:"ERROR";s:0:"";s:19:"_CONTENT_CONSTRUCTS";a:6:{i:0;s:7:"content";i:1;s:7:"summary";i:2;s:4:"info";i:3;s:5:"title";i:4;s:7:"tagline";i:5;s:9:"copyright";}s:13:"last_modified";s:31:"Fri, 29 Apr 2016 14:00:16 GMT
";}